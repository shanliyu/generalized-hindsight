{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fd7cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import plotly.express as px\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c108cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doodad not detected\n",
      "objc[2480]: Class GLFWApplicationDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a1797d778) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a179fe740). One of the two will be used. Which one is undefined.\n",
      "objc[2480]: Class GLFWWindowDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a1797d700) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a179fe768). One of the two will be used. Which one is undefined.\n",
      "objc[2480]: Class GLFWContentView is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a1797d7a0) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a179fe7b8). One of the two will be used. Which one is undefined.\n",
      "objc[2480]: Class GLFWWindow is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a1797d818) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a179fe830). One of the two will be used. Which one is undefined.\n",
      "using sparse reward if specified\n",
      "2021-05-25 19:13:32.748223 PDT | Variant:\n",
      "2021-05-25 19:13:32.750282 PDT | {\n",
      "  \"seed\": 10,\n",
      "  \"algorithm\": \"SAC\",\n",
      "  \"env_name\": \"pointreacherobs\",\n",
      "  \"algo_kwargs\": {\n",
      "    \"batch_size\": 256,\n",
      "    \"num_epochs\": 300,\n",
      "    \"num_eval_steps_per_epoch\": 1000,\n",
      "    \"num_expl_steps_per_train_loop\": 20,\n",
      "    \"num_trains_per_train_loop\": 100,\n",
      "    \"min_num_steps_before_training\": 1000,\n",
      "    \"max_path_length\": 20,\n",
      "    \"num_train_loops_per_epoch\": 5\n",
      "  },\n",
      "  \"trainer_kwargs\": {\n",
      "    \"discount\": 0.97,\n",
      "    \"soft_target_tau\": 0.005,\n",
      "    \"target_update_period\": 1,\n",
      "    \"policy_lr\": 0.003,\n",
      "    \"qf_lr\": 0.003,\n",
      "    \"reward_scale\": 1,\n",
      "    \"use_automatic_entropy_tuning\": true\n",
      "  },\n",
      "  \"replay_buffer_kwargs\": {\n",
      "    \"max_replay_buffer_size\": 2000,\n",
      "    \"latent_dim\": 6,\n",
      "    \"approx_irl\": false,\n",
      "    \"plot\": false\n",
      "  },\n",
      "  \"relabeler_kwargs\": {\n",
      "    \"relabel\": false,\n",
      "    \"use_adv\": false,\n",
      "    \"n_sampled_latents\": 5,\n",
      "    \"n_to_take\": 1,\n",
      "    \"cache\": false,\n",
      "    \"sparse_reward\": null,\n",
      "    \"fixed_ratio\": null\n",
      "  },\n",
      "  \"qf_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      300,\n",
      "      300,\n",
      "      300\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"policy_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      300,\n",
      "      300,\n",
      "      300\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"path_collector_kwargs\": {\n",
      "    \"save_videos\": false\n",
      "  },\n",
      "  \"use_advantages\": false,\n",
      "  \"proper_advantages\": true,\n",
      "  \"plot\": false,\n",
      "  \"test\": false,\n",
      "  \"gpu\": 0,\n",
      "  \"mode\": \"here_no_doodad\",\n",
      "  \"local_docker\": false,\n",
      "  \"insert_time\": false,\n",
      "  \"latent_shape_multiplier\": 1,\n",
      "  \"env_kwargs\": {\n",
      "    \"horizon\": 20\n",
      "  }\n",
      "}\n",
      "pointreacher\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "sparse reward: None\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "sparse reward: None\n",
      "2021-05-25 19:15:38.431160 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 0 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                   1100\n",
      "trainer/QF1 Loss                                        0.576872\n",
      "trainer/QF2 Loss                                        0.57548\n",
      "trainer/Policy Loss                                    -1.37884\n",
      "trainer/Q1 Predictions Mean                            -0.00126527\n",
      "trainer/Q1 Predictions Std                              0.000605492\n",
      "trainer/Q1 Predictions Max                              0.000269961\n",
      "trainer/Q1 Predictions Min                             -0.00227255\n",
      "trainer/Q2 Predictions Mean                            -4.35855e-06\n",
      "trainer/Q2 Predictions Std                              0.000609121\n",
      "trainer/Q2 Predictions Max                              0.00133012\n",
      "trainer/Q2 Predictions Min                             -0.00163013\n",
      "trainer/Q Targets Mean                                  0.514032\n",
      "trainer/Q Targets Std                                   0.557929\n",
      "trainer/Q Targets Max                                   1.83678\n",
      "trainer/Q Targets Min                                  -1.35869\n",
      "trainer/Log Pis Mean                                   -1.38018\n",
      "trainer/Log Pis Std                                     0.289063\n",
      "trainer/Log Pis Max                                    -0.577882\n",
      "trainer/Log Pis Min                                    -2.60453\n",
      "trainer/Policy mu Mean                                  0.000988936\n",
      "trainer/Policy mu Std                                   0.00058491\n",
      "trainer/Policy mu Max                                   0.00221213\n",
      "trainer/Policy mu Min                                   1.31949e-05\n",
      "trainer/Policy log std Mean                             4.21378e-05\n",
      "trainer/Policy log std Std                              0.000645919\n",
      "trainer/Policy log std Max                              0.000916166\n",
      "trainer/Policy log std Min                             -0.00127692\n",
      "trainer/Alpha                                           0.997005\n",
      "trainer/Alpha Loss                                     -0\n",
      "exploration/num steps total                          1100\n",
      "exploration/num paths total                            55\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.674579\n",
      "exploration/Rewards Std                                 0.290019\n",
      "exploration/Rewards Max                                -0.105313\n",
      "exploration/Rewards Min                                -1.25229\n",
      "exploration/Returns Mean                              -13.4916\n",
      "exploration/Returns Std                                 1.35362\n",
      "exploration/Returns Max                               -11.0651\n",
      "exploration/Returns Min                               -14.8536\n",
      "exploration/Actions Mean                                0.0632062\n",
      "exploration/Actions Std                                 0.578647\n",
      "exploration/Actions Max                                 0.98148\n",
      "exploration/Actions Min                                -0.941187\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                           -13.4916\n",
      "exploration/env_infos/final/reward_dist Mean            3.94634e-51\n",
      "exploration/env_infos/final/reward_dist Std             7.89266e-51\n",
      "exploration/env_infos/final/reward_dist Max             1.97317e-50\n",
      "exploration/env_infos/final/reward_dist Min             4.19542e-102\n",
      "exploration/env_infos/initial/reward_dist Mean          0.0344088\n",
      "exploration/env_infos/initial/reward_dist Std           0.0428659\n",
      "exploration/env_infos/initial/reward_dist Max           0.106806\n",
      "exploration/env_infos/initial/reward_dist Min           9.56386e-05\n",
      "exploration/env_infos/reward_dist Mean                  0.0187599\n",
      "exploration/env_infos/reward_dist Std                   0.0898929\n",
      "exploration/env_infos/reward_dist Max                   0.836662\n",
      "exploration/env_infos/reward_dist Min                   4.19542e-102\n",
      "exploration/env_infos/final/reward_energy Mean         -0.787813\n",
      "exploration/env_infos/final/reward_energy Std           0.299654\n",
      "exploration/env_infos/final/reward_energy Max          -0.19223\n",
      "exploration/env_infos/final/reward_energy Min          -1.00022\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.87809\n",
      "exploration/env_infos/initial/reward_energy Std         0.131384\n",
      "exploration/env_infos/initial/reward_energy Max        -0.719892\n",
      "exploration/env_infos/initial/reward_energy Min        -1.09336\n",
      "exploration/env_infos/reward_energy Mean               -0.775129\n",
      "exploration/env_infos/reward_energy Std                 0.277183\n",
      "exploration/env_infos/reward_energy Max                -0.19223\n",
      "exploration/env_infos/reward_energy Min                -1.25569\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean       0.412974\n",
      "exploration/env_infos/final/end_effector_loc Std        0.646101\n",
      "exploration/env_infos/final/end_effector_loc Max        1\n",
      "exploration/env_infos/final/end_effector_loc Min       -1\n",
      "exploration/env_infos/initial/end_effector_loc Mean     0.0216553\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.022725\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0471042\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0352084\n",
      "exploration/env_infos/end_effector_loc Mean             0.273985\n",
      "exploration/env_infos/end_effector_loc Std              0.485792\n",
      "exploration/env_infos/end_effector_loc Max              1\n",
      "exploration/env_infos/end_effector_loc Min             -1\n",
      "evaluation/num steps total                           1000\n",
      "evaluation/num paths total                             50\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.0780623\n",
      "evaluation/Rewards Std                                  0.0473164\n",
      "evaluation/Rewards Max                                  0.0425092\n",
      "evaluation/Rewards Min                                 -0.150422\n",
      "evaluation/Returns Mean                                -1.56125\n",
      "evaluation/Returns Std                                  0.945244\n",
      "evaluation/Returns Max                                  0.807234\n",
      "evaluation/Returns Min                                 -2.97967\n",
      "evaluation/Actions Mean                                 0.00098014\n",
      "evaluation/Actions Std                                  0.000587866\n",
      "evaluation/Actions Max                                  0.0020782\n",
      "evaluation/Actions Min                                  0.000173299\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -1.56125\n",
      "evaluation/env_infos/final/reward_dist Mean             0.00593428\n",
      "evaluation/env_infos/final/reward_dist Std              0.00912983\n",
      "evaluation/env_infos/final/reward_dist Max              0.0441983\n",
      "evaluation/env_infos/final/reward_dist Min              5.15938e-07\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00533279\n",
      "evaluation/env_infos/initial/reward_dist Std            0.00750724\n",
      "evaluation/env_infos/initial/reward_dist Max            0.02745\n",
      "evaluation/env_infos/initial/reward_dist Min            9.54696e-07\n",
      "evaluation/env_infos/reward_dist Mean                   0.00547818\n",
      "evaluation/env_infos/reward_dist Std                    0.0078623\n",
      "evaluation/env_infos/reward_dist Max                    0.0441983\n",
      "evaluation/env_infos/reward_dist Min                    5.15938e-07\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.00163802\n",
      "evaluation/env_infos/final/reward_energy Std            0.000251158\n",
      "evaluation/env_infos/final/reward_energy Max           -0.00111856\n",
      "evaluation/env_infos/final/reward_energy Min           -0.00210778\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.00156036\n",
      "evaluation/env_infos/initial/reward_energy Std          0.00021699\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.00111835\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.00196039\n",
      "evaluation/env_infos/reward_energy Mean                -0.00159921\n",
      "evaluation/env_infos/reward_energy Std                  0.000234643\n",
      "evaluation/env_infos/reward_energy Max                 -0.00111835\n",
      "evaluation/env_infos/reward_energy Min                 -0.00210778\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        0.0102276\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.00607673\n",
      "evaluation/env_infos/final/end_effector_loc Max         0.0207555\n",
      "evaluation/env_infos/final/end_effector_loc Min         0.00191566\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      4.80703e-05\n",
      "evaluation/env_infos/initial/end_effector_loc Std       2.81336e-05\n",
      "evaluation/env_infos/initial/end_effector_loc Max       9.61769e-05\n",
      "evaluation/env_infos/initial/end_effector_loc Min       8.66496e-06\n",
      "evaluation/env_infos/end_effector_loc Mean              0.00373819\n",
      "evaluation/env_infos/end_effector_loc Std               0.0043006\n",
      "evaluation/env_infos/end_effector_loc Max               0.0207555\n",
      "evaluation/env_infos/end_effector_loc Min               8.66496e-06\n",
      "time/data storing (s)                                   0.00687533\n",
      "time/evaluation sampling (s)                            6.60521\n",
      "time/exploration sampling (s)                           0.722718\n",
      "time/logging (s)                                        0.0234152\n",
      "time/saving (s)                                         0.0814613\n",
      "time/training (s)                                     111.244\n",
      "time/epoch (s)                                        118.684\n",
      "time/total (s)                                        140.75\n",
      "Epoch                                                   0\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:16:19.437834 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 1 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                   1200\n",
      "trainer/QF1 Loss                                        0.0142788\n",
      "trainer/QF2 Loss                                        0.0179015\n",
      "trainer/Policy Loss                                    -0.759393\n",
      "trainer/Q1 Predictions Mean                            -0.46276\n",
      "trainer/Q1 Predictions Std                              0.624373\n",
      "trainer/Q1 Predictions Max                              0.586792\n",
      "trainer/Q1 Predictions Min                             -2.04044\n",
      "trainer/Q2 Predictions Mean                            -0.537618\n",
      "trainer/Q2 Predictions Std                              0.622405\n",
      "trainer/Q2 Predictions Max                              0.567538\n",
      "trainer/Q2 Predictions Min                             -2.17126\n",
      "trainer/Q Targets Mean                                 -0.486009\n",
      "trainer/Q Targets Std                                   0.654017\n",
      "trainer/Q Targets Max                                   0.673842\n",
      "trainer/Q Targets Min                                  -2.31864\n",
      "trainer/Log Pis Mean                                   -1.18884\n",
      "trainer/Log Pis Std                                     0.472486\n",
      "trainer/Log Pis Max                                    -0.544822\n",
      "trainer/Log Pis Min                                    -3.78867\n",
      "trainer/Policy mu Mean                                  0.0285259\n",
      "trainer/Policy mu Std                                   0.0346983\n",
      "trainer/Policy mu Max                                   0.134678\n",
      "trainer/Policy mu Min                                  -0.0757775\n",
      "trainer/Policy log std Mean                            -0.526448\n",
      "trainer/Policy log std Std                              0.0696464\n",
      "trainer/Policy log std Max                             -0.366739\n",
      "trainer/Policy log std Min                             -0.693374\n",
      "trainer/Alpha                                           0.225887\n",
      "trainer/Alpha Loss                                     -4.73488\n",
      "exploration/num steps total                          1200\n",
      "exploration/num paths total                            60\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.517996\n",
      "exploration/Rewards Std                                 0.284947\n",
      "exploration/Rewards Max                                 0.0111953\n",
      "exploration/Rewards Min                                -1.26828\n",
      "exploration/Returns Mean                              -10.3599\n",
      "exploration/Returns Std                                 2.90097\n",
      "exploration/Returns Max                                -6.89275\n",
      "exploration/Returns Min                               -15.7143\n",
      "exploration/Actions Mean                                0.046027\n",
      "exploration/Actions Std                                 0.385387\n",
      "exploration/Actions Max                                 0.915877\n",
      "exploration/Actions Min                                -0.852987\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                           -10.3599\n",
      "exploration/env_infos/final/reward_dist Mean            1.0713e-53\n",
      "exploration/env_infos/final/reward_dist Std             2.14259e-53\n",
      "exploration/env_infos/final/reward_dist Max             5.35648e-53\n",
      "exploration/env_infos/final/reward_dist Min             1.19818e-151\n",
      "exploration/env_infos/initial/reward_dist Mean          0.0103099\n",
      "exploration/env_infos/initial/reward_dist Std           0.00620404\n",
      "exploration/env_infos/initial/reward_dist Max           0.017284\n",
      "exploration/env_infos/initial/reward_dist Min           2.62818e-07\n",
      "exploration/env_infos/reward_dist Mean                  0.0169753\n",
      "exploration/env_infos/reward_dist Std                   0.049046\n",
      "exploration/env_infos/reward_dist Max                   0.271489\n",
      "exploration/env_infos/reward_dist Min                   1.19818e-151\n",
      "exploration/env_infos/final/reward_energy Mean         -0.484143\n",
      "exploration/env_infos/final/reward_energy Std           0.217856\n",
      "exploration/env_infos/final/reward_energy Max          -0.152269\n",
      "exploration/env_infos/final/reward_energy Min          -0.809446\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.473428\n",
      "exploration/env_infos/initial/reward_energy Std         0.250453\n",
      "exploration/env_infos/initial/reward_energy Max        -0.136648\n",
      "exploration/env_infos/initial/reward_energy Min        -0.769096\n",
      "exploration/env_infos/reward_energy Mean               -0.491715\n",
      "exploration/env_infos/reward_energy Std                 0.243927\n",
      "exploration/env_infos/reward_energy Max                -0.0138491\n",
      "exploration/env_infos/reward_energy Min                -1.14363\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean       0.281292\n",
      "exploration/env_infos/final/end_effector_loc Std        0.753392\n",
      "exploration/env_infos/final/end_effector_loc Max        1\n",
      "exploration/env_infos/final/end_effector_loc Min       -1\n",
      "exploration/env_infos/initial/end_effector_loc Mean     0.000124701\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.0189357\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0364533\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0353364\n",
      "exploration/env_infos/end_effector_loc Mean             0.131406\n",
      "exploration/env_infos/end_effector_loc Std              0.468006\n",
      "exploration/env_infos/end_effector_loc Max              1\n",
      "exploration/env_infos/end_effector_loc Min             -1\n",
      "evaluation/num steps total                           2000\n",
      "evaluation/num paths total                            100\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.129317\n",
      "evaluation/Rewards Std                                  0.0950763\n",
      "evaluation/Rewards Max                                  0.165276\n",
      "evaluation/Rewards Min                                 -0.637119\n",
      "evaluation/Returns Mean                                -2.58635\n",
      "evaluation/Returns Std                                  1.38028\n",
      "evaluation/Returns Max                                  1.35225\n",
      "evaluation/Returns Min                                 -5.58551\n",
      "evaluation/Actions Mean                                 0.0325454\n",
      "evaluation/Actions Std                                  0.0245784\n",
      "evaluation/Actions Max                                  0.123077\n",
      "evaluation/Actions Min                                 -0.057515\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -2.58635\n",
      "evaluation/env_infos/final/reward_dist Mean             2.69966e-07\n",
      "evaluation/env_infos/final/reward_dist Std              1.09715e-06\n",
      "evaluation/env_infos/final/reward_dist Max              5.76264e-06\n",
      "evaluation/env_infos/final/reward_dist Min              3.98986e-74\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00497559\n",
      "evaluation/env_infos/initial/reward_dist Std            0.00811029\n",
      "evaluation/env_infos/initial/reward_dist Max            0.027829\n",
      "evaluation/env_infos/initial/reward_dist Min            1.12831e-06\n",
      "evaluation/env_infos/reward_dist Mean                   0.0435771\n",
      "evaluation/env_infos/reward_dist Std                    0.143624\n",
      "evaluation/env_infos/reward_dist Max                    0.954145\n",
      "evaluation/env_infos/reward_dist Min                    3.98986e-74\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.0490002\n",
      "evaluation/env_infos/final/reward_energy Std            0.00727559\n",
      "evaluation/env_infos/final/reward_energy Max           -0.0370384\n",
      "evaluation/env_infos/final/reward_energy Min           -0.0729301\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.0621672\n",
      "evaluation/env_infos/initial/reward_energy Std          0.0211793\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0401044\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.133877\n",
      "evaluation/env_infos/reward_energy Mean                -0.0556139\n",
      "evaluation/env_infos/reward_energy Std                  0.015287\n",
      "evaluation/env_infos/reward_energy Max                 -0.0370384\n",
      "evaluation/env_infos/reward_energy Min                 -0.134243\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        0.356476\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.266035\n",
      "evaluation/env_infos/final/end_effector_loc Max         0.969306\n",
      "evaluation/env_infos/final/end_effector_loc Min        -0.581257\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.00178086\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.00149003\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.00614425\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.00287575\n",
      "evaluation/env_infos/end_effector_loc Mean              0.133145\n",
      "evaluation/env_infos/end_effector_loc Std               0.172783\n",
      "evaluation/env_infos/end_effector_loc Max               0.969306\n",
      "evaluation/env_infos/end_effector_loc Min              -0.581257\n",
      "time/data storing (s)                                   0.0030834\n",
      "time/evaluation sampling (s)                            1.13712\n",
      "time/exploration sampling (s)                           0.129258\n",
      "time/logging (s)                                        0.0217114\n",
      "time/saving (s)                                         0.0287159\n",
      "time/training (s)                                      39.5197\n",
      "time/epoch (s)                                         40.8395\n",
      "time/total (s)                                        181.754\n",
      "Epoch                                                   1\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:17:00.577258 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 2 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                   1300\r\n",
      "trainer/QF1 Loss                                        0.016673\r\n",
      "trainer/QF2 Loss                                        0.0196761\r\n",
      "trainer/Policy Loss                                     1.37269\r\n",
      "trainer/Q1 Predictions Mean                            -1.49769\r\n",
      "trainer/Q1 Predictions Std                              0.976716\r\n",
      "trainer/Q1 Predictions Max                              0.332782\r\n",
      "trainer/Q1 Predictions Min                             -4.23755\r\n",
      "trainer/Q2 Predictions Mean                            -1.54115\r\n",
      "trainer/Q2 Predictions Std                              0.980129\r\n",
      "trainer/Q2 Predictions Max                              0.280805\r\n",
      "trainer/Q2 Predictions Min                             -4.01748\r\n",
      "trainer/Q Targets Mean                                 -1.5193\r\n",
      "trainer/Q Targets Std                                   1.02966\r\n",
      "trainer/Q Targets Max                                   0.350677\r\n",
      "trainer/Q Targets Min                                  -4.41199\r\n",
      "trainer/Log Pis Mean                                    0.132319\r\n",
      "trainer/Log Pis Std                                     1.14895\r\n",
      "trainer/Log Pis Max                                     3.76002\r\n",
      "trainer/Log Pis Min                                    -6.51631\r\n",
      "trainer/Policy mu Mean                                 -0.0163182\r\n",
      "trainer/Policy mu Std                                   0.341392\r\n",
      "trainer/Policy mu Max                                   1.57785\r\n",
      "trainer/Policy mu Min                                  -1.65983\r\n",
      "trainer/Policy log std Mean                            -1.28305\r\n",
      "trainer/Policy log std Std                              0.384718\r\n",
      "trainer/Policy log std Max                             -0.30159\r\n",
      "trainer/Policy log std Min                             -2.16013\r\n",
      "trainer/Alpha                                           0.0639025\r\n",
      "trainer/Alpha Loss                                     -5.133\r\n",
      "exploration/num steps total                          1300\r\n",
      "exploration/num paths total                            65\r\n",
      "exploration/path length Mean                           20\r\n",
      "exploration/path length Std                             0\r\n",
      "exploration/path length Max                            20\r\n",
      "exploration/path length Min                            20\r\n",
      "exploration/Rewards Mean                               -0.317213\r\n",
      "exploration/Rewards Std                                 0.206701\r\n",
      "exploration/Rewards Max                                -0.0785534\r\n",
      "exploration/Rewards Min                                -1.08456\r\n",
      "exploration/Returns Mean                               -6.34426\r\n",
      "exploration/Returns Std                                 3.03259\r\n",
      "exploration/Returns Max                                -4.30967\r\n",
      "exploration/Returns Min                               -12.3355\r\n",
      "exploration/Actions Mean                                0.0176146\r\n",
      "exploration/Actions Std                                 0.249469\r\n",
      "exploration/Actions Max                                 0.793571\r\n",
      "exploration/Actions Min                                -0.562318\r\n",
      "exploration/Num Paths                                   5\r\n",
      "exploration/Average Returns                            -6.34426\r\n",
      "exploration/env_infos/final/reward_dist Mean            1.10525e-05\r\n",
      "exploration/env_infos/final/reward_dist Std             2.2105e-05\r\n",
      "exploration/env_infos/final/reward_dist Max             5.52624e-05\r\n",
      "exploration/env_infos/final/reward_dist Min             8.32703e-100\r\n",
      "exploration/env_infos/initial/reward_dist Mean          0.00917166\r\n",
      "exploration/env_infos/initial/reward_dist Std           0.0115859\r\n",
      "exploration/env_infos/initial/reward_dist Max           0.0301845\r\n",
      "exploration/env_infos/initial/reward_dist Min           1.11096e-05\r\n",
      "exploration/env_infos/reward_dist Mean                  0.0291417\r\n",
      "exploration/env_infos/reward_dist Std                   0.101679\r\n",
      "exploration/env_infos/reward_dist Max                   0.609998\r\n",
      "exploration/env_infos/reward_dist Min                   1.44801e-101\r\n",
      "exploration/env_infos/final/reward_energy Mean         -0.447668\r\n",
      "exploration/env_infos/final/reward_energy Std           0.173098\r\n",
      "exploration/env_infos/final/reward_energy Max          -0.170972\r\n",
      "exploration/env_infos/final/reward_energy Min          -0.6067\r\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.416838\r\n",
      "exploration/env_infos/initial/reward_energy Std         0.144682\r\n",
      "exploration/env_infos/initial/reward_energy Max        -0.210439\r\n",
      "exploration/env_infos/initial/reward_energy Min        -0.655991\r\n",
      "exploration/env_infos/reward_energy Mean               -0.307407\r\n",
      "exploration/env_infos/reward_energy Std                 0.174901\r\n",
      "exploration/env_infos/reward_energy Max                -0.0300016\r\n",
      "exploration/env_infos/reward_energy Min                -0.815759\r\n",
      "exploration/env_infos/final/reward_safety Mean          0\r\n",
      "exploration/env_infos/final/reward_safety Std           0\r\n",
      "exploration/env_infos/final/reward_safety Max           0\r\n",
      "exploration/env_infos/final/reward_safety Min           0\r\n",
      "exploration/env_infos/initial/reward_safety Mean        0\r\n",
      "exploration/env_infos/initial/reward_safety Std         0\r\n",
      "exploration/env_infos/initial/reward_safety Max         0\r\n",
      "exploration/env_infos/initial/reward_safety Min         0\r\n",
      "exploration/env_infos/reward_safety Mean                0\r\n",
      "exploration/env_infos/reward_safety Std                 0\r\n",
      "exploration/env_infos/reward_safety Max                 0\r\n",
      "exploration/env_infos/reward_safety Min                 0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       0.324347\r\n",
      "exploration/env_infos/final/end_effector_loc Std        0.475971\r\n",
      "exploration/env_infos/final/end_effector_loc Max        0.857327\r\n",
      "exploration/env_infos/final/end_effector_loc Min       -0.902187\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean    -0.00309135\r\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.0152906\r\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0318791\r\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0214156\r\n",
      "exploration/env_infos/end_effector_loc Mean             0.143115\r\n",
      "exploration/env_infos/end_effector_loc Std              0.335973\r\n",
      "exploration/env_infos/end_effector_loc Max              0.857327\r\n",
      "exploration/env_infos/end_effector_loc Min             -1\r\n",
      "evaluation/num steps total                           3000\r\n",
      "evaluation/num paths total                            150\r\n",
      "evaluation/path length Mean                            20\r\n",
      "evaluation/path length Std                              0\r\n",
      "evaluation/path length Max                             20\r\n",
      "evaluation/path length Min                             20\r\n",
      "evaluation/Rewards Mean                                -0.171235\r\n",
      "evaluation/Rewards Std                                  0.156176\r\n",
      "evaluation/Rewards Max                                  0.0822294\r\n",
      "evaluation/Rewards Min                                 -0.979425\r\n",
      "evaluation/Returns Mean                                -3.4247\r\n",
      "evaluation/Returns Std                                  2.37878\r\n",
      "evaluation/Returns Max                                 -1.03687\r\n",
      "evaluation/Returns Min                                -12.9465\r\n",
      "evaluation/Actions Mean                                -0.00332473\r\n",
      "evaluation/Actions Std                                  0.113027\r\n",
      "evaluation/Actions Max                                  0.655579\r\n",
      "evaluation/Actions Min                                 -0.545786\r\n",
      "evaluation/Num Paths                                   50\r\n",
      "evaluation/Average Returns                             -3.4247\r\n",
      "evaluation/env_infos/final/reward_dist Mean             0.0091053\r\n",
      "evaluation/env_infos/final/reward_dist Std              0.0369815\r\n",
      "evaluation/env_infos/final/reward_dist Max              0.234501\r\n",
      "evaluation/env_infos/final/reward_dist Min              4.91896e-119\r\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00550878\r\n",
      "evaluation/env_infos/initial/reward_dist Std            0.0120953\r\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0659473\r\n",
      "evaluation/env_infos/initial/reward_dist Min            1.36827e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                   0.0420782\r\n",
      "evaluation/env_infos/reward_dist Std                    0.136436\r\n",
      "evaluation/env_infos/reward_dist Max                    0.985674\r\n",
      "evaluation/env_infos/reward_dist Min                    1.56268e-124\r\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.189778\r\n",
      "evaluation/env_infos/final/reward_energy Std            0.121088\r\n",
      "evaluation/env_infos/final/reward_energy Max           -0.00870755\r\n",
      "evaluation/env_infos/final/reward_energy Min           -0.55283\r\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.157609\r\n",
      "evaluation/env_infos/initial/reward_energy Std          0.140839\r\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.00895795\r\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.66551\r\n",
      "evaluation/env_infos/reward_energy Mean                -0.121254\r\n",
      "evaluation/env_infos/reward_energy Std                  0.104257\r\n",
      "evaluation/env_infos/reward_energy Max                 -0.00162313\r\n",
      "evaluation/env_infos/reward_energy Min                 -0.66551\r\n",
      "evaluation/env_infos/final/reward_safety Mean           0\r\n",
      "evaluation/env_infos/final/reward_safety Std            0\r\n",
      "evaluation/env_infos/final/reward_safety Max            0\r\n",
      "evaluation/env_infos/final/reward_safety Min            0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\r\n",
      "evaluation/env_infos/initial/reward_safety Std          0\r\n",
      "evaluation/env_infos/initial/reward_safety Max          0\r\n",
      "evaluation/env_infos/initial/reward_safety Min          0\r\n",
      "evaluation/env_infos/reward_safety Mean                 0\r\n",
      "evaluation/env_infos/reward_safety Std                  0\r\n",
      "evaluation/env_infos/reward_safety Max                  0\r\n",
      "evaluation/env_infos/reward_safety Min                  0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        0.123052\r\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.380354\r\n",
      "evaluation/env_infos/final/end_effector_loc Max         1\r\n",
      "evaluation/env_infos/final/end_effector_loc Min        -0.51224\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.00333402\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.00668802\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.032779\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.0145551\r\n",
      "evaluation/env_infos/end_effector_loc Mean              0.0941151\r\n",
      "evaluation/env_infos/end_effector_loc Std               0.253196\r\n",
      "evaluation/env_infos/end_effector_loc Max               1\r\n",
      "evaluation/env_infos/end_effector_loc Min              -0.71139\r\n",
      "time/data storing (s)                                   0.00303249\r\n",
      "time/evaluation sampling (s)                            1.08983\r\n",
      "time/exploration sampling (s)                           0.125043\r\n",
      "time/logging (s)                                        0.0180217\r\n",
      "time/saving (s)                                         0.0282199\r\n",
      "time/training (s)                                      39.8115\r\n",
      "time/epoch (s)                                         41.0756\r\n",
      "time/total (s)                                        222.889\r\n",
      "Epoch                                                   2\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:17:42.200003 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 3 finished\n",
      "---------------------------------------------------  --------------\n",
      "replay_buffer/size                                   1400\n",
      "trainer/QF1 Loss                                        0.00992432\n",
      "trainer/QF2 Loss                                        0.0117483\n",
      "trainer/Policy Loss                                     3.07273\n",
      "trainer/Q1 Predictions Mean                            -2.17948\n",
      "trainer/Q1 Predictions Std                              1.31205\n",
      "trainer/Q1 Predictions Max                             -0.0172831\n",
      "trainer/Q1 Predictions Min                             -7.29756\n",
      "trainer/Q2 Predictions Mean                            -2.1591\n",
      "trainer/Q2 Predictions Std                              1.29779\n",
      "trainer/Q2 Predictions Max                             -0.00159588\n",
      "trainer/Q2 Predictions Min                             -7.20492\n",
      "trainer/Q Targets Mean                                 -2.18751\n",
      "trainer/Q Targets Std                                   1.31597\n",
      "trainer/Q Targets Max                                   0.0423553\n",
      "trainer/Q Targets Min                                  -7.1498\n",
      "trainer/Log Pis Mean                                    1.20588\n",
      "trainer/Log Pis Std                                     1.36652\n",
      "trainer/Log Pis Max                                     6.7834\n",
      "trainer/Log Pis Min                                    -3.3873\n",
      "trainer/Policy mu Mean                                 -0.0163686\n",
      "trainer/Policy mu Std                                   0.633887\n",
      "trainer/Policy mu Max                                   2.41305\n",
      "trainer/Policy mu Min                                  -2.83405\n",
      "trainer/Policy log std Mean                            -1.68811\n",
      "trainer/Policy log std Std                              0.555395\n",
      "trainer/Policy log std Max                              0.351206\n",
      "trainer/Policy log std Min                             -2.70888\n",
      "trainer/Alpha                                           0.0278039\n",
      "trainer/Alpha Loss                                     -2.84399\n",
      "exploration/num steps total                          1400\n",
      "exploration/num paths total                            70\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.218513\n",
      "exploration/Rewards Std                                 0.0870036\n",
      "exploration/Rewards Max                                -0.0625078\n",
      "exploration/Rewards Min                                -0.527785\n",
      "exploration/Returns Mean                               -4.37025\n",
      "exploration/Returns Std                                 0.745795\n",
      "exploration/Returns Max                                -3.18479\n",
      "exploration/Returns Min                                -5.5153\n",
      "exploration/Actions Mean                               -0.00675753\n",
      "exploration/Actions Std                                 0.191701\n",
      "exploration/Actions Max                                 0.585023\n",
      "exploration/Actions Min                                -0.698324\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                            -4.37025\n",
      "exploration/env_infos/final/reward_dist Mean            1.95884e-07\n",
      "exploration/env_infos/final/reward_dist Std             3.91767e-07\n",
      "exploration/env_infos/final/reward_dist Max             9.79418e-07\n",
      "exploration/env_infos/final/reward_dist Min             4.13074e-31\n",
      "exploration/env_infos/initial/reward_dist Mean          0.0065823\n",
      "exploration/env_infos/initial/reward_dist Std           0.00953403\n",
      "exploration/env_infos/initial/reward_dist Max           0.0246377\n",
      "exploration/env_infos/initial/reward_dist Min           5.58389e-06\n",
      "exploration/env_infos/reward_dist Mean                  0.0741105\n",
      "exploration/env_infos/reward_dist Std                   0.213269\n",
      "exploration/env_infos/reward_dist Max                   0.996782\n",
      "exploration/env_infos/reward_dist Min                   4.13074e-31\n",
      "exploration/env_infos/final/reward_energy Mean         -0.34174\n",
      "exploration/env_infos/final/reward_energy Std           0.133296\n",
      "exploration/env_infos/final/reward_energy Max          -0.180776\n",
      "exploration/env_infos/final/reward_energy Min          -0.5564\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.194765\n",
      "exploration/env_infos/initial/reward_energy Std         0.0711915\n",
      "exploration/env_infos/initial/reward_energy Max        -0.0959558\n",
      "exploration/env_infos/initial/reward_energy Min        -0.266792\n",
      "exploration/env_infos/reward_energy Mean               -0.227697\n",
      "exploration/env_infos/reward_energy Std                 0.147458\n",
      "exploration/env_infos/reward_energy Max                -0.0210123\n",
      "exploration/env_infos/reward_energy Min                -0.725702\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean      -0.0748444\n",
      "exploration/env_infos/final/end_effector_loc Std        0.382551\n",
      "exploration/env_infos/final/end_effector_loc Max        0.524773\n",
      "exploration/env_infos/final/end_effector_loc Min       -0.723146\n",
      "exploration/env_infos/initial/end_effector_loc Mean     0.00169161\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.00713376\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0111471\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0120533\n",
      "exploration/env_infos/end_effector_loc Mean             0.00363569\n",
      "exploration/env_infos/end_effector_loc Std              0.223757\n",
      "exploration/env_infos/end_effector_loc Max              0.524773\n",
      "exploration/env_infos/end_effector_loc Min             -0.723146\n",
      "evaluation/num steps total                           4000\n",
      "evaluation/num paths total                            200\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.154526\n",
      "evaluation/Rewards Std                                  0.126546\n",
      "evaluation/Rewards Max                                  0.0852592\n",
      "evaluation/Rewards Min                                 -1.04159\n",
      "evaluation/Returns Mean                                -3.09053\n",
      "evaluation/Returns Std                                  1.68053\n",
      "evaluation/Returns Max                                 -0.385755\n",
      "evaluation/Returns Min                                 -9.44947\n",
      "evaluation/Actions Mean                                -0.0106644\n",
      "evaluation/Actions Std                                  0.115393\n",
      "evaluation/Actions Max                                  0.447187\n",
      "evaluation/Actions Min                                 -0.590926\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -3.09053\n",
      "evaluation/env_infos/final/reward_dist Mean             0.0139149\n",
      "evaluation/env_infos/final/reward_dist Std              0.0439649\n",
      "evaluation/env_infos/final/reward_dist Max              0.234655\n",
      "evaluation/env_infos/final/reward_dist Min              1.7802e-77\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00607394\n",
      "evaluation/env_infos/initial/reward_dist Std            0.00989201\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0349882\n",
      "evaluation/env_infos/initial/reward_dist Min            6.16823e-06\n",
      "evaluation/env_infos/reward_dist Mean                   0.0783719\n",
      "evaluation/env_infos/reward_dist Std                    0.188876\n",
      "evaluation/env_infos/reward_dist Max                    0.983201\n",
      "evaluation/env_infos/reward_dist Min                    1.7802e-77\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.194703\n",
      "evaluation/env_infos/final/reward_energy Std            0.119711\n",
      "evaluation/env_infos/final/reward_energy Max           -0.0241036\n",
      "evaluation/env_infos/final/reward_energy Min           -0.594926\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.175934\n",
      "evaluation/env_infos/initial/reward_energy Std          0.0973403\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0299292\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.542958\n",
      "evaluation/env_infos/reward_energy Mean                -0.136909\n",
      "evaluation/env_infos/reward_energy Std                  0.0900816\n",
      "evaluation/env_infos/reward_energy Max                 -0.00539068\n",
      "evaluation/env_infos/reward_energy Min                 -0.594926\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean       -0.111028\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.366704\n",
      "evaluation/env_infos/final/end_effector_loc Max         0.688137\n",
      "evaluation/env_infos/final/end_effector_loc Min        -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.000338744\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.00710071\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.0223594\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.018419\n",
      "evaluation/env_infos/end_effector_loc Mean             -0.0352607\n",
      "evaluation/env_infos/end_effector_loc Std               0.245378\n",
      "evaluation/env_infos/end_effector_loc Max               0.688137\n",
      "evaluation/env_infos/end_effector_loc Min              -1\n",
      "time/data storing (s)                                   0.00302715\n",
      "time/evaluation sampling (s)                            1.00204\n",
      "time/exploration sampling (s)                           0.119483\n",
      "time/logging (s)                                        0.0189059\n",
      "time/saving (s)                                         0.0290477\n",
      "time/training (s)                                      40.3817\n",
      "time/epoch (s)                                         41.5542\n",
      "time/total (s)                                        264.513\n",
      "Epoch                                                   3\n",
      "---------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:18:25.559039 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 4 finished\n",
      "---------------------------------------------------  --------------\n",
      "replay_buffer/size                                   1500\n",
      "trainer/QF1 Loss                                        0.00980622\n",
      "trainer/QF2 Loss                                        0.00970217\n",
      "trainer/Policy Loss                                     4.19382\n",
      "trainer/Q1 Predictions Mean                            -2.77101\n",
      "trainer/Q1 Predictions Std                              1.67244\n",
      "trainer/Q1 Predictions Max                             -0.181449\n",
      "trainer/Q1 Predictions Min                             -9.09773\n",
      "trainer/Q2 Predictions Mean                            -2.83605\n",
      "trainer/Q2 Predictions Std                              1.68902\n",
      "trainer/Q2 Predictions Max                             -0.220586\n",
      "trainer/Q2 Predictions Min                             -9.08909\n",
      "trainer/Q Targets Mean                                 -2.80172\n",
      "trainer/Q Targets Std                                   1.67376\n",
      "trainer/Q Targets Max                                  -0.304971\n",
      "trainer/Q Targets Min                                  -9.10335\n",
      "trainer/Log Pis Mean                                    1.73946\n",
      "trainer/Log Pis Std                                     1.47903\n",
      "trainer/Log Pis Max                                     8.6428\n",
      "trainer/Log Pis Min                                    -5.90367\n",
      "trainer/Policy mu Mean                                 -0.0116877\n",
      "trainer/Policy mu Std                                   0.768037\n",
      "trainer/Policy mu Max                                   3.12088\n",
      "trainer/Policy mu Min                                  -2.7154\n",
      "trainer/Policy log std Mean                            -1.85284\n",
      "trainer/Policy log std Std                              0.609531\n",
      "trainer/Policy log std Max                             -0.248283\n",
      "trainer/Policy log std Min                             -2.81579\n",
      "trainer/Alpha                                           0.0179338\n",
      "trainer/Alpha Loss                                     -1.04753\n",
      "exploration/num steps total                          1500\n",
      "exploration/num paths total                            75\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.185039\n",
      "exploration/Rewards Std                                 0.0864499\n",
      "exploration/Rewards Max                                -0.00210195\n",
      "exploration/Rewards Min                                -0.502919\n",
      "exploration/Returns Mean                               -3.70077\n",
      "exploration/Returns Std                                 0.686651\n",
      "exploration/Returns Max                                -2.5347\n",
      "exploration/Returns Min                                -4.67839\n",
      "exploration/Actions Mean                               -0.0191028\n",
      "exploration/Actions Std                                 0.119402\n",
      "exploration/Actions Max                                 0.34221\n",
      "exploration/Actions Min                                -0.37382\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                            -3.70077\n",
      "exploration/env_infos/final/reward_dist Mean            2.69802e-06\n",
      "exploration/env_infos/final/reward_dist Std             3.4599e-06\n",
      "exploration/env_infos/final/reward_dist Max             8.42812e-06\n",
      "exploration/env_infos/final/reward_dist Min             4.61265e-28\n",
      "exploration/env_infos/initial/reward_dist Mean          0.0120991\n",
      "exploration/env_infos/initial/reward_dist Std           0.0239685\n",
      "exploration/env_infos/initial/reward_dist Max           0.0600351\n",
      "exploration/env_infos/initial/reward_dist Min           9.77122e-06\n",
      "exploration/env_infos/reward_dist Mean                  0.0578942\n",
      "exploration/env_infos/reward_dist Std                   0.155278\n",
      "exploration/env_infos/reward_dist Max                   0.794904\n",
      "exploration/env_infos/reward_dist Min                   4.61265e-28\n",
      "exploration/env_infos/final/reward_energy Mean         -0.241985\n",
      "exploration/env_infos/final/reward_energy Std           0.0910221\n",
      "exploration/env_infos/final/reward_energy Max          -0.153321\n",
      "exploration/env_infos/final/reward_energy Min          -0.405928\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.192135\n",
      "exploration/env_infos/initial/reward_energy Std         0.0863032\n",
      "exploration/env_infos/initial/reward_energy Max        -0.124289\n",
      "exploration/env_infos/initial/reward_energy Min        -0.346944\n",
      "exploration/env_infos/reward_energy Mean               -0.150374\n",
      "exploration/env_infos/reward_energy Std                 0.0814307\n",
      "exploration/env_infos/reward_energy Max                -0.0108661\n",
      "exploration/env_infos/reward_energy Min                -0.405928\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean      -0.0977437\n",
      "exploration/env_infos/final/end_effector_loc Std        0.328272\n",
      "exploration/env_infos/final/end_effector_loc Max        0.412813\n",
      "exploration/env_infos/final/end_effector_loc Min       -0.550653\n",
      "exploration/env_infos/initial/end_effector_loc Mean     0.00133301\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.00732653\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0171105\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0098357\n",
      "exploration/env_infos/end_effector_loc Mean            -0.00177883\n",
      "exploration/env_infos/end_effector_loc Std              0.193416\n",
      "exploration/env_infos/end_effector_loc Max              0.412813\n",
      "exploration/env_infos/end_effector_loc Min             -0.550653\n",
      "evaluation/num steps total                           5000\n",
      "evaluation/num paths total                            250\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.138405\n",
      "evaluation/Rewards Std                                  0.127079\n",
      "evaluation/Rewards Max                                  0.175818\n",
      "evaluation/Rewards Min                                 -0.875634\n",
      "evaluation/Returns Mean                                -2.7681\n",
      "evaluation/Returns Std                                  1.6903\n",
      "evaluation/Returns Max                                  0.848121\n",
      "evaluation/Returns Min                                 -7.44648\n",
      "evaluation/Actions Mean                                -0.0132224\n",
      "evaluation/Actions Std                                  0.123843\n",
      "evaluation/Actions Max                                  0.558661\n",
      "evaluation/Actions Min                                 -0.756\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -2.7681\n",
      "evaluation/env_infos/final/reward_dist Mean             0.0327722\n",
      "evaluation/env_infos/final/reward_dist Std              0.0957473\n",
      "evaluation/env_infos/final/reward_dist Max              0.495844\n",
      "evaluation/env_infos/final/reward_dist Min              1.7421e-111\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.0056543\n",
      "evaluation/env_infos/initial/reward_dist Std            0.009642\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0369354\n",
      "evaluation/env_infos/initial/reward_dist Min            9.7795e-07\n",
      "evaluation/env_infos/reward_dist Mean                   0.0708348\n",
      "evaluation/env_infos/reward_dist Std                    0.177649\n",
      "evaluation/env_infos/reward_dist Max                    0.998196\n",
      "evaluation/env_infos/reward_dist Min                    1.7421e-111\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.226136\n",
      "evaluation/env_infos/final/reward_energy Std            0.142231\n",
      "evaluation/env_infos/final/reward_energy Max           -0.0337407\n",
      "evaluation/env_infos/final/reward_energy Min           -0.616076\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.24447\n",
      "evaluation/env_infos/initial/reward_energy Std          0.185361\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0357208\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.872488\n",
      "evaluation/env_infos/reward_energy Mean                -0.139616\n",
      "evaluation/env_infos/reward_energy Std                  0.107384\n",
      "evaluation/env_infos/reward_energy Max                 -0.00332395\n",
      "evaluation/env_infos/reward_energy Min                 -0.872488\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean       -0.120011\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.356042\n",
      "evaluation/env_infos/final/end_effector_loc Max         0.965892\n",
      "evaluation/env_infos/final/end_effector_loc Min        -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.000624074\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.0108289\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.027933\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.0378\n",
      "evaluation/env_infos/end_effector_loc Mean             -0.0319308\n",
      "evaluation/env_infos/end_effector_loc Std               0.22982\n",
      "evaluation/env_infos/end_effector_loc Max               0.965892\n",
      "evaluation/env_infos/end_effector_loc Min              -1\n",
      "time/data storing (s)                                   0.00306942\n",
      "time/evaluation sampling (s)                            1.09246\n",
      "time/exploration sampling (s)                           0.12108\n",
      "time/logging (s)                                        0.0190816\n",
      "time/saving (s)                                         0.0297291\n",
      "time/training (s)                                      42.0117\n",
      "time/epoch (s)                                         43.2772\n",
      "time/total (s)                                        307.872\n",
      "Epoch                                                   4\n",
      "---------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:19:08.173371 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 5 finished\r\n",
      "---------------------------------------------------  --------------\r\n",
      "replay_buffer/size                                   1600\r\n",
      "trainer/QF1 Loss                                        0.00391405\r\n",
      "trainer/QF2 Loss                                        0.00828227\r\n",
      "trainer/Policy Loss                                     4.53958\r\n",
      "trainer/Q1 Predictions Mean                            -2.71977\r\n",
      "trainer/Q1 Predictions Std                              1.61255\r\n",
      "trainer/Q1 Predictions Max                             -0.24123\r\n",
      "trainer/Q1 Predictions Min                            -10.4015\r\n",
      "trainer/Q2 Predictions Mean                            -2.6909\r\n",
      "trainer/Q2 Predictions Std                              1.61907\r\n",
      "trainer/Q2 Predictions Max                             -0.162658\r\n",
      "trainer/Q2 Predictions Min                            -10.2839\r\n",
      "trainer/Q Targets Mean                                 -2.72431\r\n",
      "trainer/Q Targets Std                                   1.61548\r\n",
      "trainer/Q Targets Max                                  -0.134168\r\n",
      "trainer/Q Targets Min                                 -10.3591\r\n",
      "trainer/Log Pis Mean                                    2.15081\r\n",
      "trainer/Log Pis Std                                     1.18245\r\n",
      "trainer/Log Pis Max                                     5.55259\r\n",
      "trainer/Log Pis Min                                    -4.33486\r\n",
      "trainer/Policy mu Mean                                 -0.0691641\r\n",
      "trainer/Policy mu Std                                   0.567645\r\n",
      "trainer/Policy mu Max                                   2.31754\r\n",
      "trainer/Policy mu Min                                  -2.55865\r\n",
      "trainer/Policy log std Mean                            -2.14175\r\n",
      "trainer/Policy log std Std                              0.568775\r\n",
      "trainer/Policy log std Max                             -0.37912\r\n",
      "trainer/Policy log std Min                             -3.00872\r\n",
      "trainer/Alpha                                           0.0152722\r\n",
      "trainer/Alpha Loss                                      0.630623\r\n",
      "exploration/num steps total                          1600\r\n",
      "exploration/num paths total                            80\r\n",
      "exploration/path length Mean                           20\r\n",
      "exploration/path length Std                             0\r\n",
      "exploration/path length Max                            20\r\n",
      "exploration/path length Min                            20\r\n",
      "exploration/Rewards Mean                               -0.159711\r\n",
      "exploration/Rewards Std                                 0.077504\r\n",
      "exploration/Rewards Max                                -0.0124729\r\n",
      "exploration/Rewards Min                                -0.407509\r\n",
      "exploration/Returns Mean                               -3.19422\r\n",
      "exploration/Returns Std                                 1.14011\r\n",
      "exploration/Returns Max                                -1.35609\r\n",
      "exploration/Returns Min                                -4.27307\r\n",
      "exploration/Actions Mean                               -0.0130702\r\n",
      "exploration/Actions Std                                 0.182171\r\n",
      "exploration/Actions Max                                 0.616261\r\n",
      "exploration/Actions Min                                -0.672497\r\n",
      "exploration/Num Paths                                   5\r\n",
      "exploration/Average Returns                            -3.19422\r\n",
      "exploration/env_infos/final/reward_dist Mean            0.129612\r\n",
      "exploration/env_infos/final/reward_dist Std             0.259203\r\n",
      "exploration/env_infos/final/reward_dist Max             0.648019\r\n",
      "exploration/env_infos/final/reward_dist Min             7.32844e-39\r\n",
      "exploration/env_infos/initial/reward_dist Mean          0.00216965\r\n",
      "exploration/env_infos/initial/reward_dist Std           0.00427531\r\n",
      "exploration/env_infos/initial/reward_dist Max           0.0107201\r\n",
      "exploration/env_infos/initial/reward_dist Min           4.88192e-06\r\n",
      "exploration/env_infos/reward_dist Mean                  0.128057\r\n",
      "exploration/env_infos/reward_dist Std                   0.281165\r\n",
      "exploration/env_infos/reward_dist Max                   0.961812\r\n",
      "exploration/env_infos/reward_dist Min                   7.32844e-39\r\n",
      "exploration/env_infos/final/reward_energy Mean         -0.245481\r\n",
      "exploration/env_infos/final/reward_energy Std           0.0895232\r\n",
      "exploration/env_infos/final/reward_energy Max          -0.107677\r\n",
      "exploration/env_infos/final/reward_energy Min          -0.347745\r\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.177843\r\n",
      "exploration/env_infos/initial/reward_energy Std         0.156465\r\n",
      "exploration/env_infos/initial/reward_energy Max        -0.0534861\r\n",
      "exploration/env_infos/initial/reward_energy Min        -0.480436\r\n",
      "exploration/env_infos/reward_energy Mean               -0.202719\r\n",
      "exploration/env_infos/reward_energy Std                 0.16006\r\n",
      "exploration/env_infos/reward_energy Max                -0.00457819\r\n",
      "exploration/env_infos/reward_energy Min                -0.768262\r\n",
      "exploration/env_infos/final/reward_safety Mean          0\r\n",
      "exploration/env_infos/final/reward_safety Std           0\r\n",
      "exploration/env_infos/final/reward_safety Max           0\r\n",
      "exploration/env_infos/final/reward_safety Min           0\r\n",
      "exploration/env_infos/initial/reward_safety Mean        0\r\n",
      "exploration/env_infos/initial/reward_safety Std         0\r\n",
      "exploration/env_infos/initial/reward_safety Max         0\r\n",
      "exploration/env_infos/initial/reward_safety Min         0\r\n",
      "exploration/env_infos/reward_safety Mean                0\r\n",
      "exploration/env_infos/reward_safety Std                 0\r\n",
      "exploration/env_infos/reward_safety Max                 0\r\n",
      "exploration/env_infos/reward_safety Min                 0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean      -0.0926429\r\n",
      "exploration/env_infos/final/end_effector_loc Std        0.236094\r\n",
      "exploration/env_infos/final/end_effector_loc Max        0.2314\r\n",
      "exploration/env_infos/final/end_effector_loc Min       -0.48896\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean    -0.00360691\r\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.00755822\r\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.00599825\r\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0230505\r\n",
      "exploration/env_infos/end_effector_loc Mean            -0.0116781\r\n",
      "exploration/env_infos/end_effector_loc Std              0.137859\r\n",
      "exploration/env_infos/end_effector_loc Max              0.266246\r\n",
      "exploration/env_infos/end_effector_loc Min             -0.48896\r\n",
      "evaluation/num steps total                           6000\r\n",
      "evaluation/num paths total                            300\r\n",
      "evaluation/path length Mean                            20\r\n",
      "evaluation/path length Std                              0\r\n",
      "evaluation/path length Max                             20\r\n",
      "evaluation/path length Min                             20\r\n",
      "evaluation/Rewards Mean                                -0.140115\r\n",
      "evaluation/Rewards Std                                  0.0954639\r\n",
      "evaluation/Rewards Max                                  0.122094\r\n",
      "evaluation/Rewards Min                                 -0.676226\r\n",
      "evaluation/Returns Mean                                -2.8023\r\n",
      "evaluation/Returns Std                                  1.18033\r\n",
      "evaluation/Returns Max                                 -0.53575\r\n",
      "evaluation/Returns Min                                 -5.67409\r\n",
      "evaluation/Actions Mean                                -0.0217011\r\n",
      "evaluation/Actions Std                                  0.0957455\r\n",
      "evaluation/Actions Max                                  0.397837\r\n",
      "evaluation/Actions Min                                 -0.409855\r\n",
      "evaluation/Num Paths                                   50\r\n",
      "evaluation/Average Returns                             -2.8023\r\n",
      "evaluation/env_infos/final/reward_dist Mean             0.0370594\r\n",
      "evaluation/env_infos/final/reward_dist Std              0.138308\r\n",
      "evaluation/env_infos/final/reward_dist Max              0.783803\r\n",
      "evaluation/env_infos/final/reward_dist Min              3.64493e-74\r\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00665165\r\n",
      "evaluation/env_infos/initial/reward_dist Std            0.00947028\r\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0324627\r\n",
      "evaluation/env_infos/initial/reward_dist Min            1.2328e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                   0.0515727\r\n",
      "evaluation/env_infos/reward_dist Std                    0.134432\r\n",
      "evaluation/env_infos/reward_dist Max                    0.9591\r\n",
      "evaluation/env_infos/reward_dist Min                    3.64493e-74\r\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.187431\r\n",
      "evaluation/env_infos/final/reward_energy Std            0.118135\r\n",
      "evaluation/env_infos/final/reward_energy Max           -0.043561\r\n",
      "evaluation/env_infos/final/reward_energy Min           -0.528155\r\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.153871\r\n",
      "evaluation/env_infos/initial/reward_energy Std          0.0687537\r\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0442669\r\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.364019\r\n",
      "evaluation/env_infos/reward_energy Mean                -0.114251\r\n",
      "evaluation/env_infos/reward_energy Std                  0.0788854\r\n",
      "evaluation/env_infos/reward_energy Max                 -0.00217171\r\n",
      "evaluation/env_infos/reward_energy Min                 -0.528155\r\n",
      "evaluation/env_infos/final/reward_safety Mean           0\r\n",
      "evaluation/env_infos/final/reward_safety Std            0\r\n",
      "evaluation/env_infos/final/reward_safety Max            0\r\n",
      "evaluation/env_infos/final/reward_safety Min            0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\r\n",
      "evaluation/env_infos/initial/reward_safety Std          0\r\n",
      "evaluation/env_infos/initial/reward_safety Max          0\r\n",
      "evaluation/env_infos/initial/reward_safety Min          0\r\n",
      "evaluation/env_infos/reward_safety Mean                 0\r\n",
      "evaluation/env_infos/reward_safety Std                  0\r\n",
      "evaluation/env_infos/reward_safety Max                  0\r\n",
      "evaluation/env_infos/reward_safety Min                  0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean       -0.201088\r\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.29125\r\n",
      "evaluation/env_infos/final/end_effector_loc Max         0.319776\r\n",
      "evaluation/env_infos/final/end_effector_loc Min        -1\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.00126295\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.00582315\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.0111058\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.0180466\r\n",
      "evaluation/env_infos/end_effector_loc Mean             -0.0514303\r\n",
      "evaluation/env_infos/end_effector_loc Std               0.188284\r\n",
      "evaluation/env_infos/end_effector_loc Max               0.333243\r\n",
      "evaluation/env_infos/end_effector_loc Min              -1\r\n",
      "time/data storing (s)                                   0.00296788\r\n",
      "time/evaluation sampling (s)                            0.94454\r\n",
      "time/exploration sampling (s)                           0.122848\r\n",
      "time/logging (s)                                        0.0192485\r\n",
      "time/saving (s)                                         0.0325745\r\n",
      "time/training (s)                                      41.3926\r\n",
      "time/epoch (s)                                         42.5148\r\n",
      "time/total (s)                                        350.486\r\n",
      "Epoch                                                   5\r\n",
      "---------------------------------------------------  --------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:19:53.753783 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 6 finished\r\n",
      "---------------------------------------------------  --------------\r\n",
      "replay_buffer/size                                   1700\r\n",
      "trainer/QF1 Loss                                        0.00401628\r\n",
      "trainer/QF2 Loss                                        0.00301326\r\n",
      "trainer/Policy Loss                                     4.5679\r\n",
      "trainer/Q1 Predictions Mean                            -2.94904\r\n",
      "trainer/Q1 Predictions Std                              1.78239\r\n",
      "trainer/Q1 Predictions Max                             -0.145669\r\n",
      "trainer/Q1 Predictions Min                             -9.25414\r\n",
      "trainer/Q2 Predictions Mean                            -2.95907\r\n",
      "trainer/Q2 Predictions Std                              1.78011\r\n",
      "trainer/Q2 Predictions Max                             -0.138948\r\n",
      "trainer/Q2 Predictions Min                             -9.21313\r\n",
      "trainer/Q Targets Mean                                 -2.96176\r\n",
      "trainer/Q Targets Std                                   1.78064\r\n",
      "trainer/Q Targets Max                                  -0.134168\r\n",
      "trainer/Q Targets Min                                  -9.25124\r\n",
      "trainer/Log Pis Mean                                    1.98999\r\n",
      "trainer/Log Pis Std                                     1.43828\r\n",
      "trainer/Log Pis Max                                     7.88639\r\n",
      "trainer/Log Pis Min                                    -3.32601\r\n",
      "trainer/Policy mu Mean                                 -0.00859346\r\n",
      "trainer/Policy mu Std                                   0.609074\r\n",
      "trainer/Policy mu Max                                   2.67305\r\n",
      "trainer/Policy mu Min                                  -2.60096\r\n",
      "trainer/Policy log std Mean                            -2.12876\r\n",
      "trainer/Policy log std Std                              0.567733\r\n",
      "trainer/Policy log std Max                             -0.163518\r\n",
      "trainer/Policy log std Min                             -3.13718\r\n",
      "trainer/Alpha                                           0.0148503\r\n",
      "trainer/Alpha Loss                                     -0.0421255\r\n",
      "exploration/num steps total                          1700\r\n",
      "exploration/num paths total                            85\r\n",
      "exploration/path length Mean                           20\r\n",
      "exploration/path length Std                             0\r\n",
      "exploration/path length Max                            20\r\n",
      "exploration/path length Min                            20\r\n",
      "exploration/Rewards Mean                               -0.12011\r\n",
      "exploration/Rewards Std                                 0.0995193\r\n",
      "exploration/Rewards Max                                 0.0391944\r\n",
      "exploration/Rewards Min                                -0.515167\r\n",
      "exploration/Returns Mean                               -2.40221\r\n",
      "exploration/Returns Std                                 1.31784\r\n",
      "exploration/Returns Max                                -0.940018\r\n",
      "exploration/Returns Min                                -4.38468\r\n",
      "exploration/Actions Mean                               -0.0201889\r\n",
      "exploration/Actions Std                                 0.124215\r\n",
      "exploration/Actions Max                                 0.457947\r\n",
      "exploration/Actions Min                                -0.424974\r\n",
      "exploration/Num Paths                                   5\r\n",
      "exploration/Average Returns                            -2.40221\r\n",
      "exploration/env_infos/final/reward_dist Mean            0.0403122\r\n",
      "exploration/env_infos/final/reward_dist Std             0.0806241\r\n",
      "exploration/env_infos/final/reward_dist Max             0.20156\r\n",
      "exploration/env_infos/final/reward_dist Min             2.46859e-46\r\n",
      "exploration/env_infos/initial/reward_dist Mean          0.0120158\r\n",
      "exploration/env_infos/initial/reward_dist Std           0.0158164\r\n",
      "exploration/env_infos/initial/reward_dist Max           0.0433042\r\n",
      "exploration/env_infos/initial/reward_dist Min           4.82377e-06\r\n",
      "exploration/env_infos/reward_dist Mean                  0.0555058\r\n",
      "exploration/env_infos/reward_dist Std                   0.0871416\r\n",
      "exploration/env_infos/reward_dist Max                   0.48784\r\n",
      "exploration/env_infos/reward_dist Min                   2.46859e-46\r\n",
      "exploration/env_infos/final/reward_energy Mean         -0.207155\r\n",
      "exploration/env_infos/final/reward_energy Std           0.0950714\r\n",
      "exploration/env_infos/final/reward_energy Max          -0.102552\r\n",
      "exploration/env_infos/final/reward_energy Min          -0.366865\r\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.229679\r\n",
      "exploration/env_infos/initial/reward_energy Std         0.161923\r\n",
      "exploration/env_infos/initial/reward_energy Max        -0.0247831\r\n",
      "exploration/env_infos/initial/reward_energy Min        -0.483169\r\n",
      "exploration/env_infos/reward_energy Mean               -0.153621\r\n",
      "exploration/env_infos/reward_energy Std                 0.0898589\r\n",
      "exploration/env_infos/reward_energy Max                -0.0215799\r\n",
      "exploration/env_infos/reward_energy Min                -0.483169\r\n",
      "exploration/env_infos/final/reward_safety Mean          0\r\n",
      "exploration/env_infos/final/reward_safety Std           0\r\n",
      "exploration/env_infos/final/reward_safety Max           0\r\n",
      "exploration/env_infos/final/reward_safety Min           0\r\n",
      "exploration/env_infos/initial/reward_safety Mean        0\r\n",
      "exploration/env_infos/initial/reward_safety Std         0\r\n",
      "exploration/env_infos/initial/reward_safety Max         0\r\n",
      "exploration/env_infos/initial/reward_safety Min         0\r\n",
      "exploration/env_infos/reward_safety Mean                0\r\n",
      "exploration/env_infos/reward_safety Std                 0\r\n",
      "exploration/env_infos/reward_safety Max                 0\r\n",
      "exploration/env_infos/reward_safety Min                 0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean      -0.185485\r\n",
      "exploration/env_infos/final/end_effector_loc Std        0.318867\r\n",
      "exploration/env_infos/final/end_effector_loc Max        0.246597\r\n",
      "exploration/env_infos/final/end_effector_loc Min       -0.711116\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean    -0.00150753\r\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.00982047\r\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.012737\r\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0212487\r\n",
      "exploration/env_infos/end_effector_loc Mean            -0.0659243\r\n",
      "exploration/env_infos/end_effector_loc Std              0.195697\r\n",
      "exploration/env_infos/end_effector_loc Max              0.252024\r\n",
      "exploration/env_infos/end_effector_loc Min             -0.711116\r\n",
      "evaluation/num steps total                           7000\r\n",
      "evaluation/num paths total                            350\r\n",
      "evaluation/path length Mean                            20\r\n",
      "evaluation/path length Std                              0\r\n",
      "evaluation/path length Max                             20\r\n",
      "evaluation/path length Min                             20\r\n",
      "evaluation/Rewards Mean                                -0.126556\r\n",
      "evaluation/Rewards Std                                  0.122697\r\n",
      "evaluation/Rewards Max                                  0.107346\r\n",
      "evaluation/Rewards Min                                 -0.939904\r\n",
      "evaluation/Returns Mean                                -2.53112\r\n",
      "evaluation/Returns Std                                  1.3597\r\n",
      "evaluation/Returns Max                                 -0.361605\r\n",
      "evaluation/Returns Min                                 -8.06351\r\n",
      "evaluation/Actions Mean                                -0.0139378\r\n",
      "evaluation/Actions Std                                  0.102267\r\n",
      "evaluation/Actions Max                                  0.43149\r\n",
      "evaluation/Actions Min                                 -0.801056\r\n",
      "evaluation/Num Paths                                   50\r\n",
      "evaluation/Average Returns                             -2.53112\r\n",
      "evaluation/env_infos/final/reward_dist Mean             0.0116841\r\n",
      "evaluation/env_infos/final/reward_dist Std              0.0428651\r\n",
      "evaluation/env_infos/final/reward_dist Max              0.245459\r\n",
      "evaluation/env_infos/final/reward_dist Min              1.55703e-63\r\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00737524\r\n",
      "evaluation/env_infos/initial/reward_dist Std            0.0124404\r\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0593836\r\n",
      "evaluation/env_infos/initial/reward_dist Min            3.99796e-07\r\n",
      "evaluation/env_infos/reward_dist Mean                   0.0540835\r\n",
      "evaluation/env_infos/reward_dist Std                    0.146721\r\n",
      "evaluation/env_infos/reward_dist Max                    0.960647\r\n",
      "evaluation/env_infos/reward_dist Min                    2.778e-70\r\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.217906\r\n",
      "evaluation/env_infos/final/reward_energy Std            0.184873\r\n",
      "evaluation/env_infos/final/reward_energy Max           -0.0116104\r\n",
      "evaluation/env_infos/final/reward_energy Min           -0.878679\r\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.154152\r\n",
      "evaluation/env_infos/initial/reward_energy Std          0.0884744\r\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0229355\r\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.448848\r\n",
      "evaluation/env_infos/reward_energy Mean                -0.107257\r\n",
      "evaluation/env_infos/reward_energy Std                  0.0990024\r\n",
      "evaluation/env_infos/reward_energy Max                 -0.00113453\r\n",
      "evaluation/env_infos/reward_energy Min                 -0.878679\r\n",
      "evaluation/env_infos/final/reward_safety Mean           0\r\n",
      "evaluation/env_infos/final/reward_safety Std            0\r\n",
      "evaluation/env_infos/final/reward_safety Max            0\r\n",
      "evaluation/env_infos/final/reward_safety Min            0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\r\n",
      "evaluation/env_infos/initial/reward_safety Std          0\r\n",
      "evaluation/env_infos/initial/reward_safety Max          0\r\n",
      "evaluation/env_infos/initial/reward_safety Min          0\r\n",
      "evaluation/env_infos/reward_safety Mean                 0\r\n",
      "evaluation/env_infos/reward_safety Std                  0\r\n",
      "evaluation/env_infos/reward_safety Max                  0\r\n",
      "evaluation/env_infos/reward_safety Min                  0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean       -0.023272\r\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.328776\r\n",
      "evaluation/env_infos/final/end_effector_loc Max         0.518937\r\n",
      "evaluation/env_infos/final/end_effector_loc Min        -1\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.0025474\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.00574448\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.0215745\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.0149535\r\n",
      "evaluation/env_infos/end_effector_loc Mean              0.0251593\r\n",
      "evaluation/env_infos/end_effector_loc Std               0.194369\r\n",
      "evaluation/env_infos/end_effector_loc Max               0.715969\r\n",
      "evaluation/env_infos/end_effector_loc Min              -1\r\n",
      "time/data storing (s)                                   0.0030781\r\n",
      "time/evaluation sampling (s)                            1.01022\r\n",
      "time/exploration sampling (s)                           0.133743\r\n",
      "time/logging (s)                                        0.0194217\r\n",
      "time/saving (s)                                         0.0297984\r\n",
      "time/training (s)                                      44.2768\r\n",
      "time/epoch (s)                                         45.473\r\n",
      "time/total (s)                                        396.066\r\n",
      "Epoch                                                   6\r\n",
      "---------------------------------------------------  --------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:20:41.537561 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 7 finished\n",
      "---------------------------------------------------  --------------\n",
      "replay_buffer/size                                   1800\n",
      "trainer/QF1 Loss                                        0.00532094\n",
      "trainer/QF2 Loss                                        0.00913572\n",
      "trainer/Policy Loss                                     4.66646\n",
      "trainer/Q1 Predictions Mean                            -3.1223\n",
      "trainer/Q1 Predictions Std                              1.9577\n",
      "trainer/Q1 Predictions Max                             -0.148908\n",
      "trainer/Q1 Predictions Min                            -10.6049\n",
      "trainer/Q2 Predictions Mean                            -3.04868\n",
      "trainer/Q2 Predictions Std                              1.9379\n",
      "trainer/Q2 Predictions Max                             -0.103933\n",
      "trainer/Q2 Predictions Min                            -10.6075\n",
      "trainer/Q Targets Mean                                 -3.09267\n",
      "trainer/Q Targets Std                                   1.95351\n",
      "trainer/Q Targets Max                                  -0.114265\n",
      "trainer/Q Targets Min                                 -10.6191\n",
      "trainer/Log Pis Mean                                    1.91905\n",
      "trainer/Log Pis Std                                     1.22466\n",
      "trainer/Log Pis Max                                     7.41546\n",
      "trainer/Log Pis Min                                    -3.28219\n",
      "trainer/Policy mu Mean                                 -0.0383501\n",
      "trainer/Policy mu Std                                   0.657596\n",
      "trainer/Policy mu Max                                   2.85599\n",
      "trainer/Policy mu Min                                  -2.87545\n",
      "trainer/Policy log std Mean                            -2.07488\n",
      "trainer/Policy log std Std                              0.596162\n",
      "trainer/Policy log std Max                             -0.0168958\n",
      "trainer/Policy log std Min                             -2.9021\n",
      "trainer/Alpha                                           0.014825\n",
      "trainer/Alpha Loss                                     -0.340906\n",
      "exploration/num steps total                          1800\n",
      "exploration/num paths total                            90\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.109248\n",
      "exploration/Rewards Std                                 0.078963\n",
      "exploration/Rewards Max                                 0.00457018\n",
      "exploration/Rewards Min                                -0.349884\n",
      "exploration/Returns Mean                               -2.18496\n",
      "exploration/Returns Std                                 1.01039\n",
      "exploration/Returns Max                                -1.00419\n",
      "exploration/Returns Min                                -4.03704\n",
      "exploration/Actions Mean                                0.00175343\n",
      "exploration/Actions Std                                 0.136674\n",
      "exploration/Actions Max                                 0.361814\n",
      "exploration/Actions Min                                -0.312836\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                            -2.18496\n",
      "exploration/env_infos/final/reward_dist Mean            0.0328673\n",
      "exploration/env_infos/final/reward_dist Std             0.054723\n",
      "exploration/env_infos/final/reward_dist Max             0.140989\n",
      "exploration/env_infos/final/reward_dist Min             4.04595e-10\n",
      "exploration/env_infos/initial/reward_dist Mean          0.00521036\n",
      "exploration/env_infos/initial/reward_dist Std           0.00504233\n",
      "exploration/env_infos/initial/reward_dist Max           0.0125878\n",
      "exploration/env_infos/initial/reward_dist Min           5.22559e-06\n",
      "exploration/env_infos/reward_dist Mean                  0.0718827\n",
      "exploration/env_infos/reward_dist Std                   0.116213\n",
      "exploration/env_infos/reward_dist Max                   0.51901\n",
      "exploration/env_infos/reward_dist Min                   4.04595e-10\n",
      "exploration/env_infos/final/reward_energy Mean         -0.192104\n",
      "exploration/env_infos/final/reward_energy Std           0.117009\n",
      "exploration/env_infos/final/reward_energy Max          -0.0606609\n",
      "exploration/env_infos/final/reward_energy Min          -0.355326\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.184392\n",
      "exploration/env_infos/initial/reward_energy Std         0.109897\n",
      "exploration/env_infos/initial/reward_energy Max        -0.0346794\n",
      "exploration/env_infos/initial/reward_energy Min        -0.367841\n",
      "exploration/env_infos/reward_energy Mean               -0.165589\n",
      "exploration/env_infos/reward_energy Std                 0.0997282\n",
      "exploration/env_infos/reward_energy Max                -0.0136825\n",
      "exploration/env_infos/reward_energy Min                -0.401459\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean      -0.00647823\n",
      "exploration/env_infos/final/end_effector_loc Std        0.250561\n",
      "exploration/env_infos/final/end_effector_loc Max        0.417808\n",
      "exploration/env_infos/final/end_effector_loc Min       -0.369255\n",
      "exploration/env_infos/initial/end_effector_loc Mean    -0.000362005\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.00758064\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.00967479\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0156418\n",
      "exploration/env_infos/end_effector_loc Mean             0.00333751\n",
      "exploration/env_infos/end_effector_loc Std              0.154675\n",
      "exploration/env_infos/end_effector_loc Max              0.417808\n",
      "exploration/env_infos/end_effector_loc Min             -0.369255\n",
      "evaluation/num steps total                           8000\n",
      "evaluation/num paths total                            400\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.120225\n",
      "evaluation/Rewards Std                                  0.114706\n",
      "evaluation/Rewards Max                                  0.10838\n",
      "evaluation/Rewards Min                                 -0.824555\n",
      "evaluation/Returns Mean                                -2.40449\n",
      "evaluation/Returns Std                                  1.31461\n",
      "evaluation/Returns Max                                 -0.224426\n",
      "evaluation/Returns Min                                 -5.54946\n",
      "evaluation/Actions Mean                                -0.012673\n",
      "evaluation/Actions Std                                  0.0939277\n",
      "evaluation/Actions Max                                  0.474986\n",
      "evaluation/Actions Min                                 -0.631368\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -2.40449\n",
      "evaluation/env_infos/final/reward_dist Mean             0.0359109\n",
      "evaluation/env_infos/final/reward_dist Std              0.126213\n",
      "evaluation/env_infos/final/reward_dist Max              0.759091\n",
      "evaluation/env_infos/final/reward_dist Min              4.1122e-70\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00473938\n",
      "evaluation/env_infos/initial/reward_dist Std            0.00780024\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0308037\n",
      "evaluation/env_infos/initial/reward_dist Min            8.80153e-07\n",
      "evaluation/env_infos/reward_dist Mean                   0.0778644\n",
      "evaluation/env_infos/reward_dist Std                    0.183512\n",
      "evaluation/env_infos/reward_dist Max                    0.993649\n",
      "evaluation/env_infos/reward_dist Min                    4.1122e-70\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.194139\n",
      "evaluation/env_infos/final/reward_energy Std            0.166278\n",
      "evaluation/env_infos/final/reward_energy Max           -0.011186\n",
      "evaluation/env_infos/final/reward_energy Min           -0.631476\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.138763\n",
      "evaluation/env_infos/initial/reward_energy Std          0.073323\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0173869\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.401362\n",
      "evaluation/env_infos/reward_energy Mean                -0.100189\n",
      "evaluation/env_infos/reward_energy Std                  0.0890407\n",
      "evaluation/env_infos/reward_energy Max                 -0.0037521\n",
      "evaluation/env_infos/reward_energy Min                 -0.631476\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean       -0.0413599\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.33914\n",
      "evaluation/env_infos/final/end_effector_loc Max         0.483354\n",
      "evaluation/env_infos/final/end_effector_loc Min        -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.00209752\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.00513709\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.0132112\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.0181754\n",
      "evaluation/env_infos/end_effector_loc Mean              0.00788367\n",
      "evaluation/env_infos/end_effector_loc Std               0.191619\n",
      "evaluation/env_infos/end_effector_loc Max               0.483354\n",
      "evaluation/env_infos/end_effector_loc Min              -1\n",
      "time/data storing (s)                                   0.00304509\n",
      "time/evaluation sampling (s)                            1.0774\n",
      "time/exploration sampling (s)                           0.129972\n",
      "time/logging (s)                                        0.0218681\n",
      "time/saving (s)                                         0.0305959\n",
      "time/training (s)                                      46.406\n",
      "time/epoch (s)                                         47.6689\n",
      "time/total (s)                                        443.852\n",
      "Epoch                                                   7\n",
      "---------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:21:28.228418 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 8 finished\n",
      "---------------------------------------------------  --------------\n",
      "replay_buffer/size                                   1900\n",
      "trainer/QF1 Loss                                        0.00315309\n",
      "trainer/QF2 Loss                                        0.0186633\n",
      "trainer/Policy Loss                                     4.69278\n",
      "trainer/Q1 Predictions Mean                            -3.04336\n",
      "trainer/Q1 Predictions Std                              1.76928\n",
      "trainer/Q1 Predictions Max                             -0.101967\n",
      "trainer/Q1 Predictions Min                            -10.13\n",
      "trainer/Q2 Predictions Mean                            -3.15417\n",
      "trainer/Q2 Predictions Std                              1.78896\n",
      "trainer/Q2 Predictions Max                             -0.116019\n",
      "trainer/Q2 Predictions Min                            -10.1045\n",
      "trainer/Q Targets Mean                                 -3.05003\n",
      "trainer/Q Targets Std                                   1.77127\n",
      "trainer/Q Targets Max                                  -0.119584\n",
      "trainer/Q Targets Min                                 -10.2059\n",
      "trainer/Log Pis Mean                                    1.89864\n",
      "trainer/Log Pis Std                                     1.52933\n",
      "trainer/Log Pis Max                                     7.42613\n",
      "trainer/Log Pis Min                                    -2.84766\n",
      "trainer/Policy mu Mean                                  0.00674611\n",
      "trainer/Policy mu Std                                   0.796232\n",
      "trainer/Policy mu Max                                   2.85768\n",
      "trainer/Policy mu Min                                  -2.94379\n",
      "trainer/Policy log std Mean                            -1.96356\n",
      "trainer/Policy log std Std                              0.660782\n",
      "trainer/Policy log std Max                              0.0122131\n",
      "trainer/Policy log std Min                             -3.01445\n",
      "trainer/Alpha                                           0.0155872\n",
      "trainer/Alpha Loss                                     -0.421793\n",
      "exploration/num steps total                          1900\n",
      "exploration/num paths total                            95\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.120469\n",
      "exploration/Rewards Std                                 0.0981408\n",
      "exploration/Rewards Max                                 0.028481\n",
      "exploration/Rewards Min                                -0.542977\n",
      "exploration/Returns Mean                               -2.40938\n",
      "exploration/Returns Std                                 0.874631\n",
      "exploration/Returns Max                                -1.33071\n",
      "exploration/Returns Min                                -3.75844\n",
      "exploration/Actions Mean                               -0.00631211\n",
      "exploration/Actions Std                                 0.187973\n",
      "exploration/Actions Max                                 0.727745\n",
      "exploration/Actions Min                                -0.506436\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                            -2.40938\n",
      "exploration/env_infos/final/reward_dist Mean            0.000490911\n",
      "exploration/env_infos/final/reward_dist Std             0.0009812\n",
      "exploration/env_infos/final/reward_dist Max             0.00245331\n",
      "exploration/env_infos/final/reward_dist Min             1.22721e-35\n",
      "exploration/env_infos/initial/reward_dist Mean          0.000109025\n",
      "exploration/env_infos/initial/reward_dist Std           0.000112665\n",
      "exploration/env_infos/initial/reward_dist Max           0.000274696\n",
      "exploration/env_infos/initial/reward_dist Min           1.33551e-05\n",
      "exploration/env_infos/reward_dist Mean                  0.0641289\n",
      "exploration/env_infos/reward_dist Std                   0.181745\n",
      "exploration/env_infos/reward_dist Max                   0.867845\n",
      "exploration/env_infos/reward_dist Min                   1.22721e-35\n",
      "exploration/env_infos/final/reward_energy Mean         -0.244852\n",
      "exploration/env_infos/final/reward_energy Std           0.0872895\n",
      "exploration/env_infos/final/reward_energy Max          -0.0963312\n",
      "exploration/env_infos/final/reward_energy Min          -0.369723\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.454919\n",
      "exploration/env_infos/initial/reward_energy Std         0.176031\n",
      "exploration/env_infos/initial/reward_energy Max        -0.249878\n",
      "exploration/env_infos/initial/reward_energy Min        -0.728648\n",
      "exploration/env_infos/reward_energy Mean               -0.224167\n",
      "exploration/env_infos/reward_energy Std                 0.143166\n",
      "exploration/env_infos/reward_energy Max                -0.0209205\n",
      "exploration/env_infos/reward_energy Min                -0.728648\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean      -0.0597196\n",
      "exploration/env_infos/final/end_effector_loc Std        0.401409\n",
      "exploration/env_infos/final/end_effector_loc Max        0.6004\n",
      "exploration/env_infos/final/end_effector_loc Min       -0.768194\n",
      "exploration/env_infos/initial/end_effector_loc Mean     0.00411278\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.0167484\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0363872\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0164456\n",
      "exploration/env_infos/end_effector_loc Mean            -0.0123202\n",
      "exploration/env_infos/end_effector_loc Std              0.265136\n",
      "exploration/env_infos/end_effector_loc Max              0.6004\n",
      "exploration/env_infos/end_effector_loc Min             -0.768194\n",
      "evaluation/num steps total                           9000\n",
      "evaluation/num paths total                            450\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.121668\n",
      "evaluation/Rewards Std                                  0.117023\n",
      "evaluation/Rewards Max                                  0.104391\n",
      "evaluation/Rewards Min                                 -0.792242\n",
      "evaluation/Returns Mean                                -2.43336\n",
      "evaluation/Returns Std                                  1.28797\n",
      "evaluation/Returns Max                                 -0.0298396\n",
      "evaluation/Returns Min                                 -5.82313\n",
      "evaluation/Actions Mean                                -0.0164039\n",
      "evaluation/Actions Std                                  0.0997085\n",
      "evaluation/Actions Max                                  0.528124\n",
      "evaluation/Actions Min                                 -0.608067\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -2.43336\n",
      "evaluation/env_infos/final/reward_dist Mean             0.0783531\n",
      "evaluation/env_infos/final/reward_dist Std              0.210993\n",
      "evaluation/env_infos/final/reward_dist Max              0.959972\n",
      "evaluation/env_infos/final/reward_dist Min              5.13075e-81\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00538389\n",
      "evaluation/env_infos/initial/reward_dist Std            0.00752556\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0352487\n",
      "evaluation/env_infos/initial/reward_dist Min            1.07641e-06\n",
      "evaluation/env_infos/reward_dist Mean                   0.0828767\n",
      "evaluation/env_infos/reward_dist Std                    0.177923\n",
      "evaluation/env_infos/reward_dist Max                    0.98926\n",
      "evaluation/env_infos/reward_dist Min                    5.13075e-81\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.144679\n",
      "evaluation/env_infos/final/reward_energy Std            0.0979144\n",
      "evaluation/env_infos/final/reward_energy Max           -0.00811962\n",
      "evaluation/env_infos/final/reward_energy Min           -0.464694\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.233305\n",
      "evaluation/env_infos/initial/reward_energy Std          0.141318\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0240861\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.632523\n",
      "evaluation/env_infos/reward_energy Mean                -0.112431\n",
      "evaluation/env_infos/reward_energy Std                  0.08821\n",
      "evaluation/env_infos/reward_energy Max                 -0.00260803\n",
      "evaluation/env_infos/reward_energy Min                 -0.632523\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean       -0.0991857\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.393903\n",
      "evaluation/env_infos/final/end_effector_loc Max         0.771833\n",
      "evaluation/env_infos/final/end_effector_loc Min        -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.00200743\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.00943255\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.0264062\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.0304034\n",
      "evaluation/env_infos/end_effector_loc Mean             -0.0129488\n",
      "evaluation/env_infos/end_effector_loc Std               0.236235\n",
      "evaluation/env_infos/end_effector_loc Max               0.771833\n",
      "evaluation/env_infos/end_effector_loc Min              -1\n",
      "time/data storing (s)                                   0.00299193\n",
      "time/evaluation sampling (s)                            1.0975\n",
      "time/exploration sampling (s)                           0.129689\n",
      "time/logging (s)                                        0.0213201\n",
      "time/saving (s)                                         0.0313278\n",
      "time/training (s)                                      45.271\n",
      "time/epoch (s)                                         46.5539\n",
      "time/total (s)                                        490.542\n",
      "Epoch                                                   8\n",
      "---------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:22:14.002617 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 9 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00445208\n",
      "trainer/QF2 Loss                                         0.00415612\n",
      "trainer/Policy Loss                                      4.46808\n",
      "trainer/Q1 Predictions Mean                             -2.91392\n",
      "trainer/Q1 Predictions Std                               1.97332\n",
      "trainer/Q1 Predictions Max                              -0.0731378\n",
      "trainer/Q1 Predictions Min                             -11.0758\n",
      "trainer/Q2 Predictions Mean                             -2.88326\n",
      "trainer/Q2 Predictions Std                               1.95605\n",
      "trainer/Q2 Predictions Max                              -0.10773\n",
      "trainer/Q2 Predictions Min                             -10.884\n",
      "trainer/Q Targets Mean                                  -2.88772\n",
      "trainer/Q Targets Std                                    1.95442\n",
      "trainer/Q Targets Max                                   -0.111562\n",
      "trainer/Q Targets Min                                  -11.063\n",
      "trainer/Log Pis Mean                                     1.88253\n",
      "trainer/Log Pis Std                                      1.37722\n",
      "trainer/Log Pis Max                                      7.06818\n",
      "trainer/Log Pis Min                                     -5.68848\n",
      "trainer/Policy mu Mean                                  -0.0142511\n",
      "trainer/Policy mu Std                                    0.634605\n",
      "trainer/Policy mu Max                                    2.53393\n",
      "trainer/Policy mu Min                                   -2.73537\n",
      "trainer/Policy log std Mean                             -2.06168\n",
      "trainer/Policy log std Std                               0.583922\n",
      "trainer/Policy log std Max                              -0.228722\n",
      "trainer/Policy log std Min                              -3.07149\n",
      "trainer/Alpha                                            0.015224\n",
      "trainer/Alpha Loss                                      -0.491583\n",
      "exploration/num steps total                           2000\n",
      "exploration/num paths total                            100\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.117089\n",
      "exploration/Rewards Std                                  0.0956642\n",
      "exploration/Rewards Max                                  0.0709816\n",
      "exploration/Rewards Min                                 -0.41412\n",
      "exploration/Returns Mean                                -2.34178\n",
      "exploration/Returns Std                                  0.93993\n",
      "exploration/Returns Max                                 -1.16824\n",
      "exploration/Returns Min                                 -3.80301\n",
      "exploration/Actions Mean                                -0.000832796\n",
      "exploration/Actions Std                                  0.16664\n",
      "exploration/Actions Max                                  0.473452\n",
      "exploration/Actions Min                                 -0.678519\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.34178\n",
      "exploration/env_infos/final/reward_dist Mean             0.0788667\n",
      "exploration/env_infos/final/reward_dist Std              0.157733\n",
      "exploration/env_infos/final/reward_dist Max              0.394332\n",
      "exploration/env_infos/final/reward_dist Min              3.88658e-19\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0134992\n",
      "exploration/env_infos/initial/reward_dist Std            0.0267896\n",
      "exploration/env_infos/initial/reward_dist Max            0.0670783\n",
      "exploration/env_infos/initial/reward_dist Min            2.26571e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.132425\n",
      "exploration/env_infos/reward_dist Std                    0.205354\n",
      "exploration/env_infos/reward_dist Max                    0.8765\n",
      "exploration/env_infos/reward_dist Min                    9.70504e-22\n",
      "exploration/env_infos/final/reward_energy Mean          -0.299579\n",
      "exploration/env_infos/final/reward_energy Std            0.21723\n",
      "exploration/env_infos/final/reward_energy Max           -0.0449317\n",
      "exploration/env_infos/final/reward_energy Min           -0.682126\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.41637\n",
      "exploration/env_infos/initial/reward_energy Std          0.21694\n",
      "exploration/env_infos/initial/reward_energy Max         -0.141798\n",
      "exploration/env_infos/initial/reward_energy Min         -0.799736\n",
      "exploration/env_infos/reward_energy Mean                -0.196532\n",
      "exploration/env_infos/reward_energy Std                  0.130056\n",
      "exploration/env_infos/reward_energy Max                 -0.0124541\n",
      "exploration/env_infos/reward_energy Min                 -0.799736\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0561878\n",
      "exploration/env_infos/final/end_effector_loc Std         0.39595\n",
      "exploration/env_infos/final/end_effector_loc Max         0.499235\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.617098\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00108787\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0165635\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0228931\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0327848\n",
      "exploration/env_infos/end_effector_loc Mean              0.0335449\n",
      "exploration/env_infos/end_effector_loc Std               0.25367\n",
      "exploration/env_infos/end_effector_loc Max               0.499235\n",
      "exploration/env_infos/end_effector_loc Min              -0.617098\n",
      "evaluation/num steps total                           10000\n",
      "evaluation/num paths total                             500\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.109354\n",
      "evaluation/Rewards Std                                   0.080017\n",
      "evaluation/Rewards Max                                   0.0688032\n",
      "evaluation/Rewards Min                                  -0.717519\n",
      "evaluation/Returns Mean                                 -2.18708\n",
      "evaluation/Returns Std                                   0.944662\n",
      "evaluation/Returns Max                                  -0.131174\n",
      "evaluation/Returns Min                                  -5.55097\n",
      "evaluation/Actions Mean                                 -0.0136973\n",
      "evaluation/Actions Std                                   0.0880017\n",
      "evaluation/Actions Max                                   0.735795\n",
      "evaluation/Actions Min                                  -0.797453\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -2.18708\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0363625\n",
      "evaluation/env_infos/final/reward_dist Std               0.118915\n",
      "evaluation/env_infos/final/reward_dist Max               0.610875\n",
      "evaluation/env_infos/final/reward_dist Min               4.08377e-65\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00587902\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0188641\n",
      "evaluation/env_infos/initial/reward_dist Max             0.131242\n",
      "evaluation/env_infos/initial/reward_dist Min             1.76679e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.0692563\n",
      "evaluation/env_infos/reward_dist Std                     0.171646\n",
      "evaluation/env_infos/reward_dist Max                     0.9802\n",
      "evaluation/env_infos/reward_dist Min                     4.08377e-65\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.139345\n",
      "evaluation/env_infos/final/reward_energy Std             0.122126\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0109747\n",
      "evaluation/env_infos/final/reward_energy Min            -0.575587\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.212727\n",
      "evaluation/env_infos/initial/reward_energy Std           0.185945\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.030087\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.04139\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0912323\n",
      "evaluation/env_infos/reward_energy Std                   0.086836\n",
      "evaluation/env_infos/reward_energy Max                  -0.00263502\n",
      "evaluation/env_infos/reward_energy Min                  -1.04139\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0561189\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.333355\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.740545\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00146743\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00988088\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0367897\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0398726\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0039207\n",
      "evaluation/env_infos/end_effector_loc Std                0.190851\n",
      "evaluation/env_infos/end_effector_loc Max                0.740545\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.00320174\n",
      "time/evaluation sampling (s)                             1.00141\n",
      "time/exploration sampling (s)                            0.131821\n",
      "time/logging (s)                                         0.0218864\n",
      "time/saving (s)                                          0.0296345\n",
      "time/training (s)                                       44.4323\n",
      "time/epoch (s)                                          45.6203\n",
      "time/total (s)                                         536.315\n",
      "Epoch                                                    9\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:23:00.578022 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 10 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00612566\r\n",
      "trainer/QF2 Loss                                         0.00450927\r\n",
      "trainer/Policy Loss                                      4.73966\r\n",
      "trainer/Q1 Predictions Mean                             -2.95715\r\n",
      "trainer/Q1 Predictions Std                               1.90926\r\n",
      "trainer/Q1 Predictions Max                              -0.161132\r\n",
      "trainer/Q1 Predictions Min                              -9.46225\r\n",
      "trainer/Q2 Predictions Mean                             -2.95376\r\n",
      "trainer/Q2 Predictions Std                               1.9247\r\n",
      "trainer/Q2 Predictions Max                              -0.174487\r\n",
      "trainer/Q2 Predictions Min                              -9.79856\r\n",
      "trainer/Q Targets Mean                                  -2.94762\r\n",
      "trainer/Q Targets Std                                    1.92437\r\n",
      "trainer/Q Targets Max                                   -0.153345\r\n",
      "trainer/Q Targets Min                                   -9.82373\r\n",
      "trainer/Log Pis Mean                                     2.04306\r\n",
      "trainer/Log Pis Std                                      1.46884\r\n",
      "trainer/Log Pis Max                                      9.24999\r\n",
      "trainer/Log Pis Min                                     -2.4602\r\n",
      "trainer/Policy mu Mean                                  -0.0136412\r\n",
      "trainer/Policy mu Std                                    0.706992\r\n",
      "trainer/Policy mu Max                                    2.57877\r\n",
      "trainer/Policy mu Min                                   -3.37981\r\n",
      "trainer/Policy log std Mean                             -2.08289\r\n",
      "trainer/Policy log std Std                               0.592796\r\n",
      "trainer/Policy log std Max                              -0.181075\r\n",
      "trainer/Policy log std Min                              -3.24415\r\n",
      "trainer/Alpha                                            0.0158679\r\n",
      "trainer/Alpha Loss                                       0.178438\r\n",
      "exploration/num steps total                           2100\r\n",
      "exploration/num paths total                            105\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.211701\r\n",
      "exploration/Rewards Std                                  0.0934085\r\n",
      "exploration/Rewards Max                                 -0.0372659\r\n",
      "exploration/Rewards Min                                 -0.556664\r\n",
      "exploration/Returns Mean                                -4.23401\r\n",
      "exploration/Returns Std                                  1.21427\r\n",
      "exploration/Returns Max                                 -2.7763\r\n",
      "exploration/Returns Min                                 -6.18214\r\n",
      "exploration/Actions Mean                                 0.00843026\r\n",
      "exploration/Actions Std                                  0.175229\r\n",
      "exploration/Actions Max                                  0.747791\r\n",
      "exploration/Actions Min                                 -0.840261\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -4.23401\r\n",
      "exploration/env_infos/final/reward_dist Mean             5.10512e-08\r\n",
      "exploration/env_infos/final/reward_dist Std              1.0208e-07\r\n",
      "exploration/env_infos/final/reward_dist Max              2.55211e-07\r\n",
      "exploration/env_infos/final/reward_dist Min              5.44369e-44\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000241264\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.000357503\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.000950018\r\n",
      "exploration/env_infos/initial/reward_dist Min            7.80915e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.0147121\r\n",
      "exploration/env_infos/reward_dist Std                    0.0792127\r\n",
      "exploration/env_infos/reward_dist Max                    0.642368\r\n",
      "exploration/env_infos/reward_dist Min                    2.06964e-45\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.233899\r\n",
      "exploration/env_infos/final/reward_energy Std            0.046255\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.189089\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.29248\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.180868\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.0902192\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.04204\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.322384\r\n",
      "exploration/env_infos/reward_energy Mean                -0.193262\r\n",
      "exploration/env_infos/reward_energy Std                  0.15557\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0153834\r\n",
      "exploration/env_infos/reward_energy Min                 -0.944226\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.00325038\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.420076\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.494579\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.991038\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000785262\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00710275\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00849682\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0159193\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.00370282\r\n",
      "exploration/env_infos/end_effector_loc Std               0.287445\r\n",
      "exploration/env_infos/end_effector_loc Max               0.503013\r\n",
      "exploration/env_infos/end_effector_loc Min              -1\r\n",
      "evaluation/num steps total                           11000\r\n",
      "evaluation/num paths total                             550\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.120884\r\n",
      "evaluation/Rewards Std                                   0.0901638\r\n",
      "evaluation/Rewards Max                                   0.108132\r\n",
      "evaluation/Rewards Min                                  -0.718647\r\n",
      "evaluation/Returns Mean                                 -2.41767\r\n",
      "evaluation/Returns Std                                   1.2018\r\n",
      "evaluation/Returns Max                                   0.582752\r\n",
      "evaluation/Returns Min                                  -5.47631\r\n",
      "evaluation/Actions Mean                                 -0.00884906\r\n",
      "evaluation/Actions Std                                   0.0849369\r\n",
      "evaluation/Actions Max                                   0.60225\r\n",
      "evaluation/Actions Min                                  -0.58994\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -2.41767\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0513876\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.162926\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.988065\r\n",
      "evaluation/env_infos/final/reward_dist Min               6.14719e-68\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00744284\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0156932\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0817627\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.13791e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.0557876\r\n",
      "evaluation/env_infos/reward_dist Std                     0.159761\r\n",
      "evaluation/env_infos/reward_dist Max                     0.988065\r\n",
      "evaluation/env_infos/reward_dist Min                     6.14719e-68\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.119466\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0836574\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0171798\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.433978\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.233227\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.165023\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0168539\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.768105\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0881556\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0825455\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000673391\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.768105\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0366146\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.332479\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.641184\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000604364\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0100831\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0301125\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.029497\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00377817\n",
      "evaluation/env_infos/end_effector_loc Std                0.206309\n",
      "evaluation/env_infos/end_effector_loc Max                0.641184\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.00316474\n",
      "time/evaluation sampling (s)                             1.10534\n",
      "time/exploration sampling (s)                            0.131173\n",
      "time/logging (s)                                         0.0194244\n",
      "time/saving (s)                                          0.0290736\n",
      "time/training (s)                                       45.1226\n",
      "time/epoch (s)                                          46.4108\n",
      "time/total (s)                                         582.887\n",
      "Epoch                                                   10\n",
      "---------------------------------------------------  ---------------\n",
      "2021-05-25 19:23:46.652384 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 11 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00576653\n",
      "trainer/QF2 Loss                                         0.00529006\n",
      "trainer/Policy Loss                                      4.54779\n",
      "trainer/Q1 Predictions Mean                             -2.77117\n",
      "trainer/Q1 Predictions Std                               1.86587\n",
      "trainer/Q1 Predictions Max                              -0.130918\n",
      "trainer/Q1 Predictions Min                             -11.2886\n",
      "trainer/Q2 Predictions Mean                             -2.74842\n",
      "trainer/Q2 Predictions Std                               1.85531\n",
      "trainer/Q2 Predictions Max                              -0.108681\n",
      "trainer/Q2 Predictions Min                             -11.2334\n",
      "trainer/Q Targets Mean                                  -2.73754\n",
      "trainer/Q Targets Std                                    1.84606\n",
      "trainer/Q Targets Max                                   -0.0946199\n",
      "trainer/Q Targets Min                                  -11.3446\n",
      "trainer/Log Pis Mean                                     2.07449\n",
      "trainer/Log Pis Std                                      1.58767\n",
      "trainer/Log Pis Max                                      9.11468\n",
      "trainer/Log Pis Min                                     -2.71209\n",
      "trainer/Policy mu Mean                                  -0.0443209\n",
      "trainer/Policy mu Std                                    0.793992\n",
      "trainer/Policy mu Max                                    3.44733\n",
      "trainer/Policy mu Min                                   -3.32761\n",
      "trainer/Policy log std Mean                             -2.00017\n",
      "trainer/Policy log std Std                               0.599633\n",
      "trainer/Policy log std Max                              -0.0475062\n",
      "trainer/Policy log std Min                              -2.97489\n",
      "trainer/Alpha                                            0.016235\n",
      "trainer/Alpha Loss                                       0.306914\n",
      "exploration/num steps total                           2200\n",
      "exploration/num paths total                            110\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.183434\n",
      "exploration/Rewards Std                                  0.109815\n",
      "exploration/Rewards Max                                  0.0568666\n",
      "exploration/Rewards Min                                 -0.583449\n",
      "exploration/Returns Mean                                -3.66867\n",
      "exploration/Returns Std                                  0.671211\n",
      "exploration/Returns Max                                 -2.63071\n",
      "exploration/Returns Min                                 -4.27238\n",
      "exploration/Actions Mean                                -0.00890265\n",
      "exploration/Actions Std                                  0.113645\n",
      "exploration/Actions Max                                  0.288116\n",
      "exploration/Actions Min                                 -0.30274\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -3.66867\n",
      "exploration/env_infos/final/reward_dist Mean             0.033112\n",
      "exploration/env_infos/final/reward_dist Std              0.0546735\n",
      "exploration/env_infos/final/reward_dist Max              0.140758\n",
      "exploration/env_infos/final/reward_dist Min              1.42662e-40\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00919615\n",
      "exploration/env_infos/initial/reward_dist Std            0.0110347\n",
      "exploration/env_infos/initial/reward_dist Max            0.027808\n",
      "exploration/env_infos/initial/reward_dist Min            6.24952e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0686175\n",
      "exploration/env_infos/reward_dist Std                    0.114971\n",
      "exploration/env_infos/reward_dist Max                    0.435792\n",
      "exploration/env_infos/reward_dist Min                    1.42662e-40\n",
      "exploration/env_infos/final/reward_energy Mean          -0.178713\n",
      "exploration/env_infos/final/reward_energy Std            0.0543983\n",
      "exploration/env_infos/final/reward_energy Max           -0.0905265\n",
      "exploration/env_infos/final/reward_energy Min           -0.243733\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.215577\n",
      "exploration/env_infos/initial/reward_energy Std          0.0491593\n",
      "exploration/env_infos/initial/reward_energy Max         -0.155156\n",
      "exploration/env_infos/initial/reward_energy Min         -0.30568\n",
      "exploration/env_infos/reward_energy Mean                -0.141997\n",
      "exploration/env_infos/reward_energy Std                  0.0763271\n",
      "exploration/env_infos/reward_energy Max                 -0.0265819\n",
      "exploration/env_infos/reward_energy Min                 -0.360992\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.11665\n",
      "exploration/env_infos/final/end_effector_loc Std         0.385239\n",
      "exploration/env_infos/final/end_effector_loc Max         0.362007\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.842267\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00209792\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00753068\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0104526\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0149354\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0550128\n",
      "exploration/env_infos/end_effector_loc Std               0.206879\n",
      "exploration/env_infos/end_effector_loc Max               0.362007\n",
      "exploration/env_infos/end_effector_loc Min              -0.842267\n",
      "evaluation/num steps total                           12000\n",
      "evaluation/num paths total                             600\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.107939\n",
      "evaluation/Rewards Std                                   0.0729217\n",
      "evaluation/Rewards Max                                   0.0645711\n",
      "evaluation/Rewards Min                                  -0.62767\n",
      "evaluation/Returns Mean                                 -2.15878\n",
      "evaluation/Returns Std                                   0.967839\n",
      "evaluation/Returns Max                                  -0.598675\n",
      "evaluation/Returns Min                                  -5.50918\n",
      "evaluation/Actions Mean                                  0.000196849\n",
      "evaluation/Actions Std                                   0.0938852\n",
      "evaluation/Actions Max                                   0.474783\n",
      "evaluation/Actions Min                                  -0.975392\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -2.15878\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0126502\n",
      "evaluation/env_infos/final/reward_dist Std               0.0580807\n",
      "evaluation/env_infos/final/reward_dist Max               0.401314\n",
      "evaluation/env_infos/final/reward_dist Min               9.40453e-47\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00976908\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0329858\n",
      "evaluation/env_infos/initial/reward_dist Max             0.22407\n",
      "evaluation/env_infos/initial/reward_dist Min             2.34212e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.0557189\n",
      "evaluation/env_infos/reward_dist Std                     0.142974\n",
      "evaluation/env_infos/reward_dist Max                     0.972434\n",
      "evaluation/env_infos/reward_dist Min                     9.40453e-47\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.118539\n",
      "evaluation/env_infos/final/reward_energy Std             0.0847105\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0212853\n",
      "evaluation/env_infos/final/reward_energy Min            -0.412104\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.248573\n",
      "evaluation/env_infos/initial/reward_energy Std           0.255942\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0150159\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.28634\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0874845\n",
      "evaluation/env_infos/reward_energy Std                   0.099877\n",
      "evaluation/env_infos/reward_energy Max                  -0.00165591\n",
      "evaluation/env_infos/reward_energy Min                  -1.28634\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0446976\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.27984\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.662738\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.964706\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00106255\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0125694\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0237391\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0487696\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0179977\n",
      "evaluation/env_infos/end_effector_loc Std                0.175489\n",
      "evaluation/env_infos/end_effector_loc Max                0.662738\n",
      "evaluation/env_infos/end_effector_loc Min               -0.964706\n",
      "time/data storing (s)                                    0.0030066\n",
      "time/evaluation sampling (s)                             1.07158\n",
      "time/exploration sampling (s)                            0.129123\n",
      "time/logging (s)                                         0.0266578\n",
      "time/saving (s)                                          0.0311853\n",
      "time/training (s)                                       44.6426\n",
      "time/epoch (s)                                          45.9042\n",
      "time/total (s)                                         628.968\n",
      "Epoch                                                   11\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:24:32.025976 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 12 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00317505\n",
      "trainer/QF2 Loss                                         0.00356741\n",
      "trainer/Policy Loss                                      4.224\n",
      "trainer/Q1 Predictions Mean                             -2.44865\n",
      "trainer/Q1 Predictions Std                               1.56015\n",
      "trainer/Q1 Predictions Max                              -0.0263941\n",
      "trainer/Q1 Predictions Min                              -9.22536\n",
      "trainer/Q2 Predictions Mean                             -2.45026\n",
      "trainer/Q2 Predictions Std                               1.55578\n",
      "trainer/Q2 Predictions Max                              -0.0634723\n",
      "trainer/Q2 Predictions Min                              -9.20636\n",
      "trainer/Q Targets Mean                                  -2.46091\n",
      "trainer/Q Targets Std                                    1.56521\n",
      "trainer/Q Targets Max                                   -0.0572312\n",
      "trainer/Q Targets Min                                   -9.26282\n",
      "trainer/Log Pis Mean                                     2.00327\n",
      "trainer/Log Pis Std                                      1.21642\n",
      "trainer/Log Pis Max                                      7.88299\n",
      "trainer/Log Pis Min                                     -2.20284\n",
      "trainer/Policy mu Mean                                  -0.00195495\n",
      "trainer/Policy mu Std                                    0.537883\n",
      "trainer/Policy mu Max                                    2.93236\n",
      "trainer/Policy mu Min                                   -2.59237\n",
      "trainer/Policy log std Mean                             -2.15926\n",
      "trainer/Policy log std Std                               0.529236\n",
      "trainer/Policy log std Max                              -0.392954\n",
      "trainer/Policy log std Min                              -3.06677\n",
      "trainer/Alpha                                            0.0164994\n",
      "trainer/Alpha Loss                                       0.0134078\n",
      "exploration/num steps total                           2300\n",
      "exploration/num paths total                            115\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.131759\n",
      "exploration/Rewards Std                                  0.0715171\n",
      "exploration/Rewards Max                                  0.0031933\n",
      "exploration/Rewards Min                                 -0.4202\n",
      "exploration/Returns Mean                                -2.63519\n",
      "exploration/Returns Std                                  0.544396\n",
      "exploration/Returns Max                                 -1.78086\n",
      "exploration/Returns Min                                 -3.45244\n",
      "exploration/Actions Mean                                -0.00353127\n",
      "exploration/Actions Std                                  0.122522\n",
      "exploration/Actions Max                                  0.581829\n",
      "exploration/Actions Min                                 -0.349097\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.63519\n",
      "exploration/env_infos/final/reward_dist Mean             0.004765\n",
      "exploration/env_infos/final/reward_dist Std              0.00936162\n",
      "exploration/env_infos/final/reward_dist Max              0.0234869\n",
      "exploration/env_infos/final/reward_dist Min              8.48965e-18\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00470963\n",
      "exploration/env_infos/initial/reward_dist Std            0.00768251\n",
      "exploration/env_infos/initial/reward_dist Max            0.0199204\n",
      "exploration/env_infos/initial/reward_dist Min            6.01152e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0420192\n",
      "exploration/env_infos/reward_dist Std                    0.113035\n",
      "exploration/env_infos/reward_dist Max                    0.570829\n",
      "exploration/env_infos/reward_dist Min                    8.48965e-18\n",
      "exploration/env_infos/final/reward_energy Mean          -0.129404\n",
      "exploration/env_infos/final/reward_energy Std            0.0663869\n",
      "exploration/env_infos/final/reward_energy Max           -0.0778513\n",
      "exploration/env_infos/final/reward_energy Min           -0.259631\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.214067\n",
      "exploration/env_infos/initial/reward_energy Std          0.120652\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0838295\n",
      "exploration/env_infos/initial/reward_energy Min         -0.411506\n",
      "exploration/env_infos/reward_energy Mean                -0.143218\n",
      "exploration/env_infos/reward_energy Std                  0.097657\n",
      "exploration/env_infos/reward_energy Max                 -0.00381842\n",
      "exploration/env_infos/reward_energy Min                 -0.589576\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0781184\n",
      "exploration/env_infos/final/end_effector_loc Std         0.267225\n",
      "exploration/env_infos/final/end_effector_loc Max         0.335246\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.405207\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.000212404\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00868514\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.014911\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0141776\n",
      "exploration/env_infos/end_effector_loc Mean             -0.047834\n",
      "exploration/env_infos/end_effector_loc Std               0.179718\n",
      "exploration/env_infos/end_effector_loc Max               0.335246\n",
      "exploration/env_infos/end_effector_loc Min              -0.493902\n",
      "evaluation/num steps total                           13000\n",
      "evaluation/num paths total                             650\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.11331\n",
      "evaluation/Rewards Std                                   0.0805047\n",
      "evaluation/Rewards Max                                   0.089257\n",
      "evaluation/Rewards Min                                  -0.596522\n",
      "evaluation/Returns Mean                                 -2.26621\n",
      "evaluation/Returns Std                                   1.08585\n",
      "evaluation/Returns Max                                   0.051371\n",
      "evaluation/Returns Min                                  -4.6357\n",
      "evaluation/Actions Mean                                 -0.00543072\n",
      "evaluation/Actions Std                                   0.080599\n",
      "evaluation/Actions Max                                   0.58027\n",
      "evaluation/Actions Min                                  -0.531828\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -2.26621\n",
      "evaluation/env_infos/final/reward_dist Mean              0.016623\n",
      "evaluation/env_infos/final/reward_dist Std               0.0590321\n",
      "evaluation/env_infos/final/reward_dist Max               0.394556\n",
      "evaluation/env_infos/final/reward_dist Min               3.77082e-97\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0056598\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00928809\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0354464\n",
      "evaluation/env_infos/initial/reward_dist Min             1.17022e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.054205\n",
      "evaluation/env_infos/reward_dist Std                     0.142844\n",
      "evaluation/env_infos/reward_dist Max                     0.994045\n",
      "evaluation/env_infos/reward_dist Min                     3.77082e-97\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.11998\n",
      "evaluation/env_infos/final/reward_energy Std             0.0784046\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0183616\n",
      "evaluation/env_infos/final/reward_energy Min            -0.349532\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.217207\n",
      "evaluation/env_infos/initial/reward_energy Std           0.132593\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0133179\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.616938\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0876509\n",
      "evaluation/env_infos/reward_energy Std                   0.0732715\n",
      "evaluation/env_infos/reward_energy Max                  -0.00223285\n",
      "evaluation/env_infos/reward_energy Min                  -0.616938\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0213813\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.364162\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.750014\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000851428\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00895683\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0290135\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0265914\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0184845\n",
      "evaluation/env_infos/end_effector_loc Std                0.211441\n",
      "evaluation/env_infos/end_effector_loc Max                0.750014\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.00310191\n",
      "time/evaluation sampling (s)                             1.03827\n",
      "time/exploration sampling (s)                            0.126059\n",
      "time/logging (s)                                         0.0214629\n",
      "time/saving (s)                                          0.0317325\n",
      "time/training (s)                                       43.9452\n",
      "time/epoch (s)                                          45.1658\n",
      "time/total (s)                                         674.336\n",
      "Epoch                                                   12\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:25:18.701206 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 13 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00537421\r\n",
      "trainer/QF2 Loss                                         0.00195803\r\n",
      "trainer/Policy Loss                                      4.03001\r\n",
      "trainer/Q1 Predictions Mean                             -2.32261\r\n",
      "trainer/Q1 Predictions Std                               1.358\r\n",
      "trainer/Q1 Predictions Max                              -0.0723802\r\n",
      "trainer/Q1 Predictions Min                              -8.56189\r\n",
      "trainer/Q2 Predictions Mean                             -2.35368\r\n",
      "trainer/Q2 Predictions Std                               1.36607\r\n",
      "trainer/Q2 Predictions Max                              -0.094653\r\n",
      "trainer/Q2 Predictions Min                              -8.55066\r\n",
      "trainer/Q Targets Mean                                  -2.36893\r\n",
      "trainer/Q Targets Std                                    1.37124\r\n",
      "trainer/Q Targets Max                                   -0.119584\r\n",
      "trainer/Q Targets Min                                   -8.51335\r\n",
      "trainer/Log Pis Mean                                     1.88783\r\n",
      "trainer/Log Pis Std                                      1.27279\r\n",
      "trainer/Log Pis Max                                      5.90759\r\n",
      "trainer/Log Pis Min                                     -4.09385\r\n",
      "trainer/Policy mu Mean                                   0.00487049\r\n",
      "trainer/Policy mu Std                                    0.467628\r\n",
      "trainer/Policy mu Max                                    2.49156\r\n",
      "trainer/Policy mu Min                                   -2.47192\r\n",
      "trainer/Policy log std Mean                             -2.18588\r\n",
      "trainer/Policy log std Std                               0.553875\r\n",
      "trainer/Policy log std Max                              -0.303618\r\n",
      "trainer/Policy log std Min                              -3.17569\r\n",
      "trainer/Alpha                                            0.0167315\r\n",
      "trainer/Alpha Loss                                      -0.458823\r\n",
      "exploration/num steps total                           2400\r\n",
      "exploration/num paths total                            120\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.113472\r\n",
      "exploration/Rewards Std                                  0.132479\r\n",
      "exploration/Rewards Max                                  0.10711\r\n",
      "exploration/Rewards Min                                 -0.519395\r\n",
      "exploration/Returns Mean                                -2.26944\r\n",
      "exploration/Returns Std                                  2.00007\r\n",
      "exploration/Returns Max                                 -0.609371\r\n",
      "exploration/Returns Min                                 -6.13734\r\n",
      "exploration/Actions Mean                                -0.0153735\r\n",
      "exploration/Actions Std                                  0.158891\r\n",
      "exploration/Actions Max                                  0.35151\r\n",
      "exploration/Actions Min                                 -0.490659\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.26944\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0238925\r\n",
      "exploration/env_infos/final/reward_dist Std              0.044568\r\n",
      "exploration/env_infos/final/reward_dist Max              0.112911\r\n",
      "exploration/env_infos/final/reward_dist Min              2.22024e-44\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00283851\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00434978\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0113688\r\n",
      "exploration/env_infos/initial/reward_dist Min            7.41113e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.237012\r\n",
      "exploration/env_infos/reward_dist Std                    0.321921\r\n",
      "exploration/env_infos/reward_dist Max                    0.931536\r\n",
      "exploration/env_infos/reward_dist Min                    2.22024e-44\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.31262\r\n",
      "exploration/env_infos/final/reward_energy Std            0.167229\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.123241\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.589717\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.34263\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.104192\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.190269\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.488087\r\n",
      "exploration/env_infos/reward_energy Mean                -0.196466\r\n",
      "exploration/env_infos/reward_energy Std                  0.111207\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0182278\r\n",
      "exploration/env_infos/reward_energy Min                 -0.589717\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0682677\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.278402\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.354944\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.60749\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.0023988\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0124322\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0175755\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0227185\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0125012\r\n",
      "exploration/env_infos/end_effector_loc Std               0.197948\r\n",
      "exploration/env_infos/end_effector_loc Max               0.395989\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.60749\r\n",
      "evaluation/num steps total                           14000\r\n",
      "evaluation/num paths total                             700\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.141833\r\n",
      "evaluation/Rewards Std                                   0.139444\r\n",
      "evaluation/Rewards Max                                   0.113222\r\n",
      "evaluation/Rewards Min                                  -0.852967\r\n",
      "evaluation/Returns Mean                                 -2.83665\r\n",
      "evaluation/Returns Std                                   1.7884\r\n",
      "evaluation/Returns Max                                  -0.407308\r\n",
      "evaluation/Returns Min                                  -7.11624\r\n",
      "evaluation/Actions Mean                                 -0.0176875\r\n",
      "evaluation/Actions Std                                   0.112689\r\n",
      "evaluation/Actions Max                                   0.460985\r\n",
      "evaluation/Actions Min                                  -0.965456\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -2.83665\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0410933\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.120438\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.64469\r\n",
      "evaluation/env_infos/final/reward_dist Min               3.17996e-71\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00902278\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0176072\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.106044\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.0219e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.0799971\r\n",
      "evaluation/env_infos/reward_dist Std                     0.17262\r\n",
      "evaluation/env_infos/reward_dist Max                     0.993073\r\n",
      "evaluation/env_infos/reward_dist Min                     3.17996e-71\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.172793\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.176303\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00893808\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.716867\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.291787\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.252017\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0272597\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.23423\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.104271\r\n",
      "evaluation/env_infos/reward_energy Std                   0.123089\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00196708\r\n",
      "evaluation/env_infos/reward_energy Min                  -1.23423\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.113795\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.381246\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.872424\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.921956\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.0018764\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0135016\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0230492\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0482728\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.040054\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.246911\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.872424\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.921956\r\n",
      "time/data storing (s)                                    0.00322372\r\n",
      "time/evaluation sampling (s)                             1.14412\r\n",
      "time/exploration sampling (s)                            0.137385\r\n",
      "time/logging (s)                                         0.0191345\r\n",
      "time/saving (s)                                          0.0305419\r\n",
      "time/training (s)                                       45.129\r\n",
      "time/epoch (s)                                          46.4634\r\n",
      "time/total (s)                                         721.008\r\n",
      "Epoch                                                   13\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:26:03.427536 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 14 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00354493\n",
      "trainer/QF2 Loss                                         0.00852515\n",
      "trainer/Policy Loss                                      4.14236\n",
      "trainer/Q1 Predictions Mean                             -2.31629\n",
      "trainer/Q1 Predictions Std                               1.62204\n",
      "trainer/Q1 Predictions Max                              -0.102295\n",
      "trainer/Q1 Predictions Min                             -11.0259\n",
      "trainer/Q2 Predictions Mean                             -2.28972\n",
      "trainer/Q2 Predictions Std                               1.62082\n",
      "trainer/Q2 Predictions Max                              -0.095188\n",
      "trainer/Q2 Predictions Min                             -10.8642\n",
      "trainer/Q Targets Mean                                  -2.35058\n",
      "trainer/Q Targets Std                                    1.62666\n",
      "trainer/Q Targets Max                                   -0.0946199\n",
      "trainer/Q Targets Min                                  -11.1702\n",
      "trainer/Log Pis Mean                                     2.04076\n",
      "trainer/Log Pis Std                                      1.41477\n",
      "trainer/Log Pis Max                                      9.44729\n",
      "trainer/Log Pis Min                                     -4.11472\n",
      "trainer/Policy mu Mean                                  -0.000519301\n",
      "trainer/Policy mu Std                                    0.591826\n",
      "trainer/Policy mu Max                                    3.58411\n",
      "trainer/Policy mu Min                                   -3.15038\n",
      "trainer/Policy log std Mean                             -2.19905\n",
      "trainer/Policy log std Std                               0.571105\n",
      "trainer/Policy log std Max                               0.267343\n",
      "trainer/Policy log std Min                              -3.24373\n",
      "trainer/Alpha                                            0.0169097\n",
      "trainer/Alpha Loss                                       0.166235\n",
      "exploration/num steps total                           2500\n",
      "exploration/num paths total                            125\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0992958\n",
      "exploration/Rewards Std                                  0.0523542\n",
      "exploration/Rewards Max                                  6.9293e-05\n",
      "exploration/Rewards Min                                 -0.265651\n",
      "exploration/Returns Mean                                -1.98592\n",
      "exploration/Returns Std                                  0.168447\n",
      "exploration/Returns Max                                 -1.75153\n",
      "exploration/Returns Min                                 -2.26618\n",
      "exploration/Actions Mean                                 0.0094125\n",
      "exploration/Actions Std                                  0.110942\n",
      "exploration/Actions Max                                  0.34979\n",
      "exploration/Actions Min                                 -0.30868\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.98592\n",
      "exploration/env_infos/final/reward_dist Mean             0.0149397\n",
      "exploration/env_infos/final/reward_dist Std              0.0293737\n",
      "exploration/env_infos/final/reward_dist Max              0.0736822\n",
      "exploration/env_infos/final/reward_dist Min              1.03509e-08\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0124829\n",
      "exploration/env_infos/initial/reward_dist Std            0.0138044\n",
      "exploration/env_infos/initial/reward_dist Max            0.0353458\n",
      "exploration/env_infos/initial/reward_dist Min            1.89945e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.13013\n",
      "exploration/env_infos/reward_dist Std                    0.200411\n",
      "exploration/env_infos/reward_dist Max                    0.952369\n",
      "exploration/env_infos/reward_dist Min                    1.03509e-08\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0966569\n",
      "exploration/env_infos/final/reward_energy Std            0.0371921\n",
      "exploration/env_infos/final/reward_energy Max           -0.0510761\n",
      "exploration/env_infos/final/reward_energy Min           -0.158774\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.261763\n",
      "exploration/env_infos/initial/reward_energy Std          0.0588845\n",
      "exploration/env_infos/initial/reward_energy Max         -0.207782\n",
      "exploration/env_infos/initial/reward_energy Min         -0.370234\n",
      "exploration/env_infos/reward_energy Mean                -0.137738\n",
      "exploration/env_infos/reward_energy Std                  0.0762989\n",
      "exploration/env_infos/reward_energy Max                 -0.0123593\n",
      "exploration/env_infos/reward_energy Min                 -0.370234\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.131153\n",
      "exploration/env_infos/final/end_effector_loc Std         0.222824\n",
      "exploration/env_infos/final/end_effector_loc Max         0.421921\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.253324\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00254492\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00913823\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0174895\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0117221\n",
      "exploration/env_infos/end_effector_loc Mean              0.0779194\n",
      "exploration/env_infos/end_effector_loc Std               0.159239\n",
      "exploration/env_infos/end_effector_loc Max               0.421921\n",
      "exploration/env_infos/end_effector_loc Min              -0.272167\n",
      "evaluation/num steps total                           15000\n",
      "evaluation/num paths total                             750\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0651951\n",
      "evaluation/Rewards Std                                   0.0671266\n",
      "evaluation/Rewards Max                                   0.120919\n",
      "evaluation/Rewards Min                                  -0.457957\n",
      "evaluation/Returns Mean                                 -1.3039\n",
      "evaluation/Returns Std                                   0.828431\n",
      "evaluation/Returns Max                                   0.492935\n",
      "evaluation/Returns Min                                  -3.027\n",
      "evaluation/Actions Mean                                  0.00805109\n",
      "evaluation/Actions Std                                   0.069193\n",
      "evaluation/Actions Max                                   0.447872\n",
      "evaluation/Actions Min                                  -0.888165\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.3039\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0835301\n",
      "evaluation/env_infos/final/reward_dist Std               0.201805\n",
      "evaluation/env_infos/final/reward_dist Max               0.894993\n",
      "evaluation/env_infos/final/reward_dist Min               2.30216e-28\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00669398\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0148527\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0835262\n",
      "evaluation/env_infos/initial/reward_dist Min             2.16964e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.146945\n",
      "evaluation/env_infos/reward_dist Std                     0.236651\n",
      "evaluation/env_infos/reward_dist Max                     0.997651\n",
      "evaluation/env_infos/reward_dist Min                     2.30216e-28\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.105141\n",
      "evaluation/env_infos/final/reward_energy Std             0.0485803\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0234537\n",
      "evaluation/env_infos/final/reward_energy Min            -0.251173\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.19599\n",
      "evaluation/env_infos/initial/reward_energy Std           0.149576\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0491944\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.05276\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0745109\n",
      "evaluation/env_infos/reward_energy Std                   0.0644446\n",
      "evaluation/env_infos/reward_energy Max                  -0.00219685\n",
      "evaluation/env_infos/reward_energy Min                  -1.05276\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0751944\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.298578\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.744223\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.800687\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000479177\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00870354\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0197703\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0444083\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0306339\n",
      "evaluation/env_infos/end_effector_loc Std                0.169931\n",
      "evaluation/env_infos/end_effector_loc Max                0.744223\n",
      "evaluation/env_infos/end_effector_loc Min               -0.800687\n",
      "time/data storing (s)                                    0.00290657\n",
      "time/evaluation sampling (s)                             0.969819\n",
      "time/exploration sampling (s)                            0.119306\n",
      "time/logging (s)                                         0.0202535\n",
      "time/saving (s)                                          0.0301116\n",
      "time/training (s)                                       43.3585\n",
      "time/epoch (s)                                          44.5009\n",
      "time/total (s)                                         765.735\n",
      "Epoch                                                   14\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:26:48.016444 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 15 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00947299\n",
      "trainer/QF2 Loss                                         0.00308359\n",
      "trainer/Policy Loss                                      3.80736\n",
      "trainer/Q1 Predictions Mean                             -1.98889\n",
      "trainer/Q1 Predictions Std                               1.50386\n",
      "trainer/Q1 Predictions Max                               0.040041\n",
      "trainer/Q1 Predictions Min                              -8.6679\n",
      "trainer/Q2 Predictions Mean                             -2.00543\n",
      "trainer/Q2 Predictions Std                               1.50119\n",
      "trainer/Q2 Predictions Max                               0.00657093\n",
      "trainer/Q2 Predictions Min                              -8.69265\n",
      "trainer/Q Targets Mean                                  -2.02206\n",
      "trainer/Q Targets Std                                    1.51506\n",
      "trainer/Q Targets Max                                   -0.00576274\n",
      "trainer/Q Targets Min                                   -8.81367\n",
      "trainer/Log Pis Mean                                     1.94306\n",
      "trainer/Log Pis Std                                      1.46155\n",
      "trainer/Log Pis Max                                      7.6618\n",
      "trainer/Log Pis Min                                     -4.12302\n",
      "trainer/Policy mu Mean                                  -0.0299617\n",
      "trainer/Policy mu Std                                    0.573395\n",
      "trainer/Policy mu Max                                    2.85293\n",
      "trainer/Policy mu Min                                   -3.0388\n",
      "trainer/Policy log std Mean                             -2.18362\n",
      "trainer/Policy log std Std                               0.603666\n",
      "trainer/Policy log std Max                              -0.196057\n",
      "trainer/Policy log std Min                              -3.25162\n",
      "trainer/Alpha                                            0.0178759\n",
      "trainer/Alpha Loss                                      -0.229084\n",
      "exploration/num steps total                           2600\n",
      "exploration/num paths total                            130\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.145228\n",
      "exploration/Rewards Std                                  0.0450999\n",
      "exploration/Rewards Max                                 -0.0601713\n",
      "exploration/Rewards Min                                 -0.285053\n",
      "exploration/Returns Mean                                -2.90456\n",
      "exploration/Returns Std                                  0.219997\n",
      "exploration/Returns Max                                 -2.55286\n",
      "exploration/Returns Min                                 -3.16032\n",
      "exploration/Actions Mean                                 0.0106482\n",
      "exploration/Actions Std                                  0.0956965\n",
      "exploration/Actions Max                                  0.232541\n",
      "exploration/Actions Min                                 -0.26368\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.90456\n",
      "exploration/env_infos/final/reward_dist Mean             0.015703\n",
      "exploration/env_infos/final/reward_dist Std              0.0251714\n",
      "exploration/env_infos/final/reward_dist Max              0.0649381\n",
      "exploration/env_infos/final/reward_dist Min              1.11619e-21\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00101802\n",
      "exploration/env_infos/initial/reward_dist Std            0.00145204\n",
      "exploration/env_infos/initial/reward_dist Max            0.00383731\n",
      "exploration/env_infos/initial/reward_dist Min            5.74231e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.065356\n",
      "exploration/env_infos/reward_dist Std                    0.142251\n",
      "exploration/env_infos/reward_dist Max                    0.758304\n",
      "exploration/env_infos/reward_dist Min                    1.11619e-21\n",
      "exploration/env_infos/final/reward_energy Mean          -0.169584\n",
      "exploration/env_infos/final/reward_energy Std            0.0858512\n",
      "exploration/env_infos/final/reward_energy Max           -0.0441417\n",
      "exploration/env_infos/final/reward_energy Min           -0.286725\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.155508\n",
      "exploration/env_infos/initial/reward_energy Std          0.0762767\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0747444\n",
      "exploration/env_infos/initial/reward_energy Min         -0.279184\n",
      "exploration/env_infos/reward_energy Mean                -0.121667\n",
      "exploration/env_infos/reward_energy Std                  0.0611518\n",
      "exploration/env_infos/reward_energy Max                 -0.0200404\n",
      "exploration/env_infos/reward_energy Min                 -0.286725\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.214847\n",
      "exploration/env_infos/final/end_effector_loc Std         0.234917\n",
      "exploration/env_infos/final/end_effector_loc Max         0.538943\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.12317\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.000820742\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00606855\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0100372\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00959922\n",
      "exploration/env_infos/end_effector_loc Mean              0.0977765\n",
      "exploration/env_infos/end_effector_loc Std               0.165075\n",
      "exploration/env_infos/end_effector_loc Max               0.538943\n",
      "exploration/env_infos/end_effector_loc Min              -0.163314\n",
      "evaluation/num steps total                           16000\n",
      "evaluation/num paths total                             800\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0744945\n",
      "evaluation/Rewards Std                                   0.0822976\n",
      "evaluation/Rewards Max                                   0.183006\n",
      "evaluation/Rewards Min                                  -0.81635\n",
      "evaluation/Returns Mean                                 -1.48989\n",
      "evaluation/Returns Std                                   1.22768\n",
      "evaluation/Returns Max                                   1.21746\n",
      "evaluation/Returns Min                                  -4.36623\n",
      "evaluation/Actions Mean                                 -0.00288425\n",
      "evaluation/Actions Std                                   0.0896244\n",
      "evaluation/Actions Max                                   0.681598\n",
      "evaluation/Actions Min                                  -0.963146\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.48989\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0982139\n",
      "evaluation/env_infos/final/reward_dist Std               0.179896\n",
      "evaluation/env_infos/final/reward_dist Max               0.620801\n",
      "evaluation/env_infos/final/reward_dist Min               3.11665e-90\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00543677\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0111258\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0505848\n",
      "evaluation/env_infos/initial/reward_dist Min             2.27344e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.133422\n",
      "evaluation/env_infos/reward_dist Std                     0.233219\n",
      "evaluation/env_infos/reward_dist Max                     0.996337\n",
      "evaluation/env_infos/reward_dist Min                     3.11665e-90\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0856777\n",
      "evaluation/env_infos/final/reward_energy Std             0.076248\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00787643\n",
      "evaluation/env_infos/final/reward_energy Min            -0.337661\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.268818\n",
      "evaluation/env_infos/initial/reward_energy Std           0.239762\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0497387\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.13845\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0826603\n",
      "evaluation/env_infos/reward_energy Std                   0.0961717\n",
      "evaluation/env_infos/reward_energy Max                  -0.00343799\n",
      "evaluation/env_infos/reward_energy Min                  -1.13845\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0137987\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.301057\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.605947\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000130705\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0127346\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0340799\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0481573\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00591288\n",
      "evaluation/env_infos/end_effector_loc Std                0.188143\n",
      "evaluation/env_infos/end_effector_loc Max                0.605947\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.00298056\n",
      "time/evaluation sampling (s)                             0.948358\n",
      "time/exploration sampling (s)                            0.119465\n",
      "time/logging (s)                                         0.019659\n",
      "time/saving (s)                                          0.0299059\n",
      "time/training (s)                                       43.2605\n",
      "time/epoch (s)                                          44.3808\n",
      "time/total (s)                                         810.323\n",
      "Epoch                                                   15\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:27:33.765339 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 16 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00364263\n",
      "trainer/QF2 Loss                                         0.00332528\n",
      "trainer/Policy Loss                                      3.88451\n",
      "trainer/Q1 Predictions Mean                             -1.92774\n",
      "trainer/Q1 Predictions Std                               1.35645\n",
      "trainer/Q1 Predictions Max                              -0.0201662\n",
      "trainer/Q1 Predictions Min                              -8.8972\n",
      "trainer/Q2 Predictions Mean                             -1.94057\n",
      "trainer/Q2 Predictions Std                               1.36417\n",
      "trainer/Q2 Predictions Max                              -0.0133821\n",
      "trainer/Q2 Predictions Min                              -9.0084\n",
      "trainer/Q Targets Mean                                  -1.95292\n",
      "trainer/Q Targets Std                                    1.3628\n",
      "trainer/Q Targets Max                                   -0.0231312\n",
      "trainer/Q Targets Min                                   -8.98498\n",
      "trainer/Log Pis Mean                                     2.06366\n",
      "trainer/Log Pis Std                                      1.39188\n",
      "trainer/Log Pis Max                                      7.54606\n",
      "trainer/Log Pis Min                                     -3.20364\n",
      "trainer/Policy mu Mean                                  -0.0310648\n",
      "trainer/Policy mu Std                                    0.532819\n",
      "trainer/Policy mu Max                                    2.96365\n",
      "trainer/Policy mu Min                                   -3.29314\n",
      "trainer/Policy log std Mean                             -2.26724\n",
      "trainer/Policy log std Std                               0.592817\n",
      "trainer/Policy log std Max                              -0.101008\n",
      "trainer/Policy log std Min                              -3.32593\n",
      "trainer/Alpha                                            0.0182089\n",
      "trainer/Alpha Loss                                       0.254999\n",
      "exploration/num steps total                           2700\n",
      "exploration/num paths total                            135\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.155666\n",
      "exploration/Rewards Std                                  0.0732162\n",
      "exploration/Rewards Max                                 -0.0239469\n",
      "exploration/Rewards Min                                 -0.354925\n",
      "exploration/Returns Mean                                -3.11332\n",
      "exploration/Returns Std                                  0.878222\n",
      "exploration/Returns Max                                 -1.71933\n",
      "exploration/Returns Min                                 -4.29106\n",
      "exploration/Actions Mean                                 0.00247802\n",
      "exploration/Actions Std                                  0.11933\n",
      "exploration/Actions Max                                  0.575437\n",
      "exploration/Actions Min                                 -0.495197\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -3.11332\n",
      "exploration/env_infos/final/reward_dist Mean             0.170989\n",
      "exploration/env_infos/final/reward_dist Std              0.336779\n",
      "exploration/env_infos/final/reward_dist Max              0.844499\n",
      "exploration/env_infos/final/reward_dist Min              6.75078e-17\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00745628\n",
      "exploration/env_infos/initial/reward_dist Std            0.0123423\n",
      "exploration/env_infos/initial/reward_dist Max            0.0319307\n",
      "exploration/env_infos/initial/reward_dist Min            0.000181903\n",
      "exploration/env_infos/reward_dist Mean                   0.11577\n",
      "exploration/env_infos/reward_dist Std                    0.262172\n",
      "exploration/env_infos/reward_dist Max                    0.998944\n",
      "exploration/env_infos/reward_dist Min                    6.75078e-17\n",
      "exploration/env_infos/final/reward_energy Mean          -0.10089\n",
      "exploration/env_infos/final/reward_energy Std            0.0689294\n",
      "exploration/env_infos/final/reward_energy Max           -0.0185651\n",
      "exploration/env_infos/final/reward_energy Min           -0.200434\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.265584\n",
      "exploration/env_infos/initial/reward_energy Std          0.226792\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0786856\n",
      "exploration/env_infos/initial/reward_energy Min         -0.712\n",
      "exploration/env_infos/reward_energy Mean                -0.137754\n",
      "exploration/env_infos/reward_energy Std                  0.097547\n",
      "exploration/env_infos/reward_energy Max                 -0.015592\n",
      "exploration/env_infos/reward_energy Min                 -0.712\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.110411\n",
      "exploration/env_infos/final/end_effector_loc Std         0.238506\n",
      "exploration/env_infos/final/end_effector_loc Max         0.480511\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.257732\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00729579\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00996162\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0287718\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00331793\n",
      "exploration/env_infos/end_effector_loc Mean              0.0827868\n",
      "exploration/env_infos/end_effector_loc Std               0.123895\n",
      "exploration/env_infos/end_effector_loc Max               0.480511\n",
      "exploration/env_infos/end_effector_loc Min              -0.257732\n",
      "evaluation/num steps total                           17000\n",
      "evaluation/num paths total                             850\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0904323\n",
      "evaluation/Rewards Std                                   0.10273\n",
      "evaluation/Rewards Max                                   0.151495\n",
      "evaluation/Rewards Min                                  -0.893192\n",
      "evaluation/Returns Mean                                 -1.80865\n",
      "evaluation/Returns Std                                   1.5718\n",
      "evaluation/Returns Max                                   1.25263\n",
      "evaluation/Returns Min                                  -7.44473\n",
      "evaluation/Actions Mean                                  0.0042963\n",
      "evaluation/Actions Std                                   0.0750943\n",
      "evaluation/Actions Max                                   0.590102\n",
      "evaluation/Actions Min                                  -0.825016\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.80865\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0720422\n",
      "evaluation/env_infos/final/reward_dist Std               0.229787\n",
      "evaluation/env_infos/final/reward_dist Max               0.978556\n",
      "evaluation/env_infos/final/reward_dist Min               2.24021e-104\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00864416\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0158566\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0767524\n",
      "evaluation/env_infos/initial/reward_dist Min             1.94032e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.118339\n",
      "evaluation/env_infos/reward_dist Std                     0.230631\n",
      "evaluation/env_infos/reward_dist Max                     0.992252\n",
      "evaluation/env_infos/reward_dist Min                     2.24021e-104\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0882389\n",
      "evaluation/env_infos/final/reward_energy Std             0.0664704\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00674174\n",
      "evaluation/env_infos/final/reward_energy Min            -0.36752\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.213732\n",
      "evaluation/env_infos/initial/reward_energy Std           0.18317\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0316562\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.03267\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0761563\n",
      "evaluation/env_infos/reward_energy Std                   0.0742661\n",
      "evaluation/env_infos/reward_energy Max                  -0.000825669\n",
      "evaluation/env_infos/reward_energy Min                  -1.03267\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0811854\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.341554\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.839028\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.001867\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00977523\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0295051\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0412508\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0367432\n",
      "evaluation/env_infos/end_effector_loc Std                0.20107\n",
      "evaluation/env_infos/end_effector_loc Max                0.839028\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.00288353\n",
      "time/evaluation sampling (s)                             0.970387\n",
      "time/exploration sampling (s)                            0.121643\n",
      "time/logging (s)                                         0.0210492\n",
      "time/saving (s)                                          0.0310227\n",
      "time/training (s)                                       44.3645\n",
      "time/epoch (s)                                          45.5114\n",
      "time/total (s)                                         856.073\n",
      "Epoch                                                   16\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:28:19.279290 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 17 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00134535\n",
      "trainer/QF2 Loss                                         0.00185492\n",
      "trainer/Policy Loss                                      3.79255\n",
      "trainer/Q1 Predictions Mean                             -1.92636\n",
      "trainer/Q1 Predictions Std                               1.23702\n",
      "trainer/Q1 Predictions Max                              -0.143558\n",
      "trainer/Q1 Predictions Min                              -6.7644\n",
      "trainer/Q2 Predictions Mean                             -1.92472\n",
      "trainer/Q2 Predictions Std                               1.23606\n",
      "trainer/Q2 Predictions Max                              -0.12905\n",
      "trainer/Q2 Predictions Min                              -6.71593\n",
      "trainer/Q Targets Mean                                  -1.91933\n",
      "trainer/Q Targets Std                                    1.23681\n",
      "trainer/Q Targets Max                                   -0.125169\n",
      "trainer/Q Targets Min                                   -6.75863\n",
      "trainer/Log Pis Mean                                     1.97943\n",
      "trainer/Log Pis Std                                      1.23391\n",
      "trainer/Log Pis Max                                      6.04687\n",
      "trainer/Log Pis Min                                     -2.26705\n",
      "trainer/Policy mu Mean                                  -0.0348583\n",
      "trainer/Policy mu Std                                    0.422634\n",
      "trainer/Policy mu Max                                    2.74933\n",
      "trainer/Policy mu Min                                   -2.5356\n",
      "trainer/Policy log std Mean                             -2.25391\n",
      "trainer/Policy log std Std                               0.517431\n",
      "trainer/Policy log std Max                              -0.132871\n",
      "trainer/Policy log std Min                              -3.09875\n",
      "trainer/Alpha                                            0.0184581\n",
      "trainer/Alpha Loss                                      -0.0821298\n",
      "exploration/num steps total                           2800\n",
      "exploration/num paths total                            140\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.105289\n",
      "exploration/Rewards Std                                  0.0546284\n",
      "exploration/Rewards Max                                 -0.0203557\n",
      "exploration/Rewards Min                                 -0.3092\n",
      "exploration/Returns Mean                                -2.10578\n",
      "exploration/Returns Std                                  0.543837\n",
      "exploration/Returns Max                                 -1.49298\n",
      "exploration/Returns Min                                 -2.94156\n",
      "exploration/Actions Mean                                -0.00807327\n",
      "exploration/Actions Std                                  0.140173\n",
      "exploration/Actions Max                                  0.43857\n",
      "exploration/Actions Min                                 -0.401733\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.10578\n",
      "exploration/env_infos/final/reward_dist Mean             0.0364483\n",
      "exploration/env_infos/final/reward_dist Std              0.0463598\n",
      "exploration/env_infos/final/reward_dist Max              0.119194\n",
      "exploration/env_infos/final/reward_dist Min              9.84639e-22\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0103968\n",
      "exploration/env_infos/initial/reward_dist Std            0.0127951\n",
      "exploration/env_infos/initial/reward_dist Max            0.0280293\n",
      "exploration/env_infos/initial/reward_dist Min            1.82944e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0785548\n",
      "exploration/env_infos/reward_dist Std                    0.17422\n",
      "exploration/env_infos/reward_dist Max                    0.825281\n",
      "exploration/env_infos/reward_dist Min                    9.84639e-22\n",
      "exploration/env_infos/final/reward_energy Mean          -0.283046\n",
      "exploration/env_infos/final/reward_energy Std            0.137616\n",
      "exploration/env_infos/final/reward_energy Max           -0.0864816\n",
      "exploration/env_infos/final/reward_energy Min           -0.464948\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.247942\n",
      "exploration/env_infos/initial/reward_energy Std          0.116682\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0871623\n",
      "exploration/env_infos/initial/reward_energy Min         -0.428858\n",
      "exploration/env_infos/reward_energy Mean                -0.171317\n",
      "exploration/env_infos/reward_energy Std                  0.100389\n",
      "exploration/env_infos/reward_energy Max                 -0.0246617\n",
      "exploration/env_infos/reward_energy Min                 -0.464948\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0579131\n",
      "exploration/env_infos/final/end_effector_loc Std         0.246385\n",
      "exploration/env_infos/final/end_effector_loc Max         0.282337\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.687232\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00223741\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00942636\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0188638\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0133595\n",
      "exploration/env_infos/end_effector_loc Mean             -0.00539084\n",
      "exploration/env_infos/end_effector_loc Std               0.138298\n",
      "exploration/env_infos/end_effector_loc Max               0.298093\n",
      "exploration/env_infos/end_effector_loc Min              -0.687232\n",
      "evaluation/num steps total                           18000\n",
      "evaluation/num paths total                             900\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.090208\n",
      "evaluation/Rewards Std                                   0.0776281\n",
      "evaluation/Rewards Max                                   0.0977076\n",
      "evaluation/Rewards Min                                  -0.589263\n",
      "evaluation/Returns Mean                                 -1.80416\n",
      "evaluation/Returns Std                                   1.14835\n",
      "evaluation/Returns Max                                   0.840507\n",
      "evaluation/Returns Min                                  -4.51516\n",
      "evaluation/Actions Mean                                 -0.00156521\n",
      "evaluation/Actions Std                                   0.0943222\n",
      "evaluation/Actions Max                                   0.818999\n",
      "evaluation/Actions Min                                  -0.914876\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.80416\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0836764\n",
      "evaluation/env_infos/final/reward_dist Std               0.200721\n",
      "evaluation/env_infos/final/reward_dist Max               0.903356\n",
      "evaluation/env_infos/final/reward_dist Min               1.15563e-64\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00705409\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0165747\n",
      "evaluation/env_infos/initial/reward_dist Max             0.104594\n",
      "evaluation/env_infos/initial/reward_dist Min             1.84652e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.0898322\n",
      "evaluation/env_infos/reward_dist Std                     0.183387\n",
      "evaluation/env_infos/reward_dist Max                     0.987575\n",
      "evaluation/env_infos/reward_dist Min                     1.15563e-64\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0884292\n",
      "evaluation/env_infos/final/reward_energy Std             0.0764876\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0108143\n",
      "evaluation/env_infos/final/reward_energy Min            -0.351331\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.268741\n",
      "evaluation/env_infos/initial/reward_energy Std           0.305338\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0132435\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.10874\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0823987\n",
      "evaluation/env_infos/reward_energy Std                   0.104922\n",
      "evaluation/env_infos/reward_energy Max                  -0.00172516\n",
      "evaluation/env_infos/reward_energy Min                  -1.10874\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0180693\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.287308\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.590674\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.91359\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000114944\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0143806\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0409499\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0457438\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0144328\n",
      "evaluation/env_infos/end_effector_loc Std                0.188083\n",
      "evaluation/env_infos/end_effector_loc Max                0.590674\n",
      "evaluation/env_infos/end_effector_loc Min               -0.91359\n",
      "time/data storing (s)                                    0.00299161\n",
      "time/evaluation sampling (s)                             1.06985\n",
      "time/exploration sampling (s)                            0.121494\n",
      "time/logging (s)                                         0.0203425\n",
      "time/saving (s)                                          0.0294728\n",
      "time/training (s)                                       44.0119\n",
      "time/epoch (s)                                          45.2561\n",
      "time/total (s)                                         901.585\n",
      "Epoch                                                   17\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:29:04.598185 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 18 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00234868\r\n",
      "trainer/QF2 Loss                                         0.00141051\r\n",
      "trainer/Policy Loss                                      3.85093\r\n",
      "trainer/Q1 Predictions Mean                             -1.87584\r\n",
      "trainer/Q1 Predictions Std                               1.35617\r\n",
      "trainer/Q1 Predictions Max                              -0.0242751\r\n",
      "trainer/Q1 Predictions Min                              -8.4857\r\n",
      "trainer/Q2 Predictions Mean                             -1.88257\r\n",
      "trainer/Q2 Predictions Std                               1.35924\r\n",
      "trainer/Q2 Predictions Max                              -0.0286584\r\n",
      "trainer/Q2 Predictions Min                              -8.47534\r\n",
      "trainer/Q Targets Mean                                  -1.88788\r\n",
      "trainer/Q Targets Std                                    1.36053\r\n",
      "trainer/Q Targets Max                                   -0.00576274\r\n",
      "trainer/Q Targets Min                                   -8.54988\r\n",
      "trainer/Log Pis Mean                                     2.06299\r\n",
      "trainer/Log Pis Std                                      1.18116\r\n",
      "trainer/Log Pis Max                                      9.20379\r\n",
      "trainer/Log Pis Min                                     -2.44228\r\n",
      "trainer/Policy mu Mean                                  -0.041886\r\n",
      "trainer/Policy mu Std                                    0.365306\r\n",
      "trainer/Policy mu Max                                    3.03972\r\n",
      "trainer/Policy mu Min                                   -2.89623\r\n",
      "trainer/Policy log std Mean                             -2.34255\r\n",
      "trainer/Policy log std Std                               0.473543\r\n",
      "trainer/Policy log std Max                              -0.212367\r\n",
      "trainer/Policy log std Min                              -3.16668\r\n",
      "trainer/Alpha                                            0.0190544\r\n",
      "trainer/Alpha Loss                                       0.249439\r\n",
      "exploration/num steps total                           2900\r\n",
      "exploration/num paths total                            145\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.123431\r\n",
      "exploration/Rewards Std                                  0.0563293\r\n",
      "exploration/Rewards Max                                  0.0334956\r\n",
      "exploration/Rewards Min                                 -0.298208\r\n",
      "exploration/Returns Mean                                -2.46862\r\n",
      "exploration/Returns Std                                  0.755072\r\n",
      "exploration/Returns Max                                 -0.978008\r\n",
      "exploration/Returns Min                                 -2.96509\r\n",
      "exploration/Actions Mean                                -0.0048313\r\n",
      "exploration/Actions Std                                  0.107572\r\n",
      "exploration/Actions Max                                  0.336576\r\n",
      "exploration/Actions Min                                 -0.26348\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.46862\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.00497595\r\n",
      "exploration/env_infos/final/reward_dist Std              0.00981316\r\n",
      "exploration/env_infos/final/reward_dist Max              0.0246017\r\n",
      "exploration/env_infos/final/reward_dist Min              1.31031e-05\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00791899\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0132966\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0344729\r\n",
      "exploration/env_infos/initial/reward_dist Min            1.63266e-05\r\n",
      "exploration/env_infos/reward_dist Mean                   0.0789486\r\n",
      "exploration/env_infos/reward_dist Std                    0.194042\r\n",
      "exploration/env_infos/reward_dist Max                    0.892121\r\n",
      "exploration/env_infos/reward_dist Min                    4.17754e-07\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.128683\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0465194\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0750233\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.18857\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.241831\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.0964385\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.097135\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.369216\r\n",
      "exploration/env_infos/reward_energy Mean                -0.133601\r\n",
      "exploration/env_infos/reward_energy Std                  0.0730825\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00744318\r\n",
      "exploration/env_infos/reward_energy Min                 -0.369216\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0642819\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.193274\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.331154\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.296752\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00299784\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00870294\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0168288\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00991487\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.0537275\r\n",
      "exploration/env_infos/end_effector_loc Std               0.141439\r\n",
      "exploration/env_infos/end_effector_loc Max               0.331154\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.296752\r\n",
      "evaluation/num steps total                           19000\r\n",
      "evaluation/num paths total                             950\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0947729\r\n",
      "evaluation/Rewards Std                                   0.0785104\r\n",
      "evaluation/Rewards Max                                   0.0890514\r\n",
      "evaluation/Rewards Min                                  -0.496971\r\n",
      "evaluation/Returns Mean                                 -1.89546\r\n",
      "evaluation/Returns Std                                   1.04921\r\n",
      "evaluation/Returns Max                                  -0.056208\r\n",
      "evaluation/Returns Min                                  -4.94838\r\n",
      "evaluation/Actions Mean                                 -0.00483825\r\n",
      "evaluation/Actions Std                                   0.0755955\r\n",
      "evaluation/Actions Max                                   0.477069\r\n",
      "evaluation/Actions Min                                  -0.831519\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.89546\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0397\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.122278\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.518291\r\n",
      "evaluation/env_infos/final/reward_dist Min               1.04003e-39\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00687103\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.011469\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0442391\r\n",
      "evaluation/env_infos/initial/reward_dist Min             8.31955e-07\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.078358\r\n",
      "evaluation/env_infos/reward_dist Std                     0.153832\r\n",
      "evaluation/env_infos/reward_dist Max                     0.939358\r\n",
      "evaluation/env_infos/reward_dist Min                     1.04003e-39\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0955743\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0728919\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0140328\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.275318\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.192148\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.197052\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00276471\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.02152\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0741009\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0773643\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00178216\r\n",
      "evaluation/env_infos/reward_energy Min                  -1.02152\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0190025\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.354451\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.752581\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.820331\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00100173\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00967907\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0238535\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0415759\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00587165\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.205025\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.752581\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.820331\r\n",
      "time/data storing (s)                                    0.00302265\r\n",
      "time/evaluation sampling (s)                             0.967623\r\n",
      "time/exploration sampling (s)                            0.125814\r\n",
      "time/logging (s)                                         0.0184923\r\n",
      "time/saving (s)                                          0.0303251\r\n",
      "time/training (s)                                       43.9077\r\n",
      "time/epoch (s)                                          45.053\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time/total (s)                                         946.901\n",
      "Epoch                                                   18\n",
      "---------------------------------------------------  ---------------\n",
      "2021-05-25 19:29:50.139677 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 19 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00251724\n",
      "trainer/QF2 Loss                                         0.00149648\n",
      "trainer/Policy Loss                                      3.6917\n",
      "trainer/Q1 Predictions Mean                             -1.82294\n",
      "trainer/Q1 Predictions Std                               1.23015\n",
      "trainer/Q1 Predictions Max                              -0.00687152\n",
      "trainer/Q1 Predictions Min                              -8.12665\n",
      "trainer/Q2 Predictions Mean                             -1.83408\n",
      "trainer/Q2 Predictions Std                               1.23274\n",
      "trainer/Q2 Predictions Max                              -0.0346256\n",
      "trainer/Q2 Predictions Min                              -8.09983\n",
      "trainer/Q Targets Mean                                  -1.84844\n",
      "trainer/Q Targets Std                                    1.24047\n",
      "trainer/Q Targets Max                                   -0.0657975\n",
      "trainer/Q Targets Min                                   -8.158\n",
      "trainer/Log Pis Mean                                     1.9549\n",
      "trainer/Log Pis Std                                      1.23686\n",
      "trainer/Log Pis Max                                      4.65368\n",
      "trainer/Log Pis Min                                     -4.7722\n",
      "trainer/Policy mu Mean                                  -0.0485406\n",
      "trainer/Policy mu Std                                    0.397637\n",
      "trainer/Policy mu Max                                    2.35706\n",
      "trainer/Policy mu Min                                   -2.70976\n",
      "trainer/Policy log std Mean                             -2.28258\n",
      "trainer/Policy log std Std                               0.519785\n",
      "trainer/Policy log std Max                               0.39747\n",
      "trainer/Policy log std Min                              -3.1943\n",
      "trainer/Alpha                                            0.0191408\n",
      "trainer/Alpha Loss                                      -0.178329\n",
      "exploration/num steps total                           3000\n",
      "exploration/num paths total                            150\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.161702\n",
      "exploration/Rewards Std                                  0.0863981\n",
      "exploration/Rewards Max                                  0.0152004\n",
      "exploration/Rewards Min                                 -0.390631\n",
      "exploration/Returns Mean                                -3.23404\n",
      "exploration/Returns Std                                  1.12267\n",
      "exploration/Returns Max                                 -2.06462\n",
      "exploration/Returns Min                                 -5.19172\n",
      "exploration/Actions Mean                                -0.00669403\n",
      "exploration/Actions Std                                  0.177658\n",
      "exploration/Actions Max                                  0.743394\n",
      "exploration/Actions Min                                 -0.631316\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -3.23404\n",
      "exploration/env_infos/final/reward_dist Mean             0.0313882\n",
      "exploration/env_infos/final/reward_dist Std              0.0378835\n",
      "exploration/env_infos/final/reward_dist Max              0.0838394\n",
      "exploration/env_infos/final/reward_dist Min              1.48617e-11\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00101911\n",
      "exploration/env_infos/initial/reward_dist Std            0.00130908\n",
      "exploration/env_infos/initial/reward_dist Max            0.0032901\n",
      "exploration/env_infos/initial/reward_dist Min            3.75583e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.102914\n",
      "exploration/env_infos/reward_dist Std                    0.219232\n",
      "exploration/env_infos/reward_dist Max                    0.895757\n",
      "exploration/env_infos/reward_dist Min                    1.48617e-11\n",
      "exploration/env_infos/final/reward_energy Mean          -0.256606\n",
      "exploration/env_infos/final/reward_energy Std            0.148254\n",
      "exploration/env_infos/final/reward_energy Max           -0.102855\n",
      "exploration/env_infos/final/reward_energy Min           -0.458756\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.401437\n",
      "exploration/env_infos/initial/reward_energy Std          0.331354\n",
      "exploration/env_infos/initial/reward_energy Max         -0.125458\n",
      "exploration/env_infos/initial/reward_energy Min         -1.03444\n",
      "exploration/env_infos/reward_energy Mean                -0.19052\n",
      "exploration/env_infos/reward_energy Std                  0.164064\n",
      "exploration/env_infos/reward_energy Max                 -0.0191299\n",
      "exploration/env_infos/reward_energy Min                 -1.03444\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0317379\n",
      "exploration/env_infos/final/end_effector_loc Std         0.278544\n",
      "exploration/env_infos/final/end_effector_loc Max         0.307163\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.482666\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.0049213\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0177331\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0371697\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0202071\n",
      "exploration/env_infos/end_effector_loc Mean              0.00809683\n",
      "exploration/env_infos/end_effector_loc Std               0.179612\n",
      "exploration/env_infos/end_effector_loc Max               0.311814\n",
      "exploration/env_infos/end_effector_loc Min              -0.482666\n",
      "evaluation/num steps total                           20000\n",
      "evaluation/num paths total                            1000\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0873154\n",
      "evaluation/Rewards Std                                   0.075299\n",
      "evaluation/Rewards Max                                   0.153521\n",
      "evaluation/Rewards Min                                  -0.351075\n",
      "evaluation/Returns Mean                                 -1.74631\n",
      "evaluation/Returns Std                                   1.12527\n",
      "evaluation/Returns Max                                   1.65978\n",
      "evaluation/Returns Min                                  -4.49939\n",
      "evaluation/Actions Mean                                 -0.00544523\n",
      "evaluation/Actions Std                                   0.0647317\n",
      "evaluation/Actions Max                                   0.615275\n",
      "evaluation/Actions Min                                  -0.573434\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.74631\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0339472\n",
      "evaluation/env_infos/final/reward_dist Std               0.106366\n",
      "evaluation/env_infos/final/reward_dist Max               0.618989\n",
      "evaluation/env_infos/final/reward_dist Min               1.33749e-25\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00379805\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00699191\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0306269\n",
      "evaluation/env_infos/initial/reward_dist Min             2.39838e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.0969367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation/env_infos/reward_dist Std                     0.19606\n",
      "evaluation/env_infos/reward_dist Max                     0.998722\n",
      "evaluation/env_infos/reward_dist Min                     1.33749e-25\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.07739\n",
      "evaluation/env_infos/final/reward_energy Std             0.0547344\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00413626\n",
      "evaluation/env_infos/final/reward_energy Min            -0.233816\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.181832\n",
      "evaluation/env_infos/initial/reward_energy Std           0.146164\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.019499\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.802407\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0664638\n",
      "evaluation/env_infos/reward_energy Std                   0.0634212\n",
      "evaluation/env_infos/reward_energy Max                  -0.000204311\n",
      "evaluation/env_infos/reward_energy Min                  -0.802407\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0184727\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.316183\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.621443\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.666319\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00175271\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00805987\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0307637\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0286717\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0247948\n",
      "evaluation/env_infos/end_effector_loc Std                0.184259\n",
      "evaluation/env_infos/end_effector_loc Max                0.621443\n",
      "evaluation/env_infos/end_effector_loc Min               -0.666319\n",
      "time/data storing (s)                                    0.00293378\n",
      "time/evaluation sampling (s)                             0.981944\n",
      "time/exploration sampling (s)                            0.121394\n",
      "time/logging (s)                                         0.0195417\n",
      "time/saving (s)                                          0.0300718\n",
      "time/training (s)                                       44.1261\n",
      "time/epoch (s)                                          45.282\n",
      "time/total (s)                                         992.443\n",
      "Epoch                                                   19\n",
      "---------------------------------------------------  ---------------\n",
      "2021-05-25 19:30:36.285778 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 20 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00248698\n",
      "trainer/QF2 Loss                                         0.00261965\n",
      "trainer/Policy Loss                                      3.59663\n",
      "trainer/Q1 Predictions Mean                             -1.69227\n",
      "trainer/Q1 Predictions Std                               1.01348\n",
      "trainer/Q1 Predictions Max                              -0.0219329\n",
      "trainer/Q1 Predictions Min                              -5.05338\n",
      "trainer/Q2 Predictions Mean                             -1.66368\n",
      "trainer/Q2 Predictions Std                               1.01852\n",
      "trainer/Q2 Predictions Max                              -0.00909691\n",
      "trainer/Q2 Predictions Min                              -5.00154\n",
      "trainer/Q Targets Mean                                  -1.69052\n",
      "trainer/Q Targets Std                                    1.02921\n",
      "trainer/Q Targets Max                                   -0.0287947\n",
      "trainer/Q Targets Min                                   -5.08097\n",
      "trainer/Log Pis Mean                                     1.95785\n",
      "trainer/Log Pis Std                                      1.32828\n",
      "trainer/Log Pis Max                                      4.61581\n",
      "trainer/Log Pis Min                                     -3.44465\n",
      "trainer/Policy mu Mean                                  -0.0179185\n",
      "trainer/Policy mu Std                                    0.210716\n",
      "trainer/Policy mu Max                                    1.41571\n",
      "trainer/Policy mu Min                                   -1.55029\n",
      "trainer/Policy log std Mean                             -2.36776\n",
      "trainer/Policy log std Std                               0.439399\n",
      "trainer/Policy log std Max                              -0.652599\n",
      "trainer/Policy log std Min                              -3.33175\n",
      "trainer/Alpha                                            0.0201313\n",
      "trainer/Alpha Loss                                      -0.16461\n",
      "exploration/num steps total                           3100\n",
      "exploration/num paths total                            155\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.134735\n",
      "exploration/Rewards Std                                  0.0527813\n",
      "exploration/Rewards Max                                 -0.0101755\n",
      "exploration/Rewards Min                                 -0.259321\n",
      "exploration/Returns Mean                                -2.69469\n",
      "exploration/Returns Std                                  0.6064\n",
      "exploration/Returns Max                                 -1.88303\n",
      "exploration/Returns Min                                 -3.51046\n",
      "exploration/Actions Mean                                -0.00356104\n",
      "exploration/Actions Std                                  0.103819\n",
      "exploration/Actions Max                                  0.332155\n",
      "exploration/Actions Min                                 -0.3876\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.69469\n",
      "exploration/env_infos/final/reward_dist Mean             7.12171e-06\n",
      "exploration/env_infos/final/reward_dist Std              1.38087e-05\n",
      "exploration/env_infos/final/reward_dist Max              3.47335e-05\n",
      "exploration/env_infos/final/reward_dist Min              1.18478e-42\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0010101\n",
      "exploration/env_infos/initial/reward_dist Std            0.00125727\n",
      "exploration/env_infos/initial/reward_dist Max            0.00342967\n",
      "exploration/env_infos/initial/reward_dist Min            4.7014e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.0190489\n",
      "exploration/env_infos/reward_dist Std                    0.0428231\n",
      "exploration/env_infos/reward_dist Max                    0.220602\n",
      "exploration/env_infos/reward_dist Min                    1.18478e-42\n",
      "exploration/env_infos/final/reward_energy Mean          -0.176152\n",
      "exploration/env_infos/final/reward_energy Std            0.0620387\n",
      "exploration/env_infos/final/reward_energy Max           -0.0785922\n",
      "exploration/env_infos/final/reward_energy Min           -0.231599\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.117049\n",
      "exploration/env_infos/initial/reward_energy Std          0.0317396\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0673138\n",
      "exploration/env_infos/initial/reward_energy Min         -0.144941\n",
      "exploration/env_infos/reward_energy Mean                -0.123626\n",
      "exploration/env_infos/reward_energy Std                  0.0793651\n",
      "exploration/env_infos/reward_energy Max                 -0.0132256\n",
      "exploration/env_infos/reward_energy Min                 -0.407985\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.000963422\n",
      "exploration/env_infos/final/end_effector_loc Std         0.371752\n",
      "exploration/env_infos/final/end_effector_loc Max         0.373629\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.760319\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00238737\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00356166\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00641923\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00344389\n",
      "exploration/env_infos/end_effector_loc Mean              0.0199854\n",
      "exploration/env_infos/end_effector_loc Std               0.201887\n",
      "exploration/env_infos/end_effector_loc Max               0.373629\n",
      "exploration/env_infos/end_effector_loc Min              -0.760319\n",
      "evaluation/num steps total                           21000\n",
      "evaluation/num paths total                            1050\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0841271\n",
      "evaluation/Rewards Std                                   0.0677043\n",
      "evaluation/Rewards Max                                   0.147359\n",
      "evaluation/Rewards Min                                  -0.582624\n",
      "evaluation/Returns Mean                                 -1.68254\n",
      "evaluation/Returns Std                                   0.865894\n",
      "evaluation/Returns Max                                   0.48494\n",
      "evaluation/Returns Min                                  -3.58603\n",
      "evaluation/Actions Mean                                 -0.00691527\n",
      "evaluation/Actions Std                                   0.0484629\n",
      "evaluation/Actions Max                                   0.399415\n",
      "evaluation/Actions Min                                  -0.284991\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.68254\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0640186\n",
      "evaluation/env_infos/final/reward_dist Std               0.160954\n",
      "evaluation/env_infos/final/reward_dist Max               0.939318\n",
      "evaluation/env_infos/final/reward_dist Min               3.20436e-44\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0045537\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00767161\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0303989\n",
      "evaluation/env_infos/initial/reward_dist Min             1.15282e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.0884866\n",
      "evaluation/env_infos/reward_dist Std                     0.179034\n",
      "evaluation/env_infos/reward_dist Max                     0.996489\n",
      "evaluation/env_infos/reward_dist Min                     3.20436e-44\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.081909\n",
      "evaluation/env_infos/final/reward_energy Std             0.0760277\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00972662\n",
      "evaluation/env_infos/final/reward_energy Min            -0.404483\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.110375\n",
      "evaluation/env_infos/initial/reward_energy Std           0.0686613\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.023516\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.318258\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0533209\n",
      "evaluation/env_infos/reward_energy Std                   0.0441569\n",
      "evaluation/env_infos/reward_energy Max                  -0.00188164\n",
      "evaluation/env_infos/reward_energy Min                  -0.404483\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0451065\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.301534\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.454465\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.848303\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00026901\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00458791\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0158921\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0142496\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0102572\n",
      "evaluation/env_infos/end_effector_loc Std                0.164542\n",
      "evaluation/env_infos/end_effector_loc Max                0.454465\n",
      "evaluation/env_infos/end_effector_loc Min               -0.848303\n",
      "time/data storing (s)                                    0.00287305\n",
      "time/evaluation sampling (s)                             0.946274\n",
      "time/exploration sampling (s)                            0.121226\n",
      "time/logging (s)                                         0.019417\n",
      "time/saving (s)                                          0.029886\n",
      "time/training (s)                                       44.7534\n",
      "time/epoch (s)                                          45.8731\n",
      "time/total (s)                                        1038.59\n",
      "Epoch                                                   20\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:31:22.049976 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 21 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00210275\n",
      "trainer/QF2 Loss                                         0.00304153\n",
      "trainer/Policy Loss                                      3.9314\n",
      "trainer/Q1 Predictions Mean                             -1.69986\n",
      "trainer/Q1 Predictions Std                               1.01105\n",
      "trainer/Q1 Predictions Max                              -0.0352082\n",
      "trainer/Q1 Predictions Min                              -6.33899\n",
      "trainer/Q2 Predictions Mean                             -1.68502\n",
      "trainer/Q2 Predictions Std                               1.01375\n",
      "trainer/Q2 Predictions Max                              -0.0391836\n",
      "trainer/Q2 Predictions Min                              -6.33102\n",
      "trainer/Q Targets Mean                                  -1.69475\n",
      "trainer/Q Targets Std                                    1.01455\n",
      "trainer/Q Targets Max                                   -0.0376383\n",
      "trainer/Q Targets Min                                   -6.50395\n",
      "trainer/Log Pis Mean                                     2.26102\n",
      "trainer/Log Pis Std                                      1.31855\n",
      "trainer/Log Pis Max                                      5.40832\n",
      "trainer/Log Pis Min                                     -3.51674\n",
      "trainer/Policy mu Mean                                  -0.0103715\n",
      "trainer/Policy mu Std                                    0.202554\n",
      "trainer/Policy mu Max                                    1.59305\n",
      "trainer/Policy mu Min                                   -2.39728\n",
      "trainer/Policy log std Mean                             -2.51204\n",
      "trainer/Policy log std Std                               0.392728\n",
      "trainer/Policy log std Max                              -0.461271\n",
      "trainer/Policy log std Min                              -3.3921\n",
      "trainer/Alpha                                            0.0202628\n",
      "trainer/Alpha Loss                                       1.01795\n",
      "exploration/num steps total                           3200\n",
      "exploration/num paths total                            160\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0569692\n",
      "exploration/Rewards Std                                  0.106616\n",
      "exploration/Rewards Max                                  0.153634\n",
      "exploration/Rewards Min                                 -0.339855\n",
      "exploration/Returns Mean                                -1.13938\n",
      "exploration/Returns Std                                  1.49792\n",
      "exploration/Returns Max                                  1.61441\n",
      "exploration/Returns Min                                 -2.55416\n",
      "exploration/Actions Mean                                 0.00352316\n",
      "exploration/Actions Std                                  0.111657\n",
      "exploration/Actions Max                                  0.355426\n",
      "exploration/Actions Min                                 -0.324545\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.13938\n",
      "exploration/env_infos/final/reward_dist Mean             0.0727321\n",
      "exploration/env_infos/final/reward_dist Std              0.127377\n",
      "exploration/env_infos/final/reward_dist Max              0.326748\n",
      "exploration/env_infos/final/reward_dist Min              1.0774e-19\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00814386\n",
      "exploration/env_infos/initial/reward_dist Std            0.0145627\n",
      "exploration/env_infos/initial/reward_dist Max            0.0372108\n",
      "exploration/env_infos/initial/reward_dist Min            2.39896e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.159732\n",
      "exploration/env_infos/reward_dist Std                    0.23699\n",
      "exploration/env_infos/reward_dist Max                    0.894881\n",
      "exploration/env_infos/reward_dist Min                    1.0774e-19\n",
      "exploration/env_infos/final/reward_energy Mean          -0.124808\n",
      "exploration/env_infos/final/reward_energy Std            0.0751601\n",
      "exploration/env_infos/final/reward_energy Max           -0.0393795\n",
      "exploration/env_infos/final/reward_energy Min           -0.230109\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.222566\n",
      "exploration/env_infos/initial/reward_energy Std          0.104817\n",
      "exploration/env_infos/initial/reward_energy Max         -0.117339\n",
      "exploration/env_infos/initial/reward_energy Min         -0.416374\n",
      "exploration/env_infos/reward_energy Mean                -0.13365\n",
      "exploration/env_infos/reward_energy Std                  0.0842442\n",
      "exploration/env_infos/reward_energy Max                 -0.00431652\n",
      "exploration/env_infos/reward_energy Min                 -0.416374\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.00459556\n",
      "exploration/env_infos/final/end_effector_loc Std         0.29192\n",
      "exploration/env_infos/final/end_effector_loc Max         0.278122\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.636882\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00243703\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00834947\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00893417\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0162272\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0148786\n",
      "exploration/env_infos/end_effector_loc Std               0.189972\n",
      "exploration/env_infos/end_effector_loc Max               0.278122\n",
      "exploration/env_infos/end_effector_loc Min              -0.636882\n",
      "evaluation/num steps total                           22000\n",
      "evaluation/num paths total                            1100\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0626847\n",
      "evaluation/Rewards Std                                   0.0745355\n",
      "evaluation/Rewards Max                                   0.111089\n",
      "evaluation/Rewards Min                                  -0.545762\n",
      "evaluation/Returns Mean                                 -1.25369\n",
      "evaluation/Returns Std                                   1.12259\n",
      "evaluation/Returns Max                                   1.22594\n",
      "evaluation/Returns Min                                  -3.49241\n",
      "evaluation/Actions Mean                                 -0.0077482\n",
      "evaluation/Actions Std                                   0.0558542\n",
      "evaluation/Actions Max                                   0.365591\n",
      "evaluation/Actions Min                                  -0.74091\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.25369\n",
      "evaluation/env_infos/final/reward_dist Mean              0.167529\n",
      "evaluation/env_infos/final/reward_dist Std               0.224108\n",
      "evaluation/env_infos/final/reward_dist Max               0.994827\n",
      "evaluation/env_infos/final/reward_dist Min               5.36533e-57\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00521255\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00933152\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0409575\n",
      "evaluation/env_infos/initial/reward_dist Min             1.33828e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.137158\n",
      "evaluation/env_infos/reward_dist Std                     0.212616\n",
      "evaluation/env_infos/reward_dist Max                     0.998779\n",
      "evaluation/env_infos/reward_dist Min                     5.36533e-57\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0706683\n",
      "evaluation/env_infos/final/reward_energy Std             0.0542872\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00334885\n",
      "evaluation/env_infos/final/reward_energy Min            -0.229215\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.157231\n",
      "evaluation/env_infos/initial/reward_energy Std           0.15346\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0164047\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.7706\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0547942\n",
      "evaluation/env_infos/reward_energy Std                   0.05794\n",
      "evaluation/env_infos/reward_energy Max                  -0.000380304\n",
      "evaluation/env_infos/reward_energy Min                  -0.7706\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0698358\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.266896\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.5753\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.919514\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.0011473\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00768266\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0182796\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0370455\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0240066\n",
      "evaluation/env_infos/end_effector_loc Std                0.156785\n",
      "evaluation/env_infos/end_effector_loc Max                0.5753\n",
      "evaluation/env_infos/end_effector_loc Min               -0.919514\n",
      "time/data storing (s)                                    0.00304119\n",
      "time/evaluation sampling (s)                             0.977421\n",
      "time/exploration sampling (s)                            0.119289\n",
      "time/logging (s)                                         0.0201635\n",
      "time/saving (s)                                          0.0316762\n",
      "time/training (s)                                       44.318\n",
      "time/epoch (s)                                          45.4696\n",
      "time/total (s)                                        1084.35\n",
      "Epoch                                                   21\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:32:07.977242 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 22 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00208177\n",
      "trainer/QF2 Loss                                         0.00075504\n",
      "trainer/Policy Loss                                      3.67203\n",
      "trainer/Q1 Predictions Mean                             -1.64912\n",
      "trainer/Q1 Predictions Std                               1.02217\n",
      "trainer/Q1 Predictions Max                              -0.0100421\n",
      "trainer/Q1 Predictions Min                              -6.28763\n",
      "trainer/Q2 Predictions Mean                             -1.63301\n",
      "trainer/Q2 Predictions Std                               1.00303\n",
      "trainer/Q2 Predictions Max                              -0.0400837\n",
      "trainer/Q2 Predictions Min                              -6.16132\n",
      "trainer/Q Targets Mean                                  -1.63218\n",
      "trainer/Q Targets Std                                    1.0066\n",
      "trainer/Q Targets Max                                   -0.0595577\n",
      "trainer/Q Targets Min                                   -6.2031\n",
      "trainer/Log Pis Mean                                     2.04664\n",
      "trainer/Log Pis Std                                      1.28886\n",
      "trainer/Log Pis Max                                      5.20349\n",
      "trainer/Log Pis Min                                     -2.11795\n",
      "trainer/Policy mu Mean                                   0.00162103\n",
      "trainer/Policy mu Std                                    0.246505\n",
      "trainer/Policy mu Max                                    2.37764\n",
      "trainer/Policy mu Min                                   -2.10731\n",
      "trainer/Policy log std Mean                             -2.39335\n",
      "trainer/Policy log std Std                               0.441233\n",
      "trainer/Policy log std Max                              -0.248241\n",
      "trainer/Policy log std Min                              -3.21867\n",
      "trainer/Alpha                                            0.0200744\n",
      "trainer/Alpha Loss                                       0.182321\n",
      "exploration/num steps total                           3300\n",
      "exploration/num paths total                            165\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0914819\n",
      "exploration/Rewards Std                                  0.0905991\n",
      "exploration/Rewards Max                                  0.0907386\n",
      "exploration/Rewards Min                                 -0.390972\n",
      "exploration/Returns Mean                                -1.82964\n",
      "exploration/Returns Std                                  1.2724\n",
      "exploration/Returns Max                                  0.109419\n",
      "exploration/Returns Min                                 -3.28081\n",
      "exploration/Actions Mean                                 0.00345199\n",
      "exploration/Actions Std                                  0.0972037\n",
      "exploration/Actions Max                                  0.330593\n",
      "exploration/Actions Min                                 -0.250378\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.82964\n",
      "exploration/env_infos/final/reward_dist Mean             0.0707797\n",
      "exploration/env_infos/final/reward_dist Std              0.137737\n",
      "exploration/env_infos/final/reward_dist Max              0.346211\n",
      "exploration/env_infos/final/reward_dist Min              3.72994e-35\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0150057\n",
      "exploration/env_infos/initial/reward_dist Std            0.0182832\n",
      "exploration/env_infos/initial/reward_dist Max            0.0434135\n",
      "exploration/env_infos/initial/reward_dist Min            1.13222e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.163333\n",
      "exploration/env_infos/reward_dist Std                    0.252853\n",
      "exploration/env_infos/reward_dist Max                    0.980687\n",
      "exploration/env_infos/reward_dist Min                    3.72994e-35\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0887638\n",
      "exploration/env_infos/final/reward_energy Std            0.0347684\n",
      "exploration/env_infos/final/reward_energy Max           -0.0281889\n",
      "exploration/env_infos/final/reward_energy Min           -0.121169\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.20304\n",
      "exploration/env_infos/initial/reward_energy Std          0.0760875\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0987474\n",
      "exploration/env_infos/initial/reward_energy Min         -0.330613\n",
      "exploration/env_infos/reward_energy Mean                -0.119357\n",
      "exploration/env_infos/reward_energy Std                  0.0683724\n",
      "exploration/env_infos/reward_energy Max                 -0.0116462\n",
      "exploration/env_infos/reward_energy Min                 -0.350358\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.166991\n",
      "exploration/env_infos/final/end_effector_loc Std         0.315079\n",
      "exploration/env_infos/final/end_effector_loc Max         0.803847\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.306483\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00346441\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00683858\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0165297\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00982475\n",
      "exploration/env_infos/end_effector_loc Mean              0.0858867\n",
      "exploration/env_infos/end_effector_loc Std               0.173915\n",
      "exploration/env_infos/end_effector_loc Max               0.803847\n",
      "exploration/env_infos/end_effector_loc Min              -0.306483\n",
      "evaluation/num steps total                           23000\n",
      "evaluation/num paths total                            1150\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0823196\n",
      "evaluation/Rewards Std                                   0.0681208\n",
      "evaluation/Rewards Max                                   0.127273\n",
      "evaluation/Rewards Min                                  -0.391238\n",
      "evaluation/Returns Mean                                 -1.64639\n",
      "evaluation/Returns Std                                   1.14259\n",
      "evaluation/Returns Max                                   1.50547\n",
      "evaluation/Returns Min                                  -4.2164\n",
      "evaluation/Actions Mean                                 -0.00348344\n",
      "evaluation/Actions Std                                   0.0552982\n",
      "evaluation/Actions Max                                   0.394784\n",
      "evaluation/Actions Min                                  -0.822481\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.64639\n",
      "evaluation/env_infos/final/reward_dist Mean              0.18094\n",
      "evaluation/env_infos/final/reward_dist Std               0.31012\n",
      "evaluation/env_infos/final/reward_dist Max               0.979428\n",
      "evaluation/env_infos/final/reward_dist Min               2.35261e-22\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00445483\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00894614\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0436059\n",
      "evaluation/env_infos/initial/reward_dist Min             1.39161e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.122351\n",
      "evaluation/env_infos/reward_dist Std                     0.221854\n",
      "evaluation/env_infos/reward_dist Max                     0.999277\n",
      "evaluation/env_infos/reward_dist Min                     2.35261e-22\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0586889\n",
      "evaluation/env_infos/final/reward_energy Std             0.0636247\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0119675\n",
      "evaluation/env_infos/final/reward_energy Min            -0.345051\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.154708\n",
      "evaluation/env_infos/initial/reward_energy Std           0.147983\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0217239\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.830936\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0523917\n",
      "evaluation/env_infos/reward_energy Std                   0.058268\n",
      "evaluation/env_infos/reward_energy Max                  -0.000632169\n",
      "evaluation/env_infos/reward_energy Min                  -0.830936\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.00954474\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.252928\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.749336\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.585671\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00049892\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00755267\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0197392\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0411241\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0129256\n",
      "evaluation/env_infos/end_effector_loc Std                0.149668\n",
      "evaluation/env_infos/end_effector_loc Max                0.749336\n",
      "evaluation/env_infos/end_effector_loc Min               -0.585671\n",
      "time/data storing (s)                                    0.00312707\n",
      "time/evaluation sampling (s)                             0.970995\n",
      "time/exploration sampling (s)                            0.118101\n",
      "time/logging (s)                                         0.0192349\n",
      "time/saving (s)                                          0.0305322\n",
      "time/training (s)                                       44.4698\n",
      "time/epoch (s)                                          45.6118\n",
      "time/total (s)                                        1130.28\n",
      "Epoch                                                   22\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:32:55.132779 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 23 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000845264\r\n",
      "trainer/QF2 Loss                                         0.000738536\r\n",
      "trainer/Policy Loss                                      3.24365\r\n",
      "trainer/Q1 Predictions Mean                             -1.47428\r\n",
      "trainer/Q1 Predictions Std                               0.926854\r\n",
      "trainer/Q1 Predictions Max                               0.0180639\r\n",
      "trainer/Q1 Predictions Min                              -3.9728\r\n",
      "trainer/Q2 Predictions Mean                             -1.46816\r\n",
      "trainer/Q2 Predictions Std                               0.923123\r\n",
      "trainer/Q2 Predictions Max                               0.0499661\r\n",
      "trainer/Q2 Predictions Min                              -3.9255\r\n",
      "trainer/Q Targets Mean                                  -1.46658\r\n",
      "trainer/Q Targets Std                                    0.92405\r\n",
      "trainer/Q Targets Max                                    0.0203456\r\n",
      "trainer/Q Targets Min                                   -3.91251\r\n",
      "trainer/Log Pis Mean                                     1.7836\r\n",
      "trainer/Log Pis Std                                      1.25738\r\n",
      "trainer/Log Pis Max                                      4.06054\r\n",
      "trainer/Log Pis Min                                     -6.73606\r\n",
      "trainer/Policy mu Mean                                   0.00486339\r\n",
      "trainer/Policy mu Std                                    0.214854\r\n",
      "trainer/Policy mu Max                                    2.09769\r\n",
      "trainer/Policy mu Min                                   -0.540247\r\n",
      "trainer/Policy log std Mean                             -2.29203\r\n",
      "trainer/Policy log std Std                               0.464893\r\n",
      "trainer/Policy log std Max                              -0.62013\r\n",
      "trainer/Policy log std Min                              -3.0852\r\n",
      "trainer/Alpha                                            0.021282\r\n",
      "trainer/Alpha Loss                                      -0.832993\r\n",
      "exploration/num steps total                           3400\r\n",
      "exploration/num paths total                            170\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.121694\r\n",
      "exploration/Rewards Std                                  0.0714952\r\n",
      "exploration/Rewards Max                                 -0.0272096\r\n",
      "exploration/Rewards Min                                 -0.376006\r\n",
      "exploration/Returns Mean                                -2.43387\r\n",
      "exploration/Returns Std                                  0.759646\r\n",
      "exploration/Returns Max                                 -1.61762\r\n",
      "exploration/Returns Min                                 -3.60402\r\n",
      "exploration/Actions Mean                                -0.00160453\r\n",
      "exploration/Actions Std                                  0.148671\r\n",
      "exploration/Actions Max                                  0.573589\r\n",
      "exploration/Actions Min                                 -0.392601\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.43387\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0357856\r\n",
      "exploration/env_infos/final/reward_dist Std              0.0382002\r\n",
      "exploration/env_infos/final/reward_dist Max              0.0975672\r\n",
      "exploration/env_infos/final/reward_dist Min              1.33244e-36\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00778208\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0106843\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.027118\r\n",
      "exploration/env_infos/initial/reward_dist Min            3.49011e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.136185\r\n",
      "exploration/env_infos/reward_dist Std                    0.209742\r\n",
      "exploration/env_infos/reward_dist Max                    0.944358\r\n",
      "exploration/env_infos/reward_dist Min                    1.33244e-36\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.133502\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0319811\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.079353\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.172768\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.332912\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.159562\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0907183\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.579413\r\n",
      "exploration/env_infos/reward_energy Mean                -0.184057\r\n",
      "exploration/env_infos/reward_energy Std                  0.101658\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0215264\r\n",
      "exploration/env_infos/reward_energy Min                 -0.579413\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0112904\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.327038\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.477011\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.755059\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00506901\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0120278\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0286795\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0138935\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.0114126\r\n",
      "exploration/env_infos/end_effector_loc Std               0.196484\r\n",
      "exploration/env_infos/end_effector_loc Max               0.477011\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.755059\r\n",
      "evaluation/num steps total                           24000\r\n",
      "evaluation/num paths total                            1200\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0668052\r\n",
      "evaluation/Rewards Std                                   0.0905556\r\n",
      "evaluation/Rewards Max                                   0.117957\r\n",
      "evaluation/Rewards Min                                  -0.587621\r\n",
      "evaluation/Returns Mean                                 -1.3361\r\n",
      "evaluation/Returns Std                                   1.32925\r\n",
      "evaluation/Returns Max                                   0.637623\r\n",
      "evaluation/Returns Min                                  -5.46529\r\n",
      "evaluation/Actions Mean                                 -0.0045642\r\n",
      "evaluation/Actions Std                                   0.070436\r\n",
      "evaluation/Actions Max                                   0.427441\r\n",
      "evaluation/Actions Min                                  -0.789231\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.3361\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.103558\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.205055\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.734825\r\n",
      "evaluation/env_infos/final/reward_dist Min               4.50247e-44\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00467618\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00749599\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0327052\r\n",
      "evaluation/env_infos/initial/reward_dist Min             6.04762e-07\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.106973\r\n",
      "evaluation/env_infos/reward_dist Std                     0.18091\r\n",
      "evaluation/env_infos/reward_dist Max                     0.988565\r\n",
      "evaluation/env_infos/reward_dist Min                     4.50247e-44\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0779269\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0656574\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00832536\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.418494\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.189587\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.168144\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0310469\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.799563\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0689684\r\n",
      "evaluation/env_infos/reward_energy Std                   0.072163\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00133464\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.799563\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0959188\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.299924\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.757402\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.683907\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00286836\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00848777\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0163386\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0394615\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.05306\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.18292\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.757402\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.683907\r\n",
      "time/data storing (s)                                    0.00303635\r\n",
      "time/evaluation sampling (s)                             0.960598\r\n",
      "time/exploration sampling (s)                            0.123955\r\n",
      "time/logging (s)                                         0.0203875\r\n",
      "time/saving (s)                                          0.0303476\r\n",
      "time/training (s)                                       45.6983\r\n",
      "time/epoch (s)                                          46.8366\r\n",
      "time/total (s)                                        1177.43\r\n",
      "Epoch                                                   23\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:33:42.143745 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 24 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00103472\r\n",
      "trainer/QF2 Loss                                         0.00262344\r\n",
      "trainer/Policy Loss                                      3.38863\r\n",
      "trainer/Q1 Predictions Mean                             -1.50708\r\n",
      "trainer/Q1 Predictions Std                               0.903178\r\n",
      "trainer/Q1 Predictions Max                               0.028404\r\n",
      "trainer/Q1 Predictions Min                              -3.87102\r\n",
      "trainer/Q2 Predictions Mean                             -1.49204\r\n",
      "trainer/Q2 Predictions Std                               0.897398\r\n",
      "trainer/Q2 Predictions Max                               0.0144268\r\n",
      "trainer/Q2 Predictions Min                              -3.91756\r\n",
      "trainer/Q Targets Mean                                  -1.49779\r\n",
      "trainer/Q Targets Std                                    0.90045\r\n",
      "trainer/Q Targets Max                                    0.0203456\r\n",
      "trainer/Q Targets Min                                   -3.89184\r\n",
      "trainer/Log Pis Mean                                     1.89091\r\n",
      "trainer/Log Pis Std                                      1.35864\r\n",
      "trainer/Log Pis Max                                      4.39738\r\n",
      "trainer/Log Pis Min                                     -3.28733\r\n",
      "trainer/Policy mu Mean                                  -0.0201926\r\n",
      "trainer/Policy mu Std                                    0.303422\r\n",
      "trainer/Policy mu Max                                    2.2092\r\n",
      "trainer/Policy mu Min                                   -2.16765\r\n",
      "trainer/Policy log std Mean                             -2.2858\r\n",
      "trainer/Policy log std Std                               0.519817\r\n",
      "trainer/Policy log std Max                              -0.418502\r\n",
      "trainer/Policy log std Min                              -3.35979\r\n",
      "trainer/Alpha                                            0.0203439\r\n",
      "trainer/Alpha Loss                                      -0.424652\r\n",
      "exploration/num steps total                           3500\r\n",
      "exploration/num paths total                            175\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.119444\r\n",
      "exploration/Rewards Std                                  0.0577276\r\n",
      "exploration/Rewards Max                                 -0.0380516\r\n",
      "exploration/Rewards Min                                 -0.390374\r\n",
      "exploration/Returns Mean                                -2.38888\r\n",
      "exploration/Returns Std                                  0.314438\r\n",
      "exploration/Returns Max                                 -1.99009\r\n",
      "exploration/Returns Min                                 -2.7715\r\n",
      "exploration/Actions Mean                                -0.00777254\r\n",
      "exploration/Actions Std                                  0.080656\r\n",
      "exploration/Actions Max                                  0.257109\r\n",
      "exploration/Actions Min                                 -0.31836\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.38888\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.000831564\r\n",
      "exploration/env_infos/final/reward_dist Std              0.00160144\r\n",
      "exploration/env_infos/final/reward_dist Max              0.00403375\r\n",
      "exploration/env_infos/final/reward_dist Min              2.83523e-14\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00235145\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00446793\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0112845\r\n",
      "exploration/env_infos/initial/reward_dist Min            4.70187e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.0256169\r\n",
      "exploration/env_infos/reward_dist Std                    0.0713609\r\n",
      "exploration/env_infos/reward_dist Max                    0.46613\r\n",
      "exploration/env_infos/reward_dist Min                    2.83523e-14\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.162422\r\n",
      "exploration/env_infos/final/reward_energy Std            0.100143\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0511202\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.345145\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.151178\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.0734582\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0657675\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.257935\r\n",
      "exploration/env_infos/reward_energy Mean                -0.0980831\r\n",
      "exploration/env_infos/reward_energy Std                  0.0592564\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0136538\r\n",
      "exploration/env_infos/reward_energy Min                 -0.345145\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0624545\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.254441\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.316923\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.556561\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00187483\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00563901\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0128554\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00828111\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0306966\r\n",
      "exploration/env_infos/end_effector_loc Std               0.14491\r\n",
      "exploration/env_infos/end_effector_loc Max               0.316923\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.556561\r\n",
      "evaluation/num steps total                           25000\r\n",
      "evaluation/num paths total                            1250\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0664353\r\n",
      "evaluation/Rewards Std                                   0.0728691\r\n",
      "evaluation/Rewards Max                                   0.12772\r\n",
      "evaluation/Rewards Min                                  -0.597638\r\n",
      "evaluation/Returns Mean                                 -1.32871\r\n",
      "evaluation/Returns Std                                   1.04792\r\n",
      "evaluation/Returns Max                                   0.472059\r\n",
      "evaluation/Returns Min                                  -3.803\r\n",
      "evaluation/Actions Mean                                 -0.00219815\r\n",
      "evaluation/Actions Std                                   0.0759196\r\n",
      "evaluation/Actions Max                                   0.933951\r\n",
      "evaluation/Actions Min                                  -0.919217\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.32871\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0767016\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.178398\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.83875\r\n",
      "evaluation/env_infos/final/reward_dist Min               1.18975e-37\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00639741\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0192363\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.132789\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.0043e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.103883\r\n",
      "evaluation/env_infos/reward_dist Std                     0.197216\r\n",
      "evaluation/env_infos/reward_dist Max                     0.973228\r\n",
      "evaluation/env_infos/reward_dist Min                     1.18975e-37\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.068687\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation/env_infos/final/reward_energy Std             0.0368885\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0120729\n",
      "evaluation/env_infos/final/reward_energy Min            -0.148536\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.215859\n",
      "evaluation/env_infos/initial/reward_energy Std           0.263531\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.028544\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.31043\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0665136\n",
      "evaluation/env_infos/reward_energy Std                   0.0843396\n",
      "evaluation/env_infos/reward_energy Max                  -0.000437994\n",
      "evaluation/env_infos/reward_energy Min                  -1.31043\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0437687\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.305752\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.757639\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.739353\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00162892\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0119332\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0466975\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0459608\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0248649\n",
      "evaluation/env_infos/end_effector_loc Std                0.189145\n",
      "evaluation/env_infos/end_effector_loc Max                0.757639\n",
      "evaluation/env_infos/end_effector_loc Min               -0.739353\n",
      "time/data storing (s)                                    0.00302701\n",
      "time/evaluation sampling (s)                             1.00349\n",
      "time/exploration sampling (s)                            0.122533\n",
      "time/logging (s)                                         0.0194177\n",
      "time/saving (s)                                          0.0283402\n",
      "time/training (s)                                       45.5049\n",
      "time/epoch (s)                                          46.6817\n",
      "time/total (s)                                        1224.44\n",
      "Epoch                                                   24\n",
      "---------------------------------------------------  ---------------\n",
      "2021-05-25 19:34:28.770955 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 25 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00192642\n",
      "trainer/QF2 Loss                                         0.0037633\n",
      "trainer/Policy Loss                                      3.61657\n",
      "trainer/Q1 Predictions Mean                             -1.44162\n",
      "trainer/Q1 Predictions Std                               0.874737\n",
      "trainer/Q1 Predictions Max                              -0.000672296\n",
      "trainer/Q1 Predictions Min                              -4.15411\n",
      "trainer/Q2 Predictions Mean                             -1.4362\n",
      "trainer/Q2 Predictions Std                               0.886233\n",
      "trainer/Q2 Predictions Max                               0.0374026\n",
      "trainer/Q2 Predictions Min                              -4.28273\n",
      "trainer/Q Targets Mean                                  -1.43645\n",
      "trainer/Q Targets Std                                    0.880393\n",
      "trainer/Q Targets Max                                    0.0350086\n",
      "trainer/Q Targets Min                                   -4.17356\n",
      "trainer/Log Pis Mean                                     2.19224\n",
      "trainer/Log Pis Std                                      1.34198\n",
      "trainer/Log Pis Max                                      4.39552\n",
      "trainer/Log Pis Min                                     -4.02113\n",
      "trainer/Policy mu Mean                                  -0.00145799\n",
      "trainer/Policy mu Std                                    0.292317\n",
      "trainer/Policy mu Max                                    2.3997\n",
      "trainer/Policy mu Min                                   -2.14093\n",
      "trainer/Policy log std Mean                             -2.41379\n",
      "trainer/Policy log std Std                               0.548914\n",
      "trainer/Policy log std Max                              -0.257925\n",
      "trainer/Policy log std Min                              -3.27876\n",
      "trainer/Alpha                                            0.020647\n",
      "trainer/Alpha Loss                                       0.745945\n",
      "exploration/num steps total                           3600\n",
      "exploration/num paths total                            180\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.188863\n",
      "exploration/Rewards Std                                  0.122258\n",
      "exploration/Rewards Max                                 -0.0628688\n",
      "exploration/Rewards Min                                 -0.742205\n",
      "exploration/Returns Mean                                -3.77726\n",
      "exploration/Returns Std                                  1.49607\n",
      "exploration/Returns Max                                 -2.45397\n",
      "exploration/Returns Min                                 -6.47996\n",
      "exploration/Actions Mean                                -0.0105934\n",
      "exploration/Actions Std                                  0.166051\n",
      "exploration/Actions Max                                  0.831515\n",
      "exploration/Actions Min                                 -0.707685\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -3.77726\n",
      "exploration/env_infos/final/reward_dist Mean             0.0199669\n",
      "exploration/env_infos/final/reward_dist Std              0.0399338\n",
      "exploration/env_infos/final/reward_dist Max              0.0998346\n",
      "exploration/env_infos/final/reward_dist Min              1.02853e-83\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00363422\n",
      "exploration/env_infos/initial/reward_dist Std            0.00333577\n",
      "exploration/env_infos/initial/reward_dist Max            0.00870617\n",
      "exploration/env_infos/initial/reward_dist Min            2.07171e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.029488\n",
      "exploration/env_infos/reward_dist Std                    0.0977123\n",
      "exploration/env_infos/reward_dist Max                    0.748551\n",
      "exploration/env_infos/reward_dist Min                    1.02853e-83\n",
      "exploration/env_infos/final/reward_energy Mean          -0.172607\n",
      "exploration/env_infos/final/reward_energy Std            0.0955641\n",
      "exploration/env_infos/final/reward_energy Max           -0.0616236\n",
      "exploration/env_infos/final/reward_energy Min           -0.321439\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.393064\n",
      "exploration/env_infos/initial/reward_energy Std          0.382627\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0980722\n",
      "exploration/env_infos/initial/reward_energy Min         -1.0919\n",
      "exploration/env_infos/reward_energy Mean                -0.175356\n",
      "exploration/env_infos/reward_energy Std                  0.156909\n",
      "exploration/env_infos/reward_energy Max                 -0.0251731\n",
      "exploration/env_infos/reward_energy Min                 -1.0919\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0834723\n",
      "exploration/env_infos/final/end_effector_loc Std         0.526149\n",
      "exploration/env_infos/final/end_effector_loc Max         0.634092\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.919464\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000959299\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0193703\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0415758\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0353842\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0260919\n",
      "exploration/env_infos/end_effector_loc Std               0.301315\n",
      "exploration/env_infos/end_effector_loc Max               0.634092\n",
      "exploration/env_infos/end_effector_loc Min              -0.919464\n",
      "evaluation/num steps total                           26000\n",
      "evaluation/num paths total                            1300\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0819103\n",
      "evaluation/Rewards Std                                   0.115202\n",
      "evaluation/Rewards Max                                   0.141297\n",
      "evaluation/Rewards Min                                  -0.749245\n",
      "evaluation/Returns Mean                                 -1.63821\n",
      "evaluation/Returns Std                                   1.90608\n",
      "evaluation/Returns Max                                   1.0731\n",
      "evaluation/Returns Min                                  -9.08768\n",
      "evaluation/Actions Mean                                  0.00163399\n",
      "evaluation/Actions Std                                   0.0945268\n",
      "evaluation/Actions Max                                   0.697414\n",
      "evaluation/Actions Min                                  -0.914458\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.63821\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0589946\n",
      "evaluation/env_infos/final/reward_dist Std               0.152116\n",
      "evaluation/env_infos/final/reward_dist Max               0.89552\n",
      "evaluation/env_infos/final/reward_dist Min               8.33754e-73\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0054505\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00889244\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0362764\n",
      "evaluation/env_infos/initial/reward_dist Min             1.58227e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.110301\n",
      "evaluation/env_infos/reward_dist Std                     0.210319\n",
      "evaluation/env_infos/reward_dist Max                     0.997319\n",
      "evaluation/env_infos/reward_dist Min                     8.33754e-73\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0667749\n",
      "evaluation/env_infos/final/reward_energy Std             0.0548726\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00602081\n",
      "evaluation/env_infos/final/reward_energy Min            -0.2698\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.24335\n",
      "evaluation/env_infos/initial/reward_energy Std           0.257038\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0147075\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.971556\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0795428\n",
      "evaluation/env_infos/reward_energy Std                   0.107466\n",
      "evaluation/env_infos/reward_energy Max                  -0.000613486\n",
      "evaluation/env_infos/reward_energy Min                  -1.10715\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0362375\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.301432\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.839747\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.844749\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00369727\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0119557\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0302735\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0457229\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0377041\n",
      "evaluation/env_infos/end_effector_loc Std                0.198189\n",
      "evaluation/env_infos/end_effector_loc Max                0.839747\n",
      "evaluation/env_infos/end_effector_loc Min               -0.844749\n",
      "time/data storing (s)                                    0.00288421\n",
      "time/evaluation sampling (s)                             0.946918\n",
      "time/exploration sampling (s)                            0.122723\n",
      "time/logging (s)                                         0.019941\n",
      "time/saving (s)                                          0.031353\n",
      "time/training (s)                                       45.1233\n",
      "time/epoch (s)                                          46.2471\n",
      "time/total (s)                                        1271.07\n",
      "Epoch                                                   25\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:35:15.352331 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 26 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00228066\n",
      "trainer/QF2 Loss                                         0.00218909\n",
      "trainer/Policy Loss                                      3.46209\n",
      "trainer/Q1 Predictions Mean                             -1.43709\n",
      "trainer/Q1 Predictions Std                               0.846088\n",
      "trainer/Q1 Predictions Max                               0.115269\n",
      "trainer/Q1 Predictions Min                              -3.89597\n",
      "trainer/Q2 Predictions Mean                             -1.40883\n",
      "trainer/Q2 Predictions Std                               0.837883\n",
      "trainer/Q2 Predictions Max                               0.0753636\n",
      "trainer/Q2 Predictions Min                              -3.87255\n",
      "trainer/Q Targets Mean                                  -1.42218\n",
      "trainer/Q Targets Std                                    0.848074\n",
      "trainer/Q Targets Max                                    0.126095\n",
      "trainer/Q Targets Min                                   -3.9578\n",
      "trainer/Log Pis Mean                                     2.0403\n",
      "trainer/Log Pis Std                                      1.21157\n",
      "trainer/Log Pis Max                                      4.0894\n",
      "trainer/Log Pis Min                                     -3.38142\n",
      "trainer/Policy mu Mean                                   0.00570102\n",
      "trainer/Policy mu Std                                    0.235485\n",
      "trainer/Policy mu Max                                    1.83575\n",
      "trainer/Policy mu Min                                   -1.07412\n",
      "trainer/Policy log std Mean                             -2.37641\n",
      "trainer/Policy log std Std                               0.473602\n",
      "trainer/Policy log std Max                              -0.612779\n",
      "trainer/Policy log std Min                              -3.316\n",
      "trainer/Alpha                                            0.0202056\n",
      "trainer/Alpha Loss                                       0.157148\n",
      "exploration/num steps total                           3700\n",
      "exploration/num paths total                            185\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0719371\n",
      "exploration/Rewards Std                                  0.0726162\n",
      "exploration/Rewards Max                                  0.0450167\n",
      "exploration/Rewards Min                                 -0.318041\n",
      "exploration/Returns Mean                                -1.43874\n",
      "exploration/Returns Std                                  0.652951\n",
      "exploration/Returns Max                                 -0.519682\n",
      "exploration/Returns Min                                 -2.48418\n",
      "exploration/Actions Mean                                -0.00549966\n",
      "exploration/Actions Std                                  0.135288\n",
      "exploration/Actions Max                                  0.439915\n",
      "exploration/Actions Min                                 -0.443709\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.43874\n",
      "exploration/env_infos/final/reward_dist Mean             0.00355946\n",
      "exploration/env_infos/final/reward_dist Std              0.00412964\n",
      "exploration/env_infos/final/reward_dist Max              0.00906741\n",
      "exploration/env_infos/final/reward_dist Min              1.01275e-10\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0227187\n",
      "exploration/env_infos/initial/reward_dist Std            0.0265253\n",
      "exploration/env_infos/initial/reward_dist Max            0.0724936\n",
      "exploration/env_infos/initial/reward_dist Min            1.30809e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.132084\n",
      "exploration/env_infos/reward_dist Std                    0.239816\n",
      "exploration/env_infos/reward_dist Max                    0.952683\n",
      "exploration/env_infos/reward_dist Min                    1.01275e-10\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0974783\n",
      "exploration/env_infos/final/reward_energy Std            0.0707078\n",
      "exploration/env_infos/final/reward_energy Max           -0.016771\n",
      "exploration/env_infos/final/reward_energy Min           -0.19294\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.29403\n",
      "exploration/env_infos/initial/reward_energy Std          0.148949\n",
      "exploration/env_infos/initial/reward_energy Max         -0.095127\n",
      "exploration/env_infos/initial/reward_energy Min         -0.473352\n",
      "exploration/env_infos/reward_energy Mean                -0.153649\n",
      "exploration/env_infos/reward_energy Std                  0.114272\n",
      "exploration/env_infos/reward_energy Max                 -0.00377446\n",
      "exploration/env_infos/reward_energy Min                 -0.473352\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.147408\n",
      "exploration/env_infos/final/end_effector_loc Std         0.261906\n",
      "exploration/env_infos/final/end_effector_loc Max         0.339589\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.526127\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00759073\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00884194\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00780837\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0221855\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0985716\n",
      "exploration/env_infos/end_effector_loc Std               0.186107\n",
      "exploration/env_infos/end_effector_loc Max               0.339589\n",
      "exploration/env_infos/end_effector_loc Min              -0.526127\n",
      "evaluation/num steps total                           27000\n",
      "evaluation/num paths total                            1350\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0824593\n",
      "evaluation/Rewards Std                                   0.0807482\n",
      "evaluation/Rewards Max                                   0.109391\n",
      "evaluation/Rewards Min                                  -0.496691\n",
      "evaluation/Returns Mean                                 -1.64919\n",
      "evaluation/Returns Std                                   1.25101\n",
      "evaluation/Returns Max                                   0.80004\n",
      "evaluation/Returns Min                                  -6.2604\n",
      "evaluation/Actions Mean                                  0.00734949\n",
      "evaluation/Actions Std                                   0.0895855\n",
      "evaluation/Actions Max                                   0.868865\n",
      "evaluation/Actions Min                                  -0.552673\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.64919\n",
      "evaluation/env_infos/final/reward_dist Mean              0.050003\n",
      "evaluation/env_infos/final/reward_dist Std               0.113415\n",
      "evaluation/env_infos/final/reward_dist Max               0.452838\n",
      "evaluation/env_infos/final/reward_dist Min               5.13232e-41\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00736621\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0180794\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0981235\n",
      "evaluation/env_infos/initial/reward_dist Min             1.07941e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.110812\n",
      "evaluation/env_infos/reward_dist Std                     0.21178\n",
      "evaluation/env_infos/reward_dist Max                     0.993859\n",
      "evaluation/env_infos/reward_dist Min                     5.13232e-41\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.060528\n",
      "evaluation/env_infos/final/reward_energy Std             0.0485656\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00707899\n",
      "evaluation/env_infos/final/reward_energy Min            -0.210945\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.251763\n",
      "evaluation/env_infos/initial/reward_energy Std           0.221763\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0139842\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.877523\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0870708\n",
      "evaluation/env_infos/reward_energy Std                   0.0926166\n",
      "evaluation/env_infos/reward_energy Max                  -0.00169746\n",
      "evaluation/env_infos/reward_energy Min                  -0.877523\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0302717\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.294874\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.582197\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.831018\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00337334\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0113721\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0434432\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0276336\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0446409\n",
      "evaluation/env_infos/end_effector_loc Std                0.196958\n",
      "evaluation/env_infos/end_effector_loc Max                0.582197\n",
      "evaluation/env_infos/end_effector_loc Min               -0.831018\n",
      "time/data storing (s)                                    0.0030427\n",
      "time/evaluation sampling (s)                             0.965245\n",
      "time/exploration sampling (s)                            0.120631\n",
      "time/logging (s)                                         0.0190762\n",
      "time/saving (s)                                          0.0294035\n",
      "time/training (s)                                       45.1167\n",
      "time/epoch (s)                                          46.2541\n",
      "time/total (s)                                        1317.65\n",
      "Epoch                                                   26\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:36:02.276591 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 27 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00117056\n",
      "trainer/QF2 Loss                                         0.000933309\n",
      "trainer/Policy Loss                                      3.38432\n",
      "trainer/Q1 Predictions Mean                             -1.43432\n",
      "trainer/Q1 Predictions Std                               0.837177\n",
      "trainer/Q1 Predictions Max                               0.272819\n",
      "trainer/Q1 Predictions Min                              -3.93213\n",
      "trainer/Q2 Predictions Mean                             -1.43306\n",
      "trainer/Q2 Predictions Std                               0.83038\n",
      "trainer/Q2 Predictions Max                               0.233711\n",
      "trainer/Q2 Predictions Min                              -3.85239\n",
      "trainer/Q Targets Mean                                  -1.43132\n",
      "trainer/Q Targets Std                                    0.831348\n",
      "trainer/Q Targets Max                                    0.243764\n",
      "trainer/Q Targets Min                                   -3.87282\n",
      "trainer/Log Pis Mean                                     1.96409\n",
      "trainer/Log Pis Std                                      1.2736\n",
      "trainer/Log Pis Max                                      4.53841\n",
      "trainer/Log Pis Min                                     -2.97317\n",
      "trainer/Policy mu Mean                                   0.0194747\n",
      "trainer/Policy mu Std                                    0.376847\n",
      "trainer/Policy mu Max                                    2.04941\n",
      "trainer/Policy mu Min                                   -2.04462\n",
      "trainer/Policy log std Mean                             -2.2632\n",
      "trainer/Policy log std Std                               0.586772\n",
      "trainer/Policy log std Max                              -0.363287\n",
      "trainer/Policy log std Min                              -3.12931\n",
      "trainer/Alpha                                            0.0206545\n",
      "trainer/Alpha Loss                                      -0.139351\n",
      "exploration/num steps total                           3800\n",
      "exploration/num paths total                            190\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0936505\n",
      "exploration/Rewards Std                                  0.0833835\n",
      "exploration/Rewards Max                                  0.119909\n",
      "exploration/Rewards Min                                 -0.242174\n",
      "exploration/Returns Mean                                -1.87301\n",
      "exploration/Returns Std                                  1.18356\n",
      "exploration/Returns Max                                  0.381063\n",
      "exploration/Returns Min                                 -3.0046\n",
      "exploration/Actions Mean                                -9.28162e-05\n",
      "exploration/Actions Std                                  0.147226\n",
      "exploration/Actions Max                                  0.731528\n",
      "exploration/Actions Min                                 -0.424503\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.87301\n",
      "exploration/env_infos/final/reward_dist Mean             0.14122\n",
      "exploration/env_infos/final/reward_dist Std              0.197063\n",
      "exploration/env_infos/final/reward_dist Max              0.502831\n",
      "exploration/env_infos/final/reward_dist Min              2.50857e-17\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00665475\n",
      "exploration/env_infos/initial/reward_dist Std            0.0129119\n",
      "exploration/env_infos/initial/reward_dist Max            0.0324748\n",
      "exploration/env_infos/initial/reward_dist Min            6.036e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.170704\n",
      "exploration/env_infos/reward_dist Std                    0.280902\n",
      "exploration/env_infos/reward_dist Max                    0.894967\n",
      "exploration/env_infos/reward_dist Min                    2.50857e-17\n",
      "exploration/env_infos/final/reward_energy Mean          -0.117\n",
      "exploration/env_infos/final/reward_energy Std            0.02686\n",
      "exploration/env_infos/final/reward_energy Max           -0.0773788\n",
      "exploration/env_infos/final/reward_energy Min           -0.14586\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.316058\n",
      "exploration/env_infos/initial/reward_energy Std          0.316338\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0763855\n",
      "exploration/env_infos/initial/reward_energy Min         -0.93236\n",
      "exploration/env_infos/reward_energy Mean                -0.155975\n",
      "exploration/env_infos/reward_energy Std                  0.137924\n",
      "exploration/env_infos/reward_energy Max                 -0.0136486\n",
      "exploration/env_infos/reward_energy Min                 -0.93236\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0689644\n",
      "exploration/env_infos/final/end_effector_loc Std         0.284833\n",
      "exploration/env_infos/final/end_effector_loc Max         0.753108\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.2885\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00292423\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0155371\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0365764\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0142319\n",
      "exploration/env_infos/end_effector_loc Mean              0.0355849\n",
      "exploration/env_infos/end_effector_loc Std               0.193668\n",
      "exploration/env_infos/end_effector_loc Max               0.753108\n",
      "exploration/env_infos/end_effector_loc Min              -0.2885\n",
      "evaluation/num steps total                           28000\n",
      "evaluation/num paths total                            1400\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0778595\n",
      "evaluation/Rewards Std                                   0.121249\n",
      "evaluation/Rewards Max                                   0.175705\n",
      "evaluation/Rewards Min                                  -0.856595\n",
      "evaluation/Returns Mean                                 -1.55719\n",
      "evaluation/Returns Std                                   2.1308\n",
      "evaluation/Returns Max                                   1.38935\n",
      "evaluation/Returns Min                                 -13.1545\n",
      "evaluation/Actions Mean                                  0.00542355\n",
      "evaluation/Actions Std                                   0.0728291\n",
      "evaluation/Actions Max                                   0.543115\n",
      "evaluation/Actions Min                                  -0.776873\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.55719\n",
      "evaluation/env_infos/final/reward_dist Mean              0.121078\n",
      "evaluation/env_infos/final/reward_dist Std               0.227312\n",
      "evaluation/env_infos/final/reward_dist Max               0.84511\n",
      "evaluation/env_infos/final/reward_dist Min               3.69766e-110\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00300244\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00723685\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0472442\n",
      "evaluation/env_infos/initial/reward_dist Min             1.49442e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.144645\n",
      "evaluation/env_infos/reward_dist Std                     0.250774\n",
      "evaluation/env_infos/reward_dist Max                     0.996554\n",
      "evaluation/env_infos/reward_dist Min                     3.69766e-110\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0578203\n",
      "evaluation/env_infos/final/reward_energy Std             0.0604497\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00587563\n",
      "evaluation/env_infos/final/reward_energy Min            -0.296658\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.178263\n",
      "evaluation/env_infos/initial/reward_energy Std           0.199052\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.011272\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.945953\n",
      "evaluation/env_infos/reward_energy Mean                 -0.061335\n",
      "evaluation/env_infos/reward_energy Std                   0.0830965\n",
      "evaluation/env_infos/reward_energy Max                  -0.000613207\n",
      "evaluation/env_infos/reward_energy Min                  -0.945953\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.046963\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.288993\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.694169\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000632911\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00942597\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0206334\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0388437\n",
      "evaluation/env_infos/end_effector_loc Mean               0.00851982\n",
      "evaluation/env_infos/end_effector_loc Std                0.187776\n",
      "evaluation/env_infos/end_effector_loc Max                0.694169\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.00288692\n",
      "time/evaluation sampling (s)                             0.970091\n",
      "time/exploration sampling (s)                            0.119592\n",
      "time/logging (s)                                         0.0198678\n",
      "time/saving (s)                                          0.0307102\n",
      "time/training (s)                                       45.4266\n",
      "time/epoch (s)                                          46.5698\n",
      "time/total (s)                                        1364.58\n",
      "Epoch                                                   27\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:36:49.457248 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 28 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000836844\n",
      "trainer/QF2 Loss                                         0.00103203\n",
      "trainer/Policy Loss                                      3.42811\n",
      "trainer/Q1 Predictions Mean                             -1.37537\n",
      "trainer/Q1 Predictions Std                               0.864069\n",
      "trainer/Q1 Predictions Max                               0.472538\n",
      "trainer/Q1 Predictions Min                              -3.8801\n",
      "trainer/Q2 Predictions Mean                             -1.39298\n",
      "trainer/Q2 Predictions Std                               0.871996\n",
      "trainer/Q2 Predictions Max                               0.40889\n",
      "trainer/Q2 Predictions Min                              -4.02032\n",
      "trainer/Q Targets Mean                                  -1.37427\n",
      "trainer/Q Targets Std                                    0.868458\n",
      "trainer/Q Targets Max                                    0.4348\n",
      "trainer/Q Targets Min                                   -4.00653\n",
      "trainer/Log Pis Mean                                     2.05911\n",
      "trainer/Log Pis Std                                      1.15668\n",
      "trainer/Log Pis Max                                      6.28543\n",
      "trainer/Log Pis Min                                     -3.77694\n",
      "trainer/Policy mu Mean                                   0.0183732\n",
      "trainer/Policy mu Std                                    0.367676\n",
      "trainer/Policy mu Max                                    2.24762\n",
      "trainer/Policy mu Min                                   -2.01615\n",
      "trainer/Policy log std Mean                             -2.3182\n",
      "trainer/Policy log std Std                               0.514683\n",
      "trainer/Policy log std Max                              -0.280837\n",
      "trainer/Policy log std Min                              -3.23126\n",
      "trainer/Alpha                                            0.0202401\n",
      "trainer/Alpha Loss                                       0.230551\n",
      "exploration/num steps total                           3900\n",
      "exploration/num paths total                            195\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.055702\n",
      "exploration/Rewards Std                                  0.0625389\n",
      "exploration/Rewards Max                                  0.0752089\n",
      "exploration/Rewards Min                                 -0.228344\n",
      "exploration/Returns Mean                                -1.11404\n",
      "exploration/Returns Std                                  0.917538\n",
      "exploration/Returns Max                                  0.421251\n",
      "exploration/Returns Min                                 -2.14373\n",
      "exploration/Actions Mean                                 0.00608474\n",
      "exploration/Actions Std                                  0.100982\n",
      "exploration/Actions Max                                  0.430569\n",
      "exploration/Actions Min                                 -0.226309\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.11404\n",
      "exploration/env_infos/final/reward_dist Mean             0.139011\n",
      "exploration/env_infos/final/reward_dist Std              0.221973\n",
      "exploration/env_infos/final/reward_dist Max              0.578239\n",
      "exploration/env_infos/final/reward_dist Min              3.05102e-08\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0119336\n",
      "exploration/env_infos/initial/reward_dist Std            0.0130206\n",
      "exploration/env_infos/initial/reward_dist Max            0.0307964\n",
      "exploration/env_infos/initial/reward_dist Min            0.000392136\n",
      "exploration/env_infos/reward_dist Mean                   0.275087\n",
      "exploration/env_infos/reward_dist Std                    0.285184\n",
      "exploration/env_infos/reward_dist Max                    0.930571\n",
      "exploration/env_infos/reward_dist Min                    3.05102e-08\n",
      "exploration/env_infos/final/reward_energy Mean          -0.11755\n",
      "exploration/env_infos/final/reward_energy Std            0.0451618\n",
      "exploration/env_infos/final/reward_energy Max           -0.065436\n",
      "exploration/env_infos/final/reward_energy Min           -0.196319\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.11201\n",
      "exploration/env_infos/initial/reward_energy Std          0.0449788\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0557613\n",
      "exploration/env_infos/initial/reward_energy Min         -0.18078\n",
      "exploration/env_infos/reward_energy Mean                -0.117153\n",
      "exploration/env_infos/reward_energy Std                  0.0821204\n",
      "exploration/env_infos/reward_energy Max                 -0.0105278\n",
      "exploration/env_infos/reward_energy Min                 -0.430907\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0929161\n",
      "exploration/env_infos/final/end_effector_loc Std         0.176735\n",
      "exploration/env_infos/final/end_effector_loc Max         0.336813\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.226047\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00296963\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00306481\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00833534\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00322619\n",
      "exploration/env_infos/end_effector_loc Mean              0.0446034\n",
      "exploration/env_infos/end_effector_loc Std               0.104675\n",
      "exploration/env_infos/end_effector_loc Max               0.336813\n",
      "exploration/env_infos/end_effector_loc Min              -0.226047\n",
      "evaluation/num steps total                           29000\n",
      "evaluation/num paths total                            1450\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0775506\n",
      "evaluation/Rewards Std                                   0.0889816\n",
      "evaluation/Rewards Max                                   0.150317\n",
      "evaluation/Rewards Min                                  -0.563364\n",
      "evaluation/Returns Mean                                 -1.55101\n",
      "evaluation/Returns Std                                   1.45118\n",
      "evaluation/Returns Max                                   1.20005\n",
      "evaluation/Returns Min                                  -8.71153\n",
      "evaluation/Actions Mean                                  0.00568594\n",
      "evaluation/Actions Std                                   0.0851227\n",
      "evaluation/Actions Max                                   0.510682\n",
      "evaluation/Actions Min                                  -0.961774\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.55101\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0536298\n",
      "evaluation/env_infos/final/reward_dist Std               0.151092\n",
      "evaluation/env_infos/final/reward_dist Max               0.688131\n",
      "evaluation/env_infos/final/reward_dist Min               1.36428e-86\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00708609\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0117662\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0420442\n",
      "evaluation/env_infos/initial/reward_dist Min             1.01616e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.0916732\n",
      "evaluation/env_infos/reward_dist Std                     0.188689\n",
      "evaluation/env_infos/reward_dist Max                     0.984362\n",
      "evaluation/env_infos/reward_dist Min                     1.71307e-88\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0823674\n",
      "evaluation/env_infos/final/reward_energy Std             0.0842128\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00826024\n",
      "evaluation/env_infos/final/reward_energy Min            -0.322326\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.216973\n",
      "evaluation/env_infos/initial/reward_energy Std           0.215967\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0215917\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.27582\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0783424\n",
      "evaluation/env_infos/reward_energy Std                   0.0917545\n",
      "evaluation/env_infos/reward_energy Max                  -0.00152775\n",
      "evaluation/env_infos/reward_energy Min                  -1.27582\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0173664\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.290968\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.543374\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00183255\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0106673\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0255341\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0480887\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00579446\n",
      "evaluation/env_infos/end_effector_loc Std                0.198391\n",
      "evaluation/env_infos/end_effector_loc Max                0.543374\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.0030741\n",
      "time/evaluation sampling (s)                             1.06335\n",
      "time/exploration sampling (s)                            0.122546\n",
      "time/logging (s)                                         0.0194515\n",
      "time/saving (s)                                          0.0297202\n",
      "time/training (s)                                       45.5561\n",
      "time/epoch (s)                                          46.7942\n",
      "time/total (s)                                        1411.75\n",
      "Epoch                                                   28\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:37:36.470574 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 29 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00143346\n",
      "trainer/QF2 Loss                                         0.00145426\n",
      "trainer/Policy Loss                                      3.40982\n",
      "trainer/Q1 Predictions Mean                             -1.34385\n",
      "trainer/Q1 Predictions Std                               0.794182\n",
      "trainer/Q1 Predictions Max                               0.338798\n",
      "trainer/Q1 Predictions Min                              -3.89966\n",
      "trainer/Q2 Predictions Mean                             -1.32362\n",
      "trainer/Q2 Predictions Std                               0.784791\n",
      "trainer/Q2 Predictions Max                               0.381921\n",
      "trainer/Q2 Predictions Min                              -3.87196\n",
      "trainer/Q Targets Mean                                  -1.33658\n",
      "trainer/Q Targets Std                                    0.784752\n",
      "trainer/Q Targets Max                                    0.396806\n",
      "trainer/Q Targets Min                                   -3.93741\n",
      "trainer/Log Pis Mean                                     2.08061\n",
      "trainer/Log Pis Std                                      1.28496\n",
      "trainer/Log Pis Max                                      4.50485\n",
      "trainer/Log Pis Min                                     -3.01126\n",
      "trainer/Policy mu Mean                                   0.00242612\n",
      "trainer/Policy mu Std                                    0.235283\n",
      "trainer/Policy mu Max                                    1.7354\n",
      "trainer/Policy mu Min                                   -1.14995\n",
      "trainer/Policy log std Mean                             -2.39065\n",
      "trainer/Policy log std Std                               0.500581\n",
      "trainer/Policy log std Max                              -0.535807\n",
      "trainer/Policy log std Min                              -3.18573\n",
      "trainer/Alpha                                            0.0207749\n",
      "trainer/Alpha Loss                                       0.312314\n",
      "exploration/num steps total                           4000\n",
      "exploration/num paths total                            200\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.101811\n",
      "exploration/Rewards Std                                  0.0752361\n",
      "exploration/Rewards Max                                  0.0854098\n",
      "exploration/Rewards Min                                 -0.320172\n",
      "exploration/Returns Mean                                -2.03623\n",
      "exploration/Returns Std                                  1.00636\n",
      "exploration/Returns Max                                 -0.826922\n",
      "exploration/Returns Min                                 -3.51336\n",
      "exploration/Actions Mean                                -0.00267165\n",
      "exploration/Actions Std                                  0.125441\n",
      "exploration/Actions Max                                  0.512903\n",
      "exploration/Actions Min                                 -0.451126\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.03623\n",
      "exploration/env_infos/final/reward_dist Mean             0.185554\n",
      "exploration/env_infos/final/reward_dist Std              0.364861\n",
      "exploration/env_infos/final/reward_dist Max              0.915219\n",
      "exploration/env_infos/final/reward_dist Min              1.05527e-18\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0103528\n",
      "exploration/env_infos/initial/reward_dist Std            0.0125724\n",
      "exploration/env_infos/initial/reward_dist Max            0.0339781\n",
      "exploration/env_infos/initial/reward_dist Min            0.000198093\n",
      "exploration/env_infos/reward_dist Mean                   0.134622\n",
      "exploration/env_infos/reward_dist Std                    0.249953\n",
      "exploration/env_infos/reward_dist Max                    0.96125\n",
      "exploration/env_infos/reward_dist Min                    1.05527e-18\n",
      "exploration/env_infos/final/reward_energy Mean          -0.112694\n",
      "exploration/env_infos/final/reward_energy Std            0.0892674\n",
      "exploration/env_infos/final/reward_energy Max           -0.0205583\n",
      "exploration/env_infos/final/reward_energy Min           -0.270084\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.295558\n",
      "exploration/env_infos/initial/reward_energy Std          0.193242\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0610989\n",
      "exploration/env_infos/initial/reward_energy Min         -0.634772\n",
      "exploration/env_infos/reward_energy Mean                -0.142418\n",
      "exploration/env_infos/reward_energy Std                  0.10584\n",
      "exploration/env_infos/reward_energy Max                 -0.00904316\n",
      "exploration/env_infos/reward_energy Min                 -0.634772\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0252763\n",
      "exploration/env_infos/final/end_effector_loc Std         0.299038\n",
      "exploration/env_infos/final/end_effector_loc Max         0.4248\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.673905\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00717461\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0102174\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0256451\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00838616\n",
      "exploration/env_infos/end_effector_loc Mean              0.0192251\n",
      "exploration/env_infos/end_effector_loc Std               0.195176\n",
      "exploration/env_infos/end_effector_loc Max               0.4248\n",
      "exploration/env_infos/end_effector_loc Min              -0.673905\n",
      "evaluation/num steps total                           30000\n",
      "evaluation/num paths total                            1500\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0476888\n",
      "evaluation/Rewards Std                                   0.076602\n",
      "evaluation/Rewards Max                                   0.137631\n",
      "evaluation/Rewards Min                                  -0.487326\n",
      "evaluation/Returns Mean                                 -0.953775\n",
      "evaluation/Returns Std                                   1.10766\n",
      "evaluation/Returns Max                                   1.03744\n",
      "evaluation/Returns Min                                  -2.87491\n",
      "evaluation/Actions Mean                                 -0.00248753\n",
      "evaluation/Actions Std                                   0.0801856\n",
      "evaluation/Actions Max                                   0.911033\n",
      "evaluation/Actions Min                                  -0.625356\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.953775\n",
      "evaluation/env_infos/final/reward_dist Mean              0.137634\n",
      "evaluation/env_infos/final/reward_dist Std               0.251972\n",
      "evaluation/env_infos/final/reward_dist Max               0.890035\n",
      "evaluation/env_infos/final/reward_dist Min               2.05399e-25\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00587226\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00960469\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0422461\n",
      "evaluation/env_infos/initial/reward_dist Min             2.35855e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.150229\n",
      "evaluation/env_infos/reward_dist Std                     0.243486\n",
      "evaluation/env_infos/reward_dist Max                     0.99847\n",
      "evaluation/env_infos/reward_dist Min                     2.05399e-25\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0898408\n",
      "evaluation/env_infos/final/reward_energy Std             0.0686247\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00632332\n",
      "evaluation/env_infos/final/reward_energy Min            -0.297953\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.205826\n",
      "evaluation/env_infos/initial/reward_energy Std           0.2116\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0105307\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.921956\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0734961\n",
      "evaluation/env_infos/reward_energy Std                   0.0864301\n",
      "evaluation/env_infos/reward_energy Max                  -0.00111476\n",
      "evaluation/env_infos/reward_energy Min                  -0.921956\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0226645\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.240071\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.466689\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.576811\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000952578\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0103931\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0455516\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0312678\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0100615\n",
      "evaluation/env_infos/end_effector_loc Std                0.157375\n",
      "evaluation/env_infos/end_effector_loc Max                0.49087\n",
      "evaluation/env_infos/end_effector_loc Min               -0.576811\n",
      "time/data storing (s)                                    0.00286261\n",
      "time/evaluation sampling (s)                             0.975112\n",
      "time/exploration sampling (s)                            0.120365\n",
      "time/logging (s)                                         0.0202353\n",
      "time/saving (s)                                          0.0297651\n",
      "time/training (s)                                       45.4521\n",
      "time/epoch (s)                                          46.6004\n",
      "time/total (s)                                        1458.77\n",
      "Epoch                                                   29\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:38:24.142208 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 30 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000868392\r\n",
      "trainer/QF2 Loss                                         0.00092665\r\n",
      "trainer/Policy Loss                                      3.17587\r\n",
      "trainer/Q1 Predictions Mean                             -1.23718\r\n",
      "trainer/Q1 Predictions Std                               0.813709\r\n",
      "trainer/Q1 Predictions Max                               0.556193\r\n",
      "trainer/Q1 Predictions Min                              -3.25137\r\n",
      "trainer/Q2 Predictions Mean                             -1.2578\r\n",
      "trainer/Q2 Predictions Std                               0.813467\r\n",
      "trainer/Q2 Predictions Max                               0.578312\r\n",
      "trainer/Q2 Predictions Min                              -3.3113\r\n",
      "trainer/Q Targets Mean                                  -1.25083\r\n",
      "trainer/Q Targets Std                                    0.816737\r\n",
      "trainer/Q Targets Max                                    0.578554\r\n",
      "trainer/Q Targets Min                                   -3.28503\r\n",
      "trainer/Log Pis Mean                                     1.92624\r\n",
      "trainer/Log Pis Std                                      1.33906\r\n",
      "trainer/Log Pis Max                                      4.33227\r\n",
      "trainer/Log Pis Min                                     -3.99861\r\n",
      "trainer/Policy mu Mean                                   0.0291556\r\n",
      "trainer/Policy mu Std                                    0.302697\r\n",
      "trainer/Policy mu Max                                    1.92191\r\n",
      "trainer/Policy mu Min                                   -1.52025\r\n",
      "trainer/Policy log std Mean                             -2.34375\r\n",
      "trainer/Policy log std Std                               0.552229\r\n",
      "trainer/Policy log std Max                              -0.321568\r\n",
      "trainer/Policy log std Min                              -3.19551\r\n",
      "trainer/Alpha                                            0.0219214\r\n",
      "trainer/Alpha Loss                                      -0.281782\r\n",
      "exploration/num steps total                           4100\r\n",
      "exploration/num paths total                            205\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.124742\r\n",
      "exploration/Rewards Std                                  0.101751\r\n",
      "exploration/Rewards Max                                  0.0233141\r\n",
      "exploration/Rewards Min                                 -0.566925\r\n",
      "exploration/Returns Mean                                -2.49483\r\n",
      "exploration/Returns Std                                  1.10729\r\n",
      "exploration/Returns Max                                 -0.990037\r\n",
      "exploration/Returns Min                                 -4.37202\r\n",
      "exploration/Actions Mean                                -0.0192899\r\n",
      "exploration/Actions Std                                  0.131652\r\n",
      "exploration/Actions Max                                  0.557396\r\n",
      "exploration/Actions Min                                 -0.515605\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.49483\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.000115024\r\n",
      "exploration/env_infos/final/reward_dist Std              0.000162676\r\n",
      "exploration/env_infos/final/reward_dist Max              0.000416184\r\n",
      "exploration/env_infos/final/reward_dist Min              3.0369e-69\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00445585\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00507897\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0132387\r\n",
      "exploration/env_infos/initial/reward_dist Min            1.48586e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.0473141\r\n",
      "exploration/env_infos/reward_dist Std                    0.132383\r\n",
      "exploration/env_infos/reward_dist Max                    0.820441\r\n",
      "exploration/env_infos/reward_dist Min                    3.0369e-69\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.176782\r\n",
      "exploration/env_infos/final/reward_energy Std            0.101689\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0817857\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.368716\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.11499\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.0772959\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.039025\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.209132\r\n",
      "exploration/env_infos/reward_energy Mean                -0.140006\r\n",
      "exploration/env_infos/reward_energy Std                  0.125726\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0039966\r\n",
      "exploration/env_infos/reward_energy Min                 -0.565691\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.188437\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.341598\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.329339\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.802257\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00087698\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0048195\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00969946\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00420323\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0700567\r\n",
      "exploration/env_infos/end_effector_loc Std               0.173904\r\n",
      "exploration/env_infos/end_effector_loc Max               0.329339\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.802257\r\n",
      "evaluation/num steps total                           31000\r\n",
      "evaluation/num paths total                            1550\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0729044\r\n",
      "evaluation/Rewards Std                                   0.0687716\r\n",
      "evaluation/Rewards Max                                   0.133651\r\n",
      "evaluation/Rewards Min                                  -0.363407\r\n",
      "evaluation/Returns Mean                                 -1.45809\r\n",
      "evaluation/Returns Std                                   0.958826\r\n",
      "evaluation/Returns Max                                   0.420791\r\n",
      "evaluation/Returns Min                                  -3.30132\r\n",
      "evaluation/Actions Mean                                  0.000209947\r\n",
      "evaluation/Actions Std                                   0.101427\r\n",
      "evaluation/Actions Max                                   0.954438\r\n",
      "evaluation/Actions Min                                  -0.886743\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.45809\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.172209\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.294937\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.900849\r\n",
      "evaluation/env_infos/final/reward_dist Min               5.27157e-26\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00578265\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0102414\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0462467\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.52378e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.121241\r\n",
      "evaluation/env_infos/reward_dist Std                     0.22821\r\n",
      "evaluation/env_infos/reward_dist Max                     0.992421\r\n",
      "evaluation/env_infos/reward_dist Min                     5.27157e-26\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0603641\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0601814\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00366965\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.284695\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.23473\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.289546\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00779645\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.30279\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.084739\r\n",
      "evaluation/env_infos/reward_energy Std                   0.115734\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00101156\r\n",
      "evaluation/env_infos/reward_energy Min                  -1.30279\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0283729\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.249677\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.542368\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.762873\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000518219\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0131681\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0477219\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0443371\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0177462\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.16411\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.548663\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.762873\r\n",
      "time/data storing (s)                                    0.00304684\r\n",
      "time/evaluation sampling (s)                             0.947271\r\n",
      "time/exploration sampling (s)                            0.126374\r\n",
      "time/logging (s)                                         0.0191306\r\n",
      "time/saving (s)                                          0.0305977\r\n",
      "time/training (s)                                       46.1618\r\n",
      "time/epoch (s)                                          47.2882\r\n",
      "time/total (s)                                        1506.44\r\n",
      "Epoch                                                   30\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:39:11.372864 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 31 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00105319\n",
      "trainer/QF2 Loss                                         0.00170622\n",
      "trainer/Policy Loss                                      3.33576\n",
      "trainer/Q1 Predictions Mean                             -1.30821\n",
      "trainer/Q1 Predictions Std                               0.841368\n",
      "trainer/Q1 Predictions Max                               0.773772\n",
      "trainer/Q1 Predictions Min                              -3.40724\n",
      "trainer/Q2 Predictions Mean                             -1.31752\n",
      "trainer/Q2 Predictions Std                               0.840216\n",
      "trainer/Q2 Predictions Max                               0.692052\n",
      "trainer/Q2 Predictions Min                              -3.38911\n",
      "trainer/Q Targets Mean                                  -1.30253\n",
      "trainer/Q Targets Std                                    0.842284\n",
      "trainer/Q Targets Max                                    0.78719\n",
      "trainer/Q Targets Min                                   -3.42878\n",
      "trainer/Log Pis Mean                                     2.02568\n",
      "trainer/Log Pis Std                                      1.28525\n",
      "trainer/Log Pis Max                                      4.27801\n",
      "trainer/Log Pis Min                                     -4.08691\n",
      "trainer/Policy mu Mean                                   0.0266079\n",
      "trainer/Policy mu Std                                    0.284967\n",
      "trainer/Policy mu Max                                    2.2716\n",
      "trainer/Policy mu Min                                   -1.65812\n",
      "trainer/Policy log std Mean                             -2.30338\n",
      "trainer/Policy log std Std                               0.558107\n",
      "trainer/Policy log std Max                              -0.192994\n",
      "trainer/Policy log std Min                              -3.22668\n",
      "trainer/Alpha                                            0.0208096\n",
      "trainer/Alpha Loss                                       0.0994424\n",
      "exploration/num steps total                           4200\n",
      "exploration/num paths total                            210\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.136397\n",
      "exploration/Rewards Std                                  0.0449336\n",
      "exploration/Rewards Max                                 -0.00982367\n",
      "exploration/Rewards Min                                 -0.268115\n",
      "exploration/Returns Mean                                -2.72794\n",
      "exploration/Returns Std                                  0.598944\n",
      "exploration/Returns Max                                 -1.74675\n",
      "exploration/Returns Min                                 -3.46367\n",
      "exploration/Actions Mean                                 0.00441041\n",
      "exploration/Actions Std                                  0.0754965\n",
      "exploration/Actions Max                                  0.279963\n",
      "exploration/Actions Min                                 -0.205511\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.72794\n",
      "exploration/env_infos/final/reward_dist Mean             0.023512\n",
      "exploration/env_infos/final/reward_dist Std              0.0355496\n",
      "exploration/env_infos/final/reward_dist Max              0.0927815\n",
      "exploration/env_infos/final/reward_dist Min              4.00104e-13\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0042544\n",
      "exploration/env_infos/initial/reward_dist Std            0.00673618\n",
      "exploration/env_infos/initial/reward_dist Max            0.0176893\n",
      "exploration/env_infos/initial/reward_dist Min            1.28221e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.0522036\n",
      "exploration/env_infos/reward_dist Std                    0.153622\n",
      "exploration/env_infos/reward_dist Max                    0.862431\n",
      "exploration/env_infos/reward_dist Min                    4.00104e-13\n",
      "exploration/env_infos/final/reward_energy Mean          -0.139108\n",
      "exploration/env_infos/final/reward_energy Std            0.0435108\n",
      "exploration/env_infos/final/reward_energy Max           -0.0916708\n",
      "exploration/env_infos/final/reward_energy Min           -0.206007\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.146633\n",
      "exploration/env_infos/initial/reward_energy Std          0.0673603\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0331241\n",
      "exploration/env_infos/initial/reward_energy Min         -0.233346\n",
      "exploration/env_infos/reward_energy Mean                -0.0953342\n",
      "exploration/env_infos/reward_energy Std                  0.0484742\n",
      "exploration/env_infos/reward_energy Max                 -0.0240482\n",
      "exploration/env_infos/reward_energy Min                 -0.293867\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0853826\n",
      "exploration/env_infos/final/end_effector_loc Std         0.222733\n",
      "exploration/env_infos/final/end_effector_loc Max         0.457031\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.169338\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00191643\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00537361\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0104349\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00677597\n",
      "exploration/env_infos/end_effector_loc Mean              0.040722\n",
      "exploration/env_infos/end_effector_loc Std               0.135335\n",
      "exploration/env_infos/end_effector_loc Max               0.457031\n",
      "exploration/env_infos/end_effector_loc Min              -0.169338\n",
      "evaluation/num steps total                           32000\n",
      "evaluation/num paths total                            1600\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0670496\n",
      "evaluation/Rewards Std                                   0.0762236\n",
      "evaluation/Rewards Max                                   0.138701\n",
      "evaluation/Rewards Min                                  -0.502041\n",
      "evaluation/Returns Mean                                 -1.34099\n",
      "evaluation/Returns Std                                   1.00427\n",
      "evaluation/Returns Max                                   0.819274\n",
      "evaluation/Returns Min                                  -3.85441\n",
      "evaluation/Actions Mean                                  0.00661723\n",
      "evaluation/Actions Std                                   0.0779997\n",
      "evaluation/Actions Max                                   0.724893\n",
      "evaluation/Actions Min                                  -0.605992\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.34099\n",
      "evaluation/env_infos/final/reward_dist Mean              0.10896\n",
      "evaluation/env_infos/final/reward_dist Std               0.199922\n",
      "evaluation/env_infos/final/reward_dist Max               0.896509\n",
      "evaluation/env_infos/final/reward_dist Min               1.64797e-26\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00685253\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0112947\n",
      "evaluation/env_infos/initial/reward_dist Max             0.043635\n",
      "evaluation/env_infos/initial/reward_dist Min             1.97466e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.152013\n",
      "evaluation/env_infos/reward_dist Std                     0.246042\n",
      "evaluation/env_infos/reward_dist Max                     0.995809\n",
      "evaluation/env_infos/reward_dist Min                     1.64797e-26\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0478993\n",
      "evaluation/env_infos/final/reward_energy Std             0.0527738\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00340879\n",
      "evaluation/env_infos/final/reward_energy Min            -0.231816\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.192832\n",
      "evaluation/env_infos/initial/reward_energy Std           0.189688\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0174097\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.81228\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0698787\n",
      "evaluation/env_infos/reward_energy Std                   0.085863\n",
      "evaluation/env_infos/reward_energy Max                  -0.00327803\n",
      "evaluation/env_infos/reward_energy Min                  -0.81228\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0637486\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.28143\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.812117\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.475605\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000404155\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00955479\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0362446\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0302996\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0207534\n",
      "evaluation/env_infos/end_effector_loc Std                0.177362\n",
      "evaluation/env_infos/end_effector_loc Max                0.812117\n",
      "evaluation/env_infos/end_effector_loc Min               -0.475605\n",
      "time/data storing (s)                                    0.00306083\n",
      "time/evaluation sampling (s)                             1.06272\n",
      "time/exploration sampling (s)                            0.125052\n",
      "time/logging (s)                                         0.0204754\n",
      "time/saving (s)                                          0.0293729\n",
      "time/training (s)                                       45.5864\n",
      "time/epoch (s)                                          46.8271\n",
      "time/total (s)                                        1553.67\n",
      "Epoch                                                   31\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:39:58.645036 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 32 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00075019\r\n",
      "trainer/QF2 Loss                                         0.000711884\r\n",
      "trainer/Policy Loss                                      3.25488\r\n",
      "trainer/Q1 Predictions Mean                             -1.25544\r\n",
      "trainer/Q1 Predictions Std                               0.785948\r\n",
      "trainer/Q1 Predictions Max                               0.821711\r\n",
      "trainer/Q1 Predictions Min                              -3.24505\r\n",
      "trainer/Q2 Predictions Mean                             -1.25161\r\n",
      "trainer/Q2 Predictions Std                               0.783929\r\n",
      "trainer/Q2 Predictions Max                               0.821986\r\n",
      "trainer/Q2 Predictions Min                              -3.24602\r\n",
      "trainer/Q Targets Mean                                  -1.2596\r\n",
      "trainer/Q Targets Std                                    0.792916\r\n",
      "trainer/Q Targets Max                                    0.840977\r\n",
      "trainer/Q Targets Min                                   -3.27868\r\n",
      "trainer/Log Pis Mean                                     2.01259\r\n",
      "trainer/Log Pis Std                                      1.27611\r\n",
      "trainer/Log Pis Max                                      4.23461\r\n",
      "trainer/Log Pis Min                                     -3.20308\r\n",
      "trainer/Policy mu Mean                                   0.0494255\r\n",
      "trainer/Policy mu Std                                    0.262388\r\n",
      "trainer/Policy mu Max                                    2.25552\r\n",
      "trainer/Policy mu Min                                   -0.902589\r\n",
      "trainer/Policy log std Mean                             -2.34858\r\n",
      "trainer/Policy log std Std                               0.543264\r\n",
      "trainer/Policy log std Max                              -0.265509\r\n",
      "trainer/Policy log std Min                              -3.14608\r\n",
      "trainer/Alpha                                            0.0229554\r\n",
      "trainer/Alpha Loss                                       0.0475195\r\n",
      "exploration/num steps total                           4300\r\n",
      "exploration/num paths total                            215\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.103459\r\n",
      "exploration/Rewards Std                                  0.0947882\r\n",
      "exploration/Rewards Max                                  0.081175\r\n",
      "exploration/Rewards Min                                 -0.336021\r\n",
      "exploration/Returns Mean                                -2.06919\r\n",
      "exploration/Returns Std                                  1.3674\r\n",
      "exploration/Returns Max                                  0.265208\r\n",
      "exploration/Returns Min                                 -3.75538\r\n",
      "exploration/Actions Mean                                 0.00442631\r\n",
      "exploration/Actions Std                                  0.203609\r\n",
      "exploration/Actions Max                                  0.910404\r\n",
      "exploration/Actions Min                                 -0.879292\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.06919\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0819507\r\n",
      "exploration/env_infos/final/reward_dist Std              0.103409\r\n",
      "exploration/env_infos/final/reward_dist Max              0.244303\r\n",
      "exploration/env_infos/final/reward_dist Min              1.84696e-20\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0172631\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0113907\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0363455\r\n",
      "exploration/env_infos/initial/reward_dist Min            0.00545669\r\n",
      "exploration/env_infos/reward_dist Mean                   0.0924413\r\n",
      "exploration/env_infos/reward_dist Std                    0.17842\r\n",
      "exploration/env_infos/reward_dist Max                    0.720027\r\n",
      "exploration/env_infos/reward_dist Min                    1.84696e-20\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.102541\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0596053\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0280562\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.210534\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.510908\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.487991\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0457979\r\n",
      "exploration/env_infos/initial/reward_energy Min         -1.2657\r\n",
      "exploration/env_infos/reward_energy Mean                -0.207598\r\n",
      "exploration/env_infos/reward_energy Std                  0.199638\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0112263\r\n",
      "exploration/env_infos/reward_energy Min                 -1.2657\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.185495\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.212326\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.72325\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.0334009\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00754701\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0238117\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0455202\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0439646\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.102439\r\n",
      "exploration/env_infos/end_effector_loc Std               0.204913\r\n",
      "exploration/env_infos/end_effector_loc Max               0.72325\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.424709\r\n",
      "evaluation/num steps total                           33000\r\n",
      "evaluation/num paths total                            1650\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0597704\r\n",
      "evaluation/Rewards Std                                   0.0790739\r\n",
      "evaluation/Rewards Max                                   0.122847\r\n",
      "evaluation/Rewards Min                                  -0.456664\r\n",
      "evaluation/Returns Mean                                 -1.19541\r\n",
      "evaluation/Returns Std                                   1.17868\r\n",
      "evaluation/Returns Max                                   1.16564\r\n",
      "evaluation/Returns Min                                  -3.30228\r\n",
      "evaluation/Actions Mean                                  0.00946841\r\n",
      "evaluation/Actions Std                                   0.0817082\r\n",
      "evaluation/Actions Max                                   0.98935\r\n",
      "evaluation/Actions Min                                  -0.758869\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.19541\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0795869\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.195023\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.952876\r\n",
      "evaluation/env_infos/final/reward_dist Min               6.67184e-31\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00708472\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0109615\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0487913\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.2844e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.137477\r\n",
      "evaluation/env_infos/reward_dist Std                     0.228536\r\n",
      "evaluation/env_infos/reward_dist Max                     0.990842\r\n",
      "evaluation/env_infos/reward_dist Min                     6.67184e-31\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0360925\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0434452\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00639578\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.262577\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.204623\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.221104\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0057304\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.24687\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0726766\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0908289\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00103516\r\n",
      "evaluation/env_infos/reward_energy Min                  -1.24687\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0655513\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.264282\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.805094\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.649783\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000438643\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0106421\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0494675\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0379435\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0121129\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.176808\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.805094\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.649783\r\n",
      "time/data storing (s)                                    0.00295029\r\n",
      "time/evaluation sampling (s)                             0.961795\r\n",
      "time/exploration sampling (s)                            0.12157\r\n",
      "time/logging (s)                                         0.0199748\r\n",
      "time/saving (s)                                          0.0304094\r\n",
      "time/training (s)                                       45.676\r\n",
      "time/epoch (s)                                          46.8127\r\n",
      "time/total (s)                                        1600.94\r\n",
      "Epoch                                                   32\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:40:47.116882 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 33 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00109556\n",
      "trainer/QF2 Loss                                         0.00108879\n",
      "trainer/Policy Loss                                      3.17819\n",
      "trainer/Q1 Predictions Mean                             -1.28395\n",
      "trainer/Q1 Predictions Std                               0.817455\n",
      "trainer/Q1 Predictions Max                               0.853905\n",
      "trainer/Q1 Predictions Min                              -3.31159\n",
      "trainer/Q2 Predictions Mean                             -1.29418\n",
      "trainer/Q2 Predictions Std                               0.816651\n",
      "trainer/Q2 Predictions Max                               0.871622\n",
      "trainer/Q2 Predictions Min                              -3.38168\n",
      "trainer/Q Targets Mean                                  -1.28498\n",
      "trainer/Q Targets Std                                    0.821735\n",
      "trainer/Q Targets Max                                    0.852357\n",
      "trainer/Q Targets Min                                   -3.3464\n",
      "trainer/Log Pis Mean                                     1.88772\n",
      "trainer/Log Pis Std                                      1.21953\n",
      "trainer/Log Pis Max                                      4.80274\n",
      "trainer/Log Pis Min                                     -2.60258\n",
      "trainer/Policy mu Mean                                   0.0277613\n",
      "trainer/Policy mu Std                                    0.277339\n",
      "trainer/Policy mu Max                                    2.29856\n",
      "trainer/Policy mu Min                                   -1.79327\n",
      "trainer/Policy log std Mean                             -2.29707\n",
      "trainer/Policy log std Std                               0.531072\n",
      "trainer/Policy log std Max                              -0.413334\n",
      "trainer/Policy log std Min                              -3.1734\n",
      "trainer/Alpha                                            0.021398\n",
      "trainer/Alpha Loss                                      -0.4316\n",
      "exploration/num steps total                           4400\n",
      "exploration/num paths total                            220\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.127169\n",
      "exploration/Rewards Std                                  0.068734\n",
      "exploration/Rewards Max                                 -0.0110828\n",
      "exploration/Rewards Min                                 -0.305158\n",
      "exploration/Returns Mean                                -2.54338\n",
      "exploration/Returns Std                                  0.540036\n",
      "exploration/Returns Max                                 -1.68848\n",
      "exploration/Returns Min                                 -3.21045\n",
      "exploration/Actions Mean                                 0.00333491\n",
      "exploration/Actions Std                                  0.1516\n",
      "exploration/Actions Max                                  0.584744\n",
      "exploration/Actions Min                                 -0.53535\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.54338\n",
      "exploration/env_infos/final/reward_dist Mean             0.00256698\n",
      "exploration/env_infos/final/reward_dist Std              0.00489217\n",
      "exploration/env_infos/final/reward_dist Max              0.0123439\n",
      "exploration/env_infos/final/reward_dist Min              5.6276e-14\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0066095\n",
      "exploration/env_infos/initial/reward_dist Std            0.00741077\n",
      "exploration/env_infos/initial/reward_dist Max            0.0163727\n",
      "exploration/env_infos/initial/reward_dist Min            0.000131801\n",
      "exploration/env_infos/reward_dist Mean                   0.0315535\n",
      "exploration/env_infos/reward_dist Std                    0.0797907\n",
      "exploration/env_infos/reward_dist Max                    0.393827\n",
      "exploration/env_infos/reward_dist Min                    5.6276e-14\n",
      "exploration/env_infos/final/reward_energy Mean          -0.161087\n",
      "exploration/env_infos/final/reward_energy Std            0.0405295\n",
      "exploration/env_infos/final/reward_energy Max           -0.101114\n",
      "exploration/env_infos/final/reward_energy Min           -0.221528\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.243577\n",
      "exploration/env_infos/initial/reward_energy Std          0.130683\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0813547\n",
      "exploration/env_infos/initial/reward_energy Min         -0.375911\n",
      "exploration/env_infos/reward_energy Mean                -0.163187\n",
      "exploration/env_infos/reward_energy Std                  0.139131\n",
      "exploration/env_infos/reward_energy Max                 -0.0153714\n",
      "exploration/env_infos/reward_energy Min                 -0.686673\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.127949\n",
      "exploration/env_infos/final/end_effector_loc Std         0.260978\n",
      "exploration/env_infos/final/end_effector_loc Max         0.269642\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.562699\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00716842\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00664254\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0018379\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0187743\n",
      "exploration/env_infos/end_effector_loc Mean             -0.111732\n",
      "exploration/env_infos/end_effector_loc Std               0.172079\n",
      "exploration/env_infos/end_effector_loc Max               0.269642\n",
      "exploration/env_infos/end_effector_loc Min              -0.562699\n",
      "evaluation/num steps total                           34000\n",
      "evaluation/num paths total                            1700\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0538329\n",
      "evaluation/Rewards Std                                   0.0730123\n",
      "evaluation/Rewards Max                                   0.140252\n",
      "evaluation/Rewards Min                                  -0.360975\n",
      "evaluation/Returns Mean                                 -1.07666\n",
      "evaluation/Returns Std                                   1.16411\n",
      "evaluation/Returns Max                                   1.23055\n",
      "evaluation/Returns Min                                  -3.85707\n",
      "evaluation/Actions Mean                                  0.00152176\n",
      "evaluation/Actions Std                                   0.0728253\n",
      "evaluation/Actions Max                                   0.945597\n",
      "evaluation/Actions Min                                  -0.339191\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.07666\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0862982\n",
      "evaluation/env_infos/final/reward_dist Std               0.187931\n",
      "evaluation/env_infos/final/reward_dist Max               0.879575\n",
      "evaluation/env_infos/final/reward_dist Min               4.95935e-25\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00678808\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0149283\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0788802\n",
      "evaluation/env_infos/initial/reward_dist Min             1.58291e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.140438\n",
      "evaluation/env_infos/reward_dist Std                     0.224561\n",
      "evaluation/env_infos/reward_dist Max                     0.991858\n",
      "evaluation/env_infos/reward_dist Min                     4.95935e-25\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0387068\n",
      "evaluation/env_infos/final/reward_energy Std             0.0371852\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00278421\n",
      "evaluation/env_infos/final/reward_energy Min            -0.177427\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.206786\n",
      "evaluation/env_infos/initial/reward_energy Std           0.230985\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0180013\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.967905\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0600531\n",
      "evaluation/env_infos/reward_energy Std                   0.0836978\n",
      "evaluation/env_infos/reward_energy Max                  -9.70881e-05\n",
      "evaluation/env_infos/reward_energy Min                  -0.967905\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.046359\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.254531\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.609016\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.544094\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00307715\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0105202\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0472798\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0169596\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0300307\n",
      "evaluation/env_infos/end_effector_loc Std                0.163301\n",
      "evaluation/env_infos/end_effector_loc Max                0.609016\n",
      "evaluation/env_infos/end_effector_loc Min               -0.544094\n",
      "time/data storing (s)                                    0.00296799\n",
      "time/evaluation sampling (s)                             1.1209\n",
      "time/exploration sampling (s)                            0.147016\n",
      "time/logging (s)                                         0.0228305\n",
      "time/saving (s)                                          0.0319142\n",
      "time/training (s)                                       46.7107\n",
      "time/epoch (s)                                          48.0363\n",
      "time/total (s)                                        1649.41\n",
      "Epoch                                                   33\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:41:34.940418 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 34 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00123744\n",
      "trainer/QF2 Loss                                         0.00117208\n",
      "trainer/Policy Loss                                      3.32772\n",
      "trainer/Q1 Predictions Mean                             -1.28273\n",
      "trainer/Q1 Predictions Std                               0.84898\n",
      "trainer/Q1 Predictions Max                               0.672572\n",
      "trainer/Q1 Predictions Min                              -3.11149\n",
      "trainer/Q2 Predictions Mean                             -1.275\n",
      "trainer/Q2 Predictions Std                               0.841071\n",
      "trainer/Q2 Predictions Max                               0.661838\n",
      "trainer/Q2 Predictions Min                              -3.11998\n",
      "trainer/Q Targets Mean                                  -1.27041\n",
      "trainer/Q Targets Std                                    0.845921\n",
      "trainer/Q Targets Max                                    0.656677\n",
      "trainer/Q Targets Min                                   -3.36522\n",
      "trainer/Log Pis Mean                                     2.04894\n",
      "trainer/Log Pis Std                                      1.10214\n",
      "trainer/Log Pis Max                                      4.00603\n",
      "trainer/Log Pis Min                                     -2.7506\n",
      "trainer/Policy mu Mean                                   0.0252811\n",
      "trainer/Policy mu Std                                    0.292933\n",
      "trainer/Policy mu Max                                    2.1663\n",
      "trainer/Policy mu Min                                   -1.55671\n",
      "trainer/Policy log std Mean                             -2.33273\n",
      "trainer/Policy log std Std                               0.54674\n",
      "trainer/Policy log std Max                               0.0783681\n",
      "trainer/Policy log std Min                              -3.25117\n",
      "trainer/Alpha                                            0.0206205\n",
      "trainer/Alpha Loss                                       0.189974\n",
      "exploration/num steps total                           4500\n",
      "exploration/num paths total                            225\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.10358\n",
      "exploration/Rewards Std                                  0.0530624\n",
      "exploration/Rewards Max                                 -0.010319\n",
      "exploration/Rewards Min                                 -0.260495\n",
      "exploration/Returns Mean                                -2.0716\n",
      "exploration/Returns Std                                  0.605178\n",
      "exploration/Returns Max                                 -1.2873\n",
      "exploration/Returns Min                                 -2.7749\n",
      "exploration/Actions Mean                                 0.00846258\n",
      "exploration/Actions Std                                  0.136673\n",
      "exploration/Actions Max                                  0.659234\n",
      "exploration/Actions Min                                 -0.568087\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.0716\n",
      "exploration/env_infos/final/reward_dist Mean             0.015277\n",
      "exploration/env_infos/final/reward_dist Std              0.0221676\n",
      "exploration/env_infos/final/reward_dist Max              0.0570412\n",
      "exploration/env_infos/final/reward_dist Min              2.64592e-11\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00646447\n",
      "exploration/env_infos/initial/reward_dist Std            0.0128014\n",
      "exploration/env_infos/initial/reward_dist Max            0.0320671\n",
      "exploration/env_infos/initial/reward_dist Min            1.75148e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.110006\n",
      "exploration/env_infos/reward_dist Std                    0.197375\n",
      "exploration/env_infos/reward_dist Max                    0.914457\n",
      "exploration/env_infos/reward_dist Min                    2.64592e-11\n",
      "exploration/env_infos/final/reward_energy Mean          -0.13615\n",
      "exploration/env_infos/final/reward_energy Std            0.140604\n",
      "exploration/env_infos/final/reward_energy Max           -0.00226691\n",
      "exploration/env_infos/final/reward_energy Min           -0.382436\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.147605\n",
      "exploration/env_infos/initial/reward_energy Std          0.0844157\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0599493\n",
      "exploration/env_infos/initial/reward_energy Min         -0.280021\n",
      "exploration/env_infos/reward_energy Mean                -0.145985\n",
      "exploration/env_infos/reward_energy Std                  0.127242\n",
      "exploration/env_infos/reward_energy Max                 -0.00226691\n",
      "exploration/env_infos/reward_energy Min                 -0.666892\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.136851\n",
      "exploration/env_infos/final/end_effector_loc Std         0.227358\n",
      "exploration/env_infos/final/end_effector_loc Max         0.489115\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.27221\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000436647\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00599591\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0118256\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0100258\n",
      "exploration/env_infos/end_effector_loc Mean              0.0616875\n",
      "exploration/env_infos/end_effector_loc Std               0.162682\n",
      "exploration/env_infos/end_effector_loc Max               0.489115\n",
      "exploration/env_infos/end_effector_loc Min              -0.324256\n",
      "evaluation/num steps total                           35000\n",
      "evaluation/num paths total                            1750\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0720636\n",
      "evaluation/Rewards Std                                   0.0964161\n",
      "evaluation/Rewards Max                                   0.12641\n",
      "evaluation/Rewards Min                                  -0.842333\n",
      "evaluation/Returns Mean                                 -1.44127\n",
      "evaluation/Returns Std                                   1.38452\n",
      "evaluation/Returns Max                                   1.29367\n",
      "evaluation/Returns Min                                  -6.45703\n",
      "evaluation/Actions Mean                                  0.00649545\n",
      "evaluation/Actions Std                                   0.0819222\n",
      "evaluation/Actions Max                                   0.986867\n",
      "evaluation/Actions Min                                  -0.536424\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.44127\n",
      "evaluation/env_infos/final/reward_dist Mean              0.138868\n",
      "evaluation/env_infos/final/reward_dist Std               0.222794\n",
      "evaluation/env_infos/final/reward_dist Max               0.889928\n",
      "evaluation/env_infos/final/reward_dist Min               1.6727e-116\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00768201\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0117812\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0425665\n",
      "evaluation/env_infos/initial/reward_dist Min             9.07773e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.155045\n",
      "evaluation/env_infos/reward_dist Std                     0.245595\n",
      "evaluation/env_infos/reward_dist Max                     0.976948\n",
      "evaluation/env_infos/reward_dist Min                     1.6727e-116\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0393703\n",
      "evaluation/env_infos/final/reward_energy Std             0.0433413\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00190631\n",
      "evaluation/env_infos/final/reward_energy Min            -0.25295\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.192133\n",
      "evaluation/env_infos/initial/reward_energy Std           0.259236\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00540779\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.09411\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0612043\n",
      "evaluation/env_infos/reward_energy Std                   0.0987973\n",
      "evaluation/env_infos/reward_energy Max                  -0.00190631\n",
      "evaluation/env_infos/reward_energy Min                  -1.09411\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.113839\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.264476\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.688718\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00374914\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0107746\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0493433\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0268212\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0567187\n",
      "evaluation/env_infos/end_effector_loc Std                0.165499\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -0.688718\n",
      "time/data storing (s)                                    0.00296333\n",
      "time/evaluation sampling (s)                             1.05178\n",
      "time/exploration sampling (s)                            0.122944\n",
      "time/logging (s)                                         0.020316\n",
      "time/saving (s)                                          0.0307414\n",
      "time/training (s)                                       46.0807\n",
      "time/epoch (s)                                          47.3095\n",
      "time/total (s)                                        1697.23\n",
      "Epoch                                                   34\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:42:22.490832 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 35 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000918973\n",
      "trainer/QF2 Loss                                         0.000874934\n",
      "trainer/Policy Loss                                      3.22342\n",
      "trainer/Q1 Predictions Mean                             -1.30244\n",
      "trainer/Q1 Predictions Std                               0.844705\n",
      "trainer/Q1 Predictions Max                               0.869148\n",
      "trainer/Q1 Predictions Min                              -3.63314\n",
      "trainer/Q2 Predictions Mean                             -1.2915\n",
      "trainer/Q2 Predictions Std                               0.846021\n",
      "trainer/Q2 Predictions Max                               0.877048\n",
      "trainer/Q2 Predictions Min                              -3.64759\n",
      "trainer/Q Targets Mean                                  -1.29583\n",
      "trainer/Q Targets Std                                    0.846769\n",
      "trainer/Q Targets Max                                    0.850652\n",
      "trainer/Q Targets Min                                   -3.65207\n",
      "trainer/Log Pis Mean                                     1.936\n",
      "trainer/Log Pis Std                                      1.23043\n",
      "trainer/Log Pis Max                                      4.27421\n",
      "trainer/Log Pis Min                                     -2.82327\n",
      "trainer/Policy mu Mean                                   0.0236413\n",
      "trainer/Policy mu Std                                    0.314129\n",
      "trainer/Policy mu Max                                    2.25861\n",
      "trainer/Policy mu Min                                   -1.72985\n",
      "trainer/Policy log std Mean                             -2.27345\n",
      "trainer/Policy log std Std                               0.557868\n",
      "trainer/Policy log std Max                              -0.3534\n",
      "trainer/Policy log std Min                              -3.12462\n",
      "trainer/Alpha                                            0.0210955\n",
      "trainer/Alpha Loss                                      -0.246942\n",
      "exploration/num steps total                           4600\n",
      "exploration/num paths total                            230\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0795748\n",
      "exploration/Rewards Std                                  0.0944403\n",
      "exploration/Rewards Max                                  0.112282\n",
      "exploration/Rewards Min                                 -0.341413\n",
      "exploration/Returns Mean                                -1.5915\n",
      "exploration/Returns Std                                  1.63505\n",
      "exploration/Returns Max                                  1.13\n",
      "exploration/Returns Min                                 -3.5346\n",
      "exploration/Actions Mean                                -0.00763083\n",
      "exploration/Actions Std                                  0.13031\n",
      "exploration/Actions Max                                  0.43694\n",
      "exploration/Actions Min                                 -0.492049\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.5915\n",
      "exploration/env_infos/final/reward_dist Mean             0.00227609\n",
      "exploration/env_infos/final/reward_dist Std              0.00438705\n",
      "exploration/env_infos/final/reward_dist Max              0.011047\n",
      "exploration/env_infos/final/reward_dist Min              6.17515e-12\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00723898\n",
      "exploration/env_infos/initial/reward_dist Std            0.0141428\n",
      "exploration/env_infos/initial/reward_dist Max            0.0355232\n",
      "exploration/env_infos/initial/reward_dist Min            1.29277e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.149889\n",
      "exploration/env_infos/reward_dist Std                    0.230354\n",
      "exploration/env_infos/reward_dist Max                    0.808675\n",
      "exploration/env_infos/reward_dist Min                    6.17515e-12\n",
      "exploration/env_infos/final/reward_energy Mean          -0.215276\n",
      "exploration/env_infos/final/reward_energy Std            0.159544\n",
      "exploration/env_infos/final/reward_energy Max           -0.0520282\n",
      "exploration/env_infos/final/reward_energy Min           -0.492142\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.270284\n",
      "exploration/env_infos/initial/reward_energy Std          0.145091\n",
      "exploration/env_infos/initial/reward_energy Max         -0.12556\n",
      "exploration/env_infos/initial/reward_energy Min         -0.540685\n",
      "exploration/env_infos/reward_energy Mean                -0.153594\n",
      "exploration/env_infos/reward_energy Std                  0.102404\n",
      "exploration/env_infos/reward_energy Max                 -0.0113371\n",
      "exploration/env_infos/reward_energy Min                 -0.540685\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0141\n",
      "exploration/env_infos/final/end_effector_loc Std         0.224407\n",
      "exploration/env_infos/final/end_effector_loc Max         0.305665\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.411739\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00540735\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0094017\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.021847\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0063274\n",
      "exploration/env_infos/end_effector_loc Mean              0.00832377\n",
      "exploration/env_infos/end_effector_loc Std               0.139918\n",
      "exploration/env_infos/end_effector_loc Max               0.305665\n",
      "exploration/env_infos/end_effector_loc Min              -0.411739\n",
      "evaluation/num steps total                           36000\n",
      "evaluation/num paths total                            1800\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0726216\n",
      "evaluation/Rewards Std                                   0.0805609\n",
      "evaluation/Rewards Max                                   0.143964\n",
      "evaluation/Rewards Min                                  -0.567815\n",
      "evaluation/Returns Mean                                 -1.45243\n",
      "evaluation/Returns Std                                   1.29939\n",
      "evaluation/Returns Max                                   1.10687\n",
      "evaluation/Returns Min                                  -4.63886\n",
      "evaluation/Actions Mean                                 -0.000746167\n",
      "evaluation/Actions Std                                   0.0728619\n",
      "evaluation/Actions Max                                   0.743739\n",
      "evaluation/Actions Min                                  -0.597957\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.45243\n",
      "evaluation/env_infos/final/reward_dist Mean              0.125576\n",
      "evaluation/env_infos/final/reward_dist Std               0.212716\n",
      "evaluation/env_infos/final/reward_dist Max               0.86688\n",
      "evaluation/env_infos/final/reward_dist Min               2.93355e-37\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00533571\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0116613\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0751584\n",
      "evaluation/env_infos/initial/reward_dist Min             1.45765e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.134894\n",
      "evaluation/env_infos/reward_dist Std                     0.23056\n",
      "evaluation/env_infos/reward_dist Max                     0.997463\n",
      "evaluation/env_infos/reward_dist Min                     2.93355e-37\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0482072\n",
      "evaluation/env_infos/final/reward_energy Std             0.0466425\n",
      "evaluation/env_infos/final/reward_energy Max            -0.000284555\n",
      "evaluation/env_infos/final/reward_energy Min            -0.253728\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.1745\n",
      "evaluation/env_infos/initial/reward_energy Std           0.204381\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0187585\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.834149\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0644544\n",
      "evaluation/env_infos/reward_energy Std                   0.0804018\n",
      "evaluation/env_infos/reward_energy Max                  -0.000284555\n",
      "evaluation/env_infos/reward_energy Min                  -0.834149\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0342238\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.26522\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.442488\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.641251\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000869935\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00946152\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.037187\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0298979\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0184406\n",
      "evaluation/env_infos/end_effector_loc Std                0.17705\n",
      "evaluation/env_infos/end_effector_loc Max                0.442488\n",
      "evaluation/env_infos/end_effector_loc Min               -0.689336\n",
      "time/data storing (s)                                    0.00296179\n",
      "time/evaluation sampling (s)                             0.952262\n",
      "time/exploration sampling (s)                            0.121938\n",
      "time/logging (s)                                         0.0202302\n",
      "time/saving (s)                                          0.0306381\n",
      "time/training (s)                                       45.9729\n",
      "time/epoch (s)                                          47.101\n",
      "time/total (s)                                        1744.78\n",
      "Epoch                                                   35\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:43:11.129649 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 36 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000605631\n",
      "trainer/QF2 Loss                                         0.000626211\n",
      "trainer/Policy Loss                                      3.27922\n",
      "trainer/Q1 Predictions Mean                             -1.2923\n",
      "trainer/Q1 Predictions Std                               0.770117\n",
      "trainer/Q1 Predictions Max                               0.316088\n",
      "trainer/Q1 Predictions Min                              -3.11537\n",
      "trainer/Q2 Predictions Mean                             -1.28661\n",
      "trainer/Q2 Predictions Std                               0.76713\n",
      "trainer/Q2 Predictions Max                               0.320936\n",
      "trainer/Q2 Predictions Min                              -3.06836\n",
      "trainer/Q Targets Mean                                  -1.29106\n",
      "trainer/Q Targets Std                                    0.770174\n",
      "trainer/Q Targets Max                                    0.33578\n",
      "trainer/Q Targets Min                                   -3.12505\n",
      "trainer/Log Pis Mean                                     1.99836\n",
      "trainer/Log Pis Std                                      1.30368\n",
      "trainer/Log Pis Max                                      4.56045\n",
      "trainer/Log Pis Min                                     -2.66712\n",
      "trainer/Policy mu Mean                                   0.0320942\n",
      "trainer/Policy mu Std                                    0.265762\n",
      "trainer/Policy mu Max                                    2.19052\n",
      "trainer/Policy mu Min                                   -1.17318\n",
      "trainer/Policy log std Mean                             -2.34607\n",
      "trainer/Policy log std Std                               0.539033\n",
      "trainer/Policy log std Max                              -0.520894\n",
      "trainer/Policy log std Min                              -3.2558\n",
      "trainer/Alpha                                            0.020808\n",
      "trainer/Alpha Loss                                      -0.0063619\n",
      "exploration/num steps total                           4700\n",
      "exploration/num paths total                            235\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.122402\n",
      "exploration/Rewards Std                                  0.0694997\n",
      "exploration/Rewards Max                                  0.0336042\n",
      "exploration/Rewards Min                                 -0.351463\n",
      "exploration/Returns Mean                                -2.44804\n",
      "exploration/Returns Std                                  0.916239\n",
      "exploration/Returns Max                                 -0.971969\n",
      "exploration/Returns Min                                 -3.5443\n",
      "exploration/Actions Mean                                 0.00713647\n",
      "exploration/Actions Std                                  0.0897322\n",
      "exploration/Actions Max                                  0.462318\n",
      "exploration/Actions Min                                 -0.26242\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.44804\n",
      "exploration/env_infos/final/reward_dist Mean             0.0353323\n",
      "exploration/env_infos/final/reward_dist Std              0.0706645\n",
      "exploration/env_infos/final/reward_dist Max              0.176661\n",
      "exploration/env_infos/final/reward_dist Min              2.54528e-33\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0045831\n",
      "exploration/env_infos/initial/reward_dist Std            0.0081152\n",
      "exploration/env_infos/initial/reward_dist Max            0.020783\n",
      "exploration/env_infos/initial/reward_dist Min            1.58025e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0461514\n",
      "exploration/env_infos/reward_dist Std                    0.153784\n",
      "exploration/env_infos/reward_dist Max                    0.838086\n",
      "exploration/env_infos/reward_dist Min                    2.54528e-33\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0893728\n",
      "exploration/env_infos/final/reward_energy Std            0.0278585\n",
      "exploration/env_infos/final/reward_energy Max           -0.0431968\n",
      "exploration/env_infos/final/reward_energy Min           -0.123315\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.15062\n",
      "exploration/env_infos/initial/reward_energy Std          0.158345\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0282364\n",
      "exploration/env_infos/initial/reward_energy Min         -0.462479\n",
      "exploration/env_infos/reward_energy Mean                -0.0997835\n",
      "exploration/env_infos/reward_energy Std                  0.0790497\n",
      "exploration/env_infos/reward_energy Max                 -0.00578606\n",
      "exploration/env_infos/reward_energy Min                 -0.462479\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0663997\n",
      "exploration/env_infos/final/end_effector_loc Std         0.275907\n",
      "exploration/env_infos/final/end_effector_loc Max         0.493978\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.392582\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00188392\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00749334\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0231159\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00475101\n",
      "exploration/env_infos/end_effector_loc Mean              0.0265999\n",
      "exploration/env_infos/end_effector_loc Std               0.160198\n",
      "exploration/env_infos/end_effector_loc Max               0.493978\n",
      "exploration/env_infos/end_effector_loc Min              -0.392582\n",
      "evaluation/num steps total                           37000\n",
      "evaluation/num paths total                            1850\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0675885\n",
      "evaluation/Rewards Std                                   0.0827899\n",
      "evaluation/Rewards Max                                   0.160845\n",
      "evaluation/Rewards Min                                  -0.480762\n",
      "evaluation/Returns Mean                                 -1.35177\n",
      "evaluation/Returns Std                                   1.32298\n",
      "evaluation/Returns Max                                   1.52933\n",
      "evaluation/Returns Min                                  -4.65521\n",
      "evaluation/Actions Mean                                  0.00619177\n",
      "evaluation/Actions Std                                   0.0895627\n",
      "evaluation/Actions Max                                   0.933886\n",
      "evaluation/Actions Min                                  -0.648239\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.35177\n",
      "evaluation/env_infos/final/reward_dist Mean              0.130794\n",
      "evaluation/env_infos/final/reward_dist Std               0.245266\n",
      "evaluation/env_infos/final/reward_dist Max               0.967942\n",
      "evaluation/env_infos/final/reward_dist Min               3.13799e-53\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00716443\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00966465\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0367492\n",
      "evaluation/env_infos/initial/reward_dist Min             1.39948e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.134682\n",
      "evaluation/env_infos/reward_dist Std                     0.22906\n",
      "evaluation/env_infos/reward_dist Max                     0.997701\n",
      "evaluation/env_infos/reward_dist Min                     3.13799e-53\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0406197\n",
      "evaluation/env_infos/final/reward_energy Std             0.035818\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00428035\n",
      "evaluation/env_infos/final/reward_energy Min            -0.205486\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.246199\n",
      "evaluation/env_infos/initial/reward_energy Std           0.261537\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0156449\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.97082\n",
      "evaluation/env_infos/reward_energy Mean                 -0.075024\n",
      "evaluation/env_infos/reward_energy Std                   0.102426\n",
      "evaluation/env_infos/reward_energy Max                  -0.00118013\n",
      "evaluation/env_infos/reward_energy Min                  -0.97082\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.105188\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.247231\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.941617\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.448258\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00412429\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0120108\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0466943\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0324119\n",
      "evaluation/env_infos/end_effector_loc Mean               0.055323\n",
      "evaluation/env_infos/end_effector_loc Std                0.176673\n",
      "evaluation/env_infos/end_effector_loc Max                0.941617\n",
      "evaluation/env_infos/end_effector_loc Min               -0.562808\n",
      "time/data storing (s)                                    0.00302539\n",
      "time/evaluation sampling (s)                             1.05598\n",
      "time/exploration sampling (s)                            0.124818\n",
      "time/logging (s)                                         0.0215925\n",
      "time/saving (s)                                          0.0293568\n",
      "time/training (s)                                       46.9494\n",
      "time/epoch (s)                                          48.1842\n",
      "time/total (s)                                        1793.42\n",
      "Epoch                                                   36\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:43:59.137022 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 37 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000478914\n",
      "trainer/QF2 Loss                                         0.00061002\n",
      "trainer/Policy Loss                                      3.20777\n",
      "trainer/Q1 Predictions Mean                             -1.25784\n",
      "trainer/Q1 Predictions Std                               0.780462\n",
      "trainer/Q1 Predictions Max                               0.10543\n",
      "trainer/Q1 Predictions Min                              -3.36135\n",
      "trainer/Q2 Predictions Mean                             -1.26037\n",
      "trainer/Q2 Predictions Std                               0.784712\n",
      "trainer/Q2 Predictions Max                               0.0904833\n",
      "trainer/Q2 Predictions Min                              -3.37937\n",
      "trainer/Q Targets Mean                                  -1.25452\n",
      "trainer/Q Targets Std                                    0.78277\n",
      "trainer/Q Targets Max                                    0.140852\n",
      "trainer/Q Targets Min                                   -3.3592\n",
      "trainer/Log Pis Mean                                     1.95308\n",
      "trainer/Log Pis Std                                      1.39233\n",
      "trainer/Log Pis Max                                      4.49226\n",
      "trainer/Log Pis Min                                     -4.11478\n",
      "trainer/Policy mu Mean                                   0.0185315\n",
      "trainer/Policy mu Std                                    0.29171\n",
      "trainer/Policy mu Max                                    2.04366\n",
      "trainer/Policy mu Min                                   -1.71033\n",
      "trainer/Policy log std Mean                             -2.35222\n",
      "trainer/Policy log std Std                               0.553983\n",
      "trainer/Policy log std Max                              -0.417703\n",
      "trainer/Policy log std Min                              -3.30163\n",
      "trainer/Alpha                                            0.0210571\n",
      "trainer/Alpha Loss                                      -0.181161\n",
      "exploration/num steps total                           4800\n",
      "exploration/num paths total                            240\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.114128\n",
      "exploration/Rewards Std                                  0.073588\n",
      "exploration/Rewards Max                                  0.024674\n",
      "exploration/Rewards Min                                 -0.338285\n",
      "exploration/Returns Mean                                -2.28257\n",
      "exploration/Returns Std                                  1.04006\n",
      "exploration/Returns Max                                 -0.948907\n",
      "exploration/Returns Min                                 -4.10179\n",
      "exploration/Actions Mean                                 0.005233\n",
      "exploration/Actions Std                                  0.0944893\n",
      "exploration/Actions Max                                  0.429682\n",
      "exploration/Actions Min                                 -0.38747\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.28257\n",
      "exploration/env_infos/final/reward_dist Mean             0.213596\n",
      "exploration/env_infos/final/reward_dist Std              0.373236\n",
      "exploration/env_infos/final/reward_dist Max              0.956741\n",
      "exploration/env_infos/final/reward_dist Min              3.59703e-06\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00584195\n",
      "exploration/env_infos/initial/reward_dist Std            0.011241\n",
      "exploration/env_infos/initial/reward_dist Max            0.0283149\n",
      "exploration/env_infos/initial/reward_dist Min            1.49293e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.171957\n",
      "exploration/env_infos/reward_dist Std                    0.277847\n",
      "exploration/env_infos/reward_dist Max                    0.99032\n",
      "exploration/env_infos/reward_dist Min                    1.49293e-06\n",
      "exploration/env_infos/final/reward_energy Mean          -0.112556\n",
      "exploration/env_infos/final/reward_energy Std            0.0197922\n",
      "exploration/env_infos/final/reward_energy Max           -0.0876452\n",
      "exploration/env_infos/final/reward_energy Min           -0.143331\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.17816\n",
      "exploration/env_infos/initial/reward_energy Std          0.138614\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0575201\n",
      "exploration/env_infos/initial/reward_energy Min         -0.440084\n",
      "exploration/env_infos/reward_energy Mean                -0.113082\n",
      "exploration/env_infos/reward_energy Std                  0.0715795\n",
      "exploration/env_infos/reward_energy Max                 -0.0170477\n",
      "exploration/env_infos/reward_energy Min                 -0.440084\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.100949\n",
      "exploration/env_infos/final/end_effector_loc Std         0.208494\n",
      "exploration/env_infos/final/end_effector_loc Max         0.348982\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.255912\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.0045759\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00653871\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0214841\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00248805\n",
      "exploration/env_infos/end_effector_loc Mean              0.0566068\n",
      "exploration/env_infos/end_effector_loc Std               0.1448\n",
      "exploration/env_infos/end_effector_loc Max               0.348982\n",
      "exploration/env_infos/end_effector_loc Min              -0.255912\n",
      "evaluation/num steps total                           38000\n",
      "evaluation/num paths total                            1900\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.072401\n",
      "evaluation/Rewards Std                                   0.0788888\n",
      "evaluation/Rewards Max                                   0.145412\n",
      "evaluation/Rewards Min                                  -0.451531\n",
      "evaluation/Returns Mean                                 -1.44802\n",
      "evaluation/Returns Std                                   1.25622\n",
      "evaluation/Returns Max                                   1.63671\n",
      "evaluation/Returns Min                                  -4.53131\n",
      "evaluation/Actions Mean                                  0.00359544\n",
      "evaluation/Actions Std                                   0.0750294\n",
      "evaluation/Actions Max                                   0.889578\n",
      "evaluation/Actions Min                                  -0.556226\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.44802\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0830005\n",
      "evaluation/env_infos/final/reward_dist Std               0.186767\n",
      "evaluation/env_infos/final/reward_dist Max               0.916391\n",
      "evaluation/env_infos/final/reward_dist Min               2.58885e-29\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00542448\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0089922\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0453284\n",
      "evaluation/env_infos/initial/reward_dist Min             1.62572e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.14984\n",
      "evaluation/env_infos/reward_dist Std                     0.254803\n",
      "evaluation/env_infos/reward_dist Max                     0.995023\n",
      "evaluation/env_infos/reward_dist Min                     2.58885e-29\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0627122\n",
      "evaluation/env_infos/final/reward_energy Std             0.0710985\n",
      "evaluation/env_infos/final/reward_energy Max            -0.003445\n",
      "evaluation/env_infos/final/reward_energy Min            -0.378386\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.202046\n",
      "evaluation/env_infos/initial/reward_energy Std           0.23204\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.000982548\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.04916\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0649262\n",
      "evaluation/env_infos/reward_energy Std                   0.084079\n",
      "evaluation/env_infos/reward_energy Max                  -0.000982548\n",
      "evaluation/env_infos/reward_energy Min                  -1.04916\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0927944\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.231278\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.592756\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.416722\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00228665\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.010635\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0444789\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0278113\n",
      "evaluation/env_infos/end_effector_loc Mean               0.045263\n",
      "evaluation/env_infos/end_effector_loc Std                0.152889\n",
      "evaluation/env_infos/end_effector_loc Max                0.592756\n",
      "evaluation/env_infos/end_effector_loc Min               -0.416722\n",
      "time/data storing (s)                                    0.00291485\n",
      "time/evaluation sampling (s)                             1.04358\n",
      "time/exploration sampling (s)                            0.122627\n",
      "time/logging (s)                                         0.019785\n",
      "time/saving (s)                                          0.0298258\n",
      "time/training (s)                                       46.2463\n",
      "time/epoch (s)                                          47.465\n",
      "time/total (s)                                        1841.43\n",
      "Epoch                                                   37\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:44:46.977338 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 38 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000709893\n",
      "trainer/QF2 Loss                                         0.000669694\n",
      "trainer/Policy Loss                                      3.16473\n",
      "trainer/Q1 Predictions Mean                             -1.24494\n",
      "trainer/Q1 Predictions Std                               0.806771\n",
      "trainer/Q1 Predictions Max                               0.823547\n",
      "trainer/Q1 Predictions Min                              -3.09551\n",
      "trainer/Q2 Predictions Mean                             -1.24495\n",
      "trainer/Q2 Predictions Std                               0.808075\n",
      "trainer/Q2 Predictions Max                               0.825101\n",
      "trainer/Q2 Predictions Min                              -3.06\n",
      "trainer/Q Targets Mean                                  -1.2452\n",
      "trainer/Q Targets Std                                    0.810376\n",
      "trainer/Q Targets Max                                    0.823385\n",
      "trainer/Q Targets Min                                   -3.14826\n",
      "trainer/Log Pis Mean                                     1.91563\n",
      "trainer/Log Pis Std                                      1.34852\n",
      "trainer/Log Pis Max                                      4.1649\n",
      "trainer/Log Pis Min                                     -4.68074\n",
      "trainer/Policy mu Mean                                   0.0130495\n",
      "trainer/Policy mu Std                                    0.230862\n",
      "trainer/Policy mu Max                                    1.7075\n",
      "trainer/Policy mu Min                                   -1.62628\n",
      "trainer/Policy log std Mean                             -2.36426\n",
      "trainer/Policy log std Std                               0.534829\n",
      "trainer/Policy log std Max                              -0.507135\n",
      "trainer/Policy log std Min                              -3.12252\n",
      "trainer/Alpha                                            0.0216825\n",
      "trainer/Alpha Loss                                      -0.323071\n",
      "exploration/num steps total                           4900\n",
      "exploration/num paths total                            245\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.119867\n",
      "exploration/Rewards Std                                  0.0591079\n",
      "exploration/Rewards Max                                  0.00324871\n",
      "exploration/Rewards Min                                 -0.337458\n",
      "exploration/Returns Mean                                -2.39735\n",
      "exploration/Returns Std                                  0.67777\n",
      "exploration/Returns Max                                 -1.43607\n",
      "exploration/Returns Min                                 -3.05605\n",
      "exploration/Actions Mean                                -0.00741399\n",
      "exploration/Actions Std                                  0.145643\n",
      "exploration/Actions Max                                  0.538114\n",
      "exploration/Actions Min                                 -0.873221\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.39735\n",
      "exploration/env_infos/final/reward_dist Mean             0.0037306\n",
      "exploration/env_infos/final/reward_dist Std              0.00720544\n",
      "exploration/env_infos/final/reward_dist Max              0.0181367\n",
      "exploration/env_infos/final/reward_dist Min              2.23936e-23\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00439563\n",
      "exploration/env_infos/initial/reward_dist Std            0.00501227\n",
      "exploration/env_infos/initial/reward_dist Max            0.0115229\n",
      "exploration/env_infos/initial/reward_dist Min            2.31067e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.0215425\n",
      "exploration/env_infos/reward_dist Std                    0.045021\n",
      "exploration/env_infos/reward_dist Max                    0.219322\n",
      "exploration/env_infos/reward_dist Min                    2.23936e-23\n",
      "exploration/env_infos/final/reward_energy Mean          -0.118173\n",
      "exploration/env_infos/final/reward_energy Std            0.0338859\n",
      "exploration/env_infos/final/reward_energy Max           -0.0563318\n",
      "exploration/env_infos/final/reward_energy Min           -0.159307\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.418842\n",
      "exploration/env_infos/initial/reward_energy Std          0.30533\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0627019\n",
      "exploration/env_infos/initial/reward_energy Min         -0.912623\n",
      "exploration/env_infos/reward_energy Mean                -0.151197\n",
      "exploration/env_infos/reward_energy Std                  0.140261\n",
      "exploration/env_infos/reward_energy Max                 -0.0100802\n",
      "exploration/env_infos/reward_energy Min                 -0.912623\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0723041\n",
      "exploration/env_infos/final/end_effector_loc Std         0.301271\n",
      "exploration/env_infos/final/end_effector_loc Max         0.572492\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.542393\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00326211\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0180327\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0211642\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0436611\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0334776\n",
      "exploration/env_infos/end_effector_loc Std               0.208733\n",
      "exploration/env_infos/end_effector_loc Max               0.572492\n",
      "exploration/env_infos/end_effector_loc Min              -0.542393\n",
      "evaluation/num steps total                           39000\n",
      "evaluation/num paths total                            1950\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0677237\n",
      "evaluation/Rewards Std                                   0.0785379\n",
      "evaluation/Rewards Max                                   0.15477\n",
      "evaluation/Rewards Min                                  -0.485431\n",
      "evaluation/Returns Mean                                 -1.35447\n",
      "evaluation/Returns Std                                   1.24179\n",
      "evaluation/Returns Max                                   1.4804\n",
      "evaluation/Returns Min                                  -3.63833\n",
      "evaluation/Actions Mean                                  0.00500738\n",
      "evaluation/Actions Std                                   0.0877938\n",
      "evaluation/Actions Max                                   0.788563\n",
      "evaluation/Actions Min                                  -0.710088\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.35447\n",
      "evaluation/env_infos/final/reward_dist Mean              0.115904\n",
      "evaluation/env_infos/final/reward_dist Std               0.197942\n",
      "evaluation/env_infos/final/reward_dist Max               0.849919\n",
      "evaluation/env_infos/final/reward_dist Min               2.40591e-15\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00524921\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0122458\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0767013\n",
      "evaluation/env_infos/initial/reward_dist Min             1.37391e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.154826\n",
      "evaluation/env_infos/reward_dist Std                     0.241374\n",
      "evaluation/env_infos/reward_dist Max                     0.996201\n",
      "evaluation/env_infos/reward_dist Min                     2.40591e-15\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0340604\n",
      "evaluation/env_infos/final/reward_energy Std             0.0304063\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00191928\n",
      "evaluation/env_infos/final/reward_energy Min            -0.177506\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.300135\n",
      "evaluation/env_infos/initial/reward_energy Std           0.258532\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00992488\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.955547\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0760907\n",
      "evaluation/env_infos/reward_energy Std                   0.0983659\n",
      "evaluation/env_infos/reward_energy Max                  -0.00136889\n",
      "evaluation/env_infos/reward_energy Min                  -0.955547\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0665309\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.238011\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.631617\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.506816\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000112451\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0140049\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0394282\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0355044\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0230084\n",
      "evaluation/env_infos/end_effector_loc Std                0.168105\n",
      "evaluation/env_infos/end_effector_loc Max                0.631617\n",
      "evaluation/env_infos/end_effector_loc Min               -0.506816\n",
      "time/data storing (s)                                    0.00289482\n",
      "time/evaluation sampling (s)                             0.95601\n",
      "time/exploration sampling (s)                            0.127868\n",
      "time/logging (s)                                         0.0198221\n",
      "time/saving (s)                                          0.0291193\n",
      "time/training (s)                                       46.1928\n",
      "time/epoch (s)                                          47.3285\n",
      "time/total (s)                                        1889.27\n",
      "Epoch                                                   38\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:45:35.112980 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 39 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000599681\n",
      "trainer/QF2 Loss                                         0.000993493\n",
      "trainer/Policy Loss                                      3.20033\n",
      "trainer/Q1 Predictions Mean                             -1.17695\n",
      "trainer/Q1 Predictions Std                               0.742562\n",
      "trainer/Q1 Predictions Max                               0.465623\n",
      "trainer/Q1 Predictions Min                              -3.06184\n",
      "trainer/Q2 Predictions Mean                             -1.17315\n",
      "trainer/Q2 Predictions Std                               0.741192\n",
      "trainer/Q2 Predictions Max                               0.458362\n",
      "trainer/Q2 Predictions Min                              -3.05362\n",
      "trainer/Q Targets Mean                                  -1.18372\n",
      "trainer/Q Targets Std                                    0.739572\n",
      "trainer/Q Targets Max                                    0.453457\n",
      "trainer/Q Targets Min                                   -3.05665\n",
      "trainer/Log Pis Mean                                     2.02124\n",
      "trainer/Log Pis Std                                      1.40537\n",
      "trainer/Log Pis Max                                      4.15086\n",
      "trainer/Log Pis Min                                     -3.56272\n",
      "trainer/Policy mu Mean                                  -0.0114388\n",
      "trainer/Policy mu Std                                    0.193637\n",
      "trainer/Policy mu Max                                    1.02487\n",
      "trainer/Policy mu Min                                   -1.74578\n",
      "trainer/Policy log std Mean                             -2.40552\n",
      "trainer/Policy log std Std                               0.533291\n",
      "trainer/Policy log std Max                              -0.620102\n",
      "trainer/Policy log std Min                              -3.16634\n",
      "trainer/Alpha                                            0.0212451\n",
      "trainer/Alpha Loss                                       0.0818544\n",
      "exploration/num steps total                           5000\n",
      "exploration/num paths total                            250\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0878613\n",
      "exploration/Rewards Std                                  0.0479503\n",
      "exploration/Rewards Max                                  0.000648297\n",
      "exploration/Rewards Min                                 -0.231784\n",
      "exploration/Returns Mean                                -1.75723\n",
      "exploration/Returns Std                                  0.620113\n",
      "exploration/Returns Max                                 -0.96165\n",
      "exploration/Returns Min                                 -2.62138\n",
      "exploration/Actions Mean                                -0.000760485\n",
      "exploration/Actions Std                                  0.076859\n",
      "exploration/Actions Max                                  0.243629\n",
      "exploration/Actions Min                                 -0.286061\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.75723\n",
      "exploration/env_infos/final/reward_dist Mean             0.144432\n",
      "exploration/env_infos/final/reward_dist Std              0.144781\n",
      "exploration/env_infos/final/reward_dist Max              0.377852\n",
      "exploration/env_infos/final/reward_dist Min              1.58067e-10\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00690956\n",
      "exploration/env_infos/initial/reward_dist Std            0.0049987\n",
      "exploration/env_infos/initial/reward_dist Max            0.0140139\n",
      "exploration/env_infos/initial/reward_dist Min            6.5385e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0743844\n",
      "exploration/env_infos/reward_dist Std                    0.114904\n",
      "exploration/env_infos/reward_dist Max                    0.448983\n",
      "exploration/env_infos/reward_dist Min                    1.58067e-10\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0575939\n",
      "exploration/env_infos/final/reward_energy Std            0.0319958\n",
      "exploration/env_infos/final/reward_energy Max           -0.0294491\n",
      "exploration/env_infos/final/reward_energy Min           -0.107363\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.0802943\n",
      "exploration/env_infos/initial/reward_energy Std          0.0554765\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0246659\n",
      "exploration/env_infos/initial/reward_energy Min         -0.148315\n",
      "exploration/env_infos/reward_energy Mean                -0.0932643\n",
      "exploration/env_infos/reward_energy Std                  0.055835\n",
      "exploration/env_infos/reward_energy Max                 -0.0177855\n",
      "exploration/env_infos/reward_energy Min                 -0.289577\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.00213125\n",
      "exploration/env_infos/final/end_effector_loc Std         0.172134\n",
      "exploration/env_infos/final/end_effector_loc Max         0.198807\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.341642\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000158937\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00344685\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00496141\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00630682\n",
      "exploration/env_infos/end_effector_loc Mean              0.00260502\n",
      "exploration/env_infos/end_effector_loc Std               0.109488\n",
      "exploration/env_infos/end_effector_loc Max               0.240129\n",
      "exploration/env_infos/end_effector_loc Min              -0.341642\n",
      "evaluation/num steps total                           40000\n",
      "evaluation/num paths total                            2000\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0620722\n",
      "evaluation/Rewards Std                                   0.0652007\n",
      "evaluation/Rewards Max                                   0.113681\n",
      "evaluation/Rewards Min                                  -0.295942\n",
      "evaluation/Returns Mean                                 -1.24144\n",
      "evaluation/Returns Std                                   1.04463\n",
      "evaluation/Returns Max                                   1.14738\n",
      "evaluation/Returns Min                                  -3.26202\n",
      "evaluation/Actions Mean                                 -0.000104147\n",
      "evaluation/Actions Std                                   0.0592643\n",
      "evaluation/Actions Max                                   0.581588\n",
      "evaluation/Actions Min                                  -0.670397\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.24144\n",
      "evaluation/env_infos/final/reward_dist Mean              0.217677\n",
      "evaluation/env_infos/final/reward_dist Std               0.308197\n",
      "evaluation/env_infos/final/reward_dist Max               0.980079\n",
      "evaluation/env_infos/final/reward_dist Min               1.10536e-12\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00620091\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0153886\n",
      "evaluation/env_infos/initial/reward_dist Max             0.080859\n",
      "evaluation/env_infos/initial/reward_dist Min             1.59024e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.189278\n",
      "evaluation/env_infos/reward_dist Std                     0.277066\n",
      "evaluation/env_infos/reward_dist Max                     0.995948\n",
      "evaluation/env_infos/reward_dist Min                     1.10536e-12\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0319272\n",
      "evaluation/env_infos/final/reward_energy Std             0.0403296\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00124271\n",
      "evaluation/env_infos/final/reward_energy Min            -0.207715\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.147048\n",
      "evaluation/env_infos/initial/reward_energy Std           0.152158\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0134551\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.685786\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0508773\n",
      "evaluation/env_infos/reward_energy Std                   0.0666036\n",
      "evaluation/env_infos/reward_energy Max                  -0.000832061\n",
      "evaluation/env_infos/reward_energy Min                  -0.685786\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.00149691\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.212658\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.388273\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.535446\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000127981\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00748014\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0290794\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0321602\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00113944\n",
      "evaluation/env_infos/end_effector_loc Std                0.139535\n",
      "evaluation/env_infos/end_effector_loc Max                0.388273\n",
      "evaluation/env_infos/end_effector_loc Min               -0.535446\n",
      "time/data storing (s)                                    0.00302447\n",
      "time/evaluation sampling (s)                             0.954889\n",
      "time/exploration sampling (s)                            0.120833\n",
      "time/logging (s)                                         0.0186417\n",
      "time/saving (s)                                          0.0289551\n",
      "time/training (s)                                       46.5005\n",
      "time/epoch (s)                                          47.6268\n",
      "time/total (s)                                        1937.4\n",
      "Epoch                                                   39\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:46:23.103550 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 40 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000472922\r\n",
      "trainer/QF2 Loss                                         0.000627347\r\n",
      "trainer/Policy Loss                                      3.57672\r\n",
      "trainer/Q1 Predictions Mean                             -1.24836\r\n",
      "trainer/Q1 Predictions Std                               0.849226\r\n",
      "trainer/Q1 Predictions Max                               0.812357\r\n",
      "trainer/Q1 Predictions Min                              -3.49059\r\n",
      "trainer/Q2 Predictions Mean                             -1.25727\r\n",
      "trainer/Q2 Predictions Std                               0.854329\r\n",
      "trainer/Q2 Predictions Max                               0.795855\r\n",
      "trainer/Q2 Predictions Min                              -3.52104\r\n",
      "trainer/Q Targets Mean                                  -1.25259\r\n",
      "trainer/Q Targets Std                                    0.852995\r\n",
      "trainer/Q Targets Max                                    0.815927\r\n",
      "trainer/Q Targets Min                                   -3.53714\r\n",
      "trainer/Log Pis Mean                                     2.33122\r\n",
      "trainer/Log Pis Std                                      1.14259\r\n",
      "trainer/Log Pis Max                                      4.52423\r\n",
      "trainer/Log Pis Min                                     -1.42159\r\n",
      "trainer/Policy mu Mean                                   0.00227673\r\n",
      "trainer/Policy mu Std                                    0.181763\r\n",
      "trainer/Policy mu Max                                    0.959961\r\n",
      "trainer/Policy mu Min                                   -1.69594\r\n",
      "trainer/Policy log std Mean                             -2.48799\r\n",
      "trainer/Policy log std Std                               0.468314\r\n",
      "trainer/Policy log std Max                              -0.298582\r\n",
      "trainer/Policy log std Min                              -3.27929\r\n",
      "trainer/Alpha                                            0.0219262\r\n",
      "trainer/Alpha Loss                                       1.26528\r\n",
      "exploration/num steps total                           5100\r\n",
      "exploration/num paths total                            255\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.128128\r\n",
      "exploration/Rewards Std                                  0.0565535\r\n",
      "exploration/Rewards Max                                  0.0141675\r\n",
      "exploration/Rewards Min                                 -0.276301\r\n",
      "exploration/Returns Mean                                -2.56256\r\n",
      "exploration/Returns Std                                  0.769104\r\n",
      "exploration/Returns Max                                 -1.10142\r\n",
      "exploration/Returns Min                                 -3.21845\r\n",
      "exploration/Actions Mean                                 0.00499555\r\n",
      "exploration/Actions Std                                  0.174426\r\n",
      "exploration/Actions Max                                  0.543405\r\n",
      "exploration/Actions Min                                 -0.684274\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.56256\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.166963\r\n",
      "exploration/env_infos/final/reward_dist Std              0.307587\r\n",
      "exploration/env_infos/final/reward_dist Max              0.780948\r\n",
      "exploration/env_infos/final/reward_dist Min              1.14756e-11\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00702498\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0139059\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0348364\r\n",
      "exploration/env_infos/initial/reward_dist Min            1.38913e-05\r\n",
      "exploration/env_infos/reward_dist Mean                   0.104056\r\n",
      "exploration/env_infos/reward_dist Std                    0.198211\r\n",
      "exploration/env_infos/reward_dist Max                    0.84455\r\n",
      "exploration/env_infos/reward_dist Min                    1.14756e-11\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0685416\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0528541\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.00923155\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.166851\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.266674\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.252018\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0588264\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.684386\r\n",
      "exploration/env_infos/reward_energy Mean                -0.18475\r\n",
      "exploration/env_infos/reward_energy Std                  0.163605\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00923155\r\n",
      "exploration/env_infos/reward_energy Min                 -0.72312\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0322486\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.262746\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.37748\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.454455\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00541435\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0117885\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00395661\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0342137\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0513177\r\n",
      "exploration/env_infos/end_effector_loc Std               0.135811\r\n",
      "exploration/env_infos/end_effector_loc Max               0.37748\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.454455\r\n",
      "evaluation/num steps total                           41000\r\n",
      "evaluation/num paths total                            2050\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0448905\r\n",
      "evaluation/Rewards Std                                   0.0774582\r\n",
      "evaluation/Rewards Max                                   0.142465\r\n",
      "evaluation/Rewards Min                                  -0.273529\r\n",
      "evaluation/Returns Mean                                 -0.89781\r\n",
      "evaluation/Returns Std                                   1.22358\r\n",
      "evaluation/Returns Max                                   1.57651\r\n",
      "evaluation/Returns Min                                  -3.41745\r\n",
      "evaluation/Actions Mean                                  0.00543332\r\n",
      "evaluation/Actions Std                                   0.0591869\r\n",
      "evaluation/Actions Max                                   0.397638\r\n",
      "evaluation/Actions Min                                  -0.451406\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.89781\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.199916\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.286096\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.950568\r\n",
      "evaluation/env_infos/final/reward_dist Min               9.71141e-43\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00522038\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00776596\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0300724\r\n",
      "evaluation/env_infos/initial/reward_dist Min             9.61035e-07\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.19379\r\n",
      "evaluation/env_infos/reward_dist Std                     0.275412\r\n",
      "evaluation/env_infos/reward_dist Max                     0.999862\r\n",
      "evaluation/env_infos/reward_dist Min                     9.71141e-43\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0355357\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.026707\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00295833\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.106319\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.155964\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.127927\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0117579\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.480844\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.056391\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0623319\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000854299\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.484473\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0627012\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.24452\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.852845\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.4289\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000719107\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00709545\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0198819\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0225703\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0231385\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.154933\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.852845\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.4289\r\n",
      "time/data storing (s)                                    0.00286824\r\n",
      "time/evaluation sampling (s)                             0.948204\r\n",
      "time/exploration sampling (s)                            0.120813\r\n",
      "time/logging (s)                                         0.0196841\r\n",
      "time/saving (s)                                          0.0327873\r\n",
      "time/training (s)                                       46.3597\r\n",
      "time/epoch (s)                                          47.4841\r\n",
      "time/total (s)                                        1985.39\r\n",
      "Epoch                                                   40\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:47:11.348564 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 41 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000747012\n",
      "trainer/QF2 Loss                                         0.000716691\n",
      "trainer/Policy Loss                                      3.22332\n",
      "trainer/Q1 Predictions Mean                             -1.24725\n",
      "trainer/Q1 Predictions Std                               0.819687\n",
      "trainer/Q1 Predictions Max                               0.372572\n",
      "trainer/Q1 Predictions Min                              -3.02345\n",
      "trainer/Q2 Predictions Mean                             -1.24745\n",
      "trainer/Q2 Predictions Std                               0.817364\n",
      "trainer/Q2 Predictions Max                               0.35331\n",
      "trainer/Q2 Predictions Min                              -3.00914\n",
      "trainer/Q Targets Mean                                  -1.24107\n",
      "trainer/Q Targets Std                                    0.815637\n",
      "trainer/Q Targets Max                                    0.345243\n",
      "trainer/Q Targets Min                                   -2.98973\n",
      "trainer/Log Pis Mean                                     1.9767\n",
      "trainer/Log Pis Std                                      1.37722\n",
      "trainer/Log Pis Max                                      4.21065\n",
      "trainer/Log Pis Min                                     -3.74701\n",
      "trainer/Policy mu Mean                                  -0.014389\n",
      "trainer/Policy mu Std                                    0.226946\n",
      "trainer/Policy mu Max                                    0.952786\n",
      "trainer/Policy mu Min                                   -1.92989\n",
      "trainer/Policy log std Mean                             -2.3508\n",
      "trainer/Policy log std Std                               0.525453\n",
      "trainer/Policy log std Max                              -0.118352\n",
      "trainer/Policy log std Min                              -3.08316\n",
      "trainer/Alpha                                            0.0222943\n",
      "trainer/Alpha Loss                                      -0.088621\n",
      "exploration/num steps total                           5200\n",
      "exploration/num paths total                            260\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.108586\n",
      "exploration/Rewards Std                                  0.089061\n",
      "exploration/Rewards Max                                  0.0780773\n",
      "exploration/Rewards Min                                 -0.363096\n",
      "exploration/Returns Mean                                -2.17172\n",
      "exploration/Returns Std                                  0.476525\n",
      "exploration/Returns Max                                 -1.513\n",
      "exploration/Returns Min                                 -2.99671\n",
      "exploration/Actions Mean                                -0.000347208\n",
      "exploration/Actions Std                                  0.126289\n",
      "exploration/Actions Max                                  0.595977\n",
      "exploration/Actions Min                                 -0.368301\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.17172\n",
      "exploration/env_infos/final/reward_dist Mean             0.0515242\n",
      "exploration/env_infos/final/reward_dist Std              0.096562\n",
      "exploration/env_infos/final/reward_dist Max              0.244467\n",
      "exploration/env_infos/final/reward_dist Min              1.37169e-21\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00943608\n",
      "exploration/env_infos/initial/reward_dist Std            0.00567965\n",
      "exploration/env_infos/initial/reward_dist Max            0.0159991\n",
      "exploration/env_infos/initial/reward_dist Min            0.000409653\n",
      "exploration/env_infos/reward_dist Mean                   0.161945\n",
      "exploration/env_infos/reward_dist Std                    0.253269\n",
      "exploration/env_infos/reward_dist Max                    0.988925\n",
      "exploration/env_infos/reward_dist Min                    1.37169e-21\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0975759\n",
      "exploration/env_infos/final/reward_energy Std            0.0689862\n",
      "exploration/env_infos/final/reward_energy Max           -0.0261655\n",
      "exploration/env_infos/final/reward_energy Min           -0.22667\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.186507\n",
      "exploration/env_infos/initial/reward_energy Std          0.112063\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0618262\n",
      "exploration/env_infos/initial/reward_energy Min         -0.368315\n",
      "exploration/env_infos/reward_energy Mean                -0.141516\n",
      "exploration/env_infos/reward_energy Std                  0.108955\n",
      "exploration/env_infos/reward_energy Max                 -0.00648332\n",
      "exploration/env_infos/reward_energy Min                 -0.597092\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0242527\n",
      "exploration/env_infos/final/end_effector_loc Std         0.294609\n",
      "exploration/env_infos/final/end_effector_loc Max         0.661142\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.379402\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000176617\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00769073\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0130303\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.018415\n",
      "exploration/env_infos/end_effector_loc Mean              0.00394223\n",
      "exploration/env_infos/end_effector_loc Std               0.200957\n",
      "exploration/env_infos/end_effector_loc Max               0.670875\n",
      "exploration/env_infos/end_effector_loc Min              -0.379402\n",
      "evaluation/num steps total                           42000\n",
      "evaluation/num paths total                            2100\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0378611\n",
      "evaluation/Rewards Std                                   0.0735909\n",
      "evaluation/Rewards Max                                   0.168612\n",
      "evaluation/Rewards Min                                  -0.359448\n",
      "evaluation/Returns Mean                                 -0.757221\n",
      "evaluation/Returns Std                                   1.16719\n",
      "evaluation/Returns Max                                   2.23396\n",
      "evaluation/Returns Min                                  -3.95632\n",
      "evaluation/Actions Mean                                  0.00121715\n",
      "evaluation/Actions Std                                   0.0748966\n",
      "evaluation/Actions Max                                   0.696414\n",
      "evaluation/Actions Min                                  -0.63322\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.757221\n",
      "evaluation/env_infos/final/reward_dist Mean              0.153036\n",
      "evaluation/env_infos/final/reward_dist Std               0.245556\n",
      "evaluation/env_infos/final/reward_dist Max               0.984794\n",
      "evaluation/env_infos/final/reward_dist Min               7.12841e-24\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00544539\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0114839\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0635339\n",
      "evaluation/env_infos/initial/reward_dist Min             4.06273e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.162361\n",
      "evaluation/env_infos/reward_dist Std                     0.244943\n",
      "evaluation/env_infos/reward_dist Max                     0.986074\n",
      "evaluation/env_infos/reward_dist Min                     7.12841e-24\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0454797\n",
      "evaluation/env_infos/final/reward_energy Std             0.0661946\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00126028\n",
      "evaluation/env_infos/final/reward_energy Min            -0.334627\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.22456\n",
      "evaluation/env_infos/initial/reward_energy Std           0.217492\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0186805\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.748676\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0646599\n",
      "evaluation/env_infos/reward_energy Std                   0.0839111\n",
      "evaluation/env_infos/reward_energy Max                  -0.00113334\n",
      "evaluation/env_infos/reward_energy Min                  -0.748676\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0621651\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.220084\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.633095\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.505645\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.0027727\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0106993\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0348207\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.031661\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0388569\n",
      "evaluation/env_infos/end_effector_loc Std                0.148087\n",
      "evaluation/env_infos/end_effector_loc Max                0.633095\n",
      "evaluation/env_infos/end_effector_loc Min               -0.505645\n",
      "time/data storing (s)                                    0.00307745\n",
      "time/evaluation sampling (s)                             1.03052\n",
      "time/exploration sampling (s)                            0.123711\n",
      "time/logging (s)                                         0.0193409\n",
      "time/saving (s)                                          0.0289266\n",
      "time/training (s)                                       46.5024\n",
      "time/epoch (s)                                          47.708\n",
      "time/total (s)                                        2033.63\n",
      "Epoch                                                   41\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:47:59.555582 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 42 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000850917\r\n",
      "trainer/QF2 Loss                                         0.000875528\r\n",
      "trainer/Policy Loss                                      3.30849\r\n",
      "trainer/Q1 Predictions Mean                             -1.26606\r\n",
      "trainer/Q1 Predictions Std                               0.840976\r\n",
      "trainer/Q1 Predictions Max                               0.398435\r\n",
      "trainer/Q1 Predictions Min                              -3.44398\r\n",
      "trainer/Q2 Predictions Mean                             -1.27072\r\n",
      "trainer/Q2 Predictions Std                               0.83733\r\n",
      "trainer/Q2 Predictions Max                               0.383272\r\n",
      "trainer/Q2 Predictions Min                              -3.41526\r\n",
      "trainer/Q Targets Mean                                  -1.26838\r\n",
      "trainer/Q Targets Std                                    0.842464\r\n",
      "trainer/Q Targets Max                                    0.432687\r\n",
      "trainer/Q Targets Min                                   -3.4265\r\n",
      "trainer/Log Pis Mean                                     2.04169\r\n",
      "trainer/Log Pis Std                                      1.29696\r\n",
      "trainer/Log Pis Max                                      4.18876\r\n",
      "trainer/Log Pis Min                                     -3.54574\r\n",
      "trainer/Policy mu Mean                                  -0.0471084\r\n",
      "trainer/Policy mu Std                                    0.2494\r\n",
      "trainer/Policy mu Max                                    0.849315\r\n",
      "trainer/Policy mu Min                                   -2.06625\r\n",
      "trainer/Policy log std Mean                             -2.34516\r\n",
      "trainer/Policy log std Std                               0.555005\r\n",
      "trainer/Policy log std Max                              -0.40944\r\n",
      "trainer/Policy log std Min                              -3.24337\r\n",
      "trainer/Alpha                                            0.0223974\r\n",
      "trainer/Alpha Loss                                       0.158394\r\n",
      "exploration/num steps total                           5300\r\n",
      "exploration/num paths total                            265\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0431316\r\n",
      "exploration/Rewards Std                                  0.0976292\r\n",
      "exploration/Rewards Max                                  0.156133\r\n",
      "exploration/Rewards Min                                 -0.231852\r\n",
      "exploration/Returns Mean                                -0.862632\r\n",
      "exploration/Returns Std                                  1.60861\r\n",
      "exploration/Returns Max                                  1.11907\r\n",
      "exploration/Returns Min                                 -2.83472\r\n",
      "exploration/Actions Mean                                 0.00843724\r\n",
      "exploration/Actions Std                                  0.146911\r\n",
      "exploration/Actions Max                                  0.72211\r\n",
      "exploration/Actions Min                                 -0.54174\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -0.862632\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.20898\r\n",
      "exploration/env_infos/final/reward_dist Std              0.371134\r\n",
      "exploration/env_infos/final/reward_dist Max              0.950795\r\n",
      "exploration/env_infos/final/reward_dist Min              0.0104473\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0069302\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.010399\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0274205\r\n",
      "exploration/env_infos/initial/reward_dist Min            8.05678e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.255152\r\n",
      "exploration/env_infos/reward_dist Std                    0.292324\r\n",
      "exploration/env_infos/reward_dist Max                    0.994649\r\n",
      "exploration/env_infos/reward_dist Min                    3.32693e-07\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.150789\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0299189\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0921139\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.17491\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.217276\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.20601\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0943882\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.628005\r\n",
      "exploration/env_infos/reward_energy Mean                -0.166247\r\n",
      "exploration/env_infos/reward_energy Std                  0.125181\r\n",
      "exploration/env_infos/reward_energy Max                 -0.013504\r\n",
      "exploration/env_infos/reward_energy Min                 -0.726907\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0832773\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.20932\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.367984\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.253348\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00335396\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0100405\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0264398\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00558222\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.0477682\r\n",
      "exploration/env_infos/end_effector_loc Std               0.142517\r\n",
      "exploration/env_infos/end_effector_loc Max               0.367984\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.279784\r\n",
      "evaluation/num steps total                           43000\r\n",
      "evaluation/num paths total                            2150\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0523175\r\n",
      "evaluation/Rewards Std                                   0.0680046\r\n",
      "evaluation/Rewards Max                                   0.113837\r\n",
      "evaluation/Rewards Min                                  -0.316292\r\n",
      "evaluation/Returns Mean                                 -1.04635\r\n",
      "evaluation/Returns Std                                   1.03263\r\n",
      "evaluation/Returns Max                                   1.00249\r\n",
      "evaluation/Returns Min                                  -3.51896\r\n",
      "evaluation/Actions Mean                                 -0.00314619\r\n",
      "evaluation/Actions Std                                   0.0586854\r\n",
      "evaluation/Actions Max                                   0.371142\r\n",
      "evaluation/Actions Min                                  -0.53612\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.04635\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.119154\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.202417\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.834441\r\n",
      "evaluation/env_infos/final/reward_dist Min               9.55731e-22\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00609618\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0104669\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0528491\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.54296e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.138369\r\n",
      "evaluation/env_infos/reward_dist Std                     0.221431\r\n",
      "evaluation/env_infos/reward_dist Max                     0.995074\r\n",
      "evaluation/env_infos/reward_dist Min                     9.55731e-22\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0400891\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0471676\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00107949\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.247729\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.153027\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.132133\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0169855\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.560632\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0537658\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0633797\r\n",
      "evaluation/env_infos/reward_energy Max                  -8.70494e-05\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.593021\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0523264\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.251819\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.66553\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.578466\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00152484\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00698358\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0185571\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.026806\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0283597\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.155945\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.66553\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.578466\r\n",
      "time/data storing (s)                                    0.00285412\r\n",
      "time/evaluation sampling (s)                             1.05903\r\n",
      "time/exploration sampling (s)                            0.121892\r\n",
      "time/logging (s)                                         0.0190323\r\n",
      "time/saving (s)                                          0.028856\r\n",
      "time/training (s)                                       46.3763\r\n",
      "time/epoch (s)                                          47.608\r\n",
      "time/total (s)                                        2081.84\r\n",
      "Epoch                                                   42\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:48:48.560189 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 43 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000701614\r\n",
      "trainer/QF2 Loss                                         0.000675195\r\n",
      "trainer/Policy Loss                                      3.07476\r\n",
      "trainer/Q1 Predictions Mean                             -1.2415\r\n",
      "trainer/Q1 Predictions Std                               0.786815\r\n",
      "trainer/Q1 Predictions Max                               0.240751\r\n",
      "trainer/Q1 Predictions Min                              -3.42513\r\n",
      "trainer/Q2 Predictions Mean                             -1.24719\r\n",
      "trainer/Q2 Predictions Std                               0.788789\r\n",
      "trainer/Q2 Predictions Max                               0.24001\r\n",
      "trainer/Q2 Predictions Min                              -3.4348\r\n",
      "trainer/Q Targets Mean                                  -1.24297\r\n",
      "trainer/Q Targets Std                                    0.793317\r\n",
      "trainer/Q Targets Max                                    0.26578\r\n",
      "trainer/Q Targets Min                                   -3.43635\r\n",
      "trainer/Log Pis Mean                                     1.83313\r\n",
      "trainer/Log Pis Std                                      1.28414\r\n",
      "trainer/Log Pis Max                                      4.44941\r\n",
      "trainer/Log Pis Min                                     -2.81072\r\n",
      "trainer/Policy mu Mean                                  -0.0552155\r\n",
      "trainer/Policy mu Std                                    0.328977\r\n",
      "trainer/Policy mu Max                                    0.936544\r\n",
      "trainer/Policy mu Min                                   -2.62048\r\n",
      "trainer/Policy log std Mean                             -2.24213\r\n",
      "trainer/Policy log std Std                               0.615682\r\n",
      "trainer/Policy log std Max                              -0.169917\r\n",
      "trainer/Policy log std Min                              -3.19286\r\n",
      "trainer/Alpha                                            0.0219053\r\n",
      "trainer/Alpha Loss                                      -0.63744\r\n",
      "exploration/num steps total                           5400\r\n",
      "exploration/num paths total                            270\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0858878\r\n",
      "exploration/Rewards Std                                  0.0689896\r\n",
      "exploration/Rewards Max                                  0.0639313\r\n",
      "exploration/Rewards Min                                 -0.269376\r\n",
      "exploration/Returns Mean                                -1.71776\r\n",
      "exploration/Returns Std                                  0.999047\r\n",
      "exploration/Returns Max                                 -0.0894993\r\n",
      "exploration/Returns Min                                 -2.7722\r\n",
      "exploration/Actions Mean                                -0.00211149\r\n",
      "exploration/Actions Std                                  0.107425\r\n",
      "exploration/Actions Max                                  0.296136\r\n",
      "exploration/Actions Min                                 -0.472578\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.71776\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.00769153\r\n",
      "exploration/env_infos/final/reward_dist Std              0.00788375\r\n",
      "exploration/env_infos/final/reward_dist Max              0.0180249\r\n",
      "exploration/env_infos/final/reward_dist Min              2.68527e-08\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00136561\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00154054\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.00411628\r\n",
      "exploration/env_infos/initial/reward_dist Min            1.95066e-05\r\n",
      "exploration/env_infos/reward_dist Mean                   0.132176\r\n",
      "exploration/env_infos/reward_dist Std                    0.217536\r\n",
      "exploration/env_infos/reward_dist Max                    0.877957\r\n",
      "exploration/env_infos/reward_dist Min                    2.68527e-08\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.116339\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0507543\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0734598\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.215515\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.193666\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.179065\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0332592\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.526532\r\n",
      "exploration/env_infos/reward_energy Mean                -0.122136\r\n",
      "exploration/env_infos/reward_energy Std                  0.0903985\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0179513\r\n",
      "exploration/env_infos/reward_energy Min                 -0.526532\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0371892\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.211536\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.347999\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.325093\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.000255723\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0093219\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0116088\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0236289\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.0223937\r\n",
      "exploration/env_infos/end_effector_loc Std               0.127985\r\n",
      "exploration/env_infos/end_effector_loc Max               0.347999\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.325093\r\n",
      "evaluation/num steps total                           44000\r\n",
      "evaluation/num paths total                            2200\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0586977\r\n",
      "evaluation/Rewards Std                                   0.0775934\r\n",
      "evaluation/Rewards Max                                   0.104497\r\n",
      "evaluation/Rewards Min                                  -0.463564\r\n",
      "evaluation/Returns Mean                                 -1.17395\r\n",
      "evaluation/Returns Std                                   1.26158\r\n",
      "evaluation/Returns Max                                   1.04113\r\n",
      "evaluation/Returns Min                                  -5.61074\r\n",
      "evaluation/Actions Mean                                 -0.0041032\r\n",
      "evaluation/Actions Std                                   0.0771713\r\n",
      "evaluation/Actions Max                                   0.477392\r\n",
      "evaluation/Actions Min                                  -0.648114\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.17395\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.126497\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.256468\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.959871\r\n",
      "evaluation/env_infos/final/reward_dist Min               5.46059e-40\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00519711\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0126464\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0833445\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.02164e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.136444\r\n",
      "evaluation/env_infos/reward_dist Std                     0.230421\r\n",
      "evaluation/env_infos/reward_dist Max                     0.99041\r\n",
      "evaluation/env_infos/reward_dist Min                     5.46059e-40\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0452094\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0382867\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00628401\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.216881\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.175961\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.17863\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00673251\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.683875\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0692886\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0845196\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.0022403\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.697775\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0322904\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.234744\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.691879\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.470574\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00112831\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00879294\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0238696\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0324057\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0144875\n",
      "evaluation/env_infos/end_effector_loc Std                0.156009\n",
      "evaluation/env_infos/end_effector_loc Max                0.691879\n",
      "evaluation/env_infos/end_effector_loc Min               -0.470574\n",
      "time/data storing (s)                                    0.00313197\n",
      "time/evaluation sampling (s)                             0.945556\n",
      "time/exploration sampling (s)                            0.124802\n",
      "time/logging (s)                                         0.0205517\n",
      "time/saving (s)                                          0.028067\n",
      "time/training (s)                                       47.3429\n",
      "time/epoch (s)                                          48.465\n",
      "time/total (s)                                        2130.84\n",
      "Epoch                                                   43\n",
      "---------------------------------------------------  ---------------\n",
      "2021-05-25 19:49:37.203950 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 44 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000748182\n",
      "trainer/QF2 Loss                                         0.00102367\n",
      "trainer/Policy Loss                                      3.47922\n",
      "trainer/Q1 Predictions Mean                             -1.23141\n",
      "trainer/Q1 Predictions Std                               0.798353\n",
      "trainer/Q1 Predictions Max                               0.464253\n",
      "trainer/Q1 Predictions Min                              -3.03907\n",
      "trainer/Q2 Predictions Mean                             -1.23692\n",
      "trainer/Q2 Predictions Std                               0.79921\n",
      "trainer/Q2 Predictions Max                               0.424\n",
      "trainer/Q2 Predictions Min                              -3.01259\n",
      "trainer/Q Targets Mean                                  -1.21919\n",
      "trainer/Q Targets Std                                    0.795592\n",
      "trainer/Q Targets Max                                    0.476011\n",
      "trainer/Q Targets Min                                   -3.00801\n",
      "trainer/Log Pis Mean                                     2.25467\n",
      "trainer/Log Pis Std                                      1.17681\n",
      "trainer/Log Pis Max                                      4.18947\n",
      "trainer/Log Pis Min                                     -2.71596\n",
      "trainer/Policy mu Mean                                  -0.00766301\n",
      "trainer/Policy mu Std                                    0.261707\n",
      "trainer/Policy mu Max                                    1.55361\n",
      "trainer/Policy mu Min                                   -1.77585\n",
      "trainer/Policy log std Mean                             -2.38917\n",
      "trainer/Policy log std Std                               0.561883\n",
      "trainer/Policy log std Max                               0.134769\n",
      "trainer/Policy log std Min                              -3.23257\n",
      "trainer/Alpha                                            0.0236469\n",
      "trainer/Alpha Loss                                       0.953495\n",
      "exploration/num steps total                           5500\n",
      "exploration/num paths total                            275\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0479709\n",
      "exploration/Rewards Std                                  0.082753\n",
      "exploration/Rewards Max                                  0.103674\n",
      "exploration/Rewards Min                                 -0.279683\n",
      "exploration/Returns Mean                                -0.959418\n",
      "exploration/Returns Std                                  1.42687\n",
      "exploration/Returns Max                                  0.840224\n",
      "exploration/Returns Min                                 -3.09562\n",
      "exploration/Actions Mean                                -0.00842861\n",
      "exploration/Actions Std                                  0.171472\n",
      "exploration/Actions Max                                  0.534538\n",
      "exploration/Actions Min                                 -0.52086\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -0.959418\n",
      "exploration/env_infos/final/reward_dist Mean             0.432434\n",
      "exploration/env_infos/final/reward_dist Std              0.39814\n",
      "exploration/env_infos/final/reward_dist Max              0.998297\n",
      "exploration/env_infos/final/reward_dist Min              1.11105e-05\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00589768\n",
      "exploration/env_infos/initial/reward_dist Std            0.00722452\n",
      "exploration/env_infos/initial/reward_dist Max            0.0196201\n",
      "exploration/env_infos/initial/reward_dist Min            1.67762e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.337003\n",
      "exploration/env_infos/reward_dist Std                    0.355849\n",
      "exploration/env_infos/reward_dist Max                    0.999681\n",
      "exploration/env_infos/reward_dist Min                    1.11105e-05\n",
      "exploration/env_infos/final/reward_energy Mean          -0.125655\n",
      "exploration/env_infos/final/reward_energy Std            0.0993442\n",
      "exploration/env_infos/final/reward_energy Max           -0.0186167\n",
      "exploration/env_infos/final/reward_energy Min           -0.300706\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.312951\n",
      "exploration/env_infos/initial/reward_energy Std          0.281435\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0352025\n",
      "exploration/env_infos/initial/reward_energy Min         -0.724098\n",
      "exploration/env_infos/reward_energy Mean                -0.196518\n",
      "exploration/env_infos/reward_energy Std                  0.142577\n",
      "exploration/env_infos/reward_energy Max                 -0.00647169\n",
      "exploration/env_infos/reward_energy Min                 -0.724098\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0622881\n",
      "exploration/env_infos/final/end_effector_loc Std         0.168431\n",
      "exploration/env_infos/final/end_effector_loc Max         0.194777\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.371081\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00201908\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0147429\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0267269\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.026043\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0321917\n",
      "exploration/env_infos/end_effector_loc Std               0.142831\n",
      "exploration/env_infos/end_effector_loc Max               0.230049\n",
      "exploration/env_infos/end_effector_loc Min              -0.371081\n",
      "evaluation/num steps total                           45000\n",
      "evaluation/num paths total                            2250\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0468926\n",
      "evaluation/Rewards Std                                   0.0632369\n",
      "evaluation/Rewards Max                                   0.12741\n",
      "evaluation/Rewards Min                                  -0.271024\n",
      "evaluation/Returns Mean                                 -0.937851\n",
      "evaluation/Returns Std                                   0.942558\n",
      "evaluation/Returns Max                                   1.45192\n",
      "evaluation/Returns Min                                  -3.01985\n",
      "evaluation/Actions Mean                                  0.0032215\n",
      "evaluation/Actions Std                                   0.0638241\n",
      "evaluation/Actions Max                                   0.461854\n",
      "evaluation/Actions Min                                  -0.476166\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.937851\n",
      "evaluation/env_infos/final/reward_dist Mean              0.106158\n",
      "evaluation/env_infos/final/reward_dist Std               0.215342\n",
      "evaluation/env_infos/final/reward_dist Max               0.848232\n",
      "evaluation/env_infos/final/reward_dist Min               1.91472e-19\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00487184\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0103034\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0579828\n",
      "evaluation/env_infos/initial/reward_dist Min             1.80714e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.188391\n",
      "evaluation/env_infos/reward_dist Std                     0.276533\n",
      "evaluation/env_infos/reward_dist Max                     0.997011\n",
      "evaluation/env_infos/reward_dist Min                     1.91472e-19\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0453677\n",
      "evaluation/env_infos/final/reward_energy Std             0.0395575\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00274374\n",
      "evaluation/env_infos/final/reward_energy Min            -0.153075\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.173995\n",
      "evaluation/env_infos/initial/reward_energy Std           0.161721\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0110972\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.608174\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0619193\n",
      "evaluation/env_infos/reward_energy Std                   0.0658315\n",
      "evaluation/env_infos/reward_energy Max                  -0.000763547\n",
      "evaluation/env_infos/reward_energy Min                  -0.608174\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0537029\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.246393\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.601495\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.580937\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.0006205\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00837553\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0230927\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0238083\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0233474\n",
      "evaluation/env_infos/end_effector_loc Std                0.152319\n",
      "evaluation/env_infos/end_effector_loc Max                0.601495\n",
      "evaluation/env_infos/end_effector_loc Min               -0.580937\n",
      "time/data storing (s)                                    0.0030357\n",
      "time/evaluation sampling (s)                             1.01629\n",
      "time/exploration sampling (s)                            0.123108\n",
      "time/logging (s)                                         0.0186348\n",
      "time/saving (s)                                          0.027622\n",
      "time/training (s)                                       46.9037\n",
      "time/epoch (s)                                          48.0923\n",
      "time/total (s)                                        2179.48\n",
      "Epoch                                                   44\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:50:26.382292 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 45 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000711372\r\n",
      "trainer/QF2 Loss                                         0.000584052\r\n",
      "trainer/Policy Loss                                      3.23355\r\n",
      "trainer/Q1 Predictions Mean                             -1.17799\r\n",
      "trainer/Q1 Predictions Std                               0.765086\r\n",
      "trainer/Q1 Predictions Max                               0.494181\r\n",
      "trainer/Q1 Predictions Min                              -3.04878\r\n",
      "trainer/Q2 Predictions Mean                             -1.19311\r\n",
      "trainer/Q2 Predictions Std                               0.764668\r\n",
      "trainer/Q2 Predictions Max                               0.47158\r\n",
      "trainer/Q2 Predictions Min                              -3.02392\r\n",
      "trainer/Q Targets Mean                                  -1.18521\r\n",
      "trainer/Q Targets Std                                    0.758649\r\n",
      "trainer/Q Targets Max                                    0.421255\r\n",
      "trainer/Q Targets Min                                   -3.01927\r\n",
      "trainer/Log Pis Mean                                     2.05547\r\n",
      "trainer/Log Pis Std                                      1.47473\r\n",
      "trainer/Log Pis Max                                      4.33901\r\n",
      "trainer/Log Pis Min                                     -4.75551\r\n",
      "trainer/Policy mu Mean                                  -0.0376243\r\n",
      "trainer/Policy mu Std                                    0.269961\r\n",
      "trainer/Policy mu Max                                    1.65856\r\n",
      "trainer/Policy mu Min                                   -1.66792\r\n",
      "trainer/Policy log std Mean                             -2.3677\r\n",
      "trainer/Policy log std Std                               0.573836\r\n",
      "trainer/Policy log std Max                               0.106461\r\n",
      "trainer/Policy log std Min                              -3.24439\r\n",
      "trainer/Alpha                                            0.0219387\r\n",
      "trainer/Alpha Loss                                       0.211859\r\n",
      "exploration/num steps total                           5600\r\n",
      "exploration/num paths total                            280\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.047284\r\n",
      "exploration/Rewards Std                                  0.0528119\r\n",
      "exploration/Rewards Max                                  0.0788098\r\n",
      "exploration/Rewards Min                                 -0.17006\r\n",
      "exploration/Returns Mean                                -0.945679\r\n",
      "exploration/Returns Std                                  0.612891\r\n",
      "exploration/Returns Max                                 -0.139005\r\n",
      "exploration/Returns Min                                 -1.76834\r\n",
      "exploration/Actions Mean                                -0.0117357\r\n",
      "exploration/Actions Std                                  0.0845483\r\n",
      "exploration/Actions Max                                  0.291401\r\n",
      "exploration/Actions Min                                 -0.330207\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -0.945679\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.225307\r\n",
      "exploration/env_infos/final/reward_dist Std              0.179444\r\n",
      "exploration/env_infos/final/reward_dist Max              0.514492\r\n",
      "exploration/env_infos/final/reward_dist Min              3.88946e-06\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00614593\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00614189\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0166657\r\n",
      "exploration/env_infos/initial/reward_dist Min            5.1473e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.244683\r\n",
      "exploration/env_infos/reward_dist Std                    0.293866\r\n",
      "exploration/env_infos/reward_dist Max                    0.994921\r\n",
      "exploration/env_infos/reward_dist Min                    3.88946e-06\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.137586\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0724317\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0577703\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.25774\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.0724467\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.0456279\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.026533\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.157387\r\n",
      "exploration/env_infos/reward_energy Mean                -0.0997514\r\n",
      "exploration/env_infos/reward_energy Std                  0.0679847\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00510332\r\n",
      "exploration/env_infos/reward_energy Min                 -0.385931\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0400647\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.169492\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.254897\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.303288\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000930076\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00288063\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00531375\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0058044\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0086292\r\n",
      "exploration/env_infos/end_effector_loc Std               0.0976801\r\n",
      "exploration/env_infos/end_effector_loc Max               0.254897\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.303288\r\n",
      "evaluation/num steps total                           46000\r\n",
      "evaluation/num paths total                            2300\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0530775\r\n",
      "evaluation/Rewards Std                                   0.0932497\r\n",
      "evaluation/Rewards Max                                   0.14873\r\n",
      "evaluation/Rewards Min                                  -0.465293\r\n",
      "evaluation/Returns Mean                                 -1.06155\r\n",
      "evaluation/Returns Std                                   1.58134\r\n",
      "evaluation/Returns Max                                   2.01854\r\n",
      "evaluation/Returns Min                                  -6.25032\r\n",
      "evaluation/Actions Mean                                  0.000519342\r\n",
      "evaluation/Actions Std                                   0.0869449\r\n",
      "evaluation/Actions Max                                   0.633883\r\n",
      "evaluation/Actions Min                                  -0.684968\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.06155\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.144997\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.228853\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.954783\r\n",
      "evaluation/env_infos/final/reward_dist Min               1.91716e-48\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0081041\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0135097\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0651729\r\n",
      "evaluation/env_infos/initial/reward_dist Min             2.87412e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.191669\r\n",
      "evaluation/env_infos/reward_dist Std                     0.271851\r\n",
      "evaluation/env_infos/reward_dist Max                     0.998671\r\n",
      "evaluation/env_infos/reward_dist Min                     1.91716e-48\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0656291\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0767479\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00926416\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.293782\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.242403\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.229313\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0165183\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.876969\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0787594\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0944263\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00126959\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.876969\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0629101\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.22908\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.723135\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.384817\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00107889\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.011748\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0316941\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0342484\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0262031\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.158691\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.723135\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.384817\r\n",
      "time/data storing (s)                                    0.00295578\r\n",
      "time/evaluation sampling (s)                             0.947224\r\n",
      "time/exploration sampling (s)                            0.123614\r\n",
      "time/logging (s)                                         0.0205267\r\n",
      "time/saving (s)                                          0.0294101\r\n",
      "time/training (s)                                       47.4979\r\n",
      "time/epoch (s)                                          48.6216\r\n",
      "time/total (s)                                        2228.66\r\n",
      "Epoch                                                   45\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:51:14.960477 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 46 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000731135\n",
      "trainer/QF2 Loss                                         0.0011455\n",
      "trainer/Policy Loss                                      3.24412\n",
      "trainer/Q1 Predictions Mean                             -1.08897\n",
      "trainer/Q1 Predictions Std                               0.826447\n",
      "trainer/Q1 Predictions Max                               0.776581\n",
      "trainer/Q1 Predictions Min                              -3.24344\n",
      "trainer/Q2 Predictions Mean                             -1.09035\n",
      "trainer/Q2 Predictions Std                               0.831832\n",
      "trainer/Q2 Predictions Max                               0.792003\n",
      "trainer/Q2 Predictions Min                              -3.24359\n",
      "trainer/Q Targets Mean                                  -1.09619\n",
      "trainer/Q Targets Std                                    0.828363\n",
      "trainer/Q Targets Max                                    0.810937\n",
      "trainer/Q Targets Min                                   -3.25975\n",
      "trainer/Log Pis Mean                                     2.16483\n",
      "trainer/Log Pis Std                                      1.27434\n",
      "trainer/Log Pis Max                                      4.41629\n",
      "trainer/Log Pis Min                                     -3.43697\n",
      "trainer/Policy mu Mean                                  -0.0261343\n",
      "trainer/Policy mu Std                                    0.350219\n",
      "trainer/Policy mu Max                                    1.53506\n",
      "trainer/Policy mu Min                                   -2.28192\n",
      "trainer/Policy log std Mean                             -2.33425\n",
      "trainer/Policy log std Std                               0.56743\n",
      "trainer/Policy log std Max                              -0.372923\n",
      "trainer/Policy log std Min                              -3.24036\n",
      "trainer/Alpha                                            0.0234274\n",
      "trainer/Alpha Loss                                       0.618813\n",
      "exploration/num steps total                           5700\n",
      "exploration/num paths total                            285\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0307633\n",
      "exploration/Rewards Std                                  0.10012\n",
      "exploration/Rewards Max                                  0.166475\n",
      "exploration/Rewards Min                                 -0.214093\n",
      "exploration/Returns Mean                                -0.615266\n",
      "exploration/Returns Std                                  1.82093\n",
      "exploration/Returns Max                                  2.323\n",
      "exploration/Returns Min                                 -2.53005\n",
      "exploration/Actions Mean                                 0.00610728\n",
      "exploration/Actions Std                                  0.15783\n",
      "exploration/Actions Max                                  0.520756\n",
      "exploration/Actions Min                                 -0.520308\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -0.615266\n",
      "exploration/env_infos/final/reward_dist Mean             0.0705296\n",
      "exploration/env_infos/final/reward_dist Std              0.0666859\n",
      "exploration/env_infos/final/reward_dist Max              0.173321\n",
      "exploration/env_infos/final/reward_dist Min              1.54049e-23\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0147467\n",
      "exploration/env_infos/initial/reward_dist Std            0.017984\n",
      "exploration/env_infos/initial/reward_dist Max            0.0503523\n",
      "exploration/env_infos/initial/reward_dist Min            0.00350369\n",
      "exploration/env_infos/reward_dist Mean                   0.222839\n",
      "exploration/env_infos/reward_dist Std                    0.31309\n",
      "exploration/env_infos/reward_dist Max                    0.997014\n",
      "exploration/env_infos/reward_dist Min                    1.54049e-23\n",
      "exploration/env_infos/final/reward_energy Mean          -0.135396\n",
      "exploration/env_infos/final/reward_energy Std            0.0543594\n",
      "exploration/env_infos/final/reward_energy Max           -0.0738233\n",
      "exploration/env_infos/final/reward_energy Min           -0.219418\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.291178\n",
      "exploration/env_infos/initial/reward_energy Std          0.241808\n",
      "exploration/env_infos/initial/reward_energy Max         -0.00166394\n",
      "exploration/env_infos/initial/reward_energy Min         -0.613194\n",
      "exploration/env_infos/reward_energy Mean                -0.171586\n",
      "exploration/env_infos/reward_energy Std                  0.143016\n",
      "exploration/env_infos/reward_energy Max                 -0.00166394\n",
      "exploration/env_infos/reward_energy Min                 -0.617841\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0167907\n",
      "exploration/env_infos/final/end_effector_loc Std         0.277538\n",
      "exploration/env_infos/final/end_effector_loc Max         0.382034\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.648924\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00770275\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0109425\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00301881\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0260154\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0363368\n",
      "exploration/env_infos/end_effector_loc Std               0.176114\n",
      "exploration/env_infos/end_effector_loc Max               0.382034\n",
      "exploration/env_infos/end_effector_loc Min              -0.648924\n",
      "evaluation/num steps total                           47000\n",
      "evaluation/num paths total                            2350\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0418353\n",
      "evaluation/Rewards Std                                   0.0687402\n",
      "evaluation/Rewards Max                                   0.165579\n",
      "evaluation/Rewards Min                                  -0.355658\n",
      "evaluation/Returns Mean                                 -0.836705\n",
      "evaluation/Returns Std                                   1.09906\n",
      "evaluation/Returns Max                                   1.28144\n",
      "evaluation/Returns Min                                  -3.28158\n",
      "evaluation/Actions Mean                                 -0.00293082\n",
      "evaluation/Actions Std                                   0.0597479\n",
      "evaluation/Actions Max                                   0.527181\n",
      "evaluation/Actions Min                                  -0.497226\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.836705\n",
      "evaluation/env_infos/final/reward_dist Mean              0.174195\n",
      "evaluation/env_infos/final/reward_dist Std               0.260631\n",
      "evaluation/env_infos/final/reward_dist Max               0.973332\n",
      "evaluation/env_infos/final/reward_dist Min               8.92665e-11\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00675324\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0129852\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0599654\n",
      "evaluation/env_infos/initial/reward_dist Min             2.00439e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.181861\n",
      "evaluation/env_infos/reward_dist Std                     0.268184\n",
      "evaluation/env_infos/reward_dist Max                     0.998878\n",
      "evaluation/env_infos/reward_dist Min                     8.92665e-11\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0349873\n",
      "evaluation/env_infos/final/reward_energy Std             0.0438961\n",
      "evaluation/env_infos/final/reward_energy Max            -0.000704192\n",
      "evaluation/env_infos/final/reward_energy Min            -0.218094\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.142178\n",
      "evaluation/env_infos/initial/reward_energy Std           0.15502\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0184667\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.628238\n",
      "evaluation/env_infos/reward_energy Mean                 -0.053339\n",
      "evaluation/env_infos/reward_energy Std                   0.0656638\n",
      "evaluation/env_infos/reward_energy Max                  -0.000704192\n",
      "evaluation/env_infos/reward_energy Min                  -0.628238\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.011154\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.208737\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.386924\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.438762\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       2.45855e-05\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00743684\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0263591\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0248613\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0010924\n",
      "evaluation/env_infos/end_effector_loc Std                0.135737\n",
      "evaluation/env_infos/end_effector_loc Max                0.386924\n",
      "evaluation/env_infos/end_effector_loc Min               -0.438762\n",
      "time/data storing (s)                                    0.00305539\n",
      "time/evaluation sampling (s)                             0.953661\n",
      "time/exploration sampling (s)                            0.121815\n",
      "time/logging (s)                                         0.0203924\n",
      "time/saving (s)                                          0.0300781\n",
      "time/training (s)                                       46.8638\n",
      "time/epoch (s)                                          47.9928\n",
      "time/total (s)                                        2277.24\n",
      "Epoch                                                   46\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:52:04.096387 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 47 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000774705\r\n",
      "trainer/QF2 Loss                                         0.000982771\r\n",
      "trainer/Policy Loss                                      3.06444\r\n",
      "trainer/Q1 Predictions Mean                             -1.14256\r\n",
      "trainer/Q1 Predictions Std                               0.847949\r\n",
      "trainer/Q1 Predictions Max                               0.584781\r\n",
      "trainer/Q1 Predictions Min                              -3.37716\r\n",
      "trainer/Q2 Predictions Mean                             -1.13176\r\n",
      "trainer/Q2 Predictions Std                               0.846155\r\n",
      "trainer/Q2 Predictions Max                               0.68082\r\n",
      "trainer/Q2 Predictions Min                              -3.35201\r\n",
      "trainer/Q Targets Mean                                  -1.14642\r\n",
      "trainer/Q Targets Std                                    0.850951\r\n",
      "trainer/Q Targets Max                                    0.621236\r\n",
      "trainer/Q Targets Min                                   -3.39536\r\n",
      "trainer/Log Pis Mean                                     1.92306\r\n",
      "trainer/Log Pis Std                                      1.39018\r\n",
      "trainer/Log Pis Max                                      4.06419\r\n",
      "trainer/Log Pis Min                                     -3.18251\r\n",
      "trainer/Policy mu Mean                                   0.00438545\r\n",
      "trainer/Policy mu Std                                    0.219616\r\n",
      "trainer/Policy mu Max                                    1.36695\r\n",
      "trainer/Policy mu Min                                   -1.24017\r\n",
      "trainer/Policy log std Mean                             -2.35851\r\n",
      "trainer/Policy log std Std                               0.537844\r\n",
      "trainer/Policy log std Max                              -0.630345\r\n",
      "trainer/Policy log std Min                              -3.17348\r\n",
      "trainer/Alpha                                            0.0222845\r\n",
      "trainer/Alpha Loss                                      -0.292622\r\n",
      "exploration/num steps total                           5800\r\n",
      "exploration/num paths total                            290\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.108283\r\n",
      "exploration/Rewards Std                                  0.0750876\r\n",
      "exploration/Rewards Max                                  0.0599312\r\n",
      "exploration/Rewards Min                                 -0.394537\r\n",
      "exploration/Returns Mean                                -2.16566\r\n",
      "exploration/Returns Std                                  0.461911\r\n",
      "exploration/Returns Max                                 -1.6923\r\n",
      "exploration/Returns Min                                 -3.05098\r\n",
      "exploration/Actions Mean                                 0.0026477\r\n",
      "exploration/Actions Std                                  0.127673\r\n",
      "exploration/Actions Max                                  0.56105\r\n",
      "exploration/Actions Min                                 -0.758632\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.16566\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0130267\r\n",
      "exploration/env_infos/final/reward_dist Std              0.0208692\r\n",
      "exploration/env_infos/final/reward_dist Max              0.0538719\r\n",
      "exploration/env_infos/final/reward_dist Min              9.44704e-12\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00165926\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00202333\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.00434629\r\n",
      "exploration/env_infos/initial/reward_dist Min            2.85964e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.0874414\r\n",
      "exploration/env_infos/reward_dist Std                    0.175431\r\n",
      "exploration/env_infos/reward_dist Max                    0.870125\r\n",
      "exploration/env_infos/reward_dist Min                    9.44704e-12\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0990031\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0625659\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0131941\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.161805\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.148668\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.179622\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0276055\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.501463\r\n",
      "exploration/env_infos/reward_energy Mean                -0.1275\r\n",
      "exploration/env_infos/reward_energy Std                  0.127901\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0109609\r\n",
      "exploration/env_infos/reward_energy Min                 -0.760287\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0609577\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.277129\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.338086\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.492948\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00317513\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00760766\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0249635\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00234285\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.0363605\r\n",
      "exploration/env_infos/end_effector_loc Std               0.180014\r\n",
      "exploration/env_infos/end_effector_loc Max               0.552751\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.492948\r\n",
      "evaluation/num steps total                           48000\r\n",
      "evaluation/num paths total                            2400\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0569282\r\n",
      "evaluation/Rewards Std                                   0.0905301\r\n",
      "evaluation/Rewards Max                                   0.159303\r\n",
      "evaluation/Rewards Min                                  -0.64312\r\n",
      "evaluation/Returns Mean                                 -1.13856\r\n",
      "evaluation/Returns Std                                   1.47482\r\n",
      "evaluation/Returns Max                                   2.39439\r\n",
      "evaluation/Returns Min                                  -8.0949\r\n",
      "evaluation/Actions Mean                                 -0.00245383\r\n",
      "evaluation/Actions Std                                   0.0776349\r\n",
      "evaluation/Actions Max                                   0.794481\r\n",
      "evaluation/Actions Min                                  -0.580887\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.13856\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.185527\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.276476\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.905666\r\n",
      "evaluation/env_infos/final/reward_dist Min               6.06237e-55\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00615371\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00906853\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0366437\r\n",
      "evaluation/env_infos/initial/reward_dist Min             3.13296e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.163577\r\n",
      "evaluation/env_infos/reward_dist Std                     0.245385\r\n",
      "evaluation/env_infos/reward_dist Max                     0.988988\r\n",
      "evaluation/env_infos/reward_dist Min                     6.05746e-55\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0570947\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0839103\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00445416\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.423823\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.180761\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.194831\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0191576\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.794753\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0656209\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0880926\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00219267\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.794753\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0159003\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.272365\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.812746\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000841064\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00935866\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0397241\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0277926\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0111766\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.173308\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.826669\r\n",
      "evaluation/env_infos/end_effector_loc Min               -1\r\n",
      "time/data storing (s)                                    0.00319415\r\n",
      "time/evaluation sampling (s)                             1.05685\r\n",
      "time/exploration sampling (s)                            0.125608\r\n",
      "time/logging (s)                                         0.0189566\r\n",
      "time/saving (s)                                          0.0299453\r\n",
      "time/training (s)                                       47.2434\r\n",
      "time/epoch (s)                                          48.478\r\n",
      "time/total (s)                                        2326.37\r\n",
      "Epoch                                                   47\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:53:01.920518 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 48 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000812128\r\n",
      "trainer/QF2 Loss                                         0.000894113\r\n",
      "trainer/Policy Loss                                      2.96098\r\n",
      "trainer/Q1 Predictions Mean                             -1.05747\r\n",
      "trainer/Q1 Predictions Std                               0.87824\r\n",
      "trainer/Q1 Predictions Max                               0.949891\r\n",
      "trainer/Q1 Predictions Min                              -3.4402\r\n",
      "trainer/Q2 Predictions Mean                             -1.04787\r\n",
      "trainer/Q2 Predictions Std                               0.875799\r\n",
      "trainer/Q2 Predictions Max                               0.918833\r\n",
      "trainer/Q2 Predictions Min                              -3.43295\r\n",
      "trainer/Q Targets Mean                                  -1.04485\r\n",
      "trainer/Q Targets Std                                    0.876854\r\n",
      "trainer/Q Targets Max                                    0.919985\r\n",
      "trainer/Q Targets Min                                   -3.38668\r\n",
      "trainer/Log Pis Mean                                     1.90101\r\n",
      "trainer/Log Pis Std                                      1.32671\r\n",
      "trainer/Log Pis Max                                      4.28463\r\n",
      "trainer/Log Pis Min                                     -2.79723\r\n",
      "trainer/Policy mu Mean                                  -0.0416288\r\n",
      "trainer/Policy mu Std                                    0.237069\r\n",
      "trainer/Policy mu Max                                    0.841938\r\n",
      "trainer/Policy mu Min                                   -1.63776\r\n",
      "trainer/Policy log std Mean                             -2.32188\r\n",
      "trainer/Policy log std Std                               0.548151\r\n",
      "trainer/Policy log std Max                              -0.451461\r\n",
      "trainer/Policy log std Min                              -3.21882\r\n",
      "trainer/Alpha                                            0.0235794\r\n",
      "trainer/Alpha Loss                                      -0.37087\r\n",
      "exploration/num steps total                           5900\r\n",
      "exploration/num paths total                            295\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.146647\r\n",
      "exploration/Rewards Std                                  0.0479953\r\n",
      "exploration/Rewards Max                                 -0.037941\r\n",
      "exploration/Rewards Min                                 -0.310911\r\n",
      "exploration/Returns Mean                                -2.93294\r\n",
      "exploration/Returns Std                                  0.546128\r\n",
      "exploration/Returns Max                                 -2.38807\r\n",
      "exploration/Returns Min                                 -3.88707\r\n",
      "exploration/Actions Mean                                -0.0157726\r\n",
      "exploration/Actions Std                                  0.0801929\r\n",
      "exploration/Actions Max                                  0.272226\r\n",
      "exploration/Actions Min                                 -0.271025\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.93294\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.183623\r\n",
      "exploration/env_infos/final/reward_dist Std              0.264698\r\n",
      "exploration/env_infos/final/reward_dist Max              0.679839\r\n",
      "exploration/env_infos/final/reward_dist Min              2.64084e-19\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00628251\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00897646\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0230351\r\n",
      "exploration/env_infos/initial/reward_dist Min            6.445e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.0394018\r\n",
      "exploration/env_infos/reward_dist Std                    0.106806\r\n",
      "exploration/env_infos/reward_dist Max                    0.679839\r\n",
      "exploration/env_infos/reward_dist Min                    2.64084e-19\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0649841\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0248938\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0240804\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.0927007\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.139215\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.0586564\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0783967\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.243009\r\n",
      "exploration/env_infos/reward_energy Mean                -0.0983774\r\n",
      "exploration/env_infos/reward_energy Std                  0.0606733\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00709111\r\n",
      "exploration/env_infos/reward_energy Min                 -0.293829\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.17989\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.228178\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.222055\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.595208\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.000749055\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00528826\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0095394\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00729682\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0631703\r\n",
      "exploration/env_infos/end_effector_loc Std               0.135452\r\n",
      "exploration/env_infos/end_effector_loc Max               0.222055\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.595208\r\n",
      "evaluation/num steps total                           49000\r\n",
      "evaluation/num paths total                            2450\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0541914\r\n",
      "evaluation/Rewards Std                                   0.0821416\r\n",
      "evaluation/Rewards Max                                   0.173135\r\n",
      "evaluation/Rewards Min                                  -0.626226\r\n",
      "evaluation/Returns Mean                                 -1.08383\r\n",
      "evaluation/Returns Std                                   1.27306\r\n",
      "evaluation/Returns Max                                   2.13822\r\n",
      "evaluation/Returns Min                                  -4.21051\r\n",
      "evaluation/Actions Mean                                 -0.0037657\r\n",
      "evaluation/Actions Std                                   0.0695748\r\n",
      "evaluation/Actions Max                                   0.48012\r\n",
      "evaluation/Actions Min                                  -0.724343\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.08383\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.113957\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.19304\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.878264\r\n",
      "evaluation/env_infos/final/reward_dist Min               1.35699e-49\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00516275\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00930889\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0381481\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.6002e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.145049\r\n",
      "evaluation/env_infos/reward_dist Std                     0.234339\r\n",
      "evaluation/env_infos/reward_dist Max                     0.998005\r\n",
      "evaluation/env_infos/reward_dist Min                     1.35699e-49\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0613291\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.127723\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00420815\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.728221\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.149562\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.157299\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00662381\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.696372\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0552113\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0816174\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000392736\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.728221\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0230599\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.26\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.580109\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00100097\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0076084\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.024006\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0321826\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0128841\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.155643\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.580109\r\n",
      "evaluation/env_infos/end_effector_loc Min               -1\r\n",
      "time/data storing (s)                                    0.00309835\r\n",
      "time/evaluation sampling (s)                             0.984943\r\n",
      "time/exploration sampling (s)                            0.135512\r\n",
      "time/logging (s)                                         0.0205539\r\n",
      "time/saving (s)                                          0.0296313\r\n",
      "time/training (s)                                       56.057\r\n",
      "time/epoch (s)                                          57.2308\r\n",
      "time/total (s)                                        2384.2\r\n",
      "Epoch                                                   48\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:53:56.268569 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 49 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000790089\r\n",
      "trainer/QF2 Loss                                         0.000909833\r\n",
      "trainer/Policy Loss                                      3.14872\r\n",
      "trainer/Q1 Predictions Mean                             -1.17362\r\n",
      "trainer/Q1 Predictions Std                               0.819447\r\n",
      "trainer/Q1 Predictions Max                               1.10229\r\n",
      "trainer/Q1 Predictions Min                              -3.27301\r\n",
      "trainer/Q2 Predictions Mean                             -1.17793\r\n",
      "trainer/Q2 Predictions Std                               0.819963\r\n",
      "trainer/Q2 Predictions Max                               1.07898\r\n",
      "trainer/Q2 Predictions Min                              -3.29812\r\n",
      "trainer/Q Targets Mean                                  -1.16026\r\n",
      "trainer/Q Targets Std                                    0.817276\r\n",
      "trainer/Q Targets Max                                    1.12376\r\n",
      "trainer/Q Targets Min                                   -3.26364\r\n",
      "trainer/Log Pis Mean                                     1.97034\r\n",
      "trainer/Log Pis Std                                      1.25284\r\n",
      "trainer/Log Pis Max                                      4.09925\r\n",
      "trainer/Log Pis Min                                     -2.16304\r\n",
      "trainer/Policy mu Mean                                  -0.0109839\r\n",
      "trainer/Policy mu Std                                    0.251774\r\n",
      "trainer/Policy mu Max                                    1.38071\r\n",
      "trainer/Policy mu Min                                   -1.64826\r\n",
      "trainer/Policy log std Mean                             -2.35616\r\n",
      "trainer/Policy log std Std                               0.557364\r\n",
      "trainer/Policy log std Max                              -0.373805\r\n",
      "trainer/Policy log std Min                              -3.25251\r\n",
      "trainer/Alpha                                            0.0230331\r\n",
      "trainer/Alpha Loss                                      -0.111848\r\n",
      "exploration/num steps total                           6000\r\n",
      "exploration/num paths total                            300\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0918758\r\n",
      "exploration/Rewards Std                                  0.079307\r\n",
      "exploration/Rewards Max                                  0.0731935\r\n",
      "exploration/Rewards Min                                 -0.253194\r\n",
      "exploration/Returns Mean                                -1.83752\r\n",
      "exploration/Returns Std                                  1.22137\r\n",
      "exploration/Returns Max                                 -0.210576\r\n",
      "exploration/Returns Min                                 -3.81122\r\n",
      "exploration/Actions Mean                                 0.0150513\r\n",
      "exploration/Actions Std                                  0.142931\r\n",
      "exploration/Actions Max                                  0.51355\r\n",
      "exploration/Actions Min                                 -0.476604\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.83752\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.227148\r\n",
      "exploration/env_infos/final/reward_dist Std              0.283692\r\n",
      "exploration/env_infos/final/reward_dist Max              0.695794\r\n",
      "exploration/env_infos/final/reward_dist Min              0.000102888\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00511143\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.009444\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0239766\r\n",
      "exploration/env_infos/initial/reward_dist Min            5.89836e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.250638\r\n",
      "exploration/env_infos/reward_dist Std                    0.303744\r\n",
      "exploration/env_infos/reward_dist Max                    0.985887\r\n",
      "exploration/env_infos/reward_dist Min                    5.89836e-06\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.142058\r\n",
      "exploration/env_infos/final/reward_energy Std            0.04869\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0927796\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.226253\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.210934\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.140638\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0809642\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.479591\r\n",
      "exploration/env_infos/reward_energy Mean                -0.16182\r\n",
      "exploration/env_infos/reward_energy Std                  0.122988\r\n",
      "exploration/env_infos/reward_energy Max                 -0.015301\r\n",
      "exploration/env_infos/reward_energy Min                 -0.585933\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0688178\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.204546\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.437379\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.273206\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00334609\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00831529\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00758251\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0238302\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.00272038\r\n",
      "exploration/env_infos/end_effector_loc Std               0.14418\r\n",
      "exploration/env_infos/end_effector_loc Max               0.437379\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.273206\r\n",
      "evaluation/num steps total                           50000\r\n",
      "evaluation/num paths total                            2500\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0645907\r\n",
      "evaluation/Rewards Std                                   0.0694995\r\n",
      "evaluation/Rewards Max                                   0.121869\r\n",
      "evaluation/Rewards Min                                  -0.51852\r\n",
      "evaluation/Returns Mean                                 -1.29181\r\n",
      "evaluation/Returns Std                                   1.04072\r\n",
      "evaluation/Returns Max                                   1.48893\r\n",
      "evaluation/Returns Min                                  -3.67947\r\n",
      "evaluation/Actions Mean                                  0.00312979\r\n",
      "evaluation/Actions Std                                   0.0800667\r\n",
      "evaluation/Actions Max                                   0.919365\r\n",
      "evaluation/Actions Min                                  -0.600676\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.29181\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.133643\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.258193\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.944719\r\n",
      "evaluation/env_infos/final/reward_dist Min               2.63041e-41\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0056937\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00796159\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0305873\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.87776e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.141818\r\n",
      "evaluation/env_infos/reward_dist Std                     0.238998\r\n",
      "evaluation/env_infos/reward_dist Max                     0.999341\r\n",
      "evaluation/env_infos/reward_dist Min                     2.63041e-41\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0553054\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0678317\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00378141\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.367493\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.207392\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.226891\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0119363\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.961091\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0723403\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0872229\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00240827\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.961091\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0672111\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.237864\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.512369\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.876454\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00135258\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0107835\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0459682\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0300338\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0340352\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.153224\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.512369\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.876454\r\n",
      "time/data storing (s)                                    0.00302927\r\n",
      "time/evaluation sampling (s)                             0.997522\r\n",
      "time/exploration sampling (s)                            0.133808\r\n",
      "time/logging (s)                                         0.02002\r\n",
      "time/saving (s)                                          0.030271\r\n",
      "time/training (s)                                       52.5345\r\n",
      "time/epoch (s)                                          53.7191\r\n",
      "time/total (s)                                        2438.55\r\n",
      "Epoch                                                   49\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:54:51.994197 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 50 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000646107\n",
      "trainer/QF2 Loss                                         0.000763435\n",
      "trainer/Policy Loss                                      3.08049\n",
      "trainer/Q1 Predictions Mean                             -1.12613\n",
      "trainer/Q1 Predictions Std                               0.847626\n",
      "trainer/Q1 Predictions Max                               1.23249\n",
      "trainer/Q1 Predictions Min                              -3.1243\n",
      "trainer/Q2 Predictions Mean                             -1.13239\n",
      "trainer/Q2 Predictions Std                               0.84789\n",
      "trainer/Q2 Predictions Max                               1.23346\n",
      "trainer/Q2 Predictions Min                              -3.14379\n",
      "trainer/Q Targets Mean                                  -1.12996\n",
      "trainer/Q Targets Std                                    0.846965\n",
      "trainer/Q Targets Max                                    1.27236\n",
      "trainer/Q Targets Min                                   -3.1479\n",
      "trainer/Log Pis Mean                                     1.94655\n",
      "trainer/Log Pis Std                                      1.29308\n",
      "trainer/Log Pis Max                                      4.42344\n",
      "trainer/Log Pis Min                                     -3.75637\n",
      "trainer/Policy mu Mean                                  -0.0159303\n",
      "trainer/Policy mu Std                                    0.25567\n",
      "trainer/Policy mu Max                                    1.33346\n",
      "trainer/Policy mu Min                                   -1.84889\n",
      "trainer/Policy log std Mean                             -2.33046\n",
      "trainer/Policy log std Std                               0.551454\n",
      "trainer/Policy log std Max                              -0.00246775\n",
      "trainer/Policy log std Min                              -3.22621\n",
      "trainer/Alpha                                            0.0226247\n",
      "trainer/Alpha Loss                                      -0.20253\n",
      "exploration/num steps total                           6100\n",
      "exploration/num paths total                            305\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0694542\n",
      "exploration/Rewards Std                                  0.0700169\n",
      "exploration/Rewards Max                                  0.107779\n",
      "exploration/Rewards Min                                 -0.236101\n",
      "exploration/Returns Mean                                -1.38908\n",
      "exploration/Returns Std                                  0.996937\n",
      "exploration/Returns Max                                  0.188735\n",
      "exploration/Returns Min                                 -2.8631\n",
      "exploration/Actions Mean                                -0.00530229\n",
      "exploration/Actions Std                                  0.0883433\n",
      "exploration/Actions Max                                  0.351393\n",
      "exploration/Actions Min                                 -0.403368\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.38908\n",
      "exploration/env_infos/final/reward_dist Mean             0.0459113\n",
      "exploration/env_infos/final/reward_dist Std              0.0604781\n",
      "exploration/env_infos/final/reward_dist Max              0.162781\n",
      "exploration/env_infos/final/reward_dist Min              1.28882e-06\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00874509\n",
      "exploration/env_infos/initial/reward_dist Std            0.0094665\n",
      "exploration/env_infos/initial/reward_dist Max            0.026402\n",
      "exploration/env_infos/initial/reward_dist Min            2.81177e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.246123\n",
      "exploration/env_infos/reward_dist Std                    0.298077\n",
      "exploration/env_infos/reward_dist Max                    0.991014\n",
      "exploration/env_infos/reward_dist Min                    1.28882e-06\n",
      "exploration/env_infos/final/reward_energy Mean          -0.160259\n",
      "exploration/env_infos/final/reward_energy Std            0.126515\n",
      "exploration/env_infos/final/reward_energy Max           -0.0814626\n",
      "exploration/env_infos/final/reward_energy Min           -0.410211\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.124858\n",
      "exploration/env_infos/initial/reward_energy Std          0.128788\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0269304\n",
      "exploration/env_infos/initial/reward_energy Min         -0.37607\n",
      "exploration/env_infos/reward_energy Mean                -0.102595\n",
      "exploration/env_infos/reward_energy Std                  0.0716914\n",
      "exploration/env_infos/reward_energy Max                 -0.00549877\n",
      "exploration/env_infos/reward_energy Min                 -0.410211\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.112286\n",
      "exploration/env_infos/final/end_effector_loc Std         0.203921\n",
      "exploration/env_infos/final/end_effector_loc Max         0.255814\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.372387\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000279998\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00633573\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0175697\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00669907\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0508431\n",
      "exploration/env_infos/end_effector_loc Std               0.12762\n",
      "exploration/env_infos/end_effector_loc Max               0.255814\n",
      "exploration/env_infos/end_effector_loc Min              -0.372387\n",
      "evaluation/num steps total                           51000\n",
      "evaluation/num paths total                            2550\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0394188\n",
      "evaluation/Rewards Std                                   0.068004\n",
      "evaluation/Rewards Max                                   0.159538\n",
      "evaluation/Rewards Min                                  -0.378376\n",
      "evaluation/Returns Mean                                 -0.788376\n",
      "evaluation/Returns Std                                   1.08727\n",
      "evaluation/Returns Max                                   1.71132\n",
      "evaluation/Returns Min                                  -2.56107\n",
      "evaluation/Actions Mean                                  8.65382e-05\n",
      "evaluation/Actions Std                                   0.0566716\n",
      "evaluation/Actions Max                                   0.715245\n",
      "evaluation/Actions Min                                  -0.668209\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.788376\n",
      "evaluation/env_infos/final/reward_dist Mean              0.226187\n",
      "evaluation/env_infos/final/reward_dist Std               0.286432\n",
      "evaluation/env_infos/final/reward_dist Max               0.915394\n",
      "evaluation/env_infos/final/reward_dist Min               5.85922e-13\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00605323\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00964549\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0418206\n",
      "evaluation/env_infos/initial/reward_dist Min             1.85426e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.171169\n",
      "evaluation/env_infos/reward_dist Std                     0.271545\n",
      "evaluation/env_infos/reward_dist Max                     0.993102\n",
      "evaluation/env_infos/reward_dist Min                     5.85922e-13\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0470937\n",
      "evaluation/env_infos/final/reward_energy Std             0.0466738\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00944293\n",
      "evaluation/env_infos/final/reward_energy Min            -0.197267\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.153707\n",
      "evaluation/env_infos/initial/reward_energy Std           0.18391\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.014149\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.755233\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0459281\n",
      "evaluation/env_infos/reward_energy Std                   0.0656808\n",
      "evaluation/env_infos/reward_energy Max                  -0.00090866\n",
      "evaluation/env_infos/reward_energy Min                  -0.755233\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.00523209\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.209677\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.638184\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.527048\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000843556\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00843206\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0357622\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0334105\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00479586\n",
      "evaluation/env_infos/end_effector_loc Std                0.131875\n",
      "evaluation/env_infos/end_effector_loc Max                0.638184\n",
      "evaluation/env_infos/end_effector_loc Min               -0.527048\n",
      "time/data storing (s)                                    0.00295374\n",
      "time/evaluation sampling (s)                             0.991506\n",
      "time/exploration sampling (s)                            0.126396\n",
      "time/logging (s)                                         0.0269704\n",
      "time/saving (s)                                          0.0916348\n",
      "time/training (s)                                       53.7835\n",
      "time/epoch (s)                                          55.023\n",
      "time/total (s)                                        2494.28\n",
      "Epoch                                                   50\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:55:47.323678 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 51 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00193865\r\n",
      "trainer/QF2 Loss                                         0.00061397\r\n",
      "trainer/Policy Loss                                      3.22226\r\n",
      "trainer/Q1 Predictions Mean                             -1.13893\r\n",
      "trainer/Q1 Predictions Std                               0.884325\r\n",
      "trainer/Q1 Predictions Max                               1.38836\r\n",
      "trainer/Q1 Predictions Min                              -3.1857\r\n",
      "trainer/Q2 Predictions Mean                             -1.14884\r\n",
      "trainer/Q2 Predictions Std                               0.887894\r\n",
      "trainer/Q2 Predictions Max                               1.36732\r\n",
      "trainer/Q2 Predictions Min                              -3.21057\r\n",
      "trainer/Q Targets Mean                                  -1.14358\r\n",
      "trainer/Q Targets Std                                    0.88556\r\n",
      "trainer/Q Targets Max                                    1.41657\r\n",
      "trainer/Q Targets Min                                   -3.18895\r\n",
      "trainer/Log Pis Mean                                     2.07997\r\n",
      "trainer/Log Pis Std                                      1.36571\r\n",
      "trainer/Log Pis Max                                      4.57685\r\n",
      "trainer/Log Pis Min                                     -4.41859\r\n",
      "trainer/Policy mu Mean                                  -0.00798388\r\n",
      "trainer/Policy mu Std                                    0.212919\r\n",
      "trainer/Policy mu Max                                    1.4918\r\n",
      "trainer/Policy mu Min                                   -1.49872\r\n",
      "trainer/Policy log std Mean                             -2.3728\r\n",
      "trainer/Policy log std Std                               0.555353\r\n",
      "trainer/Policy log std Max                              -0.622116\r\n",
      "trainer/Policy log std Min                              -3.26376\r\n",
      "trainer/Alpha                                            0.0228128\r\n",
      "trainer/Alpha Loss                                       0.302467\r\n",
      "exploration/num steps total                           6200\r\n",
      "exploration/num paths total                            310\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.100228\r\n",
      "exploration/Rewards Std                                  0.0910357\r\n",
      "exploration/Rewards Max                                  0.0732557\r\n",
      "exploration/Rewards Min                                 -0.317285\r\n",
      "exploration/Returns Mean                                -2.00456\r\n",
      "exploration/Returns Std                                  1.59692\r\n",
      "exploration/Returns Max                                  0.409792\r\n",
      "exploration/Returns Min                                 -4.09928\r\n",
      "exploration/Actions Mean                                -0.0099438\r\n",
      "exploration/Actions Std                                  0.182871\r\n",
      "exploration/Actions Max                                  0.431977\r\n",
      "exploration/Actions Min                                 -0.664595\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.00456\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.178916\r\n",
      "exploration/env_infos/final/reward_dist Std              0.357577\r\n",
      "exploration/env_infos/final/reward_dist Max              0.894069\r\n",
      "exploration/env_infos/final/reward_dist Min              2.02335e-31\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0160775\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0227473\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0606214\r\n",
      "exploration/env_infos/initial/reward_dist Min            3.5386e-05\r\n",
      "exploration/env_infos/reward_dist Mean                   0.113754\r\n",
      "exploration/env_infos/reward_dist Std                    0.213596\r\n",
      "exploration/env_infos/reward_dist Max                    0.914231\r\n",
      "exploration/env_infos/reward_dist Min                    2.02335e-31\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.301809\r\n",
      "exploration/env_infos/final/reward_energy Std            0.196542\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.151391\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.683236\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.367712\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.26079\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0804808\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.755714\r\n",
      "exploration/env_infos/reward_energy Mean                -0.214166\r\n",
      "exploration/env_infos/reward_energy Std                  0.145652\r\n",
      "exploration/env_infos/reward_energy Max                 -0.018483\r\n",
      "exploration/env_infos/reward_energy Min                 -0.755714\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.00417504\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.271127\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.357213\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.490395\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00850626\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0134786\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00852682\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0300337\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.00311436\r\n",
      "exploration/env_infos/end_effector_loc Std               0.146163\r\n",
      "exploration/env_infos/end_effector_loc Max               0.357213\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.490395\r\n",
      "evaluation/num steps total                           52000\r\n",
      "evaluation/num paths total                            2600\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0423782\r\n",
      "evaluation/Rewards Std                                   0.0763285\r\n",
      "evaluation/Rewards Max                                   0.16563\r\n",
      "evaluation/Rewards Min                                  -0.310916\r\n",
      "evaluation/Returns Mean                                 -0.847564\r\n",
      "evaluation/Returns Std                                   1.1552\r\n",
      "evaluation/Returns Max                                   2.12833\r\n",
      "evaluation/Returns Min                                  -3.09494\r\n",
      "evaluation/Actions Mean                                  0.000836034\r\n",
      "evaluation/Actions Std                                   0.0746768\r\n",
      "evaluation/Actions Max                                   0.493386\r\n",
      "evaluation/Actions Min                                  -0.66004\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.847564\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.148473\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.266156\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.906336\r\n",
      "evaluation/env_infos/final/reward_dist Min               9.88054e-18\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00543535\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0105028\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0478917\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.27419e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.165136\r\n",
      "evaluation/env_infos/reward_dist Std                     0.252478\r\n",
      "evaluation/env_infos/reward_dist Max                     0.998643\r\n",
      "evaluation/env_infos/reward_dist Min                     9.88054e-18\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0714105\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0690687\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation/env_infos/final/reward_energy Max            -0.00940822\n",
      "evaluation/env_infos/final/reward_energy Min            -0.370618\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.209358\n",
      "evaluation/env_infos/initial/reward_energy Std           0.18823\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00724747\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.661129\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0647582\n",
      "evaluation/env_infos/reward_energy Std                   0.0834327\n",
      "evaluation/env_infos/reward_energy Max                  -0.001005\n",
      "evaluation/env_infos/reward_energy Min                  -0.661129\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.023684\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.221197\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.427228\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.704078\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000745616\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00992576\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0212594\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.033002\n",
      "evaluation/env_infos/end_effector_loc Mean               0.010156\n",
      "evaluation/env_infos/end_effector_loc Std                0.142869\n",
      "evaluation/env_infos/end_effector_loc Max                0.427228\n",
      "evaluation/env_infos/end_effector_loc Min               -0.704078\n",
      "time/data storing (s)                                    0.00319745\n",
      "time/evaluation sampling (s)                             1.85315\n",
      "time/exploration sampling (s)                            0.212843\n",
      "time/logging (s)                                         0.0222986\n",
      "time/saving (s)                                          0.0304365\n",
      "time/training (s)                                       52.3963\n",
      "time/epoch (s)                                          54.5182\n",
      "time/total (s)                                        2549.6\n",
      "Epoch                                                   51\n",
      "---------------------------------------------------  ---------------\n",
      "2021-05-25 19:56:44.469819 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 52 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000564138\n",
      "trainer/QF2 Loss                                         0.000882073\n",
      "trainer/Policy Loss                                      3.13608\n",
      "trainer/Q1 Predictions Mean                             -1.06428\n",
      "trainer/Q1 Predictions Std                               0.811253\n",
      "trainer/Q1 Predictions Max                               0.961758\n",
      "trainer/Q1 Predictions Min                              -3.43856\n",
      "trainer/Q2 Predictions Mean                             -1.06456\n",
      "trainer/Q2 Predictions Std                               0.810862\n",
      "trainer/Q2 Predictions Max                               0.965712\n",
      "trainer/Q2 Predictions Min                              -3.43368\n",
      "trainer/Q Targets Mean                                  -1.06752\n",
      "trainer/Q Targets Std                                    0.814868\n",
      "trainer/Q Targets Max                                    0.953399\n",
      "trainer/Q Targets Min                                   -3.43996\n",
      "trainer/Log Pis Mean                                     2.07794\n",
      "trainer/Log Pis Std                                      1.32794\n",
      "trainer/Log Pis Max                                      4.4231\n",
      "trainer/Log Pis Min                                     -5.59204\n",
      "trainer/Policy mu Mean                                  -0.0514461\n",
      "trainer/Policy mu Std                                    0.259162\n",
      "trainer/Policy mu Max                                    1.008\n",
      "trainer/Policy mu Min                                   -1.90385\n",
      "trainer/Policy log std Mean                             -2.34018\n",
      "trainer/Policy log std Std                               0.548303\n",
      "trainer/Policy log std Max                              -0.401168\n",
      "trainer/Policy log std Min                              -3.25115\n",
      "trainer/Alpha                                            0.0214122\n",
      "trainer/Alpha Loss                                       0.299631\n",
      "exploration/num steps total                           6300\n",
      "exploration/num paths total                            315\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.117094\n",
      "exploration/Rewards Std                                  0.0536121\n",
      "exploration/Rewards Max                                 -0.00310531\n",
      "exploration/Rewards Min                                 -0.294406\n",
      "exploration/Returns Mean                                -2.34188\n",
      "exploration/Returns Std                                  0.548216\n",
      "exploration/Returns Max                                 -1.35404\n",
      "exploration/Returns Min                                 -3.02284\n",
      "exploration/Actions Mean                                 0.00205229\n",
      "exploration/Actions Std                                  0.0755326\n",
      "exploration/Actions Max                                  0.277145\n",
      "exploration/Actions Min                                 -0.228927\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.34188\n",
      "exploration/env_infos/final/reward_dist Mean             0.0734465\n",
      "exploration/env_infos/final/reward_dist Std              0.127777\n",
      "exploration/env_infos/final/reward_dist Max              0.328035\n",
      "exploration/env_infos/final/reward_dist Min              5.05137e-07\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000439209\n",
      "exploration/env_infos/initial/reward_dist Std            0.000292858\n",
      "exploration/env_infos/initial/reward_dist Max            0.000739874\n",
      "exploration/env_infos/initial/reward_dist Min            5.38416e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.05603\n",
      "exploration/env_infos/reward_dist Std                    0.174253\n",
      "exploration/env_infos/reward_dist Max                    0.915356\n",
      "exploration/env_infos/reward_dist Min                    1.09447e-07\n",
      "exploration/env_infos/final/reward_energy Mean          -0.14332\n",
      "exploration/env_infos/final/reward_energy Std            0.0622155\n",
      "exploration/env_infos/final/reward_energy Max           -0.0716452\n",
      "exploration/env_infos/final/reward_energy Min           -0.233507\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.0806226\n",
      "exploration/env_infos/initial/reward_energy Std          0.0288107\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0509701\n",
      "exploration/env_infos/initial/reward_energy Min         -0.131429\n",
      "exploration/env_infos/reward_energy Mean                -0.0907904\n",
      "exploration/env_infos/reward_energy Std                  0.0563549\n",
      "exploration/env_infos/reward_energy Max                 -0.00697259\n",
      "exploration/env_infos/reward_energy Min                 -0.307387\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.022086\n",
      "exploration/env_infos/final/end_effector_loc Std         0.167831\n",
      "exploration/env_infos/final/end_effector_loc Max         0.267382\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.372321\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00135697\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00270577\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00404251\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00469231\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0226019\n",
      "exploration/env_infos/end_effector_loc Std               0.0910463\n",
      "exploration/env_infos/end_effector_loc Max               0.267382\n",
      "exploration/env_infos/end_effector_loc Min              -0.372321\n",
      "evaluation/num steps total                           53000\n",
      "evaluation/num paths total                            2650\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0570043\n",
      "evaluation/Rewards Std                                   0.0678026\n",
      "evaluation/Rewards Max                                   0.144139\n",
      "evaluation/Rewards Min                                  -0.337982\n",
      "evaluation/Returns Mean                                 -1.14009\n",
      "evaluation/Returns Std                                   1.0589\n",
      "evaluation/Returns Max                                   1.15055\n",
      "evaluation/Returns Min                                  -3.65873\n",
      "evaluation/Actions Mean                                 -0.00157285\n",
      "evaluation/Actions Std                                   0.0630739\n",
      "evaluation/Actions Max                                   0.674026\n",
      "evaluation/Actions Min                                  -0.586379\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.14009\n",
      "evaluation/env_infos/final/reward_dist Mean              0.183789\n",
      "evaluation/env_infos/final/reward_dist Std               0.28059\n",
      "evaluation/env_infos/final/reward_dist Max               0.980919\n",
      "evaluation/env_infos/final/reward_dist Min               2.05697e-26\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00562016\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0117248\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0707175\n",
      "evaluation/env_infos/initial/reward_dist Min             3.60645e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.155375\n",
      "evaluation/env_infos/reward_dist Std                     0.236562\n",
      "evaluation/env_infos/reward_dist Max                     0.996748\n",
      "evaluation/env_infos/reward_dist Min                     2.05697e-26\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0751938\n",
      "evaluation/env_infos/final/reward_energy Std             0.0723688\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00520269\n",
      "evaluation/env_infos/final/reward_energy Min            -0.293956\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.1643\n",
      "evaluation/env_infos/initial/reward_energy Std           0.184008\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0142004\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.790587\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0525415\n",
      "evaluation/env_infos/reward_energy Std                   0.0721177\n",
      "evaluation/env_infos/reward_energy Max                  -0.000646712\n",
      "evaluation/env_infos/reward_energy Min                  -0.790587\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0372193\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.215862\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.317112\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.645088\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000574897\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00870265\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0337013\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0293189\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0190383\n",
      "evaluation/env_infos/end_effector_loc Std                0.136677\n",
      "evaluation/env_infos/end_effector_loc Max                0.317112\n",
      "evaluation/env_infos/end_effector_loc Min               -0.645088\n",
      "time/data storing (s)                                    0.00319203\n",
      "time/evaluation sampling (s)                             1.09321\n",
      "time/exploration sampling (s)                            0.141807\n",
      "time/logging (s)                                         0.0212983\n",
      "time/saving (s)                                          0.0320237\n",
      "time/training (s)                                       55.1551\n",
      "time/epoch (s)                                          56.4467\n",
      "time/total (s)                                        2606.74\n",
      "Epoch                                                   52\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:57:36.204007 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 53 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000572624\r\n",
      "trainer/QF2 Loss                                         0.000973084\r\n",
      "trainer/Policy Loss                                      2.99688\r\n",
      "trainer/Q1 Predictions Mean                             -1.15497\r\n",
      "trainer/Q1 Predictions Std                               0.849146\r\n",
      "trainer/Q1 Predictions Max                               0.737176\r\n",
      "trainer/Q1 Predictions Min                              -3.27979\r\n",
      "trainer/Q2 Predictions Mean                             -1.1408\r\n",
      "trainer/Q2 Predictions Std                               0.844036\r\n",
      "trainer/Q2 Predictions Max                               0.728662\r\n",
      "trainer/Q2 Predictions Min                              -3.27321\r\n",
      "trainer/Q Targets Mean                                  -1.15099\r\n",
      "trainer/Q Targets Std                                    0.852005\r\n",
      "trainer/Q Targets Max                                    0.70616\r\n",
      "trainer/Q Targets Min                                   -3.2992\r\n",
      "trainer/Log Pis Mean                                     1.84293\r\n",
      "trainer/Log Pis Std                                      1.39176\r\n",
      "trainer/Log Pis Max                                      4.22056\r\n",
      "trainer/Log Pis Min                                     -2.56069\r\n",
      "trainer/Policy mu Mean                                  -0.0383871\r\n",
      "trainer/Policy mu Std                                    0.241176\r\n",
      "trainer/Policy mu Max                                    1.00329\r\n",
      "trainer/Policy mu Min                                   -2.02289\r\n",
      "trainer/Policy log std Mean                             -2.32471\r\n",
      "trainer/Policy log std Std                               0.527772\r\n",
      "trainer/Policy log std Max                              -0.315075\r\n",
      "trainer/Policy log std Min                              -3.20793\r\n",
      "trainer/Alpha                                            0.0227258\r\n",
      "trainer/Alpha Loss                                      -0.59412\r\n",
      "exploration/num steps total                           6400\r\n",
      "exploration/num paths total                            320\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0904083\r\n",
      "exploration/Rewards Std                                  0.0934899\r\n",
      "exploration/Rewards Max                                  0.0521065\r\n",
      "exploration/Rewards Min                                 -0.464915\r\n",
      "exploration/Returns Mean                                -1.80817\r\n",
      "exploration/Returns Std                                  1.38176\r\n",
      "exploration/Returns Max                                 -0.662538\r\n",
      "exploration/Returns Min                                 -3.53301\r\n",
      "exploration/Actions Mean                                 0.0212716\r\n",
      "exploration/Actions Std                                  0.164074\r\n",
      "exploration/Actions Max                                  0.715719\r\n",
      "exploration/Actions Min                                 -0.518244\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.80817\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0416745\r\n",
      "exploration/env_infos/final/reward_dist Std              0.0833418\r\n",
      "exploration/env_infos/final/reward_dist Max              0.208358\r\n",
      "exploration/env_infos/final/reward_dist Min              1.15508e-93\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00106381\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00125184\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.00350076\r\n",
      "exploration/env_infos/initial/reward_dist Min            1.16555e-05\r\n",
      "exploration/env_infos/reward_dist Mean                   0.0454611\r\n",
      "exploration/env_infos/reward_dist Std                    0.0882724\r\n",
      "exploration/env_infos/reward_dist Max                    0.454585\r\n",
      "exploration/env_infos/reward_dist Min                    1.15508e-93\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.184593\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0831683\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.117049\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.33386\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.404757\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.223312\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.102838\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.685325\r\n",
      "exploration/env_infos/reward_energy Mean                -0.193288\r\n",
      "exploration/env_infos/reward_energy Std                  0.131853\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0238137\r\n",
      "exploration/env_infos/reward_energy Min                 -0.723145\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.18155\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.389973\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.900878\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.294195\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00160495\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0162648\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0319206\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0259122\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.0613797\r\n",
      "exploration/env_infos/end_effector_loc Std               0.236086\r\n",
      "exploration/env_infos/end_effector_loc Max               0.900878\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.294195\r\n",
      "evaluation/num steps total                           54000\r\n",
      "evaluation/num paths total                            2700\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0615927\r\n",
      "evaluation/Rewards Std                                   0.0875146\r\n",
      "evaluation/Rewards Max                                   0.129299\r\n",
      "evaluation/Rewards Min                                  -0.47225\r\n",
      "evaluation/Returns Mean                                 -1.23185\r\n",
      "evaluation/Returns Std                                   1.39009\r\n",
      "evaluation/Returns Max                                   1.51972\r\n",
      "evaluation/Returns Min                                  -5.00488\r\n",
      "evaluation/Actions Mean                                 -0.0032988\r\n",
      "evaluation/Actions Std                                   0.0786492\r\n",
      "evaluation/Actions Max                                   0.776927\r\n",
      "evaluation/Actions Min                                  -0.852732\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.23185\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.177501\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.263068\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.970842\r\n",
      "evaluation/env_infos/final/reward_dist Min               3.14615e-39\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00688088\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0099455\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0422336\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.33985e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.176659\r\n",
      "evaluation/env_infos/reward_dist Std                     0.259607\r\n",
      "evaluation/env_infos/reward_dist Max                     0.997383\r\n",
      "evaluation/env_infos/reward_dist Min                     3.14615e-39\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0719284\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0703338\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00300241\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.293106\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.248596\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.246405\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.012014\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.956006\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.067307\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0886732\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00120531\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.956006\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.00501051\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.239866\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.532985\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.614261\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00100322\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0123344\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0388463\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0426366\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.00247718\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.160254\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.532985\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.614261\r\n",
      "time/data storing (s)                                    0.00303298\r\n",
      "time/evaluation sampling (s)                             1.10706\r\n",
      "time/exploration sampling (s)                            0.125187\r\n",
      "time/logging (s)                                         0.0197895\r\n",
      "time/saving (s)                                          0.0290963\r\n",
      "time/training (s)                                       49.7232\r\n",
      "time/epoch (s)                                          51.0074\r\n",
      "time/total (s)                                        2658.47\r\n",
      "Epoch                                                   53\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:58:27.308835 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 54 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000602111\r\n",
      "trainer/QF2 Loss                                         0.000698058\r\n",
      "trainer/Policy Loss                                      3.1455\r\n",
      "trainer/Q1 Predictions Mean                             -1.06032\r\n",
      "trainer/Q1 Predictions Std                               0.813\r\n",
      "trainer/Q1 Predictions Max                               1.46217\r\n",
      "trainer/Q1 Predictions Min                              -3.10419\r\n",
      "trainer/Q2 Predictions Mean                             -1.05144\r\n",
      "trainer/Q2 Predictions Std                               0.812666\r\n",
      "trainer/Q2 Predictions Max                               1.49839\r\n",
      "trainer/Q2 Predictions Min                              -3.06528\r\n",
      "trainer/Q Targets Mean                                  -1.05529\r\n",
      "trainer/Q Targets Std                                    0.812686\r\n",
      "trainer/Q Targets Max                                    1.42107\r\n",
      "trainer/Q Targets Min                                   -3.08488\r\n",
      "trainer/Log Pis Mean                                     2.07855\r\n",
      "trainer/Log Pis Std                                      1.36986\r\n",
      "trainer/Log Pis Max                                      4.36102\r\n",
      "trainer/Log Pis Min                                     -4.29018\r\n",
      "trainer/Policy mu Mean                                  -0.0166128\r\n",
      "trainer/Policy mu Std                                    0.166369\r\n",
      "trainer/Policy mu Max                                    0.695085\r\n",
      "trainer/Policy mu Min                                   -2.10868\r\n",
      "trainer/Policy log std Mean                             -2.4178\r\n",
      "trainer/Policy log std Std                               0.501994\r\n",
      "trainer/Policy log std Max                              -0.518801\r\n",
      "trainer/Policy log std Min                              -3.25803\r\n",
      "trainer/Alpha                                            0.0224601\r\n",
      "trainer/Alpha Loss                                       0.298174\r\n",
      "exploration/num steps total                           6500\r\n",
      "exploration/num paths total                            325\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0896921\r\n",
      "exploration/Rewards Std                                  0.071465\r\n",
      "exploration/Rewards Max                                  0.0365829\r\n",
      "exploration/Rewards Min                                 -0.371033\r\n",
      "exploration/Returns Mean                                -1.79384\r\n",
      "exploration/Returns Std                                  0.485314\r\n",
      "exploration/Returns Max                                 -1.16159\r\n",
      "exploration/Returns Min                                 -2.61891\r\n",
      "exploration/Actions Mean                                 0.00977882\r\n",
      "exploration/Actions Std                                  0.108313\r\n",
      "exploration/Actions Max                                  0.431954\r\n",
      "exploration/Actions Min                                 -0.345894\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.79384\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.150734\r\n",
      "exploration/env_infos/final/reward_dist Std              0.242625\r\n",
      "exploration/env_infos/final/reward_dist Max              0.628069\r\n",
      "exploration/env_infos/final/reward_dist Min              2.50777e-10\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00534776\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00594692\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0150082\r\n",
      "exploration/env_infos/initial/reward_dist Min            1.95935e-05\r\n",
      "exploration/env_infos/reward_dist Mean                   0.11308\r\n",
      "exploration/env_infos/reward_dist Std                    0.154352\r\n",
      "exploration/env_infos/reward_dist Max                    0.628069\r\n",
      "exploration/env_infos/reward_dist Min                    2.50777e-10\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.255023\r\n",
      "exploration/env_infos/final/reward_energy Std            0.143467\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.118\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.517729\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.178196\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.0945371\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0520423\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.265767\r\n",
      "exploration/env_infos/reward_energy Mean                -0.129746\r\n",
      "exploration/env_infos/reward_energy Std                  0.0825865\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0190906\r\n",
      "exploration/env_infos/reward_energy Min                 -0.517729\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0316354\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.215032\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.36066\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.307917\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000861554\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00707965\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0132239\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0116155\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.0059525\r\n",
      "exploration/env_infos/end_effector_loc Std               0.135553\r\n",
      "exploration/env_infos/end_effector_loc Max               0.36066\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.311552\r\n",
      "evaluation/num steps total                           55000\r\n",
      "evaluation/num paths total                            2750\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.052766\r\n",
      "evaluation/Rewards Std                                   0.0717795\r\n",
      "evaluation/Rewards Max                                   0.128068\r\n",
      "evaluation/Rewards Min                                  -0.521683\r\n",
      "evaluation/Returns Mean                                 -1.05532\r\n",
      "evaluation/Returns Std                                   1.13114\r\n",
      "evaluation/Returns Max                                   1.14936\r\n",
      "evaluation/Returns Min                                  -5.28174\r\n",
      "evaluation/Actions Mean                                  0.00251551\r\n",
      "evaluation/Actions Std                                   0.065148\r\n",
      "evaluation/Actions Max                                   0.549805\r\n",
      "evaluation/Actions Min                                  -0.553513\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.05532\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.21628\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.292008\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.952999\r\n",
      "evaluation/env_infos/final/reward_dist Min               1.47572e-93\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00708446\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0116563\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0486307\r\n",
      "evaluation/env_infos/initial/reward_dist Min             8.98009e-07\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.199824\r\n",
      "evaluation/env_infos/reward_dist Std                     0.292146\r\n",
      "evaluation/env_infos/reward_dist Max                     0.998379\r\n",
      "evaluation/env_infos/reward_dist Min                     1.47572e-93\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0685116\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0876042\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00785327\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.587198\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.169714\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.16625\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00259799\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.749208\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0569569\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0725058\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00109553\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.749208\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0494042\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.250392\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.573149\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00150277\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00826401\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0274903\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.02698\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0244095\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.155295\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.573149\r\n",
      "evaluation/env_infos/end_effector_loc Min               -1\r\n",
      "time/data storing (s)                                    0.00302301\r\n",
      "time/evaluation sampling (s)                             0.981588\r\n",
      "time/exploration sampling (s)                            0.133723\r\n",
      "time/logging (s)                                         0.021165\r\n",
      "time/saving (s)                                          0.0327178\r\n",
      "time/training (s)                                       49.2062\r\n",
      "time/epoch (s)                                          50.3785\r\n",
      "time/total (s)                                        2709.58\r\n",
      "Epoch                                                   54\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:59:25.763428 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 55 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000593128\n",
      "trainer/QF2 Loss                                         0.000621741\n",
      "trainer/Policy Loss                                      3.03282\n",
      "trainer/Q1 Predictions Mean                             -1.07043\n",
      "trainer/Q1 Predictions Std                               0.834907\n",
      "trainer/Q1 Predictions Max                               0.695357\n",
      "trainer/Q1 Predictions Min                              -3.25502\n",
      "trainer/Q2 Predictions Mean                             -1.0609\n",
      "trainer/Q2 Predictions Std                               0.82877\n",
      "trainer/Q2 Predictions Max                               0.70105\n",
      "trainer/Q2 Predictions Min                              -3.2107\n",
      "trainer/Q Targets Mean                                  -1.06613\n",
      "trainer/Q Targets Std                                    0.836254\n",
      "trainer/Q Targets Max                                    0.716177\n",
      "trainer/Q Targets Min                                   -3.24291\n",
      "trainer/Log Pis Mean                                     1.96523\n",
      "trainer/Log Pis Std                                      1.2988\n",
      "trainer/Log Pis Max                                      4.62568\n",
      "trainer/Log Pis Min                                     -2.57531\n",
      "trainer/Policy mu Mean                                  -0.027083\n",
      "trainer/Policy mu Std                                    0.265751\n",
      "trainer/Policy mu Max                                    0.895987\n",
      "trainer/Policy mu Min                                   -1.82981\n",
      "trainer/Policy log std Mean                             -2.29266\n",
      "trainer/Policy log std Std                               0.590715\n",
      "trainer/Policy log std Max                              -0.195252\n",
      "trainer/Policy log std Min                              -3.23085\n",
      "trainer/Alpha                                            0.0230214\n",
      "trainer/Alpha Loss                                      -0.131146\n",
      "exploration/num steps total                           6600\n",
      "exploration/num paths total                            330\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0764029\n",
      "exploration/Rewards Std                                  0.104132\n",
      "exploration/Rewards Max                                  0.16182\n",
      "exploration/Rewards Min                                 -0.238052\n",
      "exploration/Returns Mean                                -1.52806\n",
      "exploration/Returns Std                                  1.93502\n",
      "exploration/Returns Max                                  1.98621\n",
      "exploration/Returns Min                                 -3.47602\n",
      "exploration/Actions Mean                                 0.00125412\n",
      "exploration/Actions Std                                  0.134406\n",
      "exploration/Actions Max                                  0.473603\n",
      "exploration/Actions Min                                 -0.880081\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.52806\n",
      "exploration/env_infos/final/reward_dist Mean             0.186837\n",
      "exploration/env_infos/final/reward_dist Std              0.324201\n",
      "exploration/env_infos/final/reward_dist Max              0.831045\n",
      "exploration/env_infos/final/reward_dist Min              4.90153e-18\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0149268\n",
      "exploration/env_infos/initial/reward_dist Std            0.0238764\n",
      "exploration/env_infos/initial/reward_dist Max            0.0623438\n",
      "exploration/env_infos/initial/reward_dist Min            5.19226e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0850211\n",
      "exploration/env_infos/reward_dist Std                    0.17913\n",
      "exploration/env_infos/reward_dist Max                    0.838509\n",
      "exploration/env_infos/reward_dist Min                    4.90153e-18\n",
      "exploration/env_infos/final/reward_energy Mean          -0.180544\n",
      "exploration/env_infos/final/reward_energy Std            0.0990073\n",
      "exploration/env_infos/final/reward_energy Max           -0.0667163\n",
      "exploration/env_infos/final/reward_energy Min           -0.349078\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.269136\n",
      "exploration/env_infos/initial/reward_energy Std          0.330088\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0220289\n",
      "exploration/env_infos/initial/reward_energy Min         -0.892314\n",
      "exploration/env_infos/reward_energy Mean                -0.131652\n",
      "exploration/env_infos/reward_energy Std                  0.137116\n",
      "exploration/env_infos/reward_energy Max                 -0.00727495\n",
      "exploration/env_infos/reward_energy Min                 -0.892314\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.033585\n",
      "exploration/env_infos/final/end_effector_loc Std         0.262782\n",
      "exploration/env_infos/final/end_effector_loc Max         0.639494\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.301857\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00335862\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0146785\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0160162\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.044004\n",
      "exploration/env_infos/end_effector_loc Mean              0.0107712\n",
      "exploration/env_infos/end_effector_loc Std               0.168243\n",
      "exploration/env_infos/end_effector_loc Max               0.639494\n",
      "exploration/env_infos/end_effector_loc Min              -0.303993\n",
      "evaluation/num steps total                           56000\n",
      "evaluation/num paths total                            2800\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0480364\n",
      "evaluation/Rewards Std                                   0.0750632\n",
      "evaluation/Rewards Max                                   0.126706\n",
      "evaluation/Rewards Min                                  -0.456857\n",
      "evaluation/Returns Mean                                 -0.960727\n",
      "evaluation/Returns Std                                   1.12656\n",
      "evaluation/Returns Max                                   1.77404\n",
      "evaluation/Returns Min                                  -4.81758\n",
      "evaluation/Actions Mean                                  0.000799285\n",
      "evaluation/Actions Std                                   0.0720022\n",
      "evaluation/Actions Max                                   0.393275\n",
      "evaluation/Actions Min                                  -0.844084\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.960727\n",
      "evaluation/env_infos/final/reward_dist Mean              0.130547\n",
      "evaluation/env_infos/final/reward_dist Std               0.233542\n",
      "evaluation/env_infos/final/reward_dist Max               0.878043\n",
      "evaluation/env_infos/final/reward_dist Min               6.89677e-78\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00950609\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0171082\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0907809\n",
      "evaluation/env_infos/initial/reward_dist Min             9.45511e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.188513\n",
      "evaluation/env_infos/reward_dist Std                     0.259368\n",
      "evaluation/env_infos/reward_dist Max                     0.997478\n",
      "evaluation/env_infos/reward_dist Min                     6.89677e-78\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0778148\n",
      "evaluation/env_infos/final/reward_energy Std             0.0925562\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00349504\n",
      "evaluation/env_infos/final/reward_energy Min            -0.548578\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.194743\n",
      "evaluation/env_infos/initial/reward_energy Std           0.192678\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0289425\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.9069\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0620299\n",
      "evaluation/env_infos/reward_energy Std                   0.0807601\n",
      "evaluation/env_infos/reward_energy Max                  -0.00182103\n",
      "evaluation/env_infos/reward_energy Min                  -0.9069\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0408104\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.27453\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.471268\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000318141\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00968045\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0196637\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0422042\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0181344\n",
      "evaluation/env_infos/end_effector_loc Std                0.172707\n",
      "evaluation/env_infos/end_effector_loc Max                0.471268\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.00313594\n",
      "time/evaluation sampling (s)                             1.10513\n",
      "time/exploration sampling (s)                            0.187586\n",
      "time/logging (s)                                         0.0200025\n",
      "time/saving (s)                                          0.0302901\n",
      "time/training (s)                                       56.3763\n",
      "time/epoch (s)                                          57.7225\n",
      "time/total (s)                                        2768.03\n",
      "Epoch                                                   55\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:00:19.720256 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 56 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00103821\n",
      "trainer/QF2 Loss                                         0.000697852\n",
      "trainer/Policy Loss                                      3.18031\n",
      "trainer/Q1 Predictions Mean                             -1.11693\n",
      "trainer/Q1 Predictions Std                               0.864548\n",
      "trainer/Q1 Predictions Max                               1.37104\n",
      "trainer/Q1 Predictions Min                              -2.99365\n",
      "trainer/Q2 Predictions Mean                             -1.13266\n",
      "trainer/Q2 Predictions Std                               0.872073\n",
      "trainer/Q2 Predictions Max                               1.38206\n",
      "trainer/Q2 Predictions Min                              -3.01791\n",
      "trainer/Q Targets Mean                                  -1.12416\n",
      "trainer/Q Targets Std                                    0.864477\n",
      "trainer/Q Targets Max                                    1.36514\n",
      "trainer/Q Targets Min                                   -3.00417\n",
      "trainer/Log Pis Mean                                     2.05176\n",
      "trainer/Log Pis Std                                      1.30808\n",
      "trainer/Log Pis Max                                      4.28097\n",
      "trainer/Log Pis Min                                     -2.14099\n",
      "trainer/Policy mu Mean                                  -0.0145926\n",
      "trainer/Policy mu Std                                    0.176873\n",
      "trainer/Policy mu Max                                    0.677015\n",
      "trainer/Policy mu Min                                   -1.3311\n",
      "trainer/Policy log std Mean                             -2.36058\n",
      "trainer/Policy log std Std                               0.559371\n",
      "trainer/Policy log std Max                              -0.578373\n",
      "trainer/Policy log std Min                              -3.36633\n",
      "trainer/Alpha                                            0.0234853\n",
      "trainer/Alpha Loss                                       0.19414\n",
      "exploration/num steps total                           6700\n",
      "exploration/num paths total                            335\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0919842\n",
      "exploration/Rewards Std                                  0.11143\n",
      "exploration/Rewards Max                                  0.111324\n",
      "exploration/Rewards Min                                 -0.540389\n",
      "exploration/Returns Mean                                -1.83968\n",
      "exploration/Returns Std                                  1.36444\n",
      "exploration/Returns Max                                  0.0752275\n",
      "exploration/Returns Min                                 -3.78938\n",
      "exploration/Actions Mean                                 0.00801843\n",
      "exploration/Actions Std                                  0.105194\n",
      "exploration/Actions Max                                  0.532639\n",
      "exploration/Actions Min                                 -0.437578\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.83968\n",
      "exploration/env_infos/final/reward_dist Mean             0.0544772\n",
      "exploration/env_infos/final/reward_dist Std              0.108954\n",
      "exploration/env_infos/final/reward_dist Max              0.272386\n",
      "exploration/env_infos/final/reward_dist Min              4.81574e-22\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00540059\n",
      "exploration/env_infos/initial/reward_dist Std            0.00649916\n",
      "exploration/env_infos/initial/reward_dist Max            0.0179335\n",
      "exploration/env_infos/initial/reward_dist Min            5.76774e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.175592\n",
      "exploration/env_infos/reward_dist Std                    0.287512\n",
      "exploration/env_infos/reward_dist Max                    0.982445\n",
      "exploration/env_infos/reward_dist Min                    4.81574e-22\n",
      "exploration/env_infos/final/reward_energy Mean          -0.209329\n",
      "exploration/env_infos/final/reward_energy Std            0.193591\n",
      "exploration/env_infos/final/reward_energy Max           -0.05436\n",
      "exploration/env_infos/final/reward_energy Min           -0.591051\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.185301\n",
      "exploration/env_infos/initial/reward_energy Std          0.142165\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0188189\n",
      "exploration/env_infos/initial/reward_energy Min         -0.437981\n",
      "exploration/env_infos/reward_energy Mean                -0.122424\n",
      "exploration/env_infos/reward_energy Std                  0.0852795\n",
      "exploration/env_infos/reward_energy Max                 -0.00298507\n",
      "exploration/env_infos/reward_energy Min                 -0.591051\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.00189352\n",
      "exploration/env_infos/final/end_effector_loc Std         0.374657\n",
      "exploration/env_infos/final/end_effector_loc Max         0.495658\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.759981\n",
      "exploration/env_infos/initial/end_effector_loc Mean      2.25967e-05\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00825732\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00994174\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0218789\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0140311\n",
      "exploration/env_infos/end_effector_loc Std               0.212849\n",
      "exploration/env_infos/end_effector_loc Max               0.495658\n",
      "exploration/env_infos/end_effector_loc Min              -0.759981\n",
      "evaluation/num steps total                           57000\n",
      "evaluation/num paths total                            2850\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0460892\n",
      "evaluation/Rewards Std                                   0.107942\n",
      "evaluation/Rewards Max                                   0.144132\n",
      "evaluation/Rewards Min                                  -0.896632\n",
      "evaluation/Returns Mean                                 -0.921784\n",
      "evaluation/Returns Std                                   1.7149\n",
      "evaluation/Returns Max                                   1.88585\n",
      "evaluation/Returns Min                                  -9.6688\n",
      "evaluation/Actions Mean                                 -0.000575136\n",
      "evaluation/Actions Std                                   0.0851187\n",
      "evaluation/Actions Max                                   0.434438\n",
      "evaluation/Actions Min                                  -0.774292\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.921784\n",
      "evaluation/env_infos/final/reward_dist Mean              0.154316\n",
      "evaluation/env_infos/final/reward_dist Std               0.264287\n",
      "evaluation/env_infos/final/reward_dist Max               0.991114\n",
      "evaluation/env_infos/final/reward_dist Min               3.78169e-77\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00856504\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0243941\n",
      "evaluation/env_infos/initial/reward_dist Max             0.160821\n",
      "evaluation/env_infos/initial/reward_dist Min             1.79399e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.228195\n",
      "evaluation/env_infos/reward_dist Std                     0.296986\n",
      "evaluation/env_infos/reward_dist Max                     0.997694\n",
      "evaluation/env_infos/reward_dist Min                     3.78169e-77\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0711521\n",
      "evaluation/env_infos/final/reward_energy Std             0.105453\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00555379\n",
      "evaluation/env_infos/final/reward_energy Min            -0.596977\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.26154\n",
      "evaluation/env_infos/initial/reward_energy Std           0.224334\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0222993\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.860797\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0705596\n",
      "evaluation/env_infos/reward_energy Std                   0.0975315\n",
      "evaluation/env_infos/reward_energy Max                  -0.00178729\n",
      "evaluation/env_infos/reward_energy Min                  -0.860797\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0369735\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.286926\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.669286\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00131289\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0121115\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0217219\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0387146\n",
      "evaluation/env_infos/end_effector_loc Mean               0.015287\n",
      "evaluation/env_infos/end_effector_loc Std                0.18299\n",
      "evaluation/env_infos/end_effector_loc Max                0.669286\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.00313623\n",
      "time/evaluation sampling (s)                             1.01286\n",
      "time/exploration sampling (s)                            0.142791\n",
      "time/logging (s)                                         0.0229765\n",
      "time/saving (s)                                          0.0330532\n",
      "time/training (s)                                       52.0391\n",
      "time/epoch (s)                                          53.2539\n",
      "time/total (s)                                        2821.99\n",
      "Epoch                                                   56\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:01:17.516643 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 57 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000736703\n",
      "trainer/QF2 Loss                                         0.00139431\n",
      "trainer/Policy Loss                                      3.07645\n",
      "trainer/Q1 Predictions Mean                             -1.11824\n",
      "trainer/Q1 Predictions Std                               0.848807\n",
      "trainer/Q1 Predictions Max                               1.3714\n",
      "trainer/Q1 Predictions Min                              -3.08349\n",
      "trainer/Q2 Predictions Mean                             -1.13871\n",
      "trainer/Q2 Predictions Std                               0.854693\n",
      "trainer/Q2 Predictions Max                               1.36565\n",
      "trainer/Q2 Predictions Min                              -3.14645\n",
      "trainer/Q Targets Mean                                  -1.11986\n",
      "trainer/Q Targets Std                                    0.850055\n",
      "trainer/Q Targets Max                                    1.36535\n",
      "trainer/Q Targets Min                                   -3.10122\n",
      "trainer/Log Pis Mean                                     1.93985\n",
      "trainer/Log Pis Std                                      1.39609\n",
      "trainer/Log Pis Max                                      4.15604\n",
      "trainer/Log Pis Min                                     -2.32912\n",
      "trainer/Policy mu Mean                                  -0.0272378\n",
      "trainer/Policy mu Std                                    0.218933\n",
      "trainer/Policy mu Max                                    0.883041\n",
      "trainer/Policy mu Min                                   -1.63654\n",
      "trainer/Policy log std Mean                             -2.361\n",
      "trainer/Policy log std Std                               0.612402\n",
      "trainer/Policy log std Max                               0.165138\n",
      "trainer/Policy log std Min                              -3.25185\n",
      "trainer/Alpha                                            0.0238173\n",
      "trainer/Alpha Loss                                      -0.224707\n",
      "exploration/num steps total                           6800\n",
      "exploration/num paths total                            340\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.107745\n",
      "exploration/Rewards Std                                  0.0765248\n",
      "exploration/Rewards Max                                  0.0742506\n",
      "exploration/Rewards Min                                 -0.340967\n",
      "exploration/Returns Mean                                -2.15491\n",
      "exploration/Returns Std                                  1.07939\n",
      "exploration/Returns Max                                 -0.139814\n",
      "exploration/Returns Min                                 -3.1687\n",
      "exploration/Actions Mean                                 0.0111467\n",
      "exploration/Actions Std                                  0.132408\n",
      "exploration/Actions Max                                  0.521979\n",
      "exploration/Actions Min                                 -0.439911\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.15491\n",
      "exploration/env_infos/final/reward_dist Mean             0.141021\n",
      "exploration/env_infos/final/reward_dist Std              0.206453\n",
      "exploration/env_infos/final/reward_dist Max              0.531406\n",
      "exploration/env_infos/final/reward_dist Min              8.0462e-19\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000176101\n",
      "exploration/env_infos/initial/reward_dist Std            0.000147691\n",
      "exploration/env_infos/initial/reward_dist Max            0.000420959\n",
      "exploration/env_infos/initial/reward_dist Min            1.01247e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.19069\n",
      "exploration/env_infos/reward_dist Std                    0.291601\n",
      "exploration/env_infos/reward_dist Max                    0.967412\n",
      "exploration/env_infos/reward_dist Min                    8.0462e-19\n",
      "exploration/env_infos/final/reward_energy Mean          -0.115605\n",
      "exploration/env_infos/final/reward_energy Std            0.0603252\n",
      "exploration/env_infos/final/reward_energy Max           -0.0468414\n",
      "exploration/env_infos/final/reward_energy Min           -0.223197\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.325379\n",
      "exploration/env_infos/initial/reward_energy Std          0.167988\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0845183\n",
      "exploration/env_infos/initial/reward_energy Min         -0.57063\n",
      "exploration/env_infos/reward_energy Mean                -0.153218\n",
      "exploration/env_infos/reward_energy Std                  0.108795\n",
      "exploration/env_infos/reward_energy Max                 -0.0131685\n",
      "exploration/env_infos/reward_energy Min                 -0.57063\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0460593\n",
      "exploration/env_infos/final/end_effector_loc Std         0.321278\n",
      "exploration/env_infos/final/end_effector_loc Max         0.572841\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.348545\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00305663\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0125806\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0260989\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0219955\n",
      "exploration/env_infos/end_effector_loc Mean              0.01469\n",
      "exploration/env_infos/end_effector_loc Std               0.211635\n",
      "exploration/env_infos/end_effector_loc Max               0.572841\n",
      "exploration/env_infos/end_effector_loc Min              -0.393952\n",
      "evaluation/num steps total                           58000\n",
      "evaluation/num paths total                            2900\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0628498\n",
      "evaluation/Rewards Std                                   0.0752698\n",
      "evaluation/Rewards Max                                   0.147402\n",
      "evaluation/Rewards Min                                  -0.558639\n",
      "evaluation/Returns Mean                                 -1.257\n",
      "evaluation/Returns Std                                   1.06187\n",
      "evaluation/Returns Max                                   0.936845\n",
      "evaluation/Returns Min                                  -3.73891\n",
      "evaluation/Actions Mean                                 -0.00691471\n",
      "evaluation/Actions Std                                   0.0696975\n",
      "evaluation/Actions Max                                   0.430153\n",
      "evaluation/Actions Min                                  -0.696\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.257\n",
      "evaluation/env_infos/final/reward_dist Mean              0.130742\n",
      "evaluation/env_infos/final/reward_dist Std               0.247027\n",
      "evaluation/env_infos/final/reward_dist Max               0.95122\n",
      "evaluation/env_infos/final/reward_dist Min               3.98917e-53\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00562622\n",
      "evaluation/env_infos/initial/reward_dist Std             0.01203\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0605262\n",
      "evaluation/env_infos/initial/reward_dist Min             1.13677e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.157453\n",
      "evaluation/env_infos/reward_dist Std                     0.260578\n",
      "evaluation/env_infos/reward_dist Max                     0.995197\n",
      "evaluation/env_infos/reward_dist Min                     3.98917e-53\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0693599\n",
      "evaluation/env_infos/final/reward_energy Std             0.0884549\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00376967\n",
      "evaluation/env_infos/final/reward_energy Min            -0.410207\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.186663\n",
      "evaluation/env_infos/initial/reward_energy Std           0.182228\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0117582\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.797529\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0584573\n",
      "evaluation/env_infos/reward_energy Std                   0.0799616\n",
      "evaluation/env_infos/reward_energy Max                  -0.000445657\n",
      "evaluation/env_infos/reward_energy Min                  -0.797529\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0719455\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.286102\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.561632\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.954515\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00153214\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00909479\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0215076\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0348\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0310406\n",
      "evaluation/env_infos/end_effector_loc Std                0.166966\n",
      "evaluation/env_infos/end_effector_loc Max                0.561632\n",
      "evaluation/env_infos/end_effector_loc Min               -0.954515\n",
      "time/data storing (s)                                    0.00323538\n",
      "time/evaluation sampling (s)                             1.18516\n",
      "time/exploration sampling (s)                            0.138189\n",
      "time/logging (s)                                         0.0216149\n",
      "time/saving (s)                                          0.0384991\n",
      "time/training (s)                                       55.4912\n",
      "time/epoch (s)                                          56.8779\n",
      "time/total (s)                                        2879.78\n",
      "Epoch                                                   57\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:02:08.336329 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 58 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000976209\n",
      "trainer/QF2 Loss                                         0.000556483\n",
      "trainer/Policy Loss                                      3.10385\n",
      "trainer/Q1 Predictions Mean                             -1.13657\n",
      "trainer/Q1 Predictions Std                               0.913435\n",
      "trainer/Q1 Predictions Max                               1.4514\n",
      "trainer/Q1 Predictions Min                              -3.15312\n",
      "trainer/Q2 Predictions Mean                             -1.13386\n",
      "trainer/Q2 Predictions Std                               0.906887\n",
      "trainer/Q2 Predictions Max                               1.41451\n",
      "trainer/Q2 Predictions Min                              -3.12819\n",
      "trainer/Q Targets Mean                                  -1.13305\n",
      "trainer/Q Targets Std                                    0.906633\n",
      "trainer/Q Targets Max                                    1.41203\n",
      "trainer/Q Targets Min                                   -3.12796\n",
      "trainer/Log Pis Mean                                     1.96587\n",
      "trainer/Log Pis Std                                      1.36929\n",
      "trainer/Log Pis Max                                      4.27113\n",
      "trainer/Log Pis Min                                     -2.901\n",
      "trainer/Policy mu Mean                                  -0.0107412\n",
      "trainer/Policy mu Std                                    0.289848\n",
      "trainer/Policy mu Max                                    2.03518\n",
      "trainer/Policy mu Min                                   -2.12561\n",
      "trainer/Policy log std Mean                             -2.34451\n",
      "trainer/Policy log std Std                               0.555469\n",
      "trainer/Policy log std Max                              -0.483498\n",
      "trainer/Policy log std Min                              -3.17089\n",
      "trainer/Alpha                                            0.0237251\n",
      "trainer/Alpha Loss                                      -0.127666\n",
      "exploration/num steps total                           6900\n",
      "exploration/num paths total                            345\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0637124\n",
      "exploration/Rewards Std                                  0.0641904\n",
      "exploration/Rewards Max                                  0.0882144\n",
      "exploration/Rewards Min                                 -0.232782\n",
      "exploration/Returns Mean                                -1.27425\n",
      "exploration/Returns Std                                  0.839123\n",
      "exploration/Returns Max                                 -0.545506\n",
      "exploration/Returns Min                                 -2.78509\n",
      "exploration/Actions Mean                                -0.00674216\n",
      "exploration/Actions Std                                  0.10708\n",
      "exploration/Actions Max                                  0.399047\n",
      "exploration/Actions Min                                 -0.369892\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.27425\n",
      "exploration/env_infos/final/reward_dist Mean             0.225606\n",
      "exploration/env_infos/final/reward_dist Std              0.201701\n",
      "exploration/env_infos/final/reward_dist Max              0.478563\n",
      "exploration/env_infos/final/reward_dist Min              1.75888e-07\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0102332\n",
      "exploration/env_infos/initial/reward_dist Std            0.0187589\n",
      "exploration/env_infos/initial/reward_dist Max            0.04772\n",
      "exploration/env_infos/initial/reward_dist Min            1.12261e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.218896\n",
      "exploration/env_infos/reward_dist Std                    0.298107\n",
      "exploration/env_infos/reward_dist Max                    0.969618\n",
      "exploration/env_infos/reward_dist Min                    1.75888e-07\n",
      "exploration/env_infos/final/reward_energy Mean          -0.140288\n",
      "exploration/env_infos/final/reward_energy Std            0.081693\n",
      "exploration/env_infos/final/reward_energy Max           -0.0446855\n",
      "exploration/env_infos/final/reward_energy Min           -0.268648\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.174828\n",
      "exploration/env_infos/initial/reward_energy Std          0.160067\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0265545\n",
      "exploration/env_infos/initial/reward_energy Min         -0.451147\n",
      "exploration/env_infos/reward_energy Mean                -0.120361\n",
      "exploration/env_infos/reward_energy Std                  0.092392\n",
      "exploration/env_infos/reward_energy Max                 -0.00737304\n",
      "exploration/env_infos/reward_energy Min                 -0.451147\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0113732\n",
      "exploration/env_infos/final/end_effector_loc Std         0.219228\n",
      "exploration/env_infos/final/end_effector_loc Max         0.415162\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.319477\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.000944217\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00832714\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0199523\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0105232\n",
      "exploration/env_infos/end_effector_loc Mean              0.0129345\n",
      "exploration/env_infos/end_effector_loc Std               0.138301\n",
      "exploration/env_infos/end_effector_loc Max               0.415162\n",
      "exploration/env_infos/end_effector_loc Min              -0.319477\n",
      "evaluation/num steps total                           59000\n",
      "evaluation/num paths total                            2950\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0580023\n",
      "evaluation/Rewards Std                                   0.0984882\n",
      "evaluation/Rewards Max                                   0.149515\n",
      "evaluation/Rewards Min                                  -0.625158\n",
      "evaluation/Returns Mean                                 -1.16005\n",
      "evaluation/Returns Std                                   1.535\n",
      "evaluation/Returns Max                                   1.24575\n",
      "evaluation/Returns Min                                  -6.05891\n",
      "evaluation/Actions Mean                                 -0.00258568\n",
      "evaluation/Actions Std                                   0.0869172\n",
      "evaluation/Actions Max                                   0.848333\n",
      "evaluation/Actions Min                                  -0.75162\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.16005\n",
      "evaluation/env_infos/final/reward_dist Mean              0.172223\n",
      "evaluation/env_infos/final/reward_dist Std               0.278525\n",
      "evaluation/env_infos/final/reward_dist Max               0.993757\n",
      "evaluation/env_infos/final/reward_dist Min               4.2049e-42\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00447183\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00827516\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0426785\n",
      "evaluation/env_infos/initial/reward_dist Min             4.92073e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.182244\n",
      "evaluation/env_infos/reward_dist Std                     0.270307\n",
      "evaluation/env_infos/reward_dist Max                     0.998749\n",
      "evaluation/env_infos/reward_dist Min                     4.2049e-42\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0623619\n",
      "evaluation/env_infos/final/reward_energy Std             0.0708515\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00242769\n",
      "evaluation/env_infos/final/reward_energy Min            -0.30588\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.265714\n",
      "evaluation/env_infos/initial/reward_energy Std           0.246532\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0175726\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.0769\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0700888\n",
      "evaluation/env_infos/reward_energy Std                   0.101045\n",
      "evaluation/env_infos/reward_energy Max                  -0.000338659\n",
      "evaluation/env_infos/reward_energy Min                  -1.0769\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.00128081\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.283692\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.754932\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.731241\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00163343\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0127106\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0424167\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.037581\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0126277\n",
      "evaluation/env_infos/end_effector_loc Std                0.188323\n",
      "evaluation/env_infos/end_effector_loc Max                0.754932\n",
      "evaluation/env_infos/end_effector_loc Min               -0.731241\n",
      "time/data storing (s)                                    0.00311883\n",
      "time/evaluation sampling (s)                             1.1009\n",
      "time/exploration sampling (s)                            0.123831\n",
      "time/logging (s)                                         0.0219677\n",
      "time/saving (s)                                          0.0334427\n",
      "time/training (s)                                       48.7375\n",
      "time/epoch (s)                                          50.0208\n",
      "time/total (s)                                        2930.6\n",
      "Epoch                                                   58\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:03:04.873486 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 59 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000782608\n",
      "trainer/QF2 Loss                                         0.000914096\n",
      "trainer/Policy Loss                                      3.0445\n",
      "trainer/Q1 Predictions Mean                             -1.0383\n",
      "trainer/Q1 Predictions Std                               0.875598\n",
      "trainer/Q1 Predictions Max                               1.14027\n",
      "trainer/Q1 Predictions Min                              -3.20058\n",
      "trainer/Q2 Predictions Mean                             -1.03925\n",
      "trainer/Q2 Predictions Std                               0.867133\n",
      "trainer/Q2 Predictions Max                               1.09196\n",
      "trainer/Q2 Predictions Min                              -3.14994\n",
      "trainer/Q Targets Mean                                  -1.03407\n",
      "trainer/Q Targets Std                                    0.868148\n",
      "trainer/Q Targets Max                                    1.12238\n",
      "trainer/Q Targets Min                                   -3.15873\n",
      "trainer/Log Pis Mean                                     2.00652\n",
      "trainer/Log Pis Std                                      1.33477\n",
      "trainer/Log Pis Max                                      4.30143\n",
      "trainer/Log Pis Min                                     -3.96297\n",
      "trainer/Policy mu Mean                                   0.00192278\n",
      "trainer/Policy mu Std                                    0.244312\n",
      "trainer/Policy mu Max                                    1.40005\n",
      "trainer/Policy mu Min                                   -1.86538\n",
      "trainer/Policy log std Mean                             -2.3422\n",
      "trainer/Policy log std Std                               0.574239\n",
      "trainer/Policy log std Max                              -0.494797\n",
      "trainer/Policy log std Min                              -3.3438\n",
      "trainer/Alpha                                            0.0246097\n",
      "trainer/Alpha Loss                                       0.0241708\n",
      "exploration/num steps total                           7000\n",
      "exploration/num paths total                            350\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.106712\n",
      "exploration/Rewards Std                                  0.063485\n",
      "exploration/Rewards Max                                  0.0208196\n",
      "exploration/Rewards Min                                 -0.299185\n",
      "exploration/Returns Mean                                -2.13424\n",
      "exploration/Returns Std                                  0.724292\n",
      "exploration/Returns Max                                 -0.834191\n",
      "exploration/Returns Min                                 -2.89898\n",
      "exploration/Actions Mean                                -0.0113687\n",
      "exploration/Actions Std                                  0.0964343\n",
      "exploration/Actions Max                                  0.491671\n",
      "exploration/Actions Min                                 -0.342736\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.13424\n",
      "exploration/env_infos/final/reward_dist Mean             0.0055943\n",
      "exploration/env_infos/final/reward_dist Std              0.0111753\n",
      "exploration/env_infos/final/reward_dist Max              0.0279448\n",
      "exploration/env_infos/final/reward_dist Min              4.81377e-18\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0115378\n",
      "exploration/env_infos/initial/reward_dist Std            0.0144357\n",
      "exploration/env_infos/initial/reward_dist Max            0.0387283\n",
      "exploration/env_infos/initial/reward_dist Min            2.96002e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.178113\n",
      "exploration/env_infos/reward_dist Std                    0.261905\n",
      "exploration/env_infos/reward_dist Max                    0.923988\n",
      "exploration/env_infos/reward_dist Min                    4.81377e-18\n",
      "exploration/env_infos/final/reward_energy Mean          -0.200212\n",
      "exploration/env_infos/final/reward_energy Std            0.0742213\n",
      "exploration/env_infos/final/reward_energy Max           -0.0821549\n",
      "exploration/env_infos/final/reward_energy Min           -0.285096\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.166925\n",
      "exploration/env_infos/initial/reward_energy Std          0.109903\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0626359\n",
      "exploration/env_infos/initial/reward_energy Min         -0.344821\n",
      "exploration/env_infos/reward_energy Mean                -0.112005\n",
      "exploration/env_infos/reward_energy Std                  0.0794508\n",
      "exploration/env_infos/reward_energy Max                 -0.00264672\n",
      "exploration/env_infos/reward_energy Min                 -0.496845\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0859518\n",
      "exploration/env_infos/final/end_effector_loc Std         0.330942\n",
      "exploration/env_infos/final/end_effector_loc Max         0.648032\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.40822\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00121525\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00696071\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0113109\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.012883\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0270654\n",
      "exploration/env_infos/end_effector_loc Std               0.199471\n",
      "exploration/env_infos/end_effector_loc Max               0.648032\n",
      "exploration/env_infos/end_effector_loc Min              -0.40822\n",
      "evaluation/num steps total                           60000\n",
      "evaluation/num paths total                            3000\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0538115\n",
      "evaluation/Rewards Std                                   0.0700771\n",
      "evaluation/Rewards Max                                   0.152198\n",
      "evaluation/Rewards Min                                  -0.327423\n",
      "evaluation/Returns Mean                                 -1.07623\n",
      "evaluation/Returns Std                                   1.0822\n",
      "evaluation/Returns Max                                   1.15248\n",
      "evaluation/Returns Min                                  -3.57458\n",
      "evaluation/Actions Mean                                  0.00159971\n",
      "evaluation/Actions Std                                   0.0588618\n",
      "evaluation/Actions Max                                   0.579086\n",
      "evaluation/Actions Min                                  -0.658098\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.07623\n",
      "evaluation/env_infos/final/reward_dist Mean              0.163761\n",
      "evaluation/env_infos/final/reward_dist Std               0.240206\n",
      "evaluation/env_infos/final/reward_dist Max               0.943602\n",
      "evaluation/env_infos/final/reward_dist Min               1.57219e-28\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00632625\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0118294\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0648479\n",
      "evaluation/env_infos/initial/reward_dist Min             1.21272e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.176089\n",
      "evaluation/env_infos/reward_dist Std                     0.237339\n",
      "evaluation/env_infos/reward_dist Max                     0.991186\n",
      "evaluation/env_infos/reward_dist Min                     1.57219e-28\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0439887\n",
      "evaluation/env_infos/final/reward_energy Std             0.0433368\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00175422\n",
      "evaluation/env_infos/final/reward_energy Min            -0.231913\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.165517\n",
      "evaluation/env_infos/initial/reward_energy Std           0.168008\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0144981\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.725906\n",
      "evaluation/env_infos/reward_energy Mean                 -0.05179\n",
      "evaluation/env_infos/reward_energy Std                   0.0652099\n",
      "evaluation/env_infos/reward_energy Max                  -0.000944197\n",
      "evaluation/env_infos/reward_energy Min                  -0.725906\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0350867\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.228897\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.767799\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.50932\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00153801\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00819529\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0289543\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0329049\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0202614\n",
      "evaluation/env_infos/end_effector_loc Std                0.146125\n",
      "evaluation/env_infos/end_effector_loc Max                0.767799\n",
      "evaluation/env_infos/end_effector_loc Min               -0.50932\n",
      "time/data storing (s)                                    0.00313444\n",
      "time/evaluation sampling (s)                             1.07539\n",
      "time/exploration sampling (s)                            0.136294\n",
      "time/logging (s)                                         0.0214411\n",
      "time/saving (s)                                          0.0321326\n",
      "time/training (s)                                       54.503\n",
      "time/epoch (s)                                          55.7714\n",
      "time/total (s)                                        2987.14\n",
      "Epoch                                                   59\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:03:59.500220 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 60 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000758772\n",
      "trainer/QF2 Loss                                         0.00123063\n",
      "trainer/Policy Loss                                      3.00413\n",
      "trainer/Q1 Predictions Mean                             -1.0597\n",
      "trainer/Q1 Predictions Std                               0.871687\n",
      "trainer/Q1 Predictions Max                               0.854861\n",
      "trainer/Q1 Predictions Min                              -3.29896\n",
      "trainer/Q2 Predictions Mean                             -1.0733\n",
      "trainer/Q2 Predictions Std                               0.871997\n",
      "trainer/Q2 Predictions Max                               0.820439\n",
      "trainer/Q2 Predictions Min                              -3.35344\n",
      "trainer/Q Targets Mean                                  -1.05068\n",
      "trainer/Q Targets Std                                    0.869957\n",
      "trainer/Q Targets Max                                    0.835995\n",
      "trainer/Q Targets Min                                   -3.41339\n",
      "trainer/Log Pis Mean                                     1.9389\n",
      "trainer/Log Pis Std                                      1.3448\n",
      "trainer/Log Pis Max                                      4.24585\n",
      "trainer/Log Pis Min                                     -2.92971\n",
      "trainer/Policy mu Mean                                  -0.00811573\n",
      "trainer/Policy mu Std                                    0.266706\n",
      "trainer/Policy mu Max                                    1.74583\n",
      "trainer/Policy mu Min                                   -1.85107\n",
      "trainer/Policy log std Mean                             -2.27666\n",
      "trainer/Policy log std Std                               0.559586\n",
      "trainer/Policy log std Max                              -0.270967\n",
      "trainer/Policy log std Min                              -3.30598\n",
      "trainer/Alpha                                            0.0239073\n",
      "trainer/Alpha Loss                                      -0.22809\n",
      "exploration/num steps total                           7100\n",
      "exploration/num paths total                            355\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0826143\n",
      "exploration/Rewards Std                                  0.0992189\n",
      "exploration/Rewards Max                                  0.112004\n",
      "exploration/Rewards Min                                 -0.444243\n",
      "exploration/Returns Mean                                -1.65229\n",
      "exploration/Returns Std                                  1.37535\n",
      "exploration/Returns Max                                  0.639751\n",
      "exploration/Returns Min                                 -3.38329\n",
      "exploration/Actions Mean                                 0.0265292\n",
      "exploration/Actions Std                                  0.20738\n",
      "exploration/Actions Max                                  0.839389\n",
      "exploration/Actions Min                                 -0.615247\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.65229\n",
      "exploration/env_infos/final/reward_dist Mean             0.11065\n",
      "exploration/env_infos/final/reward_dist Std              0.141825\n",
      "exploration/env_infos/final/reward_dist Max              0.342756\n",
      "exploration/env_infos/final/reward_dist Min              3.12357e-39\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00251599\n",
      "exploration/env_infos/initial/reward_dist Std            0.00384338\n",
      "exploration/env_infos/initial/reward_dist Max            0.00994946\n",
      "exploration/env_infos/initial/reward_dist Min            2.30252e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.142978\n",
      "exploration/env_infos/reward_dist Std                    0.25517\n",
      "exploration/env_infos/reward_dist Max                    0.944986\n",
      "exploration/env_infos/reward_dist Min                    3.12357e-39\n",
      "exploration/env_infos/final/reward_energy Mean          -0.344343\n",
      "exploration/env_infos/final/reward_energy Std            0.180763\n",
      "exploration/env_infos/final/reward_energy Max           -0.0719853\n",
      "exploration/env_infos/final/reward_energy Min           -0.570626\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.363396\n",
      "exploration/env_infos/initial/reward_energy Std          0.266043\n",
      "exploration/env_infos/initial/reward_energy Max         -0.111392\n",
      "exploration/env_infos/initial/reward_energy Min         -0.860793\n",
      "exploration/env_infos/reward_energy Mean                -0.233865\n",
      "exploration/env_infos/reward_energy Std                  0.180909\n",
      "exploration/env_infos/reward_energy Max                 -0.0200967\n",
      "exploration/env_infos/reward_energy Min                 -0.860793\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.204942\n",
      "exploration/env_infos/final/end_effector_loc Std         0.341885\n",
      "exploration/env_infos/final/end_effector_loc Max         0.746935\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.158708\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00604595\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0147306\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0419695\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0198725\n",
      "exploration/env_infos/end_effector_loc Mean              0.101134\n",
      "exploration/env_infos/end_effector_loc Std               0.215568\n",
      "exploration/env_infos/end_effector_loc Max               0.746935\n",
      "exploration/env_infos/end_effector_loc Min              -0.239929\n",
      "evaluation/num steps total                           61000\n",
      "evaluation/num paths total                            3050\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.065043\n",
      "evaluation/Rewards Std                                   0.0998449\n",
      "evaluation/Rewards Max                                   0.145244\n",
      "evaluation/Rewards Min                                  -0.958442\n",
      "evaluation/Returns Mean                                 -1.30086\n",
      "evaluation/Returns Std                                   1.5564\n",
      "evaluation/Returns Max                                   1.77085\n",
      "evaluation/Returns Min                                  -7.72202\n",
      "evaluation/Actions Mean                                 -0.00232126\n",
      "evaluation/Actions Std                                   0.0773594\n",
      "evaluation/Actions Max                                   0.392522\n",
      "evaluation/Actions Min                                  -0.836239\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.30086\n",
      "evaluation/env_infos/final/reward_dist Mean              0.110964\n",
      "evaluation/env_infos/final/reward_dist Std               0.184167\n",
      "evaluation/env_infos/final/reward_dist Max               0.801857\n",
      "evaluation/env_infos/final/reward_dist Min               2.65837e-80\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00695834\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0153477\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0891525\n",
      "evaluation/env_infos/initial/reward_dist Min             1.01567e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.142707\n",
      "evaluation/env_infos/reward_dist Std                     0.228521\n",
      "evaluation/env_infos/reward_dist Max                     0.991639\n",
      "evaluation/env_infos/reward_dist Min                     2.65837e-80\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0532923\n",
      "evaluation/env_infos/final/reward_energy Std             0.0682248\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00497583\n",
      "evaluation/env_infos/final/reward_energy Min            -0.405238\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.218493\n",
      "evaluation/env_infos/initial/reward_energy Std           0.239794\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00649765\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.929943\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0625255\n",
      "evaluation/env_infos/reward_energy Std                   0.0898349\n",
      "evaluation/env_infos/reward_energy Max                  -0.000964328\n",
      "evaluation/env_infos/reward_energy Min                  -0.929943\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0824807\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.296863\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.513556\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00470444\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0104603\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0196261\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.041812\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0583683\n",
      "evaluation/env_infos/end_effector_loc Std                0.188871\n",
      "evaluation/env_infos/end_effector_loc Max                0.513556\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.00306863\n",
      "time/evaluation sampling (s)                             1.44581\n",
      "time/exploration sampling (s)                            0.132341\n",
      "time/logging (s)                                         0.0230264\n",
      "time/saving (s)                                          0.0319096\n",
      "time/training (s)                                       52.1053\n",
      "time/epoch (s)                                          53.7415\n",
      "time/total (s)                                        3041.76\n",
      "Epoch                                                   60\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:04:51.661158 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 61 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.0012214\n",
      "trainer/QF2 Loss                                         0.000619664\n",
      "trainer/Policy Loss                                      3.28278\n",
      "trainer/Q1 Predictions Mean                             -1.16082\n",
      "trainer/Q1 Predictions Std                               0.840847\n",
      "trainer/Q1 Predictions Max                               1.28081\n",
      "trainer/Q1 Predictions Min                              -3.25172\n",
      "trainer/Q2 Predictions Mean                             -1.16785\n",
      "trainer/Q2 Predictions Std                               0.84773\n",
      "trainer/Q2 Predictions Max                               1.24534\n",
      "trainer/Q2 Predictions Min                              -3.2589\n",
      "trainer/Q Targets Mean                                  -1.16183\n",
      "trainer/Q Targets Std                                    0.843013\n",
      "trainer/Q Targets Max                                    1.23618\n",
      "trainer/Q Targets Min                                   -3.24207\n",
      "trainer/Log Pis Mean                                     2.11582\n",
      "trainer/Log Pis Std                                      1.28537\n",
      "trainer/Log Pis Max                                      4.27045\n",
      "trainer/Log Pis Min                                     -2.6927\n",
      "trainer/Policy mu Mean                                  -0.0272072\n",
      "trainer/Policy mu Std                                    0.291138\n",
      "trainer/Policy mu Max                                    1.64686\n",
      "trainer/Policy mu Min                                   -2.60137\n",
      "trainer/Policy log std Mean                             -2.3803\n",
      "trainer/Policy log std Std                               0.554214\n",
      "trainer/Policy log std Max                               0.476095\n",
      "trainer/Policy log std Min                              -3.29479\n",
      "trainer/Alpha                                            0.023034\n",
      "trainer/Alpha Loss                                       0.436729\n",
      "exploration/num steps total                           7200\n",
      "exploration/num paths total                            360\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0865088\n",
      "exploration/Rewards Std                                  0.0776299\n",
      "exploration/Rewards Max                                  0.0716411\n",
      "exploration/Rewards Min                                 -0.337966\n",
      "exploration/Returns Mean                                -1.73018\n",
      "exploration/Returns Std                                  0.946684\n",
      "exploration/Returns Max                                 -0.550522\n",
      "exploration/Returns Min                                 -3.23919\n",
      "exploration/Actions Mean                                -6.5368e-05\n",
      "exploration/Actions Std                                  0.167597\n",
      "exploration/Actions Max                                  0.645976\n",
      "exploration/Actions Min                                 -0.746401\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.73018\n",
      "exploration/env_infos/final/reward_dist Mean             0.1487\n",
      "exploration/env_infos/final/reward_dist Std              0.292279\n",
      "exploration/env_infos/final/reward_dist Max              0.733225\n",
      "exploration/env_infos/final/reward_dist Min              5.06713e-13\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0100371\n",
      "exploration/env_infos/initial/reward_dist Std            0.0146866\n",
      "exploration/env_infos/initial/reward_dist Max            0.0381545\n",
      "exploration/env_infos/initial/reward_dist Min            8.16505e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.170701\n",
      "exploration/env_infos/reward_dist Std                    0.276374\n",
      "exploration/env_infos/reward_dist Max                    0.945495\n",
      "exploration/env_infos/reward_dist Min                    2.81892e-13\n",
      "exploration/env_infos/final/reward_energy Mean          -0.224791\n",
      "exploration/env_infos/final/reward_energy Std            0.115314\n",
      "exploration/env_infos/final/reward_energy Max           -0.0611035\n",
      "exploration/env_infos/final/reward_energy Min           -0.384905\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.366105\n",
      "exploration/env_infos/initial/reward_energy Std          0.286352\n",
      "exploration/env_infos/initial/reward_energy Max         -0.124378\n",
      "exploration/env_infos/initial/reward_energy Min         -0.923587\n",
      "exploration/env_infos/reward_energy Mean                -0.186817\n",
      "exploration/env_infos/reward_energy Std                  0.145867\n",
      "exploration/env_infos/reward_energy Max                 -0.0280366\n",
      "exploration/env_infos/reward_energy Min                 -0.923587\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0293502\n",
      "exploration/env_infos/final/end_effector_loc Std         0.235665\n",
      "exploration/env_infos/final/end_effector_loc Max         0.306177\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.50276\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00500199\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0156531\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0109284\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0373201\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0286744\n",
      "exploration/env_infos/end_effector_loc Std               0.166731\n",
      "exploration/env_infos/end_effector_loc Max               0.306177\n",
      "exploration/env_infos/end_effector_loc Min              -0.505157\n",
      "evaluation/num steps total                           62000\n",
      "evaluation/num paths total                            3100\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0511719\n",
      "evaluation/Rewards Std                                   0.0959191\n",
      "evaluation/Rewards Max                                   0.174871\n",
      "evaluation/Rewards Min                                  -0.790145\n",
      "evaluation/Returns Mean                                 -1.02344\n",
      "evaluation/Returns Std                                   1.43992\n",
      "evaluation/Returns Max                                   2.24638\n",
      "evaluation/Returns Min                                  -7.16074\n",
      "evaluation/Actions Mean                                 -0.00581291\n",
      "evaluation/Actions Std                                   0.0783518\n",
      "evaluation/Actions Max                                   0.528124\n",
      "evaluation/Actions Min                                  -0.880301\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.02344\n",
      "evaluation/env_infos/final/reward_dist Mean              0.178361\n",
      "evaluation/env_infos/final/reward_dist Std               0.269663\n",
      "evaluation/env_infos/final/reward_dist Max               0.910572\n",
      "evaluation/env_infos/final/reward_dist Min               3.23083e-63\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00778542\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0121605\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0510564\n",
      "evaluation/env_infos/initial/reward_dist Min             1.71364e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.207906\n",
      "evaluation/env_infos/reward_dist Std                     0.279442\n",
      "evaluation/env_infos/reward_dist Max                     0.992906\n",
      "evaluation/env_infos/reward_dist Min                     3.23083e-63\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0563884\n",
      "evaluation/env_infos/final/reward_energy Std             0.0577781\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00594085\n",
      "evaluation/env_infos/final/reward_energy Min            -0.339755\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.236974\n",
      "evaluation/env_infos/initial/reward_energy Std           0.22122\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00544037\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.914412\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0706185\n",
      "evaluation/env_infos/reward_energy Std                   0.0857824\n",
      "evaluation/env_infos/reward_energy Max                  -0.00037715\n",
      "evaluation/env_infos/reward_energy Min                  -0.914412\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0697611\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.250344\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.373101\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00319469\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0110074\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0264062\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0440151\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0375194\n",
      "evaluation/env_infos/end_effector_loc Std                0.170166\n",
      "evaluation/env_infos/end_effector_loc Max                0.373101\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.00307126\n",
      "time/evaluation sampling (s)                             1.26943\n",
      "time/exploration sampling (s)                            0.133659\n",
      "time/logging (s)                                         0.0235034\n",
      "time/saving (s)                                          0.0314882\n",
      "time/training (s)                                       49.8557\n",
      "time/epoch (s)                                          51.3169\n",
      "time/total (s)                                        3093.92\n",
      "Epoch                                                   61\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:05:47.245635 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 62 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000634692\r\n",
      "trainer/QF2 Loss                                         0.000627221\r\n",
      "trainer/Policy Loss                                      2.91604\r\n",
      "trainer/Q1 Predictions Mean                             -0.997989\r\n",
      "trainer/Q1 Predictions Std                               0.871205\r\n",
      "trainer/Q1 Predictions Max                               1.2956\r\n",
      "trainer/Q1 Predictions Min                              -3.42981\r\n",
      "trainer/Q2 Predictions Mean                             -0.998782\r\n",
      "trainer/Q2 Predictions Std                               0.872183\r\n",
      "trainer/Q2 Predictions Max                               1.29475\r\n",
      "trainer/Q2 Predictions Min                              -3.37904\r\n",
      "trainer/Q Targets Mean                                  -0.996887\r\n",
      "trainer/Q Targets Std                                    0.871294\r\n",
      "trainer/Q Targets Max                                    1.25639\r\n",
      "trainer/Q Targets Min                                   -3.41908\r\n",
      "trainer/Log Pis Mean                                     1.91374\r\n",
      "trainer/Log Pis Std                                      1.28089\r\n",
      "trainer/Log Pis Max                                      5.30689\r\n",
      "trainer/Log Pis Min                                     -2.26128\r\n",
      "trainer/Policy mu Mean                                  -0.00171755\r\n",
      "trainer/Policy mu Std                                    0.300823\r\n",
      "trainer/Policy mu Max                                    2.14798\r\n",
      "trainer/Policy mu Min                                   -2.33292\r\n",
      "trainer/Policy log std Mean                             -2.32926\r\n",
      "trainer/Policy log std Std                               0.536209\r\n",
      "trainer/Policy log std Max                              -0.296443\r\n",
      "trainer/Policy log std Min                              -3.27395\r\n",
      "trainer/Alpha                                            0.0224037\r\n",
      "trainer/Alpha Loss                                      -0.327583\r\n",
      "exploration/num steps total                           7300\r\n",
      "exploration/num paths total                            365\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.103345\r\n",
      "exploration/Rewards Std                                  0.126713\r\n",
      "exploration/Rewards Max                                  0.116165\r\n",
      "exploration/Rewards Min                                 -0.393746\r\n",
      "exploration/Returns Mean                                -2.06691\r\n",
      "exploration/Returns Std                                  2.15482\r\n",
      "exploration/Returns Max                                  0.494875\r\n",
      "exploration/Returns Min                                 -5.61846\r\n",
      "exploration/Actions Mean                                -0.0226414\r\n",
      "exploration/Actions Std                                  0.154638\r\n",
      "exploration/Actions Max                                  0.511255\r\n",
      "exploration/Actions Min                                 -0.784482\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.06691\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0740026\r\n",
      "exploration/env_infos/final/reward_dist Std              0.139397\r\n",
      "exploration/env_infos/final/reward_dist Max              0.352465\r\n",
      "exploration/env_infos/final/reward_dist Min              2.8596e-56\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00767819\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0113012\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0300937\r\n",
      "exploration/env_infos/initial/reward_dist Min            5.81273e-05\r\n",
      "exploration/env_infos/reward_dist Mean                   0.204585\r\n",
      "exploration/env_infos/reward_dist Std                    0.326299\r\n",
      "exploration/env_infos/reward_dist Max                    0.995997\r\n",
      "exploration/env_infos/reward_dist Min                    1.41095e-57\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.236766\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0976469\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.129846\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.386049\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.361647\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.286251\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0770294\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.787053\r\n",
      "exploration/env_infos/reward_energy Mean                -0.174346\r\n",
      "exploration/env_infos/reward_energy Std                  0.135848\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00539338\r\n",
      "exploration/env_infos/reward_energy Min                 -0.787053\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.283214\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.415396\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.202268\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -1\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00581344\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0152353\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00858226\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0392241\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.16644\r\n",
      "exploration/env_infos/end_effector_loc Std               0.324875\r\n",
      "exploration/env_infos/end_effector_loc Max               0.202268\r\n",
      "exploration/env_infos/end_effector_loc Min              -1\r\n",
      "evaluation/num steps total                           63000\r\n",
      "evaluation/num paths total                            3150\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0621365\r\n",
      "evaluation/Rewards Std                                   0.0926426\r\n",
      "evaluation/Rewards Max                                   0.161391\r\n",
      "evaluation/Rewards Min                                  -0.826141\r\n",
      "evaluation/Returns Mean                                 -1.24273\r\n",
      "evaluation/Returns Std                                   1.37406\r\n",
      "evaluation/Returns Max                                   1.89437\r\n",
      "evaluation/Returns Min                                  -7.17047\r\n",
      "evaluation/Actions Mean                                 -0.00401785\r\n",
      "evaluation/Actions Std                                   0.065011\r\n",
      "evaluation/Actions Max                                   0.409663\r\n",
      "evaluation/Actions Min                                  -0.661217\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.24273\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.210435\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.284031\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.970319\r\n",
      "evaluation/env_infos/final/reward_dist Min               2.15102e-70\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00690663\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0111925\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0350946\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.24206e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.188523\r\n",
      "evaluation/env_infos/reward_dist Std                     0.266471\r\n",
      "evaluation/env_infos/reward_dist Max                     0.996731\r\n",
      "evaluation/env_infos/reward_dist Min                     2.15102e-70\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0532387\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0676217\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00431878\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.428684\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.182515\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.172348\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0174584\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.751374\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0571062\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0722774\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00110665\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.751374\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0597645\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.259055\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.638273\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00161082\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00872781\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0204832\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0330609\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0315314\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.167403\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.638273\r\n",
      "evaluation/env_infos/end_effector_loc Min               -1\r\n",
      "time/data storing (s)                                    0.00312113\r\n",
      "time/evaluation sampling (s)                             1.16687\r\n",
      "time/exploration sampling (s)                            0.128678\r\n",
      "time/logging (s)                                         0.0215988\r\n",
      "time/saving (s)                                          0.030707\r\n",
      "time/training (s)                                       53.3896\r\n",
      "time/epoch (s)                                          54.7406\r\n",
      "time/total (s)                                        3149.51\r\n",
      "Epoch                                                   62\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:06:45.850578 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 63 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00106572\n",
      "trainer/QF2 Loss                                         0.000923544\n",
      "trainer/Policy Loss                                      2.98495\n",
      "trainer/Q1 Predictions Mean                             -0.98084\n",
      "trainer/Q1 Predictions Std                               0.863898\n",
      "trainer/Q1 Predictions Max                               1.26089\n",
      "trainer/Q1 Predictions Min                              -3.51502\n",
      "trainer/Q2 Predictions Mean                             -0.986053\n",
      "trainer/Q2 Predictions Std                               0.864993\n",
      "trainer/Q2 Predictions Max                               1.25423\n",
      "trainer/Q2 Predictions Min                              -3.55425\n",
      "trainer/Q Targets Mean                                  -0.982552\n",
      "trainer/Q Targets Std                                    0.862493\n",
      "trainer/Q Targets Max                                    1.26187\n",
      "trainer/Q Targets Min                                   -3.50164\n",
      "trainer/Log Pis Mean                                     1.99904\n",
      "trainer/Log Pis Std                                      1.48848\n",
      "trainer/Log Pis Max                                      4.51535\n",
      "trainer/Log Pis Min                                     -4.5159\n",
      "trainer/Policy mu Mean                                  -0.0435677\n",
      "trainer/Policy mu Std                                    0.300672\n",
      "trainer/Policy mu Max                                    1.33234\n",
      "trainer/Policy mu Min                                   -1.96731\n",
      "trainer/Policy log std Mean                             -2.35595\n",
      "trainer/Policy log std Std                               0.644975\n",
      "trainer/Policy log std Max                              -0.0728238\n",
      "trainer/Policy log std Min                              -3.29243\n",
      "trainer/Alpha                                            0.0219921\n",
      "trainer/Alpha Loss                                      -0.00367916\n",
      "exploration/num steps total                           7400\n",
      "exploration/num paths total                            370\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0892055\n",
      "exploration/Rewards Std                                  0.0449565\n",
      "exploration/Rewards Max                                  0.00168235\n",
      "exploration/Rewards Min                                 -0.211877\n",
      "exploration/Returns Mean                                -1.78411\n",
      "exploration/Returns Std                                  0.243676\n",
      "exploration/Returns Max                                 -1.55699\n",
      "exploration/Returns Min                                 -2.14896\n",
      "exploration/Actions Mean                                 0.00524486\n",
      "exploration/Actions Std                                  0.0660066\n",
      "exploration/Actions Max                                  0.175894\n",
      "exploration/Actions Min                                 -0.244261\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.78411\n",
      "exploration/env_infos/final/reward_dist Mean             0.117463\n",
      "exploration/env_infos/final/reward_dist Std              0.177927\n",
      "exploration/env_infos/final/reward_dist Max              0.470724\n",
      "exploration/env_infos/final/reward_dist Min              0.00337346\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00246474\n",
      "exploration/env_infos/initial/reward_dist Std            0.00344925\n",
      "exploration/env_infos/initial/reward_dist Max            0.00929892\n",
      "exploration/env_infos/initial/reward_dist Min            0.000224256\n",
      "exploration/env_infos/reward_dist Mean                   0.174359\n",
      "exploration/env_infos/reward_dist Std                    0.280385\n",
      "exploration/env_infos/reward_dist Max                    0.967187\n",
      "exploration/env_infos/reward_dist Min                    0.000224256\n",
      "exploration/env_infos/final/reward_energy Mean          -0.119796\n",
      "exploration/env_infos/final/reward_energy Std            0.0752\n",
      "exploration/env_infos/final/reward_energy Max           -0.0389277\n",
      "exploration/env_infos/final/reward_energy Min           -0.251704\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.120366\n",
      "exploration/env_infos/initial/reward_energy Std          0.0347554\n",
      "exploration/env_infos/initial/reward_energy Max         -0.086151\n",
      "exploration/env_infos/initial/reward_energy Min         -0.172405\n",
      "exploration/env_infos/reward_energy Mean                -0.0816451\n",
      "exploration/env_infos/reward_energy Std                  0.0458567\n",
      "exploration/env_infos/reward_energy Max                 -0.00361415\n",
      "exploration/env_infos/reward_energy Min                 -0.251704\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0960533\n",
      "exploration/env_infos/final/end_effector_loc Std         0.17954\n",
      "exploration/env_infos/final/end_effector_loc Max         0.310679\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.17368\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00115417\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00427643\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00791804\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00523494\n",
      "exploration/env_infos/end_effector_loc Mean              0.0432469\n",
      "exploration/env_infos/end_effector_loc Std               0.100648\n",
      "exploration/env_infos/end_effector_loc Max               0.310679\n",
      "exploration/env_infos/end_effector_loc Min              -0.17368\n",
      "evaluation/num steps total                           64000\n",
      "evaluation/num paths total                            3200\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0729532\n",
      "evaluation/Rewards Std                                   0.119224\n",
      "evaluation/Rewards Max                                   0.132267\n",
      "evaluation/Rewards Min                                  -0.998626\n",
      "evaluation/Returns Mean                                 -1.45906\n",
      "evaluation/Returns Std                                   1.94882\n",
      "evaluation/Returns Max                                   1.05038\n",
      "evaluation/Returns Min                                 -12.8342\n",
      "evaluation/Actions Mean                                 -0.00645751\n",
      "evaluation/Actions Std                                   0.0909429\n",
      "evaluation/Actions Max                                   0.731594\n",
      "evaluation/Actions Min                                  -0.938262\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.45906\n",
      "evaluation/env_infos/final/reward_dist Mean              0.053989\n",
      "evaluation/env_infos/final/reward_dist Std               0.108936\n",
      "evaluation/env_infos/final/reward_dist Max               0.59619\n",
      "evaluation/env_infos/final/reward_dist Min               2.61209e-82\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00641776\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0116615\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0504439\n",
      "evaluation/env_infos/initial/reward_dist Min             1.43281e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.128383\n",
      "evaluation/env_infos/reward_dist Std                     0.224211\n",
      "evaluation/env_infos/reward_dist Max                     0.993204\n",
      "evaluation/env_infos/reward_dist Min                     2.99727e-111\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0520669\n",
      "evaluation/env_infos/final/reward_energy Std             0.0679275\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00124606\n",
      "evaluation/env_infos/final/reward_energy Min            -0.290667\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.245273\n",
      "evaluation/env_infos/initial/reward_energy Std           0.260467\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00425869\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.956161\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0671221\n",
      "evaluation/env_infos/reward_energy Std                   0.110087\n",
      "evaluation/env_infos/reward_energy Max                  -0.000193892\n",
      "evaluation/env_infos/reward_energy Min                  -0.956161\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0874433\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.307375\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.597116\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00122363\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0125899\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0365797\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0469131\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0404215\n",
      "evaluation/env_infos/end_effector_loc Std                0.205834\n",
      "evaluation/env_infos/end_effector_loc Max                0.597116\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.00341353\n",
      "time/evaluation sampling (s)                             1.20634\n",
      "time/exploration sampling (s)                            0.1379\n",
      "time/logging (s)                                         0.02293\n",
      "time/saving (s)                                          0.038193\n",
      "time/training (s)                                       56.1633\n",
      "time/epoch (s)                                          57.5721\n",
      "time/total (s)                                        3208.11\n",
      "Epoch                                                   63\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:07:43.234897 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 64 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00146345\n",
      "trainer/QF2 Loss                                         0.000959836\n",
      "trainer/Policy Loss                                      2.95088\n",
      "trainer/Q1 Predictions Mean                             -0.985467\n",
      "trainer/Q1 Predictions Std                               0.837433\n",
      "trainer/Q1 Predictions Max                               1.41956\n",
      "trainer/Q1 Predictions Min                              -2.83056\n",
      "trainer/Q2 Predictions Mean                             -0.980667\n",
      "trainer/Q2 Predictions Std                               0.840654\n",
      "trainer/Q2 Predictions Max                               1.40363\n",
      "trainer/Q2 Predictions Min                              -2.85136\n",
      "trainer/Q Targets Mean                                  -0.983918\n",
      "trainer/Q Targets Std                                    0.83444\n",
      "trainer/Q Targets Max                                    1.39927\n",
      "trainer/Q Targets Min                                   -2.84695\n",
      "trainer/Log Pis Mean                                     1.97365\n",
      "trainer/Log Pis Std                                      1.38198\n",
      "trainer/Log Pis Max                                      4.30566\n",
      "trainer/Log Pis Min                                     -3.60185\n",
      "trainer/Policy mu Mean                                  -0.0430279\n",
      "trainer/Policy mu Std                                    0.312673\n",
      "trainer/Policy mu Max                                    1.56305\n",
      "trainer/Policy mu Min                                   -1.5937\n",
      "trainer/Policy log std Mean                             -2.29679\n",
      "trainer/Policy log std Std                               0.629537\n",
      "trainer/Policy log std Max                              -0.351987\n",
      "trainer/Policy log std Min                              -3.29245\n",
      "trainer/Alpha                                            0.0225247\n",
      "trainer/Alpha Loss                                      -0.0999372\n",
      "exploration/num steps total                           7500\n",
      "exploration/num paths total                            375\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0918094\n",
      "exploration/Rewards Std                                  0.0751304\n",
      "exploration/Rewards Max                                  0.0656775\n",
      "exploration/Rewards Min                                 -0.305532\n",
      "exploration/Returns Mean                                -1.83619\n",
      "exploration/Returns Std                                  1.16984\n",
      "exploration/Returns Max                                 -0.964449\n",
      "exploration/Returns Min                                 -3.98421\n",
      "exploration/Actions Mean                                -0.00280096\n",
      "exploration/Actions Std                                  0.156518\n",
      "exploration/Actions Max                                  0.701314\n",
      "exploration/Actions Min                                 -0.713328\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.83619\n",
      "exploration/env_infos/final/reward_dist Mean             0.356948\n",
      "exploration/env_infos/final/reward_dist Std              0.381137\n",
      "exploration/env_infos/final/reward_dist Max              0.904839\n",
      "exploration/env_infos/final/reward_dist Min              3.28296e-08\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00833009\n",
      "exploration/env_infos/initial/reward_dist Std            0.0118972\n",
      "exploration/env_infos/initial/reward_dist Max            0.0306929\n",
      "exploration/env_infos/initial/reward_dist Min            1.95232e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.238959\n",
      "exploration/env_infos/reward_dist Std                    0.324704\n",
      "exploration/env_infos/reward_dist Max                    0.986571\n",
      "exploration/env_infos/reward_dist Min                    3.28296e-08\n",
      "exploration/env_infos/final/reward_energy Mean          -0.110257\n",
      "exploration/env_infos/final/reward_energy Std            0.134651\n",
      "exploration/env_infos/final/reward_energy Max           -0.0056416\n",
      "exploration/env_infos/final/reward_energy Min           -0.375983\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.181091\n",
      "exploration/env_infos/initial/reward_energy Std          0.161119\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0251869\n",
      "exploration/env_infos/initial/reward_energy Min         -0.48828\n",
      "exploration/env_infos/reward_energy Mean                -0.158781\n",
      "exploration/env_infos/reward_energy Std                  0.154273\n",
      "exploration/env_infos/reward_energy Max                 -0.0056416\n",
      "exploration/env_infos/reward_energy Min                 -0.831194\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0140176\n",
      "exploration/env_infos/final/end_effector_loc Std         0.24606\n",
      "exploration/env_infos/final/end_effector_loc Max         0.342668\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.358879\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00124133\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00847943\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0234854\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00666941\n",
      "exploration/env_infos/end_effector_loc Mean              0.0118163\n",
      "exploration/env_infos/end_effector_loc Std               0.163333\n",
      "exploration/env_infos/end_effector_loc Max               0.342668\n",
      "exploration/env_infos/end_effector_loc Min              -0.358879\n",
      "evaluation/num steps total                           65000\n",
      "evaluation/num paths total                            3250\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0592212\n",
      "evaluation/Rewards Std                                   0.0819531\n",
      "evaluation/Rewards Max                                   0.144569\n",
      "evaluation/Rewards Min                                  -0.575305\n",
      "evaluation/Returns Mean                                 -1.18442\n",
      "evaluation/Returns Std                                   1.29447\n",
      "evaluation/Returns Max                                   1.40656\n",
      "evaluation/Returns Min                                  -5.3822\n",
      "evaluation/Actions Mean                                 -0.00310106\n",
      "evaluation/Actions Std                                   0.0782224\n",
      "evaluation/Actions Max                                   0.757156\n",
      "evaluation/Actions Min                                  -0.893266\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.18442\n",
      "evaluation/env_infos/final/reward_dist Mean              0.103723\n",
      "evaluation/env_infos/final/reward_dist Std               0.215073\n",
      "evaluation/env_infos/final/reward_dist Max               0.933461\n",
      "evaluation/env_infos/final/reward_dist Min               4.08878e-32\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00727164\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00949375\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0374116\n",
      "evaluation/env_infos/initial/reward_dist Min             9.22651e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.163312\n",
      "evaluation/env_infos/reward_dist Std                     0.255998\n",
      "evaluation/env_infos/reward_dist Max                     0.990845\n",
      "evaluation/env_infos/reward_dist Min                     4.08878e-32\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0328447\n",
      "evaluation/env_infos/final/reward_energy Std             0.0488877\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00564629\n",
      "evaluation/env_infos/final/reward_energy Min            -0.326877\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.220762\n",
      "evaluation/env_infos/initial/reward_energy Std           0.255839\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0124398\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.939393\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0606826\n",
      "evaluation/env_infos/reward_energy Std                   0.0925978\n",
      "evaluation/env_infos/reward_energy Max                  -0.00117461\n",
      "evaluation/env_infos/reward_energy Min                  -0.939393\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0927931\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.250779\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.417388\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.695747\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00256128\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0116695\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0378578\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0446633\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.048938\n",
      "evaluation/env_infos/end_effector_loc Std                0.170599\n",
      "evaluation/env_infos/end_effector_loc Max                0.417388\n",
      "evaluation/env_infos/end_effector_loc Min               -0.695747\n",
      "time/data storing (s)                                    0.00317143\n",
      "time/evaluation sampling (s)                             1.14854\n",
      "time/exploration sampling (s)                            0.146649\n",
      "time/logging (s)                                         0.0200496\n",
      "time/saving (s)                                          0.030865\n",
      "time/training (s)                                       55.0775\n",
      "time/epoch (s)                                          56.4268\n",
      "time/total (s)                                        3265.49\n",
      "Epoch                                                   64\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:08:40.970330 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 65 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00111018\n",
      "trainer/QF2 Loss                                         0.0011456\n",
      "trainer/Policy Loss                                      2.99685\n",
      "trainer/Q1 Predictions Mean                             -1.11838\n",
      "trainer/Q1 Predictions Std                               0.872252\n",
      "trainer/Q1 Predictions Max                               1.03295\n",
      "trainer/Q1 Predictions Min                              -3.53844\n",
      "trainer/Q2 Predictions Mean                             -1.12378\n",
      "trainer/Q2 Predictions Std                               0.873145\n",
      "trainer/Q2 Predictions Max                               1.05597\n",
      "trainer/Q2 Predictions Min                              -3.54662\n",
      "trainer/Q Targets Mean                                  -1.12275\n",
      "trainer/Q Targets Std                                    0.870789\n",
      "trainer/Q Targets Max                                    0.995587\n",
      "trainer/Q Targets Min                                   -3.54026\n",
      "trainer/Log Pis Mean                                     1.87769\n",
      "trainer/Log Pis Std                                      1.41446\n",
      "trainer/Log Pis Max                                      4.43374\n",
      "trainer/Log Pis Min                                     -5.73508\n",
      "trainer/Policy mu Mean                                  -0.035743\n",
      "trainer/Policy mu Std                                    0.290049\n",
      "trainer/Policy mu Max                                    1.43821\n",
      "trainer/Policy mu Min                                   -1.74043\n",
      "trainer/Policy log std Mean                             -2.29613\n",
      "trainer/Policy log std Std                               0.624719\n",
      "trainer/Policy log std Max                              -0.317344\n",
      "trainer/Policy log std Min                              -3.29857\n",
      "trainer/Alpha                                            0.0226675\n",
      "trainer/Alpha Loss                                      -0.463112\n",
      "exploration/num steps total                           7600\n",
      "exploration/num paths total                            380\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0904829\n",
      "exploration/Rewards Std                                  0.0723519\n",
      "exploration/Rewards Max                                  0.0842216\n",
      "exploration/Rewards Min                                 -0.319509\n",
      "exploration/Returns Mean                                -1.80966\n",
      "exploration/Returns Std                                  0.637551\n",
      "exploration/Returns Max                                 -0.868496\n",
      "exploration/Returns Min                                 -2.53577\n",
      "exploration/Actions Mean                                 0.00380706\n",
      "exploration/Actions Std                                  0.162694\n",
      "exploration/Actions Max                                  0.435481\n",
      "exploration/Actions Min                                 -0.533255\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.80966\n",
      "exploration/env_infos/final/reward_dist Mean             0.0867236\n",
      "exploration/env_infos/final/reward_dist Std              0.101871\n",
      "exploration/env_infos/final/reward_dist Max              0.275649\n",
      "exploration/env_infos/final/reward_dist Min              1.191e-11\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00586635\n",
      "exploration/env_infos/initial/reward_dist Std            0.00878111\n",
      "exploration/env_infos/initial/reward_dist Max            0.0227394\n",
      "exploration/env_infos/initial/reward_dist Min            2.90312e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.112475\n",
      "exploration/env_infos/reward_dist Std                    0.199378\n",
      "exploration/env_infos/reward_dist Max                    0.948732\n",
      "exploration/env_infos/reward_dist Min                    1.191e-11\n",
      "exploration/env_infos/final/reward_energy Mean          -0.224514\n",
      "exploration/env_infos/final/reward_energy Std            0.126094\n",
      "exploration/env_infos/final/reward_energy Max           -0.0981367\n",
      "exploration/env_infos/final/reward_energy Min           -0.449233\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.274589\n",
      "exploration/env_infos/initial/reward_energy Std          0.0983146\n",
      "exploration/env_infos/initial/reward_energy Max         -0.121018\n",
      "exploration/env_infos/initial/reward_energy Min         -0.419779\n",
      "exploration/env_infos/reward_energy Mean                -0.192393\n",
      "exploration/env_infos/reward_energy Std                  0.126303\n",
      "exploration/env_infos/reward_energy Max                 -0.0189515\n",
      "exploration/env_infos/reward_energy Min                 -0.604316\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0716393\n",
      "exploration/env_infos/final/end_effector_loc Std         0.253295\n",
      "exploration/env_infos/final/end_effector_loc Max         0.554907\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.252142\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00347871\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00970721\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0157297\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0146996\n",
      "exploration/env_infos/end_effector_loc Mean              0.0356649\n",
      "exploration/env_infos/end_effector_loc Std               0.178066\n",
      "exploration/env_infos/end_effector_loc Max               0.554907\n",
      "exploration/env_infos/end_effector_loc Min              -0.276975\n",
      "evaluation/num steps total                           66000\n",
      "evaluation/num paths total                            3300\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0672046\n",
      "evaluation/Rewards Std                                   0.0765211\n",
      "evaluation/Rewards Max                                   0.113239\n",
      "evaluation/Rewards Min                                  -0.437832\n",
      "evaluation/Returns Mean                                 -1.34409\n",
      "evaluation/Returns Std                                   1.22498\n",
      "evaluation/Returns Max                                   0.806703\n",
      "evaluation/Returns Min                                  -4.07027\n",
      "evaluation/Actions Mean                                 -0.00254544\n",
      "evaluation/Actions Std                                   0.0704033\n",
      "evaluation/Actions Max                                   0.386287\n",
      "evaluation/Actions Min                                  -0.822902\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.34409\n",
      "evaluation/env_infos/final/reward_dist Mean              0.108959\n",
      "evaluation/env_infos/final/reward_dist Std               0.226537\n",
      "evaluation/env_infos/final/reward_dist Max               0.983472\n",
      "evaluation/env_infos/final/reward_dist Min               2.0964e-42\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00516189\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00870364\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0499665\n",
      "evaluation/env_infos/initial/reward_dist Min             1.21681e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.144947\n",
      "evaluation/env_infos/reward_dist Std                     0.231538\n",
      "evaluation/env_infos/reward_dist Max                     0.989499\n",
      "evaluation/env_infos/reward_dist Min                     2.0964e-42\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0327534\n",
      "evaluation/env_infos/final/reward_energy Std             0.0287464\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00019799\n",
      "evaluation/env_infos/final/reward_energy Min            -0.17201\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.207138\n",
      "evaluation/env_infos/initial/reward_energy Std           0.237337\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0112435\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.904715\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0540245\n",
      "evaluation/env_infos/reward_energy Std                   0.0837111\n",
      "evaluation/env_infos/reward_energy Max                  -0.00019799\n",
      "evaluation/env_infos/reward_energy Min                  -0.904715\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0588115\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.27292\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.548047\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.994406\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00244908\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0108649\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0193143\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0411451\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0307573\n",
      "evaluation/env_infos/end_effector_loc Std                0.174223\n",
      "evaluation/env_infos/end_effector_loc Max                0.548047\n",
      "evaluation/env_infos/end_effector_loc Min               -0.994406\n",
      "time/data storing (s)                                    0.00302316\n",
      "time/evaluation sampling (s)                             0.970594\n",
      "time/exploration sampling (s)                            0.131454\n",
      "time/logging (s)                                         0.0334038\n",
      "time/saving (s)                                          0.0688737\n",
      "time/training (s)                                       55.7049\n",
      "time/epoch (s)                                          56.9122\n",
      "time/total (s)                                        3323.24\n",
      "Epoch                                                   65\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:09:39.755117 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 66 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000759035\n",
      "trainer/QF2 Loss                                         0.000647573\n",
      "trainer/Policy Loss                                      3.02721\n",
      "trainer/Q1 Predictions Mean                             -1.01486\n",
      "trainer/Q1 Predictions Std                               0.809172\n",
      "trainer/Q1 Predictions Max                               1.45411\n",
      "trainer/Q1 Predictions Min                              -3.25609\n",
      "trainer/Q2 Predictions Mean                             -1.02544\n",
      "trainer/Q2 Predictions Std                               0.817793\n",
      "trainer/Q2 Predictions Max                               1.46488\n",
      "trainer/Q2 Predictions Min                              -3.26749\n",
      "trainer/Q Targets Mean                                  -1.02067\n",
      "trainer/Q Targets Std                                    0.816052\n",
      "trainer/Q Targets Max                                    1.46706\n",
      "trainer/Q Targets Min                                   -3.28621\n",
      "trainer/Log Pis Mean                                     2.01245\n",
      "trainer/Log Pis Std                                      1.41913\n",
      "trainer/Log Pis Max                                      4.26551\n",
      "trainer/Log Pis Min                                     -6.26971\n",
      "trainer/Policy mu Mean                                  -0.0241908\n",
      "trainer/Policy mu Std                                    0.356843\n",
      "trainer/Policy mu Max                                    2.2086\n",
      "trainer/Policy mu Min                                   -1.60176\n",
      "trainer/Policy log std Mean                             -2.28841\n",
      "trainer/Policy log std Std                               0.63886\n",
      "trainer/Policy log std Max                              -0.259482\n",
      "trainer/Policy log std Min                              -3.2367\n",
      "trainer/Alpha                                            0.0225177\n",
      "trainer/Alpha Loss                                       0.0472552\n",
      "exploration/num steps total                           7700\n",
      "exploration/num paths total                            385\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.108111\n",
      "exploration/Rewards Std                                  0.059479\n",
      "exploration/Rewards Max                                  0.00093035\n",
      "exploration/Rewards Min                                 -0.218068\n",
      "exploration/Returns Mean                                -2.16223\n",
      "exploration/Returns Std                                  0.948068\n",
      "exploration/Returns Max                                 -1.00734\n",
      "exploration/Returns Min                                 -3.39744\n",
      "exploration/Actions Mean                                -0.00118048\n",
      "exploration/Actions Std                                  0.0629602\n",
      "exploration/Actions Max                                  0.147206\n",
      "exploration/Actions Min                                 -0.239547\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.16223\n",
      "exploration/env_infos/final/reward_dist Mean             0.106805\n",
      "exploration/env_infos/final/reward_dist Std              0.117899\n",
      "exploration/env_infos/final/reward_dist Max              0.264615\n",
      "exploration/env_infos/final/reward_dist Min              0.00100071\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00371643\n",
      "exploration/env_infos/initial/reward_dist Std            0.00484532\n",
      "exploration/env_infos/initial/reward_dist Max            0.0120798\n",
      "exploration/env_infos/initial/reward_dist Min            4.24627e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0624837\n",
      "exploration/env_infos/reward_dist Std                    0.110854\n",
      "exploration/env_infos/reward_dist Max                    0.52481\n",
      "exploration/env_infos/reward_dist Min                    4.24627e-06\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0790392\n",
      "exploration/env_infos/final/reward_energy Std            0.0658179\n",
      "exploration/env_infos/final/reward_energy Max           -0.0215403\n",
      "exploration/env_infos/final/reward_energy Min           -0.203739\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.114406\n",
      "exploration/env_infos/initial/reward_energy Std          0.0379014\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0653145\n",
      "exploration/env_infos/initial/reward_energy Min         -0.165814\n",
      "exploration/env_infos/reward_energy Mean                -0.0771334\n",
      "exploration/env_infos/reward_energy Std                  0.0445106\n",
      "exploration/env_infos/reward_energy Max                 -0.00861514\n",
      "exploration/env_infos/reward_energy Min                 -0.241468\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.011071\n",
      "exploration/env_infos/final/end_effector_loc Std         0.154221\n",
      "exploration/env_infos/final/end_effector_loc Max         0.360802\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.181624\n",
      "exploration/env_infos/initial/end_effector_loc Mean      5.5099e-05\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00426069\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0067763\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00660314\n",
      "exploration/env_infos/end_effector_loc Mean              0.00610171\n",
      "exploration/env_infos/end_effector_loc Std               0.0868714\n",
      "exploration/env_infos/end_effector_loc Max               0.360802\n",
      "exploration/env_infos/end_effector_loc Min              -0.181624\n",
      "evaluation/num steps total                           67000\n",
      "evaluation/num paths total                            3350\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0560858\n",
      "evaluation/Rewards Std                                   0.0728026\n",
      "evaluation/Rewards Max                                   0.144882\n",
      "evaluation/Rewards Min                                  -0.451024\n",
      "evaluation/Returns Mean                                 -1.12172\n",
      "evaluation/Returns Std                                   1.1978\n",
      "evaluation/Returns Max                                   2.02571\n",
      "evaluation/Returns Min                                  -3.89042\n",
      "evaluation/Actions Mean                                 -0.00236113\n",
      "evaluation/Actions Std                                   0.0746259\n",
      "evaluation/Actions Max                                   0.731236\n",
      "evaluation/Actions Min                                  -0.933932\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.12172\n",
      "evaluation/env_infos/final/reward_dist Mean              0.114248\n",
      "evaluation/env_infos/final/reward_dist Std               0.223237\n",
      "evaluation/env_infos/final/reward_dist Max               0.955604\n",
      "evaluation/env_infos/final/reward_dist Min               7.40154e-18\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00761311\n",
      "evaluation/env_infos/initial/reward_dist Std             0.018282\n",
      "evaluation/env_infos/initial/reward_dist Max             0.110233\n",
      "evaluation/env_infos/initial/reward_dist Min             1.12122e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.14064\n",
      "evaluation/env_infos/reward_dist Std                     0.247623\n",
      "evaluation/env_infos/reward_dist Max                     0.999347\n",
      "evaluation/env_infos/reward_dist Min                     7.40154e-18\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0400683\n",
      "evaluation/env_infos/final/reward_energy Std             0.0387507\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0020063\n",
      "evaluation/env_infos/final/reward_energy Min            -0.184611\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.215374\n",
      "evaluation/env_infos/initial/reward_energy Std           0.272116\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00197372\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.965591\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0541837\n",
      "evaluation/env_infos/reward_energy Std                   0.0906274\n",
      "evaluation/env_infos/reward_energy Max                  -0.000797285\n",
      "evaluation/env_infos/reward_energy Min                  -0.965591\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0739885\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.209977\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.470421\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.456137\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.0033332\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0118081\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0365618\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0466966\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0447561\n",
      "evaluation/env_infos/end_effector_loc Std                0.141382\n",
      "evaluation/env_infos/end_effector_loc Max                0.470421\n",
      "evaluation/env_infos/end_effector_loc Min               -0.539406\n",
      "time/data storing (s)                                    0.00319937\n",
      "time/evaluation sampling (s)                             1.15496\n",
      "time/exploration sampling (s)                            0.170824\n",
      "time/logging (s)                                         0.0232352\n",
      "time/saving (s)                                          0.0308555\n",
      "time/training (s)                                       56.2197\n",
      "time/epoch (s)                                          57.6028\n",
      "time/total (s)                                        3382.01\n",
      "Epoch                                                   66\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:10:37.056547 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 67 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00138736\r\n",
      "trainer/QF2 Loss                                         0.00131741\r\n",
      "trainer/Policy Loss                                      3.22321\r\n",
      "trainer/Q1 Predictions Mean                             -1.17844\r\n",
      "trainer/Q1 Predictions Std                               0.822925\r\n",
      "trainer/Q1 Predictions Max                               1.12253\r\n",
      "trainer/Q1 Predictions Min                              -3.28685\r\n",
      "trainer/Q2 Predictions Mean                             -1.18412\r\n",
      "trainer/Q2 Predictions Std                               0.831319\r\n",
      "trainer/Q2 Predictions Max                               1.17507\r\n",
      "trainer/Q2 Predictions Min                              -3.31397\r\n",
      "trainer/Q Targets Mean                                  -1.19035\r\n",
      "trainer/Q Targets Std                                    0.833043\r\n",
      "trainer/Q Targets Max                                    1.09842\r\n",
      "trainer/Q Targets Min                                   -3.29937\r\n",
      "trainer/Log Pis Mean                                     2.04886\r\n",
      "trainer/Log Pis Std                                      1.26423\r\n",
      "trainer/Log Pis Max                                      4.15641\r\n",
      "trainer/Log Pis Min                                     -2.24546\r\n",
      "trainer/Policy mu Mean                                  -0.0110192\r\n",
      "trainer/Policy mu Std                                    0.381566\r\n",
      "trainer/Policy mu Max                                    2.09145\r\n",
      "trainer/Policy mu Min                                   -1.73722\r\n",
      "trainer/Policy log std Mean                             -2.24308\r\n",
      "trainer/Policy log std Std                               0.667282\r\n",
      "trainer/Policy log std Max                               0.0805831\r\n",
      "trainer/Policy log std Min                              -3.12712\r\n",
      "trainer/Alpha                                            0.0236443\r\n",
      "trainer/Alpha Loss                                       0.183072\r\n",
      "exploration/num steps total                           7800\r\n",
      "exploration/num paths total                            390\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.119585\r\n",
      "exploration/Rewards Std                                  0.0843299\r\n",
      "exploration/Rewards Max                                  0.073158\r\n",
      "exploration/Rewards Min                                 -0.332855\r\n",
      "exploration/Returns Mean                                -2.3917\r\n",
      "exploration/Returns Std                                  1.29422\r\n",
      "exploration/Returns Max                                 -0.457542\r\n",
      "exploration/Returns Min                                 -4.27129\r\n",
      "exploration/Actions Mean                                 0.00250776\r\n",
      "exploration/Actions Std                                  0.0782392\r\n",
      "exploration/Actions Max                                  0.243326\r\n",
      "exploration/Actions Min                                 -0.385816\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.3917\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.166595\r\n",
      "exploration/env_infos/final/reward_dist Std              0.186827\r\n",
      "exploration/env_infos/final/reward_dist Max              0.435503\r\n",
      "exploration/env_infos/final/reward_dist Min              0.000106283\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00575197\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.010757\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0272548\r\n",
      "exploration/env_infos/initial/reward_dist Min            3.07816e-05\r\n",
      "exploration/env_infos/reward_dist Mean                   0.0942637\r\n",
      "exploration/env_infos/reward_dist Std                    0.141065\r\n",
      "exploration/env_infos/reward_dist Max                    0.500407\r\n",
      "exploration/env_infos/reward_dist Min                    3.07816e-05\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.065404\r\n",
      "exploration/env_infos/final/reward_energy Std            0.021657\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0396475\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.0934896\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.122478\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.145565\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0261379\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.407206\r\n",
      "exploration/env_infos/reward_energy Mean                -0.0903008\r\n",
      "exploration/env_infos/reward_energy Std                  0.0640397\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00138165\r\n",
      "exploration/env_infos/reward_energy Min                 -0.407206\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0245658\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.167298\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.147302\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.355609\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00251515\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00623792\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00343912\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0192908\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.021229\r\n",
      "exploration/env_infos/end_effector_loc Std               0.111621\r\n",
      "exploration/env_infos/end_effector_loc Max               0.154897\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.401651\r\n",
      "evaluation/num steps total                           68000\r\n",
      "evaluation/num paths total                            3400\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0463511\r\n",
      "evaluation/Rewards Std                                   0.0761563\r\n",
      "evaluation/Rewards Max                                   0.15573\r\n",
      "evaluation/Rewards Min                                  -0.740937\r\n",
      "evaluation/Returns Mean                                 -0.927022\r\n",
      "evaluation/Returns Std                                   1.0236\r\n",
      "evaluation/Returns Max                                   1.52996\r\n",
      "evaluation/Returns Min                                  -4.07462\r\n",
      "evaluation/Actions Mean                                  0.00486511\r\n",
      "evaluation/Actions Std                                   0.0776408\r\n",
      "evaluation/Actions Max                                   0.492104\r\n",
      "evaluation/Actions Min                                  -0.905732\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.927022\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.163287\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.241353\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.947334\r\n",
      "evaluation/env_infos/final/reward_dist Min               8.00598e-19\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00490888\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00936141\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0458326\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.12382e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.183521\r\n",
      "evaluation/env_infos/reward_dist Std                     0.254716\r\n",
      "evaluation/env_infos/reward_dist Max                     0.98955\r\n",
      "evaluation/env_infos/reward_dist Min                     8.00598e-19\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0652995\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.102098\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00237647\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.497693\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.221858\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.249592\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0124987\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.988887\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0640896\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0894207\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.0010393\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.988887\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0153496\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.248185\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.545767\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.563308\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.0021244\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0116139\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0229086\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0452866\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00409039\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.163927\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.545767\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.579242\r\n",
      "time/data storing (s)                                    0.00350561\r\n",
      "time/evaluation sampling (s)                             1.08555\r\n",
      "time/exploration sampling (s)                            0.16984\r\n",
      "time/logging (s)                                         0.0205993\r\n",
      "time/saving (s)                                          0.0312247\r\n",
      "time/training (s)                                       55.0298\r\n",
      "time/epoch (s)                                          56.3405\r\n",
      "time/total (s)                                        3439.31\r\n",
      "Epoch                                                   67\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:11:35.748439 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 68 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00172722\n",
      "trainer/QF2 Loss                                         0.000777282\n",
      "trainer/Policy Loss                                      3.22243\n",
      "trainer/Q1 Predictions Mean                             -1.13803\n",
      "trainer/Q1 Predictions Std                               0.760872\n",
      "trainer/Q1 Predictions Max                               0.676283\n",
      "trainer/Q1 Predictions Min                              -3.28053\n",
      "trainer/Q2 Predictions Mean                             -1.12633\n",
      "trainer/Q2 Predictions Std                               0.755157\n",
      "trainer/Q2 Predictions Max                               0.671782\n",
      "trainer/Q2 Predictions Min                              -3.29702\n",
      "trainer/Q Targets Mean                                  -1.12269\n",
      "trainer/Q Targets Std                                    0.755176\n",
      "trainer/Q Targets Max                                    0.654324\n",
      "trainer/Q Targets Min                                   -3.28438\n",
      "trainer/Log Pis Mean                                     2.09192\n",
      "trainer/Log Pis Std                                      1.32213\n",
      "trainer/Log Pis Max                                      4.543\n",
      "trainer/Log Pis Min                                     -2.33965\n",
      "trainer/Policy mu Mean                                   0.00024379\n",
      "trainer/Policy mu Std                                    0.384885\n",
      "trainer/Policy mu Max                                    1.97334\n",
      "trainer/Policy mu Min                                   -1.74154\n",
      "trainer/Policy log std Mean                             -2.30036\n",
      "trainer/Policy log std Std                               0.655229\n",
      "trainer/Policy log std Max                               0.104348\n",
      "trainer/Policy log std Min                              -3.20241\n",
      "trainer/Alpha                                            0.0234401\n",
      "trainer/Alpha Loss                                       0.345104\n",
      "exploration/num steps total                           7900\n",
      "exploration/num paths total                            395\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.115336\n",
      "exploration/Rewards Std                                  0.147503\n",
      "exploration/Rewards Max                                  0.06014\n",
      "exploration/Rewards Min                                 -0.632688\n",
      "exploration/Returns Mean                                -2.30672\n",
      "exploration/Returns Std                                  2.5205\n",
      "exploration/Returns Max                                  0.292477\n",
      "exploration/Returns Min                                 -6.73799\n",
      "exploration/Actions Mean                                -0.00740298\n",
      "exploration/Actions Std                                  0.19192\n",
      "exploration/Actions Max                                  0.598612\n",
      "exploration/Actions Min                                 -0.712987\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.30672\n",
      "exploration/env_infos/final/reward_dist Mean             0.273079\n",
      "exploration/env_infos/final/reward_dist Std              0.364135\n",
      "exploration/env_infos/final/reward_dist Max              0.968844\n",
      "exploration/env_infos/final/reward_dist Min              1.30113e-31\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00736255\n",
      "exploration/env_infos/initial/reward_dist Std            0.00654993\n",
      "exploration/env_infos/initial/reward_dist Max            0.0176867\n",
      "exploration/env_infos/initial/reward_dist Min            0.000226222\n",
      "exploration/env_infos/reward_dist Mean                   0.227021\n",
      "exploration/env_infos/reward_dist Std                    0.324032\n",
      "exploration/env_infos/reward_dist Max                    0.999999\n",
      "exploration/env_infos/reward_dist Min                    1.30113e-31\n",
      "exploration/env_infos/final/reward_energy Mean          -0.328671\n",
      "exploration/env_infos/final/reward_energy Std            0.19416\n",
      "exploration/env_infos/final/reward_energy Max           -0.115863\n",
      "exploration/env_infos/final/reward_energy Min           -0.681776\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.324851\n",
      "exploration/env_infos/initial/reward_energy Std          0.0874401\n",
      "exploration/env_infos/initial/reward_energy Max         -0.200166\n",
      "exploration/env_infos/initial/reward_energy Min         -0.413335\n",
      "exploration/env_infos/reward_energy Mean                -0.228935\n",
      "exploration/env_infos/reward_energy Std                  0.146167\n",
      "exploration/env_infos/reward_energy Max                 -0.0211623\n",
      "exploration/env_infos/reward_energy Min                 -0.807032\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.135059\n",
      "exploration/env_infos/final/end_effector_loc Std         0.264153\n",
      "exploration/env_infos/final/end_effector_loc Max         0.269729\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.537556\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00227542\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0116743\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0183508\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0180107\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0749623\n",
      "exploration/env_infos/end_effector_loc Std               0.18569\n",
      "exploration/env_infos/end_effector_loc Max               0.276773\n",
      "exploration/env_infos/end_effector_loc Min              -0.537556\n",
      "evaluation/num steps total                           69000\n",
      "evaluation/num paths total                            3450\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0629682\n",
      "evaluation/Rewards Std                                   0.0732746\n",
      "evaluation/Rewards Max                                   0.0909085\n",
      "evaluation/Rewards Min                                  -0.422571\n",
      "evaluation/Returns Mean                                 -1.25936\n",
      "evaluation/Returns Std                                   1.03871\n",
      "evaluation/Returns Max                                   0.727792\n",
      "evaluation/Returns Min                                  -3.40581\n",
      "evaluation/Actions Mean                                 -0.000187787\n",
      "evaluation/Actions Std                                   0.0813938\n",
      "evaluation/Actions Max                                   0.730927\n",
      "evaluation/Actions Min                                  -0.903367\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.25936\n",
      "evaluation/env_infos/final/reward_dist Mean              0.135996\n",
      "evaluation/env_infos/final/reward_dist Std               0.224241\n",
      "evaluation/env_infos/final/reward_dist Max               0.896155\n",
      "evaluation/env_infos/final/reward_dist Min               2.10406e-23\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00705512\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0171036\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0899132\n",
      "evaluation/env_infos/initial/reward_dist Min             1.4271e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.165155\n",
      "evaluation/env_infos/reward_dist Std                     0.247343\n",
      "evaluation/env_infos/reward_dist Max                     0.996618\n",
      "evaluation/env_infos/reward_dist Min                     2.10406e-23\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0458434\n",
      "evaluation/env_infos/final/reward_energy Std             0.0398149\n",
      "evaluation/env_infos/final/reward_energy Max            -0.000655285\n",
      "evaluation/env_infos/final/reward_energy Min            -0.170266\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.246643\n",
      "evaluation/env_infos/initial/reward_energy Std           0.234207\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0161244\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.958\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0680376\n",
      "evaluation/env_infos/reward_energy Std                   0.0928486\n",
      "evaluation/env_infos/reward_energy Max                  -0.00027507\n",
      "evaluation/env_infos/reward_energy Min                  -0.958\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0159061\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.261089\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.564158\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.630649\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -9.83276e-05\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0120249\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0365463\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0451683\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00828856\n",
      "evaluation/env_infos/end_effector_loc Std                0.167835\n",
      "evaluation/env_infos/end_effector_loc Max                0.564158\n",
      "evaluation/env_infos/end_effector_loc Min               -0.630649\n",
      "time/data storing (s)                                    0.00315523\n",
      "time/evaluation sampling (s)                             1.37873\n",
      "time/exploration sampling (s)                            0.133263\n",
      "time/logging (s)                                         0.0357784\n",
      "time/saving (s)                                          0.0395127\n",
      "time/training (s)                                       56.1782\n",
      "time/epoch (s)                                          57.7686\n",
      "time/total (s)                                        3498.01\n",
      "Epoch                                                   68\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:12:29.423829 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 69 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000933154\r\n",
      "trainer/QF2 Loss                                         0.000682523\r\n",
      "trainer/Policy Loss                                      3.07447\r\n",
      "trainer/Q1 Predictions Mean                             -1.11253\r\n",
      "trainer/Q1 Predictions Std                               0.843683\r\n",
      "trainer/Q1 Predictions Max                               0.838345\r\n",
      "trainer/Q1 Predictions Min                              -3.43732\r\n",
      "trainer/Q2 Predictions Mean                             -1.11363\r\n",
      "trainer/Q2 Predictions Std                               0.842185\r\n",
      "trainer/Q2 Predictions Max                               0.849407\r\n",
      "trainer/Q2 Predictions Min                              -3.43083\r\n",
      "trainer/Q Targets Mean                                  -1.11499\r\n",
      "trainer/Q Targets Std                                    0.844636\r\n",
      "trainer/Q Targets Max                                    0.845606\r\n",
      "trainer/Q Targets Min                                   -3.41585\r\n",
      "trainer/Log Pis Mean                                     1.97151\r\n",
      "trainer/Log Pis Std                                      1.47977\r\n",
      "trainer/Log Pis Max                                      4.3675\r\n",
      "trainer/Log Pis Min                                     -7.78597\r\n",
      "trainer/Policy mu Mean                                   0.0141081\r\n",
      "trainer/Policy mu Std                                    0.308223\r\n",
      "trainer/Policy mu Max                                    1.42802\r\n",
      "trainer/Policy mu Min                                   -1.58876\r\n",
      "trainer/Policy log std Mean                             -2.30164\r\n",
      "trainer/Policy log std Std                               0.672328\r\n",
      "trainer/Policy log std Max                               0.53859\r\n",
      "trainer/Policy log std Min                              -3.23721\r\n",
      "trainer/Alpha                                            0.0224914\r\n",
      "trainer/Alpha Loss                                      -0.108064\r\n",
      "exploration/num steps total                           8000\r\n",
      "exploration/num paths total                            400\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.10127\r\n",
      "exploration/Rewards Std                                  0.0577385\r\n",
      "exploration/Rewards Max                                  0.012892\r\n",
      "exploration/Rewards Min                                 -0.285635\r\n",
      "exploration/Returns Mean                                -2.02541\r\n",
      "exploration/Returns Std                                  0.250597\r\n",
      "exploration/Returns Max                                 -1.75535\r\n",
      "exploration/Returns Min                                 -2.42366\r\n",
      "exploration/Actions Mean                                 0.00349451\r\n",
      "exploration/Actions Std                                  0.0994182\r\n",
      "exploration/Actions Max                                  0.332261\r\n",
      "exploration/Actions Min                                 -0.360477\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.02541\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0537053\r\n",
      "exploration/env_infos/final/reward_dist Std              0.106457\r\n",
      "exploration/env_infos/final/reward_dist Max              0.266616\r\n",
      "exploration/env_infos/final/reward_dist Min              2.96158e-07\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000434665\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.000494509\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0013831\r\n",
      "exploration/env_infos/initial/reward_dist Min            4.68113e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.110503\r\n",
      "exploration/env_infos/reward_dist Std                    0.212478\r\n",
      "exploration/env_infos/reward_dist Max                    0.979295\r\n",
      "exploration/env_infos/reward_dist Min                    2.96158e-07\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.121605\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0442026\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0591545\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.182283\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.19603\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.0890731\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0961772\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.348777\r\n",
      "exploration/env_infos/reward_energy Mean                -0.114617\r\n",
      "exploration/env_infos/reward_energy Std                  0.08158\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00967994\r\n",
      "exploration/env_infos/reward_energy Min                 -0.408036\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.172271\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.184701\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.467517\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.163024\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00358183\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00671733\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.013139\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0114665\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.105474\r\n",
      "exploration/env_infos/end_effector_loc Std               0.121177\r\n",
      "exploration/env_infos/end_effector_loc Max               0.467517\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.163024\r\n",
      "evaluation/num steps total                           70000\r\n",
      "evaluation/num paths total                            3500\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0532707\r\n",
      "evaluation/Rewards Std                                   0.0712409\r\n",
      "evaluation/Rewards Max                                   0.111361\r\n",
      "evaluation/Rewards Min                                  -0.629201\r\n",
      "evaluation/Returns Mean                                 -1.06541\r\n",
      "evaluation/Returns Std                                   1.02132\r\n",
      "evaluation/Returns Max                                   0.983864\r\n",
      "evaluation/Returns Min                                  -3.70963\r\n",
      "evaluation/Actions Mean                                  0.00187879\r\n",
      "evaluation/Actions Std                                   0.0728665\r\n",
      "evaluation/Actions Max                                   0.688405\r\n",
      "evaluation/Actions Min                                  -0.745729\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.06541\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0685799\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.134646\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.545118\r\n",
      "evaluation/env_infos/final/reward_dist Min               2.97538e-33\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00566398\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00932576\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.038538\r\n",
      "evaluation/env_infos/initial/reward_dist Min             9.12586e-07\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.165822\r\n",
      "evaluation/env_infos/reward_dist Std                     0.245469\r\n",
      "evaluation/env_infos/reward_dist Max                     0.999444\r\n",
      "evaluation/env_infos/reward_dist Min                     2.97538e-33\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0367164\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0464634\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00170104\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.273491\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.231512\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.230245\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0159139\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.868661\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0609396\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0831413\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00146658\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.868661\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.00256867\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.264259\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.492451\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.618989\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00120672\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0114807\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0344202\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0372864\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00540467\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.16763\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.492451\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.618989\r\n",
      "time/data storing (s)                                    0.00333639\r\n",
      "time/evaluation sampling (s)                             1.00677\r\n",
      "time/exploration sampling (s)                            0.134294\r\n",
      "time/logging (s)                                         0.0194182\r\n",
      "time/saving (s)                                          0.0305215\r\n",
      "time/training (s)                                       51.4725\r\n",
      "time/epoch (s)                                          52.6668\r\n",
      "time/total (s)                                        3551.67\r\n",
      "Epoch                                                   69\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:13:27.267504 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 70 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00067761\r\n",
      "trainer/QF2 Loss                                         0.00082893\r\n",
      "trainer/Policy Loss                                      3.28411\r\n",
      "trainer/Q1 Predictions Mean                             -1.26548\r\n",
      "trainer/Q1 Predictions Std                               0.881988\r\n",
      "trainer/Q1 Predictions Max                               0.863735\r\n",
      "trainer/Q1 Predictions Min                              -3.31856\r\n",
      "trainer/Q2 Predictions Mean                             -1.26307\r\n",
      "trainer/Q2 Predictions Std                               0.881034\r\n",
      "trainer/Q2 Predictions Max                               0.858544\r\n",
      "trainer/Q2 Predictions Min                              -3.33858\r\n",
      "trainer/Q Targets Mean                                  -1.25928\r\n",
      "trainer/Q Targets Std                                    0.881933\r\n",
      "trainer/Q Targets Max                                    0.847508\r\n",
      "trainer/Q Targets Min                                   -3.3003\r\n",
      "trainer/Log Pis Mean                                     2.02841\r\n",
      "trainer/Log Pis Std                                      1.47298\r\n",
      "trainer/Log Pis Max                                      4.61068\r\n",
      "trainer/Log Pis Min                                     -4.23207\r\n",
      "trainer/Policy mu Mean                                   0.00976759\r\n",
      "trainer/Policy mu Std                                    0.308773\r\n",
      "trainer/Policy mu Max                                    1.36709\r\n",
      "trainer/Policy mu Min                                   -1.80422\r\n",
      "trainer/Policy log std Mean                             -2.34348\r\n",
      "trainer/Policy log std Std                               0.623164\r\n",
      "trainer/Policy log std Max                              -0.155634\r\n",
      "trainer/Policy log std Min                              -3.25313\r\n",
      "trainer/Alpha                                            0.0226011\r\n",
      "trainer/Alpha Loss                                       0.107683\r\n",
      "exploration/num steps total                           8100\r\n",
      "exploration/num paths total                            405\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.107869\r\n",
      "exploration/Rewards Std                                  0.0516748\r\n",
      "exploration/Rewards Max                                  0.0359467\r\n",
      "exploration/Rewards Min                                 -0.302525\r\n",
      "exploration/Returns Mean                                -2.15739\r\n",
      "exploration/Returns Std                                  0.385669\r\n",
      "exploration/Returns Max                                 -1.70303\r\n",
      "exploration/Returns Min                                 -2.62353\r\n",
      "exploration/Actions Mean                                 0.00210449\r\n",
      "exploration/Actions Std                                  0.136825\r\n",
      "exploration/Actions Max                                  0.501863\r\n",
      "exploration/Actions Min                                 -0.935936\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.15739\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.000709981\r\n",
      "exploration/env_infos/final/reward_dist Std              0.00141995\r\n",
      "exploration/env_infos/final/reward_dist Max              0.00354988\r\n",
      "exploration/env_infos/final/reward_dist Min              4.1737e-26\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0219133\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0271101\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0690864\r\n",
      "exploration/env_infos/initial/reward_dist Min            1.308e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.0990189\r\n",
      "exploration/env_infos/reward_dist Std                    0.233166\r\n",
      "exploration/env_infos/reward_dist Max                    0.983897\r\n",
      "exploration/env_infos/reward_dist Min                    4.1737e-26\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.148502\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0489454\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0998077\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.232283\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.329278\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.371611\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0427611\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.985159\r\n",
      "exploration/env_infos/reward_energy Mean                -0.144381\r\n",
      "exploration/env_infos/reward_energy Std                  0.12886\r\n",
      "exploration/env_infos/reward_energy Max                 -0.013158\r\n",
      "exploration/env_infos/reward_energy Min                 -0.985159\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0634185\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.266352\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.546713\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.245968\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00354208\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0171931\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0250931\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0467968\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.0236789\r\n",
      "exploration/env_infos/end_effector_loc Std               0.193137\r\n",
      "exploration/env_infos/end_effector_loc Max               0.551667\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.417148\r\n",
      "evaluation/num steps total                           71000\r\n",
      "evaluation/num paths total                            3550\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.048161\r\n",
      "evaluation/Rewards Std                                   0.076957\r\n",
      "evaluation/Rewards Max                                   0.16054\r\n",
      "evaluation/Rewards Min                                  -0.373781\r\n",
      "evaluation/Returns Mean                                 -0.96322\r\n",
      "evaluation/Returns Std                                   1.16668\r\n",
      "evaluation/Returns Max                                   1.15382\r\n",
      "evaluation/Returns Min                                  -3.08357\r\n",
      "evaluation/Actions Mean                                  0.0029595\r\n",
      "evaluation/Actions Std                                   0.0792346\r\n",
      "evaluation/Actions Max                                   0.494885\r\n",
      "evaluation/Actions Min                                  -0.986349\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.96322\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.29648\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.327964\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.999525\r\n",
      "evaluation/env_infos/final/reward_dist Min               2.12241e-21\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0108816\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0197919\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.105143\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.77161e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.235278\r\n",
      "evaluation/env_infos/reward_dist Std                     0.296738\r\n",
      "evaluation/env_infos/reward_dist Max                     0.999525\r\n",
      "evaluation/env_infos/reward_dist Min                     2.12241e-21\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0539763\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.084284\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0037007\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.518926\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.237599\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.255794\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0132959\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.21304\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0644339\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0917715\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000902422\r\n",
      "evaluation/env_infos/reward_energy Min                  -1.21304\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.00756622\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.253989\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.682794\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.488399\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00165384\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0122319\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0247442\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0493174\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00263564\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.167649\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.682794\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.488399\r\n",
      "time/data storing (s)                                    0.00301248\r\n",
      "time/evaluation sampling (s)                             1.0198\r\n",
      "time/exploration sampling (s)                            0.146484\r\n",
      "time/logging (s)                                         0.0212836\r\n",
      "time/saving (s)                                          0.030317\r\n",
      "time/training (s)                                       55.6773\r\n",
      "time/epoch (s)                                          56.8982\r\n",
      "time/total (s)                                        3609.51\r\n",
      "Epoch                                                   70\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:14:24.129796 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 71 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000933849\n",
      "trainer/QF2 Loss                                         0.000966241\n",
      "trainer/Policy Loss                                      2.91713\n",
      "trainer/Q1 Predictions Mean                             -1.07943\n",
      "trainer/Q1 Predictions Std                               0.866178\n",
      "trainer/Q1 Predictions Max                               1.22607\n",
      "trainer/Q1 Predictions Min                              -3.43242\n",
      "trainer/Q2 Predictions Mean                             -1.07468\n",
      "trainer/Q2 Predictions Std                               0.864412\n",
      "trainer/Q2 Predictions Max                               1.21117\n",
      "trainer/Q2 Predictions Min                              -3.44985\n",
      "trainer/Q Targets Mean                                  -1.0712\n",
      "trainer/Q Targets Std                                    0.864888\n",
      "trainer/Q Targets Max                                    1.18584\n",
      "trainer/Q Targets Min                                   -3.47762\n",
      "trainer/Log Pis Mean                                     1.83648\n",
      "trainer/Log Pis Std                                      1.27768\n",
      "trainer/Log Pis Max                                      4.11516\n",
      "trainer/Log Pis Min                                     -2.48852\n",
      "trainer/Policy mu Mean                                   0.0303272\n",
      "trainer/Policy mu Std                                    0.295716\n",
      "trainer/Policy mu Max                                    1.64069\n",
      "trainer/Policy mu Min                                   -1.30455\n",
      "trainer/Policy log std Mean                             -2.23488\n",
      "trainer/Policy log std Std                               0.608597\n",
      "trainer/Policy log std Max                              -0.319722\n",
      "trainer/Policy log std Min                              -3.23815\n",
      "trainer/Alpha                                            0.0226957\n",
      "trainer/Alpha Loss                                      -0.61891\n",
      "exploration/num steps total                           8200\n",
      "exploration/num paths total                            410\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0808401\n",
      "exploration/Rewards Std                                  0.0718923\n",
      "exploration/Rewards Max                                  0.0353937\n",
      "exploration/Rewards Min                                 -0.594055\n",
      "exploration/Returns Mean                                -1.6168\n",
      "exploration/Returns Std                                  0.512901\n",
      "exploration/Returns Max                                 -1.06121\n",
      "exploration/Returns Min                                 -2.54159\n",
      "exploration/Actions Mean                                 0.00823882\n",
      "exploration/Actions Std                                  0.133173\n",
      "exploration/Actions Max                                  0.578739\n",
      "exploration/Actions Min                                 -0.773362\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.6168\n",
      "exploration/env_infos/final/reward_dist Mean             0.164572\n",
      "exploration/env_infos/final/reward_dist Std              0.202397\n",
      "exploration/env_infos/final/reward_dist Max              0.44223\n",
      "exploration/env_infos/final/reward_dist Min              1.97065e-06\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0120119\n",
      "exploration/env_infos/initial/reward_dist Std            0.0199204\n",
      "exploration/env_infos/initial/reward_dist Max            0.0515621\n",
      "exploration/env_infos/initial/reward_dist Min            0.000344107\n",
      "exploration/env_infos/reward_dist Mean                   0.152894\n",
      "exploration/env_infos/reward_dist Std                    0.239948\n",
      "exploration/env_infos/reward_dist Max                    0.934106\n",
      "exploration/env_infos/reward_dist Min                    7.55207e-07\n",
      "exploration/env_infos/final/reward_energy Mean          -0.124072\n",
      "exploration/env_infos/final/reward_energy Std            0.0615268\n",
      "exploration/env_infos/final/reward_energy Max           -0.0430175\n",
      "exploration/env_infos/final/reward_energy Min           -0.229763\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.404912\n",
      "exploration/env_infos/initial/reward_energy Std          0.316674\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0300679\n",
      "exploration/env_infos/initial/reward_energy Min         -0.814621\n",
      "exploration/env_infos/reward_energy Mean                -0.128832\n",
      "exploration/env_infos/reward_energy Std                  0.137869\n",
      "exploration/env_infos/reward_energy Max                 -0.0180296\n",
      "exploration/env_infos/reward_energy Min                 -0.814621\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0361425\n",
      "exploration/env_infos/final/end_effector_loc Std         0.24588\n",
      "exploration/env_infos/final/end_effector_loc Max         0.512964\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.434238\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00278875\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0179588\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0222173\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0386681\n",
      "exploration/env_infos/end_effector_loc Mean              0.00388596\n",
      "exploration/env_infos/end_effector_loc Std               0.181597\n",
      "exploration/env_infos/end_effector_loc Max               0.512964\n",
      "exploration/env_infos/end_effector_loc Min              -0.469832\n",
      "evaluation/num steps total                           72000\n",
      "evaluation/num paths total                            3600\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0512251\n",
      "evaluation/Rewards Std                                   0.079413\n",
      "evaluation/Rewards Max                                   0.149412\n",
      "evaluation/Rewards Min                                  -0.694786\n",
      "evaluation/Returns Mean                                 -1.0245\n",
      "evaluation/Returns Std                                   1.13961\n",
      "evaluation/Returns Max                                   2.11206\n",
      "evaluation/Returns Min                                  -5.17624\n",
      "evaluation/Actions Mean                                  0.00355475\n",
      "evaluation/Actions Std                                   0.0797162\n",
      "evaluation/Actions Max                                   0.661872\n",
      "evaluation/Actions Min                                  -0.867439\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.0245\n",
      "evaluation/env_infos/final/reward_dist Mean              0.233588\n",
      "evaluation/env_infos/final/reward_dist Std               0.320058\n",
      "evaluation/env_infos/final/reward_dist Max               0.988518\n",
      "evaluation/env_infos/final/reward_dist Min               8.61387e-97\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00712585\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0106051\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0424511\n",
      "evaluation/env_infos/initial/reward_dist Min             1.23273e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.21517\n",
      "evaluation/env_infos/reward_dist Std                     0.282242\n",
      "evaluation/env_infos/reward_dist Max                     1\n",
      "evaluation/env_infos/reward_dist Min                     8.61387e-97\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0755723\n",
      "evaluation/env_infos/final/reward_energy Std             0.139757\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0040871\n",
      "evaluation/env_infos/final/reward_energy Min            -0.754334\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.224537\n",
      "evaluation/env_infos/initial/reward_energy Std           0.224555\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00397889\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.906472\n",
      "evaluation/env_infos/reward_energy Mean                 -0.064667\n",
      "evaluation/env_infos/reward_energy Std                   0.0924814\n",
      "evaluation/env_infos/reward_energy Max                  -0.000855171\n",
      "evaluation/env_infos/reward_energy Min                  -0.906472\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0406184\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.249537\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.414779\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000912248\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0111902\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.025133\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0433719\n",
      "evaluation/env_infos/end_effector_loc Mean               0.02153\n",
      "evaluation/env_infos/end_effector_loc Std                0.153493\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -0.414779\n",
      "time/data storing (s)                                    0.00333149\n",
      "time/evaluation sampling (s)                             1.09641\n",
      "time/exploration sampling (s)                            0.145133\n",
      "time/logging (s)                                         0.0213523\n",
      "time/saving (s)                                          0.0307612\n",
      "time/training (s)                                       54.6074\n",
      "time/epoch (s)                                          55.9044\n",
      "time/total (s)                                        3666.37\n",
      "Epoch                                                   71\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:15:21.248398 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 72 finished\r\n",
      "---------------------------------------------------  ----------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000909551\r\n",
      "trainer/QF2 Loss                                         0.000760931\r\n",
      "trainer/Policy Loss                                      3.45846\r\n",
      "trainer/Q1 Predictions Mean                             -1.25861\r\n",
      "trainer/Q1 Predictions Std                               0.863\r\n",
      "trainer/Q1 Predictions Max                               0.897449\r\n",
      "trainer/Q1 Predictions Min                              -3.47107\r\n",
      "trainer/Q2 Predictions Mean                             -1.25147\r\n",
      "trainer/Q2 Predictions Std                               0.863416\r\n",
      "trainer/Q2 Predictions Max                               0.854898\r\n",
      "trainer/Q2 Predictions Min                              -3.47101\r\n",
      "trainer/Q Targets Mean                                  -1.24906\r\n",
      "trainer/Q Targets Std                                    0.861997\r\n",
      "trainer/Q Targets Max                                    0.873771\r\n",
      "trainer/Q Targets Min                                   -3.52495\r\n",
      "trainer/Log Pis Mean                                     2.20937\r\n",
      "trainer/Log Pis Std                                      1.28884\r\n",
      "trainer/Log Pis Max                                      4.69985\r\n",
      "trainer/Log Pis Min                                     -2.17544\r\n",
      "trainer/Policy mu Mean                                   0.00859512\r\n",
      "trainer/Policy mu Std                                    0.231722\r\n",
      "trainer/Policy mu Max                                    1.17068\r\n",
      "trainer/Policy mu Min                                   -1.55767\r\n",
      "trainer/Policy log std Mean                             -2.42016\r\n",
      "trainer/Policy log std Std                               0.550277\r\n",
      "trainer/Policy log std Max                              -0.70136\r\n",
      "trainer/Policy log std Min                              -3.42124\r\n",
      "trainer/Alpha                                            0.0230616\r\n",
      "trainer/Alpha Loss                                       0.78942\r\n",
      "exploration/num steps total                           8300\r\n",
      "exploration/num paths total                            415\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.109283\r\n",
      "exploration/Rewards Std                                  0.110154\r\n",
      "exploration/Rewards Max                                  0.0683411\r\n",
      "exploration/Rewards Min                                 -0.688974\r\n",
      "exploration/Returns Mean                                -2.18565\r\n",
      "exploration/Returns Std                                  1.63198\r\n",
      "exploration/Returns Max                                 -0.429391\r\n",
      "exploration/Returns Min                                 -5.07478\r\n",
      "exploration/Actions Mean                                -1.79031e-05\r\n",
      "exploration/Actions Std                                  0.163321\r\n",
      "exploration/Actions Max                                  0.650414\r\n",
      "exploration/Actions Min                                 -0.449145\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -2.18565\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0512322\r\n",
      "exploration/env_infos/final/reward_dist Std              0.0655489\r\n",
      "exploration/env_infos/final/reward_dist Max              0.158111\r\n",
      "exploration/env_infos/final/reward_dist Min              2.38494e-06\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0106987\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0113032\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0265981\r\n",
      "exploration/env_infos/initial/reward_dist Min            3.0235e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.0773017\r\n",
      "exploration/env_infos/reward_dist Std                    0.164347\r\n",
      "exploration/env_infos/reward_dist Max                    0.936217\r\n",
      "exploration/env_infos/reward_dist Min                    8.95082e-07\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.176516\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0696335\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.113575\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.311935\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.322216\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.187518\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0427308\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.532375\r\n",
      "exploration/env_infos/reward_energy Mean                -0.195703\r\n",
      "exploration/env_infos/reward_energy Std                  0.12267\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0159297\r\n",
      "exploration/env_infos/reward_energy Min                 -0.685912\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0154771\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.263539\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.481457\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.445834\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00401514\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0125543\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0232033\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0224572\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.00756241\r\n",
      "exploration/env_infos/end_effector_loc Std               0.200361\r\n",
      "exploration/env_infos/end_effector_loc Max               0.556836\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.450517\r\n",
      "evaluation/num steps total                           73000\r\n",
      "evaluation/num paths total                            3650\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0567901\r\n",
      "evaluation/Rewards Std                                   0.0760603\r\n",
      "evaluation/Rewards Max                                   0.129603\r\n",
      "evaluation/Rewards Min                                  -0.469098\r\n",
      "evaluation/Returns Mean                                 -1.1358\r\n",
      "evaluation/Returns Std                                   1.09119\r\n",
      "evaluation/Returns Max                                   0.678061\r\n",
      "evaluation/Returns Min                                  -4.64711\r\n",
      "evaluation/Actions Mean                                  0.00297403\r\n",
      "evaluation/Actions Std                                   0.0818852\r\n",
      "evaluation/Actions Max                                   0.692129\r\n",
      "evaluation/Actions Min                                  -0.798767\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.1358\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.165829\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.242373\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.910344\r\n",
      "evaluation/env_infos/final/reward_dist Min               4.32495e-100\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0073267\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0108329\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0364556\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.71578e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.205232\r\n",
      "evaluation/env_infos/reward_dist Std                     0.269432\r\n",
      "evaluation/env_infos/reward_dist Max                     0.997623\r\n",
      "evaluation/env_infos/reward_dist Min                     4.32495e-100\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.056135\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.118099\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00218629\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.772063\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.225882\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.228103\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00681566\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.848953\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0632312\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0971075\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00117835\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.848953\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.054149\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.260944\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.6707\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.0036676\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0107408\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0346064\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0399384\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0395369\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.168158\r\n",
      "evaluation/env_infos/end_effector_loc Max                1\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.6707\r\n",
      "time/data storing (s)                                    0.00303809\r\n",
      "time/evaluation sampling (s)                             1.15978\r\n",
      "time/exploration sampling (s)                            0.155748\r\n",
      "time/logging (s)                                         0.0200252\r\n",
      "time/saving (s)                                          0.0279882\r\n",
      "time/training (s)                                       54.7922\r\n",
      "time/epoch (s)                                          56.1588\r\n",
      "time/total (s)                                        3723.49\r\n",
      "Epoch                                                   72\r\n",
      "---------------------------------------------------  ----------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:16:14.925718 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 73 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000566648\n",
      "trainer/QF2 Loss                                         0.000790792\n",
      "trainer/Policy Loss                                      2.92605\n",
      "trainer/Q1 Predictions Mean                             -1.04742\n",
      "trainer/Q1 Predictions Std                               0.865891\n",
      "trainer/Q1 Predictions Max                               1.22881\n",
      "trainer/Q1 Predictions Min                              -3.77677\n",
      "trainer/Q2 Predictions Mean                             -1.05444\n",
      "trainer/Q2 Predictions Std                               0.867626\n",
      "trainer/Q2 Predictions Max                               1.23928\n",
      "trainer/Q2 Predictions Min                              -3.77882\n",
      "trainer/Q Targets Mean                                  -1.04559\n",
      "trainer/Q Targets Std                                    0.860448\n",
      "trainer/Q Targets Max                                    1.20028\n",
      "trainer/Q Targets Min                                   -3.74542\n",
      "trainer/Log Pis Mean                                     1.8762\n",
      "trainer/Log Pis Std                                      1.37929\n",
      "trainer/Log Pis Max                                      4.2\n",
      "trainer/Log Pis Min                                     -3.73588\n",
      "trainer/Policy mu Mean                                   0.0413386\n",
      "trainer/Policy mu Std                                    0.300251\n",
      "trainer/Policy mu Max                                    1.70749\n",
      "trainer/Policy mu Min                                   -1.31965\n",
      "trainer/Policy log std Mean                             -2.2799\n",
      "trainer/Policy log std Std                               0.611822\n",
      "trainer/Policy log std Max                              -0.188869\n",
      "trainer/Policy log std Min                              -3.3546\n",
      "trainer/Alpha                                            0.0222377\n",
      "trainer/Alpha Loss                                      -0.471162\n",
      "exploration/num steps total                           8400\n",
      "exploration/num paths total                            420\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0911932\n",
      "exploration/Rewards Std                                  0.0782359\n",
      "exploration/Rewards Max                                  0.117461\n",
      "exploration/Rewards Min                                 -0.379602\n",
      "exploration/Returns Mean                                -1.82386\n",
      "exploration/Returns Std                                  0.942118\n",
      "exploration/Returns Max                                 -0.260485\n",
      "exploration/Returns Min                                 -2.77934\n",
      "exploration/Actions Mean                                -0.00272344\n",
      "exploration/Actions Std                                  0.0987086\n",
      "exploration/Actions Max                                  0.388581\n",
      "exploration/Actions Min                                 -0.328086\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.82386\n",
      "exploration/env_infos/final/reward_dist Mean             0.0789832\n",
      "exploration/env_infos/final/reward_dist Std              0.106286\n",
      "exploration/env_infos/final/reward_dist Max              0.27457\n",
      "exploration/env_infos/final/reward_dist Min              3.05045e-05\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000150886\n",
      "exploration/env_infos/initial/reward_dist Std            0.00010756\n",
      "exploration/env_infos/initial/reward_dist Max            0.000309439\n",
      "exploration/env_infos/initial/reward_dist Min            1.31151e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.133266\n",
      "exploration/env_infos/reward_dist Std                    0.22121\n",
      "exploration/env_infos/reward_dist Max                    0.992348\n",
      "exploration/env_infos/reward_dist Min                    1.31151e-05\n",
      "exploration/env_infos/final/reward_energy Mean          -0.129938\n",
      "exploration/env_infos/final/reward_energy Std            0.0880274\n",
      "exploration/env_infos/final/reward_energy Max           -0.0183847\n",
      "exploration/env_infos/final/reward_energy Min           -0.245748\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.212073\n",
      "exploration/env_infos/initial/reward_energy Std          0.110676\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0296867\n",
      "exploration/env_infos/initial/reward_energy Min         -0.365458\n",
      "exploration/env_infos/reward_energy Mean                -0.113404\n",
      "exploration/env_infos/reward_energy Std                  0.0814929\n",
      "exploration/env_infos/reward_energy Max                 -0.00195488\n",
      "exploration/env_infos/reward_energy Min                 -0.43976\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.027224\n",
      "exploration/env_infos/final/end_effector_loc Std         0.230915\n",
      "exploration/env_infos/final/end_effector_loc Max         0.408961\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.408729\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00055035\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00843963\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0164053\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00804759\n",
      "exploration/env_infos/end_effector_loc Mean              0.0278061\n",
      "exploration/env_infos/end_effector_loc Std               0.160744\n",
      "exploration/env_infos/end_effector_loc Max               0.408961\n",
      "exploration/env_infos/end_effector_loc Min              -0.408729\n",
      "evaluation/num steps total                           74000\n",
      "evaluation/num paths total                            3700\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0461172\n",
      "evaluation/Rewards Std                                   0.0795324\n",
      "evaluation/Rewards Max                                   0.179078\n",
      "evaluation/Rewards Min                                  -0.459704\n",
      "evaluation/Returns Mean                                 -0.922344\n",
      "evaluation/Returns Std                                   1.20038\n",
      "evaluation/Returns Max                                   2.49339\n",
      "evaluation/Returns Min                                  -3.22894\n",
      "evaluation/Actions Mean                                 -0.00243599\n",
      "evaluation/Actions Std                                   0.0729711\n",
      "evaluation/Actions Max                                   0.599325\n",
      "evaluation/Actions Min                                  -0.821327\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.922344\n",
      "evaluation/env_infos/final/reward_dist Mean              0.141079\n",
      "evaluation/env_infos/final/reward_dist Std               0.187026\n",
      "evaluation/env_infos/final/reward_dist Max               0.761096\n",
      "evaluation/env_infos/final/reward_dist Min               3.85985e-17\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00777009\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0226754\n",
      "evaluation/env_infos/initial/reward_dist Max             0.130532\n",
      "evaluation/env_infos/initial/reward_dist Min             1.97142e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.15891\n",
      "evaluation/env_infos/reward_dist Std                     0.235335\n",
      "evaluation/env_infos/reward_dist Max                     0.996795\n",
      "evaluation/env_infos/reward_dist Min                     3.85985e-17\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.034279\n",
      "evaluation/env_infos/final/reward_energy Std             0.0302135\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00208216\n",
      "evaluation/env_infos/final/reward_energy Min            -0.119497\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.247821\n",
      "evaluation/env_infos/initial/reward_energy Std           0.26772\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0184779\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.03769\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0577981\n",
      "evaluation/env_infos/reward_energy Std                   0.0855618\n",
      "evaluation/env_infos/reward_energy Max                  -0.00174582\n",
      "evaluation/env_infos/reward_energy Min                  -1.03769\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0699772\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.232147\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.582343\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.676979\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00401759\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0122564\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0299662\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0410663\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0394361\n",
      "evaluation/env_infos/end_effector_loc Std                0.151999\n",
      "evaluation/env_infos/end_effector_loc Max                0.582343\n",
      "evaluation/env_infos/end_effector_loc Min               -0.676979\n",
      "time/data storing (s)                                    0.00311539\n",
      "time/evaluation sampling (s)                             0.981728\n",
      "time/exploration sampling (s)                            0.137873\n",
      "time/logging (s)                                         0.020241\n",
      "time/saving (s)                                          0.028947\n",
      "time/training (s)                                       51.5816\n",
      "time/epoch (s)                                          52.7535\n",
      "time/total (s)                                        3777.17\n",
      "Epoch                                                   73\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:17:09.905185 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 74 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000568868\n",
      "trainer/QF2 Loss                                         0.000566029\n",
      "trainer/Policy Loss                                      3.06203\n",
      "trainer/Q1 Predictions Mean                             -1.13993\n",
      "trainer/Q1 Predictions Std                               0.822353\n",
      "trainer/Q1 Predictions Max                               0.853925\n",
      "trainer/Q1 Predictions Min                              -3.6003\n",
      "trainer/Q2 Predictions Mean                             -1.14077\n",
      "trainer/Q2 Predictions Std                               0.821583\n",
      "trainer/Q2 Predictions Max                               0.848635\n",
      "trainer/Q2 Predictions Min                              -3.58567\n",
      "trainer/Q Targets Mean                                  -1.14386\n",
      "trainer/Q Targets Std                                    0.822877\n",
      "trainer/Q Targets Max                                    0.872227\n",
      "trainer/Q Targets Min                                   -3.59972\n",
      "trainer/Log Pis Mean                                     1.92213\n",
      "trainer/Log Pis Std                                      1.30932\n",
      "trainer/Log Pis Max                                      4.41955\n",
      "trainer/Log Pis Min                                     -2.5893\n",
      "trainer/Policy mu Mean                                   0.0255634\n",
      "trainer/Policy mu Std                                    0.274383\n",
      "trainer/Policy mu Max                                    1.68296\n",
      "trainer/Policy mu Min                                   -1.36368\n",
      "trainer/Policy log std Mean                             -2.29775\n",
      "trainer/Policy log std Std                               0.574466\n",
      "trainer/Policy log std Max                              -0.00674558\n",
      "trainer/Policy log std Min                              -3.21397\n",
      "trainer/Alpha                                            0.0240624\n",
      "trainer/Alpha Loss                                      -0.29023\n",
      "exploration/num steps total                           8500\n",
      "exploration/num paths total                            425\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0767073\n",
      "exploration/Rewards Std                                  0.051329\n",
      "exploration/Rewards Max                                  0.0556786\n",
      "exploration/Rewards Min                                 -0.22084\n",
      "exploration/Returns Mean                                -1.53415\n",
      "exploration/Returns Std                                  0.68672\n",
      "exploration/Returns Max                                 -0.287155\n",
      "exploration/Returns Min                                 -2.25788\n",
      "exploration/Actions Mean                                 0.0105182\n",
      "exploration/Actions Std                                  0.0777301\n",
      "exploration/Actions Max                                  0.22853\n",
      "exploration/Actions Min                                 -0.226932\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.53415\n",
      "exploration/env_infos/final/reward_dist Mean             0.188062\n",
      "exploration/env_infos/final/reward_dist Std              0.340725\n",
      "exploration/env_infos/final/reward_dist Max              0.868174\n",
      "exploration/env_infos/final/reward_dist Min              3.90563e-15\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00814117\n",
      "exploration/env_infos/initial/reward_dist Std            0.00963569\n",
      "exploration/env_infos/initial/reward_dist Max            0.0220829\n",
      "exploration/env_infos/initial/reward_dist Min            1.69474e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.167786\n",
      "exploration/env_infos/reward_dist Std                    0.247747\n",
      "exploration/env_infos/reward_dist Max                    0.983608\n",
      "exploration/env_infos/reward_dist Min                    3.90563e-15\n",
      "exploration/env_infos/final/reward_energy Mean          -0.0947197\n",
      "exploration/env_infos/final/reward_energy Std            0.0452672\n",
      "exploration/env_infos/final/reward_energy Max           -0.0295305\n",
      "exploration/env_infos/final/reward_energy Min           -0.160418\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.113964\n",
      "exploration/env_infos/initial/reward_energy Std          0.053806\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0580438\n",
      "exploration/env_infos/initial/reward_energy Min         -0.17813\n",
      "exploration/env_infos/reward_energy Mean                -0.0968238\n",
      "exploration/env_infos/reward_energy Std                  0.0541327\n",
      "exploration/env_infos/reward_energy Max                 -0.00919411\n",
      "exploration/env_infos/reward_energy Min                 -0.228613\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0937939\n",
      "exploration/env_infos/final/end_effector_loc Std         0.275244\n",
      "exploration/env_infos/final/end_effector_loc Max         0.702439\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.326922\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00190791\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00402661\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00724638\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00735162\n",
      "exploration/env_infos/end_effector_loc Mean              0.0319783\n",
      "exploration/env_infos/end_effector_loc Std               0.152127\n",
      "exploration/env_infos/end_effector_loc Max               0.702439\n",
      "exploration/env_infos/end_effector_loc Min              -0.326922\n",
      "evaluation/num steps total                           75000\n",
      "evaluation/num paths total                            3750\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0382309\n",
      "evaluation/Rewards Std                                   0.0651618\n",
      "evaluation/Rewards Max                                   0.15911\n",
      "evaluation/Rewards Min                                  -0.436894\n",
      "evaluation/Returns Mean                                 -0.764619\n",
      "evaluation/Returns Std                                   0.919025\n",
      "evaluation/Returns Max                                   1.37177\n",
      "evaluation/Returns Min                                  -2.54442\n",
      "evaluation/Actions Mean                                  0.00235383\n",
      "evaluation/Actions Std                                   0.0746204\n",
      "evaluation/Actions Max                                   0.616858\n",
      "evaluation/Actions Min                                  -0.757315\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.764619\n",
      "evaluation/env_infos/final/reward_dist Mean              0.210313\n",
      "evaluation/env_infos/final/reward_dist Std               0.276512\n",
      "evaluation/env_infos/final/reward_dist Max               0.905114\n",
      "evaluation/env_infos/final/reward_dist Min               1.18453e-08\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00928486\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0165834\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0752171\n",
      "evaluation/env_infos/initial/reward_dist Min             9.89679e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.210817\n",
      "evaluation/env_infos/reward_dist Std                     0.260714\n",
      "evaluation/env_infos/reward_dist Max                     0.99045\n",
      "evaluation/env_infos/reward_dist Min                     1.18453e-08\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0487056\n",
      "evaluation/env_infos/final/reward_energy Std             0.0405916\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00350038\n",
      "evaluation/env_infos/final/reward_energy Min            -0.152252\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.235426\n",
      "evaluation/env_infos/initial/reward_energy Std           0.224121\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00763535\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.996583\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0662758\n",
      "evaluation/env_infos/reward_energy Std                   0.0821888\n",
      "evaluation/env_infos/reward_energy Max                  -0.000972626\n",
      "evaluation/env_infos/reward_energy Min                  -0.996583\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0065735\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.219285\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.42146\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.548808\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000209474\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0114902\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0308429\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0378657\n",
      "evaluation/env_infos/end_effector_loc Mean               0.00121764\n",
      "evaluation/env_infos/end_effector_loc Std                0.148723\n",
      "evaluation/env_infos/end_effector_loc Max                0.42146\n",
      "evaluation/env_infos/end_effector_loc Min               -0.548808\n",
      "time/data storing (s)                                    0.00302774\n",
      "time/evaluation sampling (s)                             0.989136\n",
      "time/exploration sampling (s)                            0.15485\n",
      "time/logging (s)                                         0.0211581\n",
      "time/saving (s)                                          0.0296168\n",
      "time/training (s)                                       52.7859\n",
      "time/epoch (s)                                          53.9837\n",
      "time/total (s)                                        3832.14\n",
      "Epoch                                                   74\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:18:14.487215 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 75 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.00109703\r\n",
      "trainer/QF2 Loss                                         0.000609169\r\n",
      "trainer/Policy Loss                                      3.14343\r\n",
      "trainer/Q1 Predictions Mean                             -1.14385\r\n",
      "trainer/Q1 Predictions Std                               0.768\r\n",
      "trainer/Q1 Predictions Max                               0.365679\r\n",
      "trainer/Q1 Predictions Min                              -3.70483\r\n",
      "trainer/Q2 Predictions Mean                             -1.16125\r\n",
      "trainer/Q2 Predictions Std                               0.767302\r\n",
      "trainer/Q2 Predictions Max                               0.354781\r\n",
      "trainer/Q2 Predictions Min                              -3.72756\r\n",
      "trainer/Q Targets Mean                                  -1.15883\r\n",
      "trainer/Q Targets Std                                    0.771834\r\n",
      "trainer/Q Targets Max                                    0.354594\r\n",
      "trainer/Q Targets Min                                   -3.7169\r\n",
      "trainer/Log Pis Mean                                     1.98808\r\n",
      "trainer/Log Pis Std                                      1.31029\r\n",
      "trainer/Log Pis Max                                      4.10618\r\n",
      "trainer/Log Pis Min                                     -3.15809\r\n",
      "trainer/Policy mu Mean                                   0.041226\r\n",
      "trainer/Policy mu Std                                    0.284205\r\n",
      "trainer/Policy mu Max                                    2.04668\r\n",
      "trainer/Policy mu Min                                   -1.55748\r\n",
      "trainer/Policy log std Mean                             -2.36995\r\n",
      "trainer/Policy log std Std                               0.548574\r\n",
      "trainer/Policy log std Max                              -0.190122\r\n",
      "trainer/Policy log std Min                              -3.15162\r\n",
      "trainer/Alpha                                            0.0243954\r\n",
      "trainer/Alpha Loss                                      -0.0442638\r\n",
      "exploration/num steps total                           8600\r\n",
      "exploration/num paths total                            430\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0840368\r\n",
      "exploration/Rewards Std                                  0.0913895\r\n",
      "exploration/Rewards Max                                  0.160845\r\n",
      "exploration/Rewards Min                                 -0.345285\r\n",
      "exploration/Returns Mean                                -1.68074\r\n",
      "exploration/Returns Std                                  1.32874\r\n",
      "exploration/Returns Max                                  0.847767\r\n",
      "exploration/Returns Min                                 -3.05087\r\n",
      "exploration/Actions Mean                                -0.0022916\r\n",
      "exploration/Actions Std                                  0.11744\r\n",
      "exploration/Actions Max                                  0.419474\r\n",
      "exploration/Actions Min                                 -0.783506\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.68074\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0331423\r\n",
      "exploration/env_infos/final/reward_dist Std              0.0393288\r\n",
      "exploration/env_infos/final/reward_dist Max              0.0899947\r\n",
      "exploration/env_infos/final/reward_dist Min              2.43245e-22\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00425004\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00508209\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0131382\r\n",
      "exploration/env_infos/initial/reward_dist Min            1.63794e-05\r\n",
      "exploration/env_infos/reward_dist Mean                   0.178369\r\n",
      "exploration/env_infos/reward_dist Std                    0.268962\r\n",
      "exploration/env_infos/reward_dist Max                    0.99865\r\n",
      "exploration/env_infos/reward_dist Min                    2.43245e-22\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.134306\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0501627\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0656202\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.216808\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.280322\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.331166\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0647969\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.937381\r\n",
      "exploration/env_infos/reward_energy Mean                -0.121262\r\n",
      "exploration/env_infos/reward_energy Std                  0.113535\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0136057\r\n",
      "exploration/env_infos/reward_energy Min                 -0.937381\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0499793\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.34763\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.747365\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.557843\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00705919\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0136192\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00576906\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0391753\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0256254\r\n",
      "exploration/env_infos/end_effector_loc Std               0.189447\r\n",
      "exploration/env_infos/end_effector_loc Max               0.747365\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.557843\r\n",
      "evaluation/num steps total                           76000\r\n",
      "evaluation/num paths total                            3800\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0405435\r\n",
      "evaluation/Rewards Std                                   0.0707536\r\n",
      "evaluation/Rewards Max                                   0.153829\r\n",
      "evaluation/Rewards Min                                  -0.388672\r\n",
      "evaluation/Returns Mean                                 -0.810871\r\n",
      "evaluation/Returns Std                                   1.05435\r\n",
      "evaluation/Returns Max                                   1.8556\r\n",
      "evaluation/Returns Min                                  -3.28564\r\n",
      "evaluation/Actions Mean                                 -0.00171409\r\n",
      "evaluation/Actions Std                                   0.0537755\r\n",
      "evaluation/Actions Max                                   0.501258\r\n",
      "evaluation/Actions Min                                  -0.538252\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.810871\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.215551\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.298207\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.940517\r\n",
      "evaluation/env_infos/final/reward_dist Min               3.53455e-19\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00948148\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0148197\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0562403\r\n",
      "evaluation/env_infos/initial/reward_dist Min             3.31646e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.212013\r\n",
      "evaluation/env_infos/reward_dist Std                     0.278361\r\n",
      "evaluation/env_infos/reward_dist Max                     0.998574\r\n",
      "evaluation/env_infos/reward_dist Min                     3.53455e-19\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.050862\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0749639\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0019198\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.490521\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.168348\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.136164\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0188881\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.5636\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0505823\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0568412\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000631781\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.5636\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0422619\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.219297\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.526796\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.477871\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000140212\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00765391\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0250629\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0269126\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0182229\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.141081\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.526796\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.477871\r\n",
      "time/data storing (s)                                    0.0032106\r\n",
      "time/evaluation sampling (s)                             1.02615\r\n",
      "time/exploration sampling (s)                            0.129903\r\n",
      "time/logging (s)                                         0.0203199\r\n",
      "time/saving (s)                                          0.0285628\r\n",
      "time/training (s)                                       62.3659\r\n",
      "time/epoch (s)                                          63.574\r\n",
      "time/total (s)                                        3896.72\r\n",
      "Epoch                                                   75\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:19:11.328691 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 76 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000699217\n",
      "trainer/QF2 Loss                                         0.000724117\n",
      "trainer/Policy Loss                                      3.29609\n",
      "trainer/Q1 Predictions Mean                             -1.23935\n",
      "trainer/Q1 Predictions Std                               0.85254\n",
      "trainer/Q1 Predictions Max                               0.582245\n",
      "trainer/Q1 Predictions Min                              -3.36701\n",
      "trainer/Q2 Predictions Mean                             -1.25216\n",
      "trainer/Q2 Predictions Std                               0.856632\n",
      "trainer/Q2 Predictions Max                               0.552655\n",
      "trainer/Q2 Predictions Min                              -3.38267\n",
      "trainer/Q Targets Mean                                  -1.24821\n",
      "trainer/Q Targets Std                                    0.857338\n",
      "trainer/Q Targets Max                                    0.545001\n",
      "trainer/Q Targets Min                                   -3.39643\n",
      "trainer/Log Pis Mean                                     2.05215\n",
      "trainer/Log Pis Std                                      1.41557\n",
      "trainer/Log Pis Max                                      4.62379\n",
      "trainer/Log Pis Min                                     -4.26523\n",
      "trainer/Policy mu Mean                                   0.0338589\n",
      "trainer/Policy mu Std                                    0.293676\n",
      "trainer/Policy mu Max                                    1.99545\n",
      "trainer/Policy mu Min                                   -1.66\n",
      "trainer/Policy log std Mean                             -2.32103\n",
      "trainer/Policy log std Std                               0.624684\n",
      "trainer/Policy log std Max                              -0.414215\n",
      "trainer/Policy log std Min                              -3.30564\n",
      "trainer/Alpha                                            0.0240141\n",
      "trainer/Alpha Loss                                       0.194536\n",
      "exploration/num steps total                           8700\n",
      "exploration/num paths total                            435\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0706069\n",
      "exploration/Rewards Std                                  0.0788181\n",
      "exploration/Rewards Max                                  0.110892\n",
      "exploration/Rewards Min                                 -0.23281\n",
      "exploration/Returns Mean                                -1.41214\n",
      "exploration/Returns Std                                  1.10277\n",
      "exploration/Returns Max                                  0.668489\n",
      "exploration/Returns Min                                 -2.38336\n",
      "exploration/Actions Mean                                 0.00412315\n",
      "exploration/Actions Std                                  0.0893249\n",
      "exploration/Actions Max                                  0.24145\n",
      "exploration/Actions Min                                 -0.27714\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.41214\n",
      "exploration/env_infos/final/reward_dist Mean             0.226044\n",
      "exploration/env_infos/final/reward_dist Std              0.313037\n",
      "exploration/env_infos/final/reward_dist Max              0.840062\n",
      "exploration/env_infos/final/reward_dist Min              0.00120318\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00437537\n",
      "exploration/env_infos/initial/reward_dist Std            0.00825455\n",
      "exploration/env_infos/initial/reward_dist Max            0.0208673\n",
      "exploration/env_infos/initial/reward_dist Min            1.81296e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.211182\n",
      "exploration/env_infos/reward_dist Std                    0.2539\n",
      "exploration/env_infos/reward_dist Max                    0.950794\n",
      "exploration/env_infos/reward_dist Min                    1.81296e-06\n",
      "exploration/env_infos/final/reward_energy Mean          -0.181793\n",
      "exploration/env_infos/final/reward_energy Std            0.0267401\n",
      "exploration/env_infos/final/reward_energy Max           -0.145712\n",
      "exploration/env_infos/final/reward_energy Min           -0.210818\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.13447\n",
      "exploration/env_infos/initial/reward_energy Std          0.074452\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0531016\n",
      "exploration/env_infos/initial/reward_energy Min         -0.254803\n",
      "exploration/env_infos/reward_energy Mean                -0.108633\n",
      "exploration/env_infos/reward_energy Std                  0.0647361\n",
      "exploration/env_infos/reward_energy Max                 -0.0143442\n",
      "exploration/env_infos/reward_energy Min                 -0.387866\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0169698\n",
      "exploration/env_infos/final/end_effector_loc Std         0.234812\n",
      "exploration/env_infos/final/end_effector_loc Max         0.466491\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.233426\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.000682583\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00539127\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0120725\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00807097\n",
      "exploration/env_infos/end_effector_loc Mean              0.00419939\n",
      "exploration/env_infos/end_effector_loc Std               0.145484\n",
      "exploration/env_infos/end_effector_loc Max               0.466491\n",
      "exploration/env_infos/end_effector_loc Min              -0.233426\n",
      "evaluation/num steps total                           77000\n",
      "evaluation/num paths total                            3850\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0459767\n",
      "evaluation/Rewards Std                                   0.0657171\n",
      "evaluation/Rewards Max                                   0.112751\n",
      "evaluation/Rewards Min                                  -0.474306\n",
      "evaluation/Returns Mean                                 -0.919534\n",
      "evaluation/Returns Std                                   0.918455\n",
      "evaluation/Returns Max                                   0.768208\n",
      "evaluation/Returns Min                                  -3.58232\n",
      "evaluation/Actions Mean                                 -0.0012804\n",
      "evaluation/Actions Std                                   0.0665048\n",
      "evaluation/Actions Max                                   0.472785\n",
      "evaluation/Actions Min                                  -0.673214\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.919534\n",
      "evaluation/env_infos/final/reward_dist Mean              0.151071\n",
      "evaluation/env_infos/final/reward_dist Std               0.222921\n",
      "evaluation/env_infos/final/reward_dist Max               0.732301\n",
      "evaluation/env_infos/final/reward_dist Min               4.34969e-14\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00692078\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0165045\n",
      "evaluation/env_infos/initial/reward_dist Max             0.104163\n",
      "evaluation/env_infos/initial/reward_dist Min             1.0071e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.174642\n",
      "evaluation/env_infos/reward_dist Std                     0.258021\n",
      "evaluation/env_infos/reward_dist Max                     0.996708\n",
      "evaluation/env_infos/reward_dist Min                     4.34969e-14\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0371784\n",
      "evaluation/env_infos/final/reward_energy Std             0.0359974\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00222343\n",
      "evaluation/env_infos/final/reward_energy Min            -0.165411\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.19597\n",
      "evaluation/env_infos/initial/reward_energy Std           0.192388\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0299668\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.810293\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0572625\n",
      "evaluation/env_infos/reward_energy Std                   0.0746328\n",
      "evaluation/env_infos/reward_energy Max                  -0.00107476\n",
      "evaluation/env_infos/reward_energy Min                  -0.810293\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0387727\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.245579\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.415993\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.630214\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.0017826\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00954433\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0236392\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0336607\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0241056\n",
      "evaluation/env_infos/end_effector_loc Std                0.158245\n",
      "evaluation/env_infos/end_effector_loc Max                0.415993\n",
      "evaluation/env_infos/end_effector_loc Min               -0.630214\n",
      "time/data storing (s)                                    0.0029736\n",
      "time/evaluation sampling (s)                             1.02905\n",
      "time/exploration sampling (s)                            0.143854\n",
      "time/logging (s)                                         0.0275734\n",
      "time/saving (s)                                          0.0487313\n",
      "time/training (s)                                       54.5058\n",
      "time/epoch (s)                                          55.758\n",
      "time/total (s)                                        3953.57\n",
      "Epoch                                                   76\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:20:07.991612 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 77 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000839078\r\n",
      "trainer/QF2 Loss                                         0.000786215\r\n",
      "trainer/Policy Loss                                      3.00661\r\n",
      "trainer/Q1 Predictions Mean                             -1.02398\r\n",
      "trainer/Q1 Predictions Std                               0.799134\r\n",
      "trainer/Q1 Predictions Max                               0.4346\r\n",
      "trainer/Q1 Predictions Min                              -3.49235\r\n",
      "trainer/Q2 Predictions Mean                             -1.02236\r\n",
      "trainer/Q2 Predictions Std                               0.795281\r\n",
      "trainer/Q2 Predictions Max                               0.420096\r\n",
      "trainer/Q2 Predictions Min                              -3.49057\r\n",
      "trainer/Q Targets Mean                                  -1.01386\r\n",
      "trainer/Q Targets Std                                    0.795108\r\n",
      "trainer/Q Targets Max                                    0.440746\r\n",
      "trainer/Q Targets Min                                   -3.4655\r\n",
      "trainer/Log Pis Mean                                     1.98253\r\n",
      "trainer/Log Pis Std                                      1.32034\r\n",
      "trainer/Log Pis Max                                      4.11892\r\n",
      "trainer/Log Pis Min                                     -2.77286\r\n",
      "trainer/Policy mu Mean                                   0.0152095\r\n",
      "trainer/Policy mu Std                                    0.193015\r\n",
      "trainer/Policy mu Max                                    1.3738\r\n",
      "trainer/Policy mu Min                                   -1.21425\r\n",
      "trainer/Policy log std Mean                             -2.36466\r\n",
      "trainer/Policy log std Std                               0.540161\r\n",
      "trainer/Policy log std Max                               0.159771\r\n",
      "trainer/Policy log std Min                              -3.19223\r\n",
      "trainer/Alpha                                            0.024106\r\n",
      "trainer/Alpha Loss                                      -0.0651017\r\n",
      "exploration/num steps total                           8800\r\n",
      "exploration/num paths total                            440\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0572293\r\n",
      "exploration/Rewards Std                                  0.0767772\r\n",
      "exploration/Rewards Max                                  0.110654\r\n",
      "exploration/Rewards Min                                 -0.27328\r\n",
      "exploration/Returns Mean                                -1.14459\r\n",
      "exploration/Returns Std                                  1.2199\r\n",
      "exploration/Returns Max                                  0.964196\r\n",
      "exploration/Returns Min                                 -2.55233\r\n",
      "exploration/Actions Mean                                 0.00590121\r\n",
      "exploration/Actions Std                                  0.200684\r\n",
      "exploration/Actions Max                                  0.581985\r\n",
      "exploration/Actions Min                                 -0.914668\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.14459\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.00433651\r\n",
      "exploration/env_infos/final/reward_dist Std              0.0046994\r\n",
      "exploration/env_infos/final/reward_dist Max              0.0128666\r\n",
      "exploration/env_infos/final/reward_dist Min              1.22444e-07\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000898927\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.000829877\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.00219669\r\n",
      "exploration/env_infos/initial/reward_dist Min            4.14857e-05\r\n",
      "exploration/env_infos/reward_dist Mean                   0.113666\r\n",
      "exploration/env_infos/reward_dist Std                    0.180575\r\n",
      "exploration/env_infos/reward_dist Max                    0.895583\r\n",
      "exploration/env_infos/reward_dist Min                    1.87166e-09\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.141727\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0803665\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0576639\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.264657\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.38672\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.285667\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0853373\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.728782\r\n",
      "exploration/env_infos/reward_energy Mean                -0.216293\r\n",
      "exploration/env_infos/reward_energy Std                  0.183942\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00853861\r\n",
      "exploration/env_infos/reward_energy Min                 -1.00752\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0346721\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.251208\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.305851\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.531605\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00412069\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0164914\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0287606\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0355473\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0271409\r\n",
      "exploration/env_infos/end_effector_loc Std               0.184134\r\n",
      "exploration/env_infos/end_effector_loc Max               0.305851\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.531605\r\n",
      "evaluation/num steps total                           78000\r\n",
      "evaluation/num paths total                            3900\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0546061\r\n",
      "evaluation/Rewards Std                                   0.0783644\r\n",
      "evaluation/Rewards Max                                   0.153748\r\n",
      "evaluation/Rewards Min                                  -0.565301\r\n",
      "evaluation/Returns Mean                                 -1.09212\r\n",
      "evaluation/Returns Std                                   1.07089\r\n",
      "evaluation/Returns Max                                   1.73441\r\n",
      "evaluation/Returns Min                                  -4.19288\r\n",
      "evaluation/Actions Mean                                  0.00436934\r\n",
      "evaluation/Actions Std                                   0.0662377\r\n",
      "evaluation/Actions Max                                   0.3883\r\n",
      "evaluation/Actions Min                                  -0.558574\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.09212\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.139764\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.252627\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.954868\r\n",
      "evaluation/env_infos/final/reward_dist Min               3.87648e-39\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00779978\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0180695\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0933218\r\n",
      "evaluation/env_infos/initial/reward_dist Min             9.96933e-07\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.158876\r\n",
      "evaluation/env_infos/reward_dist Std                     0.253578\r\n",
      "evaluation/env_infos/reward_dist Max                     0.995081\r\n",
      "evaluation/env_infos/reward_dist Min                     1.18593e-39\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0523407\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0610357\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00225912\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.349962\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.167176\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.165676\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0110628\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.680294\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0606432\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0716621\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000703361\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.680294\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0104652\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.277402\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.713604\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00111212\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00824672\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.019415\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0279287\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00482812\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.171087\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.713604\r\n",
      "evaluation/env_infos/end_effector_loc Min               -1\r\n",
      "time/data storing (s)                                    0.00328843\r\n",
      "time/evaluation sampling (s)                             1.27157\r\n",
      "time/exploration sampling (s)                            0.152887\r\n",
      "time/logging (s)                                         0.0239501\r\n",
      "time/saving (s)                                          0.0324031\r\n",
      "time/training (s)                                       54.1652\r\n",
      "time/epoch (s)                                          55.6493\r\n",
      "time/total (s)                                        4010.23\r\n",
      "Epoch                                                   77\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:21:04.724526 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 78 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000702262\n",
      "trainer/QF2 Loss                                         0.00057913\n",
      "trainer/Policy Loss                                      3.18069\n",
      "trainer/Q1 Predictions Mean                             -1.15093\n",
      "trainer/Q1 Predictions Std                               0.819447\n",
      "trainer/Q1 Predictions Max                               0.535822\n",
      "trainer/Q1 Predictions Min                              -3.45505\n",
      "trainer/Q2 Predictions Mean                             -1.15526\n",
      "trainer/Q2 Predictions Std                               0.818222\n",
      "trainer/Q2 Predictions Max                               0.53946\n",
      "trainer/Q2 Predictions Min                              -3.45605\n",
      "trainer/Q Targets Mean                                  -1.15542\n",
      "trainer/Q Targets Std                                    0.823061\n",
      "trainer/Q Targets Max                                    0.517948\n",
      "trainer/Q Targets Min                                   -3.49687\n",
      "trainer/Log Pis Mean                                     2.03592\n",
      "trainer/Log Pis Std                                      1.25414\n",
      "trainer/Log Pis Max                                      4.27327\n",
      "trainer/Log Pis Min                                     -3.78618\n",
      "trainer/Policy mu Mean                                   0.0466617\n",
      "trainer/Policy mu Std                                    0.291193\n",
      "trainer/Policy mu Max                                    2.19756\n",
      "trainer/Policy mu Min                                   -1.83475\n",
      "trainer/Policy log std Mean                             -2.33277\n",
      "trainer/Policy log std Std                               0.549802\n",
      "trainer/Policy log std Max                              -0.148577\n",
      "trainer/Policy log std Min                              -3.18623\n",
      "trainer/Alpha                                            0.024881\n",
      "trainer/Alpha Loss                                       0.132717\n",
      "exploration/num steps total                           8900\n",
      "exploration/num paths total                            445\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0776395\n",
      "exploration/Rewards Std                                  0.0922157\n",
      "exploration/Rewards Max                                  0.139795\n",
      "exploration/Rewards Min                                 -0.236431\n",
      "exploration/Returns Mean                                -1.55279\n",
      "exploration/Returns Std                                  1.58656\n",
      "exploration/Returns Max                                  0.358998\n",
      "exploration/Returns Min                                 -3.31004\n",
      "exploration/Actions Mean                                 0.00371972\n",
      "exploration/Actions Std                                  0.204237\n",
      "exploration/Actions Max                                  0.652673\n",
      "exploration/Actions Min                                 -0.936933\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.55279\n",
      "exploration/env_infos/final/reward_dist Mean             0.180429\n",
      "exploration/env_infos/final/reward_dist Std              0.307033\n",
      "exploration/env_infos/final/reward_dist Max              0.793662\n",
      "exploration/env_infos/final/reward_dist Min              0.000534932\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00302532\n",
      "exploration/env_infos/initial/reward_dist Std            0.00410415\n",
      "exploration/env_infos/initial/reward_dist Max            0.0109659\n",
      "exploration/env_infos/initial/reward_dist Min            1.45446e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.166083\n",
      "exploration/env_infos/reward_dist Std                    0.264226\n",
      "exploration/env_infos/reward_dist Max                    0.964446\n",
      "exploration/env_infos/reward_dist Min                    7.11653e-08\n",
      "exploration/env_infos/final/reward_energy Mean          -0.102476\n",
      "exploration/env_infos/final/reward_energy Std            0.0584363\n",
      "exploration/env_infos/final/reward_energy Max           -0.0197578\n",
      "exploration/env_infos/final/reward_energy Min           -0.19336\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.436664\n",
      "exploration/env_infos/initial/reward_energy Std          0.426903\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0606459\n",
      "exploration/env_infos/initial/reward_energy Min         -1.00639\n",
      "exploration/env_infos/reward_energy Mean                -0.209823\n",
      "exploration/env_infos/reward_energy Std                  0.198564\n",
      "exploration/env_infos/reward_energy Max                 -0.01339\n",
      "exploration/env_infos/reward_energy Min                 -1.00639\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0420519\n",
      "exploration/env_infos/final/end_effector_loc Std         0.197908\n",
      "exploration/env_infos/final/end_effector_loc Max         0.239627\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.321416\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.0121834\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0178247\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00459797\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0468467\n",
      "exploration/env_infos/end_effector_loc Mean             -0.058267\n",
      "exploration/env_infos/end_effector_loc Std               0.15824\n",
      "exploration/env_infos/end_effector_loc Max               0.239627\n",
      "exploration/env_infos/end_effector_loc Min              -0.45703\n",
      "evaluation/num steps total                           79000\n",
      "evaluation/num paths total                            3950\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.050001\n",
      "evaluation/Rewards Std                                   0.0708965\n",
      "evaluation/Rewards Max                                   0.137244\n",
      "evaluation/Rewards Min                                  -0.373947\n",
      "evaluation/Returns Mean                                 -1.00002\n",
      "evaluation/Returns Std                                   1.06671\n",
      "evaluation/Returns Max                                   1.70195\n",
      "evaluation/Returns Min                                  -3.74396\n",
      "evaluation/Actions Mean                                  0.00160698\n",
      "evaluation/Actions Std                                   0.0748365\n",
      "evaluation/Actions Max                                   0.498003\n",
      "evaluation/Actions Min                                  -0.70241\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.00002\n",
      "evaluation/env_infos/final/reward_dist Mean              0.197982\n",
      "evaluation/env_infos/final/reward_dist Std               0.276561\n",
      "evaluation/env_infos/final/reward_dist Max               0.981777\n",
      "evaluation/env_infos/final/reward_dist Min               8.85074e-17\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00678391\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0105987\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0365472\n",
      "evaluation/env_infos/initial/reward_dist Min             1.44131e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.191003\n",
      "evaluation/env_infos/reward_dist Std                     0.275413\n",
      "evaluation/env_infos/reward_dist Max                     0.991446\n",
      "evaluation/env_infos/reward_dist Min                     8.85074e-17\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0408598\n",
      "evaluation/env_infos/final/reward_energy Std             0.0498846\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00313898\n",
      "evaluation/env_infos/final/reward_energy Min            -0.260334\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.229876\n",
      "evaluation/env_infos/initial/reward_energy Std           0.233432\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0068349\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.950922\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0650972\n",
      "evaluation/env_infos/reward_energy Std                   0.0834776\n",
      "evaluation/env_infos/reward_energy Max                  -0.00113963\n",
      "evaluation/env_infos/reward_energy Min                  -0.950922\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0176422\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.212489\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.461251\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.718104\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00263548\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0112792\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0249002\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0351205\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0187645\n",
      "evaluation/env_infos/end_effector_loc Std                0.144023\n",
      "evaluation/env_infos/end_effector_loc Max                0.461251\n",
      "evaluation/env_infos/end_effector_loc Min               -0.718104\n",
      "time/data storing (s)                                    0.0032007\n",
      "time/evaluation sampling (s)                             1.08\n",
      "time/exploration sampling (s)                            0.139794\n",
      "time/logging (s)                                         0.0216454\n",
      "time/saving (s)                                          0.0287482\n",
      "time/training (s)                                       54.3422\n",
      "time/epoch (s)                                          55.6156\n",
      "time/total (s)                                        4066.96\n",
      "Epoch                                                   78\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:22:08.035690 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 79 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000796271\n",
      "trainer/QF2 Loss                                         0.000755338\n",
      "trainer/Policy Loss                                      3.13744\n",
      "trainer/Q1 Predictions Mean                             -1.21219\n",
      "trainer/Q1 Predictions Std                               0.947012\n",
      "trainer/Q1 Predictions Max                               0.445473\n",
      "trainer/Q1 Predictions Min                              -4.00425\n",
      "trainer/Q2 Predictions Mean                             -1.21542\n",
      "trainer/Q2 Predictions Std                               0.947659\n",
      "trainer/Q2 Predictions Max                               0.439016\n",
      "trainer/Q2 Predictions Min                              -3.99707\n",
      "trainer/Q Targets Mean                                  -1.21259\n",
      "trainer/Q Targets Std                                    0.94569\n",
      "trainer/Q Targets Max                                    0.435708\n",
      "trainer/Q Targets Min                                   -3.96539\n",
      "trainer/Log Pis Mean                                     1.92253\n",
      "trainer/Log Pis Std                                      1.34187\n",
      "trainer/Log Pis Max                                      4.23194\n",
      "trainer/Log Pis Min                                     -2.98636\n",
      "trainer/Policy mu Mean                                   0.018632\n",
      "trainer/Policy mu Std                                    0.295609\n",
      "trainer/Policy mu Max                                    1.84593\n",
      "trainer/Policy mu Min                                   -1.89781\n",
      "trainer/Policy log std Mean                             -2.30487\n",
      "trainer/Policy log std Std                               0.602611\n",
      "trainer/Policy log std Max                              -0.150855\n",
      "trainer/Policy log std Min                              -3.18923\n",
      "trainer/Alpha                                            0.023201\n",
      "trainer/Alpha Loss                                      -0.291532\n",
      "exploration/num steps total                           9000\n",
      "exploration/num paths total                            450\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0474926\n",
      "exploration/Rewards Std                                  0.0917056\n",
      "exploration/Rewards Max                                  0.162367\n",
      "exploration/Rewards Min                                 -0.228052\n",
      "exploration/Returns Mean                                -0.949851\n",
      "exploration/Returns Std                                  1.64756\n",
      "exploration/Returns Max                                  2.18351\n",
      "exploration/Returns Min                                 -2.34786\n",
      "exploration/Actions Mean                                 0.00446668\n",
      "exploration/Actions Std                                  0.117659\n",
      "exploration/Actions Max                                  0.461918\n",
      "exploration/Actions Min                                 -0.511791\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -0.949851\n",
      "exploration/env_infos/final/reward_dist Mean             0.209684\n",
      "exploration/env_infos/final/reward_dist Std              0.375911\n",
      "exploration/env_infos/final/reward_dist Max              0.95834\n",
      "exploration/env_infos/final/reward_dist Min              1.36101e-06\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000830026\n",
      "exploration/env_infos/initial/reward_dist Std            0.000582802\n",
      "exploration/env_infos/initial/reward_dist Max            0.00155427\n",
      "exploration/env_infos/initial/reward_dist Min            0.000134896\n",
      "exploration/env_infos/reward_dist Mean                   0.209197\n",
      "exploration/env_infos/reward_dist Std                    0.32168\n",
      "exploration/env_infos/reward_dist Max                    0.99992\n",
      "exploration/env_infos/reward_dist Min                    1.36101e-06\n",
      "exploration/env_infos/final/reward_energy Mean          -0.105401\n",
      "exploration/env_infos/final/reward_energy Std            0.0727751\n",
      "exploration/env_infos/final/reward_energy Max           -0.0282466\n",
      "exploration/env_infos/final/reward_energy Min           -0.200376\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.156144\n",
      "exploration/env_infos/initial/reward_energy Std          0.0860235\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0617517\n",
      "exploration/env_infos/initial/reward_energy Min         -0.285902\n",
      "exploration/env_infos/reward_energy Mean                -0.122898\n",
      "exploration/env_infos/reward_energy Std                  0.112353\n",
      "exploration/env_infos/reward_energy Max                 -0.00229004\n",
      "exploration/env_infos/reward_energy Min                 -0.580878\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0550092\n",
      "exploration/env_infos/final/end_effector_loc Std         0.233881\n",
      "exploration/env_infos/final/end_effector_loc Max         0.53215\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.307795\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.000190916\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00629998\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0112868\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0107924\n",
      "exploration/env_infos/end_effector_loc Mean              0.0261026\n",
      "exploration/env_infos/end_effector_loc Std               0.138634\n",
      "exploration/env_infos/end_effector_loc Max               0.53215\n",
      "exploration/env_infos/end_effector_loc Min              -0.307795\n",
      "evaluation/num steps total                           80000\n",
      "evaluation/num paths total                            4000\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0525616\n",
      "evaluation/Rewards Std                                   0.0701012\n",
      "evaluation/Rewards Max                                   0.13698\n",
      "evaluation/Rewards Min                                  -0.367176\n",
      "evaluation/Returns Mean                                 -1.05123\n",
      "evaluation/Returns Std                                   1.0651\n",
      "evaluation/Returns Max                                   1.33051\n",
      "evaluation/Returns Min                                  -3.017\n",
      "evaluation/Actions Mean                                  0.00341609\n",
      "evaluation/Actions Std                                   0.0832639\n",
      "evaluation/Actions Max                                   0.776508\n",
      "evaluation/Actions Min                                  -0.818485\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.05123\n",
      "evaluation/env_infos/final/reward_dist Mean              0.155782\n",
      "evaluation/env_infos/final/reward_dist Std               0.25991\n",
      "evaluation/env_infos/final/reward_dist Max               0.981161\n",
      "evaluation/env_infos/final/reward_dist Min               8.88201e-46\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00572943\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0114806\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0526692\n",
      "evaluation/env_infos/initial/reward_dist Min             9.41027e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.146851\n",
      "evaluation/env_infos/reward_dist Std                     0.23456\n",
      "evaluation/env_infos/reward_dist Max                     0.992819\n",
      "evaluation/env_infos/reward_dist Min                     8.88201e-46\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0610072\n",
      "evaluation/env_infos/final/reward_energy Std             0.0681232\n",
      "evaluation/env_infos/final/reward_energy Max            -0.000749923\n",
      "evaluation/env_infos/final/reward_energy Min            -0.424836\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.236879\n",
      "evaluation/env_infos/initial/reward_energy Std           0.235126\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0182627\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.06816\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0655236\n",
      "evaluation/env_infos/reward_energy Std                   0.097958\n",
      "evaluation/env_infos/reward_energy Max                  -0.000749923\n",
      "evaluation/env_infos/reward_energy Min                  -1.06816\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.00957148\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.256363\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.954862\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.603625\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00204662\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0116214\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0304915\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0409243\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0145459\n",
      "evaluation/env_infos/end_effector_loc Std                0.165964\n",
      "evaluation/env_infos/end_effector_loc Max                0.954862\n",
      "evaluation/env_infos/end_effector_loc Min               -0.603625\n",
      "time/data storing (s)                                    0.00311456\n",
      "time/evaluation sampling (s)                             1.07101\n",
      "time/exploration sampling (s)                            0.317443\n",
      "time/logging (s)                                         0.0271201\n",
      "time/saving (s)                                          0.0359923\n",
      "time/training (s)                                       60.7518\n",
      "time/epoch (s)                                          62.2065\n",
      "time/total (s)                                        4130.27\n",
      "Epoch                                                   79\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:23:02.347753 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 80 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000679713\n",
      "trainer/QF2 Loss                                         0.000766436\n",
      "trainer/Policy Loss                                      3.1449\n",
      "trainer/Q1 Predictions Mean                             -1.15067\n",
      "trainer/Q1 Predictions Std                               0.875893\n",
      "trainer/Q1 Predictions Max                               0.573801\n",
      "trainer/Q1 Predictions Min                              -3.86465\n",
      "trainer/Q2 Predictions Mean                             -1.15048\n",
      "trainer/Q2 Predictions Std                               0.875497\n",
      "trainer/Q2 Predictions Max                               0.579091\n",
      "trainer/Q2 Predictions Min                              -3.84228\n",
      "trainer/Q Targets Mean                                  -1.14899\n",
      "trainer/Q Targets Std                                    0.871182\n",
      "trainer/Q Targets Max                                    0.576972\n",
      "trainer/Q Targets Min                                   -3.82047\n",
      "trainer/Log Pis Mean                                     1.99743\n",
      "trainer/Log Pis Std                                      1.34482\n",
      "trainer/Log Pis Max                                      4.17201\n",
      "trainer/Log Pis Min                                     -2.50222\n",
      "trainer/Policy mu Mean                                   0.0143022\n",
      "trainer/Policy mu Std                                    0.265359\n",
      "trainer/Policy mu Max                                    1.79566\n",
      "trainer/Policy mu Min                                   -1.50122\n",
      "trainer/Policy log std Mean                             -2.35807\n",
      "trainer/Policy log std Std                               0.556564\n",
      "trainer/Policy log std Max                              -0.26023\n",
      "trainer/Policy log std Min                              -3.18018\n",
      "trainer/Alpha                                            0.023244\n",
      "trainer/Alpha Loss                                      -0.0096694\n",
      "exploration/num steps total                           9100\n",
      "exploration/num paths total                            455\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0730276\n",
      "exploration/Rewards Std                                  0.0787583\n",
      "exploration/Rewards Max                                  0.0986001\n",
      "exploration/Rewards Min                                 -0.222559\n",
      "exploration/Returns Mean                                -1.46055\n",
      "exploration/Returns Std                                  1.27688\n",
      "exploration/Returns Max                                  0.632581\n",
      "exploration/Returns Min                                 -3.16492\n",
      "exploration/Actions Mean                                -0.00182581\n",
      "exploration/Actions Std                                  0.133262\n",
      "exploration/Actions Max                                  0.582742\n",
      "exploration/Actions Min                                 -0.632351\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.46055\n",
      "exploration/env_infos/final/reward_dist Mean             0.0970719\n",
      "exploration/env_infos/final/reward_dist Std              0.1726\n",
      "exploration/env_infos/final/reward_dist Max              0.440554\n",
      "exploration/env_infos/final/reward_dist Min              1.0324e-11\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0179606\n",
      "exploration/env_infos/initial/reward_dist Std            0.0248005\n",
      "exploration/env_infos/initial/reward_dist Max            0.0633179\n",
      "exploration/env_infos/initial/reward_dist Min            2.02383e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.138082\n",
      "exploration/env_infos/reward_dist Std                    0.202126\n",
      "exploration/env_infos/reward_dist Max                    0.815419\n",
      "exploration/env_infos/reward_dist Min                    1.0324e-11\n",
      "exploration/env_infos/final/reward_energy Mean          -0.125562\n",
      "exploration/env_infos/final/reward_energy Std            0.0990056\n",
      "exploration/env_infos/final/reward_energy Max           -0.0231003\n",
      "exploration/env_infos/final/reward_energy Min           -0.304492\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.291661\n",
      "exploration/env_infos/initial/reward_energy Std          0.285577\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0290967\n",
      "exploration/env_infos/initial/reward_energy Min         -0.670648\n",
      "exploration/env_infos/reward_energy Mean                -0.135668\n",
      "exploration/env_infos/reward_energy Std                  0.130836\n",
      "exploration/env_infos/reward_energy Max                 -0.00738137\n",
      "exploration/env_infos/reward_energy Min                 -0.670648\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.00142423\n",
      "exploration/env_infos/final/end_effector_loc Std         0.223778\n",
      "exploration/env_infos/final/end_effector_loc Max         0.344043\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.459281\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00092868\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0144018\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0291371\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0316176\n",
      "exploration/env_infos/end_effector_loc Mean              0.00646764\n",
      "exploration/env_infos/end_effector_loc Std               0.133524\n",
      "exploration/env_infos/end_effector_loc Max               0.347011\n",
      "exploration/env_infos/end_effector_loc Min              -0.459281\n",
      "evaluation/num steps total                           81000\n",
      "evaluation/num paths total                            4050\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0664632\n",
      "evaluation/Rewards Std                                   0.0829162\n",
      "evaluation/Rewards Max                                   0.0952115\n",
      "evaluation/Rewards Min                                  -0.751546\n",
      "evaluation/Returns Mean                                 -1.32926\n",
      "evaluation/Returns Std                                   1.15436\n",
      "evaluation/Returns Max                                   0.812738\n",
      "evaluation/Returns Min                                  -4.04634\n",
      "evaluation/Actions Mean                                  0.00422942\n",
      "evaluation/Actions Std                                   0.0974431\n",
      "evaluation/Actions Max                                   0.751401\n",
      "evaluation/Actions Min                                  -0.96724\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.32926\n",
      "evaluation/env_infos/final/reward_dist Mean              0.206629\n",
      "evaluation/env_infos/final/reward_dist Std               0.282226\n",
      "evaluation/env_infos/final/reward_dist Max               0.947758\n",
      "evaluation/env_infos/final/reward_dist Min               9.08859e-53\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0115702\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0250304\n",
      "evaluation/env_infos/initial/reward_dist Max             0.112165\n",
      "evaluation/env_infos/initial/reward_dist Min             2.32712e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.149051\n",
      "evaluation/env_infos/reward_dist Std                     0.246926\n",
      "evaluation/env_infos/reward_dist Max                     0.99623\n",
      "evaluation/env_infos/reward_dist Min                     9.08859e-53\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0554786\n",
      "evaluation/env_infos/final/reward_energy Std             0.119005\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00231332\n",
      "evaluation/env_infos/final/reward_energy Min            -0.838819\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.294981\n",
      "evaluation/env_infos/initial/reward_energy Std           0.308288\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00978314\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.09223\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0732416\n",
      "evaluation/env_infos/reward_energy Std                   0.116883\n",
      "evaluation/env_infos/reward_energy Max                  -0.00064512\n",
      "evaluation/env_infos/reward_energy Min                  -1.09223\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0244347\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.281825\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.66502\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.0051289\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0141867\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0211887\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.048362\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.032526\n",
      "evaluation/env_infos/end_effector_loc Std                0.175653\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -0.66502\n",
      "time/data storing (s)                                    0.00327959\n",
      "time/evaluation sampling (s)                             1.11432\n",
      "time/exploration sampling (s)                            0.146187\n",
      "time/logging (s)                                         0.0216719\n",
      "time/saving (s)                                          0.0323428\n",
      "time/training (s)                                       51.6593\n",
      "time/epoch (s)                                          52.9771\n",
      "time/total (s)                                        4184.57\n",
      "Epoch                                                   80\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:23:56.034611 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 81 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000501914\n",
      "trainer/QF2 Loss                                         0.000812817\n",
      "trainer/Policy Loss                                      3.27369\n",
      "trainer/Q1 Predictions Mean                             -1.18551\n",
      "trainer/Q1 Predictions Std                               0.89019\n",
      "trainer/Q1 Predictions Max                               0.300216\n",
      "trainer/Q1 Predictions Min                              -3.56582\n",
      "trainer/Q2 Predictions Mean                             -1.18698\n",
      "trainer/Q2 Predictions Std                               0.889841\n",
      "trainer/Q2 Predictions Max                               0.270419\n",
      "trainer/Q2 Predictions Min                              -3.57703\n",
      "trainer/Q Targets Mean                                  -1.18013\n",
      "trainer/Q Targets Std                                    0.892682\n",
      "trainer/Q Targets Max                                    0.274576\n",
      "trainer/Q Targets Min                                   -3.5356\n",
      "trainer/Log Pis Mean                                     2.0906\n",
      "trainer/Log Pis Std                                      1.40047\n",
      "trainer/Log Pis Max                                      4.41417\n",
      "trainer/Log Pis Min                                     -3.86845\n",
      "trainer/Policy mu Mean                                   0.0195433\n",
      "trainer/Policy mu Std                                    0.262279\n",
      "trainer/Policy mu Max                                    1.68268\n",
      "trainer/Policy mu Min                                   -1.74498\n",
      "trainer/Policy log std Mean                             -2.40118\n",
      "trainer/Policy log std Std                               0.587813\n",
      "trainer/Policy log std Max                              -0.0769494\n",
      "trainer/Policy log std Min                              -3.2624\n",
      "trainer/Alpha                                            0.023733\n",
      "trainer/Alpha Loss                                       0.338905\n",
      "exploration/num steps total                           9200\n",
      "exploration/num paths total                            460\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0937772\n",
      "exploration/Rewards Std                                  0.0677251\n",
      "exploration/Rewards Max                                  0.121206\n",
      "exploration/Rewards Min                                 -0.332113\n",
      "exploration/Returns Mean                                -1.87554\n",
      "exploration/Returns Std                                  0.765732\n",
      "exploration/Returns Max                                 -0.679487\n",
      "exploration/Returns Min                                 -2.98573\n",
      "exploration/Actions Mean                                 0.000957297\n",
      "exploration/Actions Std                                  0.103705\n",
      "exploration/Actions Max                                  0.365997\n",
      "exploration/Actions Min                                 -0.539242\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.87554\n",
      "exploration/env_infos/final/reward_dist Mean             0.165378\n",
      "exploration/env_infos/final/reward_dist Std              0.323246\n",
      "exploration/env_infos/final/reward_dist Max              0.811792\n",
      "exploration/env_infos/final/reward_dist Min              2.70561e-11\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0104887\n",
      "exploration/env_infos/initial/reward_dist Std            0.0130221\n",
      "exploration/env_infos/initial/reward_dist Max            0.032101\n",
      "exploration/env_infos/initial/reward_dist Min            9.66414e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.136261\n",
      "exploration/env_infos/reward_dist Std                    0.25204\n",
      "exploration/env_infos/reward_dist Max                    0.963598\n",
      "exploration/env_infos/reward_dist Min                    2.70561e-11\n",
      "exploration/env_infos/final/reward_energy Mean          -0.130591\n",
      "exploration/env_infos/final/reward_energy Std            0.0764548\n",
      "exploration/env_infos/final/reward_energy Max           -0.0331466\n",
      "exploration/env_infos/final/reward_energy Min           -0.266098\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.183905\n",
      "exploration/env_infos/initial/reward_energy Std          0.181686\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0421181\n",
      "exploration/env_infos/initial/reward_energy Min         -0.542123\n",
      "exploration/env_infos/reward_energy Mean                -0.112348\n",
      "exploration/env_infos/reward_energy Std                  0.0942817\n",
      "exploration/env_infos/reward_energy Max                 -0.00555508\n",
      "exploration/env_infos/reward_energy Min                 -0.542123\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0360206\n",
      "exploration/env_infos/final/end_effector_loc Std         0.237847\n",
      "exploration/env_infos/final/end_effector_loc Max         0.314664\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.347579\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00416952\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0081335\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00487872\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0269621\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0326138\n",
      "exploration/env_infos/end_effector_loc Std               0.128067\n",
      "exploration/env_infos/end_effector_loc Max               0.314664\n",
      "exploration/env_infos/end_effector_loc Min              -0.347579\n",
      "evaluation/num steps total                           82000\n",
      "evaluation/num paths total                            4100\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0548885\n",
      "evaluation/Rewards Std                                   0.100443\n",
      "evaluation/Rewards Max                                   0.15758\n",
      "evaluation/Rewards Min                                  -0.760129\n",
      "evaluation/Returns Mean                                 -1.09777\n",
      "evaluation/Returns Std                                   1.4825\n",
      "evaluation/Returns Max                                   1.63514\n",
      "evaluation/Returns Min                                  -5.67063\n",
      "evaluation/Actions Mean                                  0.0129991\n",
      "evaluation/Actions Std                                   0.114505\n",
      "evaluation/Actions Max                                   0.868372\n",
      "evaluation/Actions Min                                  -0.65571\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.09777\n",
      "evaluation/env_infos/final/reward_dist Mean              0.161199\n",
      "evaluation/env_infos/final/reward_dist Std               0.236473\n",
      "evaluation/env_infos/final/reward_dist Max               0.940226\n",
      "evaluation/env_infos/final/reward_dist Min               1.95073e-105\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0107524\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0161723\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0715394\n",
      "evaluation/env_infos/initial/reward_dist Min             1.31279e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.17005\n",
      "evaluation/env_infos/reward_dist Std                     0.255779\n",
      "evaluation/env_infos/reward_dist Max                     0.995173\n",
      "evaluation/env_infos/reward_dist Min                     1.95073e-105\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0980509\n",
      "evaluation/env_infos/final/reward_energy Std             0.209188\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00194174\n",
      "evaluation/env_infos/final/reward_energy Min            -0.975538\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.23985\n",
      "evaluation/env_infos/initial/reward_energy Std           0.248776\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00581004\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.868034\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0836414\n",
      "evaluation/env_infos/reward_energy Std                   0.139875\n",
      "evaluation/env_infos/reward_energy Max                  -0.000939147\n",
      "evaluation/env_infos/reward_energy Min                  -0.975538\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0351502\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.302549\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.450905\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000970512\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0121791\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0371899\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0327855\n",
      "evaluation/env_infos/end_effector_loc Mean               0.00447512\n",
      "evaluation/env_infos/end_effector_loc Std                0.188927\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -0.518941\n",
      "time/data storing (s)                                    0.00332646\n",
      "time/evaluation sampling (s)                             1.19958\n",
      "time/exploration sampling (s)                            0.130749\n",
      "time/logging (s)                                         0.0216194\n",
      "time/saving (s)                                          0.0324738\n",
      "time/training (s)                                       51.1057\n",
      "time/epoch (s)                                          52.4934\n",
      "time/total (s)                                        4238.26\n",
      "Epoch                                                   81\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:24:51.474211 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 82 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000624508\n",
      "trainer/QF2 Loss                                         0.000726083\n",
      "trainer/Policy Loss                                      3.16976\n",
      "trainer/Q1 Predictions Mean                             -1.20486\n",
      "trainer/Q1 Predictions Std                               0.861983\n",
      "trainer/Q1 Predictions Max                               0.486261\n",
      "trainer/Q1 Predictions Min                              -3.48362\n",
      "trainer/Q2 Predictions Mean                             -1.20125\n",
      "trainer/Q2 Predictions Std                               0.863261\n",
      "trainer/Q2 Predictions Max                               0.490338\n",
      "trainer/Q2 Predictions Min                              -3.47104\n",
      "trainer/Q Targets Mean                                  -1.20341\n",
      "trainer/Q Targets Std                                    0.854031\n",
      "trainer/Q Targets Max                                    0.498528\n",
      "trainer/Q Targets Min                                   -3.48417\n",
      "trainer/Log Pis Mean                                     1.96605\n",
      "trainer/Log Pis Std                                      1.49546\n",
      "trainer/Log Pis Max                                      4.48759\n",
      "trainer/Log Pis Min                                     -4.55692\n",
      "trainer/Policy mu Mean                                   0.0412045\n",
      "trainer/Policy mu Std                                    0.238788\n",
      "trainer/Policy mu Max                                    2.06931\n",
      "trainer/Policy mu Min                                   -1.03584\n",
      "trainer/Policy log std Mean                             -2.33697\n",
      "trainer/Policy log std Std                               0.601505\n",
      "trainer/Policy log std Max                               0.133116\n",
      "trainer/Policy log std Min                              -3.18393\n",
      "trainer/Alpha                                            0.0246573\n",
      "trainer/Alpha Loss                                      -0.125703\n",
      "exploration/num steps total                           9300\n",
      "exploration/num paths total                            465\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0991734\n",
      "exploration/Rewards Std                                  0.0640419\n",
      "exploration/Rewards Max                                  0.0164013\n",
      "exploration/Rewards Min                                 -0.283169\n",
      "exploration/Returns Mean                                -1.98347\n",
      "exploration/Returns Std                                  0.675818\n",
      "exploration/Returns Max                                 -1.04208\n",
      "exploration/Returns Min                                 -2.91907\n",
      "exploration/Actions Mean                                -0.0094017\n",
      "exploration/Actions Std                                  0.160359\n",
      "exploration/Actions Max                                  0.546498\n",
      "exploration/Actions Min                                 -0.507404\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.98347\n",
      "exploration/env_infos/final/reward_dist Mean             0.0826754\n",
      "exploration/env_infos/final/reward_dist Std              0.164924\n",
      "exploration/env_infos/final/reward_dist Max              0.412522\n",
      "exploration/env_infos/final/reward_dist Min              1.13822e-11\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000378723\n",
      "exploration/env_infos/initial/reward_dist Std            0.000609849\n",
      "exploration/env_infos/initial/reward_dist Max            0.0015895\n",
      "exploration/env_infos/initial/reward_dist Min            1.07075e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.093575\n",
      "exploration/env_infos/reward_dist Std                    0.23091\n",
      "exploration/env_infos/reward_dist Max                    0.986114\n",
      "exploration/env_infos/reward_dist Min                    1.13822e-11\n",
      "exploration/env_infos/final/reward_energy Mean          -0.18314\n",
      "exploration/env_infos/final/reward_energy Std            0.0727217\n",
      "exploration/env_infos/final/reward_energy Max           -0.0807634\n",
      "exploration/env_infos/final/reward_energy Min           -0.260826\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.17704\n",
      "exploration/env_infos/initial/reward_energy Std          0.0770149\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0936821\n",
      "exploration/env_infos/initial/reward_energy Min         -0.287202\n",
      "exploration/env_infos/reward_energy Mean                -0.189661\n",
      "exploration/env_infos/reward_energy Std                  0.125042\n",
      "exploration/env_infos/reward_energy Max                 -0.0120968\n",
      "exploration/env_infos/reward_energy Min                 -0.574931\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0204503\n",
      "exploration/env_infos/final/end_effector_loc Std         0.193362\n",
      "exploration/env_infos/final/end_effector_loc Max         0.243918\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.464843\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00415736\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00541381\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0132426\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00476134\n",
      "exploration/env_infos/end_effector_loc Mean              0.0431561\n",
      "exploration/env_infos/end_effector_loc Std               0.150079\n",
      "exploration/env_infos/end_effector_loc Max               0.367726\n",
      "exploration/env_infos/end_effector_loc Min              -0.465\n",
      "evaluation/num steps total                           83000\n",
      "evaluation/num paths total                            4150\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0620006\n",
      "evaluation/Rewards Std                                   0.108704\n",
      "evaluation/Rewards Max                                   0.123184\n",
      "evaluation/Rewards Min                                  -0.807144\n",
      "evaluation/Returns Mean                                 -1.24001\n",
      "evaluation/Returns Std                                   1.5477\n",
      "evaluation/Returns Max                                   1.22329\n",
      "evaluation/Returns Min                                  -8.129\n",
      "evaluation/Actions Mean                                  0.0204754\n",
      "evaluation/Actions Std                                   0.143312\n",
      "evaluation/Actions Max                                   0.98739\n",
      "evaluation/Actions Min                                  -0.988171\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.24001\n",
      "evaluation/env_infos/final/reward_dist Mean              0.152226\n",
      "evaluation/env_infos/final/reward_dist Std               0.213322\n",
      "evaluation/env_infos/final/reward_dist Max               0.748834\n",
      "evaluation/env_infos/final/reward_dist Min               8.07708e-120\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00715763\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0138692\n",
      "evaluation/env_infos/initial/reward_dist Max             0.076537\n",
      "evaluation/env_infos/initial/reward_dist Min             1.97017e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.120622\n",
      "evaluation/env_infos/reward_dist Std                     0.200044\n",
      "evaluation/env_infos/reward_dist Max                     0.982176\n",
      "evaluation/env_infos/reward_dist Min                     2.9298e-123\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.116247\n",
      "evaluation/env_infos/final/reward_energy Std             0.232813\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00414608\n",
      "evaluation/env_infos/final/reward_energy Min            -1.33831\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.306321\n",
      "evaluation/env_infos/initial/reward_energy Std           0.32445\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.000217716\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.25535\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0984051\n",
      "evaluation/env_infos/reward_energy Std                   0.179531\n",
      "evaluation/env_infos/reward_energy Max                  -0.000217716\n",
      "evaluation/env_infos/reward_energy Min                  -1.33831\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.078785\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.30718\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.549252\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00335411\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0154151\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0328633\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0494086\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0129987\n",
      "evaluation/env_infos/end_effector_loc Std                0.205026\n",
      "evaluation/env_infos/end_effector_loc Max                1\n",
      "evaluation/env_infos/end_effector_loc Min               -0.600959\n",
      "time/data storing (s)                                    0.00299029\n",
      "time/evaluation sampling (s)                             1.07867\n",
      "time/exploration sampling (s)                            0.13567\n",
      "time/logging (s)                                         0.0226805\n",
      "time/saving (s)                                          0.0305459\n",
      "time/training (s)                                       53.1779\n",
      "time/epoch (s)                                          54.4485\n",
      "time/total (s)                                        4293.7\n",
      "Epoch                                                   82\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:25:43.773815 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 83 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000802997\n",
      "trainer/QF2 Loss                                         0.000728589\n",
      "trainer/Policy Loss                                      3.12219\n",
      "trainer/Q1 Predictions Mean                             -1.07807\n",
      "trainer/Q1 Predictions Std                               0.798062\n",
      "trainer/Q1 Predictions Max                               0.625394\n",
      "trainer/Q1 Predictions Min                              -3.13844\n",
      "trainer/Q2 Predictions Mean                             -1.07892\n",
      "trainer/Q2 Predictions Std                               0.798842\n",
      "trainer/Q2 Predictions Max                               0.628263\n",
      "trainer/Q2 Predictions Min                              -3.13457\n",
      "trainer/Q Targets Mean                                  -1.08678\n",
      "trainer/Q Targets Std                                    0.799065\n",
      "trainer/Q Targets Max                                    0.637205\n",
      "trainer/Q Targets Min                                   -3.14338\n",
      "trainer/Log Pis Mean                                     2.04393\n",
      "trainer/Log Pis Std                                      1.47663\n",
      "trainer/Log Pis Max                                      4.45773\n",
      "trainer/Log Pis Min                                     -4.44094\n",
      "trainer/Policy mu Mean                                   0.0555146\n",
      "trainer/Policy mu Std                                    0.293363\n",
      "trainer/Policy mu Max                                    2.06583\n",
      "trainer/Policy mu Min                                   -1.57229\n",
      "trainer/Policy log std Mean                             -2.33591\n",
      "trainer/Policy log std Std                               0.633862\n",
      "trainer/Policy log std Max                               0.102997\n",
      "trainer/Policy log std Min                              -3.20876\n",
      "trainer/Alpha                                            0.0248268\n",
      "trainer/Alpha Loss                                       0.162348\n",
      "exploration/num steps total                           9400\n",
      "exploration/num paths total                            470\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.100622\n",
      "exploration/Rewards Std                                  0.159106\n",
      "exploration/Rewards Max                                  0.105981\n",
      "exploration/Rewards Min                                 -0.701694\n",
      "exploration/Returns Mean                                -2.01245\n",
      "exploration/Returns Std                                  2.03215\n",
      "exploration/Returns Max                                  0.832667\n",
      "exploration/Returns Min                                 -5.46242\n",
      "exploration/Actions Mean                                -0.010638\n",
      "exploration/Actions Std                                  0.20874\n",
      "exploration/Actions Max                                  0.951553\n",
      "exploration/Actions Min                                 -0.971894\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.01245\n",
      "exploration/env_infos/final/reward_dist Mean             0.32504\n",
      "exploration/env_infos/final/reward_dist Std              0.308045\n",
      "exploration/env_infos/final/reward_dist Max              0.800821\n",
      "exploration/env_infos/final/reward_dist Min              1.38855e-63\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00116555\n",
      "exploration/env_infos/initial/reward_dist Std            0.00118257\n",
      "exploration/env_infos/initial/reward_dist Max            0.0026958\n",
      "exploration/env_infos/initial/reward_dist Min            2.9348e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.236133\n",
      "exploration/env_infos/reward_dist Std                    0.313118\n",
      "exploration/env_infos/reward_dist Max                    0.97419\n",
      "exploration/env_infos/reward_dist Min                    1.38855e-63\n",
      "exploration/env_infos/final/reward_energy Mean          -0.104883\n",
      "exploration/env_infos/final/reward_energy Std            0.0513993\n",
      "exploration/env_infos/final/reward_energy Max           -0.0546693\n",
      "exploration/env_infos/final/reward_energy Min           -0.185233\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.287152\n",
      "exploration/env_infos/initial/reward_energy Std          0.218961\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0756704\n",
      "exploration/env_infos/initial/reward_energy Min         -0.564252\n",
      "exploration/env_infos/reward_energy Mean                -0.197064\n",
      "exploration/env_infos/reward_energy Std                  0.220311\n",
      "exploration/env_infos/reward_energy Max                 -0.0115961\n",
      "exploration/env_infos/reward_energy Min                 -1.25978\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.125205\n",
      "exploration/env_infos/final/end_effector_loc Std         0.392401\n",
      "exploration/env_infos/final/end_effector_loc Max         0.304062\n",
      "exploration/env_infos/final/end_effector_loc Min        -1\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000831577\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.01274\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0266949\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0260856\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0435148\n",
      "exploration/env_infos/end_effector_loc Std               0.249425\n",
      "exploration/env_infos/end_effector_loc Max               0.304062\n",
      "exploration/env_infos/end_effector_loc Min              -1\n",
      "evaluation/num steps total                           84000\n",
      "evaluation/num paths total                            4200\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0515327\n",
      "evaluation/Rewards Std                                   0.0780406\n",
      "evaluation/Rewards Max                                   0.136748\n",
      "evaluation/Rewards Min                                  -0.307914\n",
      "evaluation/Returns Mean                                 -1.03065\n",
      "evaluation/Returns Std                                   1.21853\n",
      "evaluation/Returns Max                                   1.83176\n",
      "evaluation/Returns Min                                  -3.60723\n",
      "evaluation/Actions Mean                                  0.00411539\n",
      "evaluation/Actions Std                                   0.0866956\n",
      "evaluation/Actions Max                                   0.731663\n",
      "evaluation/Actions Min                                  -0.843167\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.03065\n",
      "evaluation/env_infos/final/reward_dist Mean              0.194708\n",
      "evaluation/env_infos/final/reward_dist Std               0.278293\n",
      "evaluation/env_infos/final/reward_dist Max               0.977219\n",
      "evaluation/env_infos/final/reward_dist Min               4.03447e-09\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0072118\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0186561\n",
      "evaluation/env_infos/initial/reward_dist Max             0.117427\n",
      "evaluation/env_infos/initial/reward_dist Min             1.05836e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.199292\n",
      "evaluation/env_infos/reward_dist Std                     0.265534\n",
      "evaluation/env_infos/reward_dist Max                     0.996633\n",
      "evaluation/env_infos/reward_dist Min                     1.5478e-26\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0546571\n",
      "evaluation/env_infos/final/reward_energy Std             0.0667798\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00635457\n",
      "evaluation/env_infos/final/reward_energy Min            -0.389303\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.230729\n",
      "evaluation/env_infos/initial/reward_energy Std           0.223137\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00611514\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.955398\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0672942\n",
      "evaluation/env_infos/reward_energy Std                   0.102653\n",
      "evaluation/env_infos/reward_energy Max                  -0.000464299\n",
      "evaluation/env_infos/reward_energy Min                  -0.955398\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0383224\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.225179\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.479811\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.477778\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000628841\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0113308\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0285488\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0421583\n",
      "evaluation/env_infos/end_effector_loc Mean               0.012545\n",
      "evaluation/env_infos/end_effector_loc Std                0.151521\n",
      "evaluation/env_infos/end_effector_loc Max                0.552201\n",
      "evaluation/env_infos/end_effector_loc Min               -0.477778\n",
      "time/data storing (s)                                    0.00329423\n",
      "time/evaluation sampling (s)                             1.31725\n",
      "time/exploration sampling (s)                            0.131958\n",
      "time/logging (s)                                         0.0241131\n",
      "time/saving (s)                                          0.0347733\n",
      "time/training (s)                                       49.6288\n",
      "time/epoch (s)                                          51.1402\n",
      "time/total (s)                                        4346\n",
      "Epoch                                                   83\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:26:38.040238 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 84 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000751667\r\n",
      "trainer/QF2 Loss                                         0.000705779\r\n",
      "trainer/Policy Loss                                      3.10635\r\n",
      "trainer/Q1 Predictions Mean                             -1.17131\r\n",
      "trainer/Q1 Predictions Std                               0.903222\r\n",
      "trainer/Q1 Predictions Max                               0.707973\r\n",
      "trainer/Q1 Predictions Min                              -3.62542\r\n",
      "trainer/Q2 Predictions Mean                             -1.16984\r\n",
      "trainer/Q2 Predictions Std                               0.902014\r\n",
      "trainer/Q2 Predictions Max                               0.693504\r\n",
      "trainer/Q2 Predictions Min                              -3.61057\r\n",
      "trainer/Q Targets Mean                                  -1.17083\r\n",
      "trainer/Q Targets Std                                    0.90419\r\n",
      "trainer/Q Targets Max                                    0.716348\r\n",
      "trainer/Q Targets Min                                   -3.58481\r\n",
      "trainer/Log Pis Mean                                     1.93474\r\n",
      "trainer/Log Pis Std                                      1.43514\r\n",
      "trainer/Log Pis Max                                      4.2128\r\n",
      "trainer/Log Pis Min                                     -2.99211\r\n",
      "trainer/Policy mu Mean                                   0.0519202\r\n",
      "trainer/Policy mu Std                                    0.221896\r\n",
      "trainer/Policy mu Max                                    1.50276\r\n",
      "trainer/Policy mu Min                                   -0.561848\r\n",
      "trainer/Policy log std Mean                             -2.34007\r\n",
      "trainer/Policy log std Std                               0.603693\r\n",
      "trainer/Policy log std Max                              -0.269094\r\n",
      "trainer/Policy log std Min                              -3.16929\r\n",
      "trainer/Alpha                                            0.023956\r\n",
      "trainer/Alpha Loss                                      -0.243462\r\n",
      "exploration/num steps total                           9500\r\n",
      "exploration/num paths total                            475\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0875493\r\n",
      "exploration/Rewards Std                                  0.0938802\r\n",
      "exploration/Rewards Max                                  0.0602913\r\n",
      "exploration/Rewards Min                                 -0.403812\r\n",
      "exploration/Returns Mean                                -1.75099\r\n",
      "exploration/Returns Std                                  1.47369\r\n",
      "exploration/Returns Max                                 -0.114906\r\n",
      "exploration/Returns Min                                 -4.03441\r\n",
      "exploration/Actions Mean                                 0.0113549\r\n",
      "exploration/Actions Std                                  0.14819\r\n",
      "exploration/Actions Max                                  0.474245\r\n",
      "exploration/Actions Min                                 -0.531655\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.75099\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.0894367\r\n",
      "exploration/env_infos/final/reward_dist Std              0.155276\r\n",
      "exploration/env_infos/final/reward_dist Max              0.397608\r\n",
      "exploration/env_infos/final/reward_dist Min              1.1131e-82\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00725961\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00615325\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0157032\r\n",
      "exploration/env_infos/initial/reward_dist Min            1.63028e-05\r\n",
      "exploration/env_infos/reward_dist Mean                   0.165233\r\n",
      "exploration/env_infos/reward_dist Std                    0.252508\r\n",
      "exploration/env_infos/reward_dist Max                    0.963572\r\n",
      "exploration/env_infos/reward_dist Min                    1.1131e-82\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.183541\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0763726\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0826769\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.299847\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.295377\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.169579\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0554643\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.480806\r\n",
      "exploration/env_infos/reward_energy Mean                -0.171598\r\n",
      "exploration/env_infos/reward_energy Std                  0.121379\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0139685\r\n",
      "exploration/env_infos/reward_energy Min                 -0.590943\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.102304\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.448656\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.985917\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.439669\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00212934\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0118521\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0237122\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0197018\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.0377609\r\n",
      "exploration/env_infos/end_effector_loc Std               0.247678\r\n",
      "exploration/env_infos/end_effector_loc Max               0.985917\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.439669\r\n",
      "evaluation/num steps total                           85000\r\n",
      "evaluation/num paths total                            4250\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0707834\r\n",
      "evaluation/Rewards Std                                   0.0794524\r\n",
      "evaluation/Rewards Max                                   0.110934\r\n",
      "evaluation/Rewards Min                                  -0.475107\r\n",
      "evaluation/Returns Mean                                 -1.41567\r\n",
      "evaluation/Returns Std                                   1.25408\r\n",
      "evaluation/Returns Max                                   0.942785\r\n",
      "evaluation/Returns Min                                  -6.3838\r\n",
      "evaluation/Actions Mean                                  0.0109766\r\n",
      "evaluation/Actions Std                                   0.0944548\r\n",
      "evaluation/Actions Max                                   0.883904\r\n",
      "evaluation/Actions Min                                  -0.798296\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.41567\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0441786\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.0958876\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.390884\r\n",
      "evaluation/env_infos/final/reward_dist Min               7.69454e-35\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0074676\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0150878\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0869212\r\n",
      "evaluation/env_infos/initial/reward_dist Min             2.20235e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.111048\r\n",
      "evaluation/env_infos/reward_dist Std                     0.212007\r\n",
      "evaluation/env_infos/reward_dist Max                     0.993214\r\n",
      "evaluation/env_infos/reward_dist Min                     3.1203e-100\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.066275\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0843609\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00353066\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.452562\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.166574\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.196738\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00902033\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.901841\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0718194\r\n",
      "evaluation/env_infos/reward_energy Std                   0.113694\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000757595\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.984006\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.116495\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.257772\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.71681\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.5124\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       9.12571e-06\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00911406\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0386118\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0321199\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0484285\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.17143\r\n",
      "evaluation/env_infos/end_effector_loc Max                1\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.5124\r\n",
      "time/data storing (s)                                    0.00305284\r\n",
      "time/evaluation sampling (s)                             1.04895\r\n",
      "time/exploration sampling (s)                            0.146891\r\n",
      "time/logging (s)                                         0.0212341\r\n",
      "time/saving (s)                                          0.0297017\r\n",
      "time/training (s)                                       51.8767\r\n",
      "time/epoch (s)                                          53.1265\r\n",
      "time/total (s)                                        4400.26\r\n",
      "Epoch                                                   84\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:27:37.719934 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 85 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000855532\n",
      "trainer/QF2 Loss                                         0.000963405\n",
      "trainer/Policy Loss                                      3.16513\n",
      "trainer/Q1 Predictions Mean                             -1.16825\n",
      "trainer/Q1 Predictions Std                               1.00838\n",
      "trainer/Q1 Predictions Max                               0.924561\n",
      "trainer/Q1 Predictions Min                              -4.13899\n",
      "trainer/Q2 Predictions Mean                             -1.1718\n",
      "trainer/Q2 Predictions Std                               1.00159\n",
      "trainer/Q2 Predictions Max                               0.877535\n",
      "trainer/Q2 Predictions Min                              -4.10344\n",
      "trainer/Q Targets Mean                                  -1.17555\n",
      "trainer/Q Targets Std                                    1.00967\n",
      "trainer/Q Targets Max                                    0.891863\n",
      "trainer/Q Targets Min                                   -4.11423\n",
      "trainer/Log Pis Mean                                     2.00454\n",
      "trainer/Log Pis Std                                      1.30367\n",
      "trainer/Log Pis Max                                      6.24955\n",
      "trainer/Log Pis Min                                     -3.44088\n",
      "trainer/Policy mu Mean                                   0.0513605\n",
      "trainer/Policy mu Std                                    0.331835\n",
      "trainer/Policy mu Max                                    2.26121\n",
      "trainer/Policy mu Min                                   -1.19138\n",
      "trainer/Policy log std Mean                             -2.2443\n",
      "trainer/Policy log std Std                               0.6423\n",
      "trainer/Policy log std Max                               0.0606581\n",
      "trainer/Policy log std Min                              -3.18303\n",
      "trainer/Alpha                                            0.0237196\n",
      "trainer/Alpha Loss                                       0.0169725\n",
      "exploration/num steps total                           9600\n",
      "exploration/num paths total                            480\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0940113\n",
      "exploration/Rewards Std                                  0.0877212\n",
      "exploration/Rewards Max                                  0.0834497\n",
      "exploration/Rewards Min                                 -0.327191\n",
      "exploration/Returns Mean                                -1.88023\n",
      "exploration/Returns Std                                  0.997942\n",
      "exploration/Returns Max                                 -0.899154\n",
      "exploration/Returns Min                                 -3.67311\n",
      "exploration/Actions Mean                                -0.00923733\n",
      "exploration/Actions Std                                  0.121246\n",
      "exploration/Actions Max                                  0.454258\n",
      "exploration/Actions Min                                 -0.546875\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.88023\n",
      "exploration/env_infos/final/reward_dist Mean             0.0478526\n",
      "exploration/env_infos/final/reward_dist Std              0.0953706\n",
      "exploration/env_infos/final/reward_dist Max              0.238594\n",
      "exploration/env_infos/final/reward_dist Min              3.66281e-14\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00858526\n",
      "exploration/env_infos/initial/reward_dist Std            0.0138561\n",
      "exploration/env_infos/initial/reward_dist Max            0.036058\n",
      "exploration/env_infos/initial/reward_dist Min            1.24269e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.177428\n",
      "exploration/env_infos/reward_dist Std                    0.265888\n",
      "exploration/env_infos/reward_dist Max                    0.978666\n",
      "exploration/env_infos/reward_dist Min                    3.66281e-14\n",
      "exploration/env_infos/final/reward_energy Mean          -0.244279\n",
      "exploration/env_infos/final/reward_energy Std            0.175954\n",
      "exploration/env_infos/final/reward_energy Max           -0.0237224\n",
      "exploration/env_infos/final/reward_energy Min           -0.479927\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.241581\n",
      "exploration/env_infos/initial/reward_energy Std          0.134951\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0965781\n",
      "exploration/env_infos/initial/reward_energy Min         -0.46201\n",
      "exploration/env_infos/reward_energy Mean                -0.135361\n",
      "exploration/env_infos/reward_energy Std                  0.106062\n",
      "exploration/env_infos/reward_energy Max                 -0.0184605\n",
      "exploration/env_infos/reward_energy Min                 -0.566015\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.133946\n",
      "exploration/env_infos/final/end_effector_loc Std         0.274934\n",
      "exploration/env_infos/final/end_effector_loc Max         0.295825\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.586996\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000881553\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00974368\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0227129\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0137665\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0456989\n",
      "exploration/env_infos/end_effector_loc Std               0.194054\n",
      "exploration/env_infos/end_effector_loc Max               0.319806\n",
      "exploration/env_infos/end_effector_loc Min              -0.586996\n",
      "evaluation/num steps total                           86000\n",
      "evaluation/num paths total                            4300\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0568031\n",
      "evaluation/Rewards Std                                   0.0950642\n",
      "evaluation/Rewards Max                                   0.135685\n",
      "evaluation/Rewards Min                                  -0.500726\n",
      "evaluation/Returns Mean                                 -1.13606\n",
      "evaluation/Returns Std                                   1.47621\n",
      "evaluation/Returns Max                                   1.36832\n",
      "evaluation/Returns Min                                  -6.97271\n",
      "evaluation/Actions Mean                                  0.00514501\n",
      "evaluation/Actions Std                                   0.100675\n",
      "evaluation/Actions Max                                   0.777781\n",
      "evaluation/Actions Min                                  -0.627673\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.13606\n",
      "evaluation/env_infos/final/reward_dist Mean              0.103041\n",
      "evaluation/env_infos/final/reward_dist Std               0.210808\n",
      "evaluation/env_infos/final/reward_dist Max               0.967063\n",
      "evaluation/env_infos/final/reward_dist Min               2.55266e-41\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0090488\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0126075\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0471988\n",
      "evaluation/env_infos/initial/reward_dist Min             2.64548e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.181395\n",
      "evaluation/env_infos/reward_dist Std                     0.256165\n",
      "evaluation/env_infos/reward_dist Max                     0.995307\n",
      "evaluation/env_infos/reward_dist Min                     2.55266e-41\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0705127\n",
      "evaluation/env_infos/final/reward_energy Std             0.0663465\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00696136\n",
      "evaluation/env_infos/final/reward_energy Min            -0.443711\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.295688\n",
      "evaluation/env_infos/initial/reward_energy Std           0.206119\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0225626\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.710746\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0864135\n",
      "evaluation/env_infos/reward_energy Std                   0.113387\n",
      "evaluation/env_infos/reward_energy Max                  -0.00316886\n",
      "evaluation/env_infos/reward_energy Min                  -0.844356\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0116608\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.300108\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.816215\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.69929\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000785667\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0127192\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0289242\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0305362\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00212452\n",
      "evaluation/env_infos/end_effector_loc Std                0.198984\n",
      "evaluation/env_infos/end_effector_loc Max                0.816215\n",
      "evaluation/env_infos/end_effector_loc Min               -0.69929\n",
      "time/data storing (s)                                    0.0032596\n",
      "time/evaluation sampling (s)                             1.72145\n",
      "time/exploration sampling (s)                            0.163914\n",
      "time/logging (s)                                         0.0242301\n",
      "time/saving (s)                                          0.0330817\n",
      "time/training (s)                                       56.2944\n",
      "time/epoch (s)                                          58.2403\n",
      "time/total (s)                                        4459.94\n",
      "Epoch                                                   85\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:28:40.817179 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 86 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00081931\n",
      "trainer/QF2 Loss                                         0.000836693\n",
      "trainer/Policy Loss                                      3.03035\n",
      "trainer/Q1 Predictions Mean                             -1.125\n",
      "trainer/Q1 Predictions Std                               0.936117\n",
      "trainer/Q1 Predictions Max                               0.962572\n",
      "trainer/Q1 Predictions Min                              -3.92476\n",
      "trainer/Q2 Predictions Mean                             -1.12644\n",
      "trainer/Q2 Predictions Std                               0.939584\n",
      "trainer/Q2 Predictions Max                               0.991322\n",
      "trainer/Q2 Predictions Min                              -3.92007\n",
      "trainer/Q Targets Mean                                  -1.12588\n",
      "trainer/Q Targets Std                                    0.937968\n",
      "trainer/Q Targets Max                                    1.01837\n",
      "trainer/Q Targets Min                                   -3.9435\n",
      "trainer/Log Pis Mean                                     1.9083\n",
      "trainer/Log Pis Std                                      1.50904\n",
      "trainer/Log Pis Max                                      4.32252\n",
      "trainer/Log Pis Min                                     -4.30051\n",
      "trainer/Policy mu Mean                                   0.0687053\n",
      "trainer/Policy mu Std                                    0.350528\n",
      "trainer/Policy mu Max                                    2.23132\n",
      "trainer/Policy mu Min                                   -1.80152\n",
      "trainer/Policy log std Mean                             -2.25008\n",
      "trainer/Policy log std Std                               0.668669\n",
      "trainer/Policy log std Max                               0.00763177\n",
      "trainer/Policy log std Min                              -3.25716\n",
      "trainer/Alpha                                            0.0246952\n",
      "trainer/Alpha Loss                                      -0.339355\n",
      "exploration/num steps total                           9700\n",
      "exploration/num paths total                            485\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.156608\n",
      "exploration/Rewards Std                                  0.110325\n",
      "exploration/Rewards Max                                  0.0296039\n",
      "exploration/Rewards Min                                 -0.477681\n",
      "exploration/Returns Mean                                -3.13217\n",
      "exploration/Returns Std                                  1.36198\n",
      "exploration/Returns Max                                 -2.04634\n",
      "exploration/Returns Min                                 -5.73329\n",
      "exploration/Actions Mean                                 0.0104222\n",
      "exploration/Actions Std                                  0.203443\n",
      "exploration/Actions Max                                  0.698733\n",
      "exploration/Actions Min                                 -0.811316\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -3.13217\n",
      "exploration/env_infos/final/reward_dist Mean             0.107084\n",
      "exploration/env_infos/final/reward_dist Std              0.194842\n",
      "exploration/env_infos/final/reward_dist Max              0.495542\n",
      "exploration/env_infos/final/reward_dist Min              4.65517e-38\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00777185\n",
      "exploration/env_infos/initial/reward_dist Std            0.0118895\n",
      "exploration/env_infos/initial/reward_dist Max            0.0312708\n",
      "exploration/env_infos/initial/reward_dist Min            4.91535e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.103352\n",
      "exploration/env_infos/reward_dist Std                    0.236001\n",
      "exploration/env_infos/reward_dist Max                    0.948472\n",
      "exploration/env_infos/reward_dist Min                    4.65517e-38\n",
      "exploration/env_infos/final/reward_energy Mean          -0.260993\n",
      "exploration/env_infos/final/reward_energy Std            0.166338\n",
      "exploration/env_infos/final/reward_energy Max           -0.0816403\n",
      "exploration/env_infos/final/reward_energy Min           -0.553158\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.319056\n",
      "exploration/env_infos/initial/reward_energy Std          0.152971\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0723563\n",
      "exploration/env_infos/initial/reward_energy Min         -0.50494\n",
      "exploration/env_infos/reward_energy Mean                -0.223699\n",
      "exploration/env_infos/reward_energy Std                  0.181533\n",
      "exploration/env_infos/reward_energy Max                 -0.00675088\n",
      "exploration/env_infos/reward_energy Min                 -0.886753\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.225965\n",
      "exploration/env_infos/final/end_effector_loc Std         0.297271\n",
      "exploration/env_infos/final/end_effector_loc Max         0.602847\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.290822\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00702379\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0103519\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0252356\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00408863\n",
      "exploration/env_infos/end_effector_loc Mean              0.142583\n",
      "exploration/env_infos/end_effector_loc Std               0.197046\n",
      "exploration/env_infos/end_effector_loc Max               0.602847\n",
      "exploration/env_infos/end_effector_loc Min              -0.290822\n",
      "evaluation/num steps total                           87000\n",
      "evaluation/num paths total                            4350\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0453707\n",
      "evaluation/Rewards Std                                   0.0742961\n",
      "evaluation/Rewards Max                                   0.153327\n",
      "evaluation/Rewards Min                                  -0.512343\n",
      "evaluation/Returns Mean                                 -0.907414\n",
      "evaluation/Returns Std                                   1.11827\n",
      "evaluation/Returns Max                                   1.85325\n",
      "evaluation/Returns Min                                  -2.95858\n",
      "evaluation/Actions Mean                                  0.0069006\n",
      "evaluation/Actions Std                                   0.0809212\n",
      "evaluation/Actions Max                                   0.673162\n",
      "evaluation/Actions Min                                  -0.797242\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.907414\n",
      "evaluation/env_infos/final/reward_dist Mean              0.170603\n",
      "evaluation/env_infos/final/reward_dist Std               0.258682\n",
      "evaluation/env_infos/final/reward_dist Max               0.936887\n",
      "evaluation/env_infos/final/reward_dist Min               2.71062e-48\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00608185\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0122879\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0648421\n",
      "evaluation/env_infos/initial/reward_dist Min             2.44074e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.206823\n",
      "evaluation/env_infos/reward_dist Std                     0.285879\n",
      "evaluation/env_infos/reward_dist Max                     0.99968\n",
      "evaluation/env_infos/reward_dist Min                     2.71062e-48\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0763778\n",
      "evaluation/env_infos/final/reward_energy Std             0.140963\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00133194\n",
      "evaluation/env_infos/final/reward_energy Min            -0.772978\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.240526\n",
      "evaluation/env_infos/initial/reward_energy Std           0.216141\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00680561\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.05712\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0683702\n",
      "evaluation/env_infos/reward_energy Std                   0.092289\n",
      "evaluation/env_infos/reward_energy Max                  -0.00089725\n",
      "evaluation/env_infos/reward_energy Min                  -1.05712\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0712399\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.246841\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.976319\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.50621\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000429362\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0114249\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.023658\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0398621\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0285739\n",
      "evaluation/env_infos/end_effector_loc Std                0.162503\n",
      "evaluation/env_infos/end_effector_loc Max                0.976319\n",
      "evaluation/env_infos/end_effector_loc Min               -0.50621\n",
      "time/data storing (s)                                    0.00329656\n",
      "time/evaluation sampling (s)                             1.77892\n",
      "time/exploration sampling (s)                            0.161973\n",
      "time/logging (s)                                         0.0182825\n",
      "time/saving (s)                                          0.0324898\n",
      "time/training (s)                                       59.3288\n",
      "time/epoch (s)                                          61.3238\n",
      "time/total (s)                                        4523.03\n",
      "Epoch                                                   86\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:29:38.288298 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 87 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000967391\n",
      "trainer/QF2 Loss                                         0.000740711\n",
      "trainer/Policy Loss                                      3.10805\n",
      "trainer/Q1 Predictions Mean                             -1.09719\n",
      "trainer/Q1 Predictions Std                               0.898056\n",
      "trainer/Q1 Predictions Max                               1.07411\n",
      "trainer/Q1 Predictions Min                              -3.83963\n",
      "trainer/Q2 Predictions Mean                             -1.09717\n",
      "trainer/Q2 Predictions Std                               0.895508\n",
      "trainer/Q2 Predictions Max                               1.04275\n",
      "trainer/Q2 Predictions Min                              -3.82989\n",
      "trainer/Q Targets Mean                                  -1.09238\n",
      "trainer/Q Targets Std                                    0.894355\n",
      "trainer/Q Targets Max                                    1.06338\n",
      "trainer/Q Targets Min                                   -3.83702\n",
      "trainer/Log Pis Mean                                     2.01512\n",
      "trainer/Log Pis Std                                      1.28843\n",
      "trainer/Log Pis Max                                      4.52608\n",
      "trainer/Log Pis Min                                     -2.25408\n",
      "trainer/Policy mu Mean                                   0.0118531\n",
      "trainer/Policy mu Std                                    0.341171\n",
      "trainer/Policy mu Max                                    2.08294\n",
      "trainer/Policy mu Min                                   -1.97468\n",
      "trainer/Policy log std Mean                             -2.26922\n",
      "trainer/Policy log std Std                               0.592561\n",
      "trainer/Policy log std Max                              -0.0713712\n",
      "trainer/Policy log std Min                              -3.24393\n",
      "trainer/Alpha                                            0.0245906\n",
      "trainer/Alpha Loss                                       0.0560372\n",
      "exploration/num steps total                           9800\n",
      "exploration/num paths total                            490\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0348848\n",
      "exploration/Rewards Std                                  0.0751926\n",
      "exploration/Rewards Max                                  0.119248\n",
      "exploration/Rewards Min                                 -0.182206\n",
      "exploration/Returns Mean                                -0.697696\n",
      "exploration/Returns Std                                  0.987555\n",
      "exploration/Returns Max                                  0.731453\n",
      "exploration/Returns Min                                 -1.96429\n",
      "exploration/Actions Mean                                 0.00423574\n",
      "exploration/Actions Std                                  0.153079\n",
      "exploration/Actions Max                                  0.551274\n",
      "exploration/Actions Min                                 -0.551642\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -0.697696\n",
      "exploration/env_infos/final/reward_dist Mean             0.137535\n",
      "exploration/env_infos/final/reward_dist Std              0.154808\n",
      "exploration/env_infos/final/reward_dist Max              0.382346\n",
      "exploration/env_infos/final/reward_dist Min              1.24052e-07\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000601893\n",
      "exploration/env_infos/initial/reward_dist Std            0.000720201\n",
      "exploration/env_infos/initial/reward_dist Max            0.00189981\n",
      "exploration/env_infos/initial/reward_dist Min            7.73048e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.192923\n",
      "exploration/env_infos/reward_dist Std                    0.240579\n",
      "exploration/env_infos/reward_dist Max                    0.905998\n",
      "exploration/env_infos/reward_dist Min                    1.24052e-07\n",
      "exploration/env_infos/final/reward_energy Mean          -0.12808\n",
      "exploration/env_infos/final/reward_energy Std            0.0328142\n",
      "exploration/env_infos/final/reward_energy Max           -0.08518\n",
      "exploration/env_infos/final/reward_energy Min           -0.167604\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.305314\n",
      "exploration/env_infos/initial/reward_energy Std          0.185088\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0502204\n",
      "exploration/env_infos/initial/reward_energy Min         -0.526035\n",
      "exploration/env_infos/reward_energy Mean                -0.175031\n",
      "exploration/env_infos/reward_energy Std                  0.127539\n",
      "exploration/env_infos/reward_energy Max                 -0.0066996\n",
      "exploration/env_infos/reward_energy Min                 -0.66289\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.00151801\n",
      "exploration/env_infos/final/end_effector_loc Std         0.245833\n",
      "exploration/env_infos/final/end_effector_loc Max         0.260676\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.515441\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00348638\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0121321\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0252777\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00882859\n",
      "exploration/env_infos/end_effector_loc Mean             -0.00734038\n",
      "exploration/env_infos/end_effector_loc Std               0.164489\n",
      "exploration/env_infos/end_effector_loc Max               0.261854\n",
      "exploration/env_infos/end_effector_loc Min              -0.515441\n",
      "evaluation/num steps total                           88000\n",
      "evaluation/num paths total                            4400\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.044048\n",
      "evaluation/Rewards Std                                   0.0780151\n",
      "evaluation/Rewards Max                                   0.10574\n",
      "evaluation/Rewards Min                                  -0.667833\n",
      "evaluation/Returns Mean                                 -0.88096\n",
      "evaluation/Returns Std                                   1.25006\n",
      "evaluation/Returns Max                                   1.12782\n",
      "evaluation/Returns Min                                  -4.8447\n",
      "evaluation/Actions Mean                                  0.00322819\n",
      "evaluation/Actions Std                                   0.0753415\n",
      "evaluation/Actions Max                                   0.608989\n",
      "evaluation/Actions Min                                  -0.763478\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.88096\n",
      "evaluation/env_infos/final/reward_dist Mean              0.20437\n",
      "evaluation/env_infos/final/reward_dist Std               0.281357\n",
      "evaluation/env_infos/final/reward_dist Max               0.993613\n",
      "evaluation/env_infos/final/reward_dist Min               4.60494e-32\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00625269\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00972274\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0331669\n",
      "evaluation/env_infos/initial/reward_dist Min             2.58418e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.174688\n",
      "evaluation/env_infos/reward_dist Std                     0.244664\n",
      "evaluation/env_infos/reward_dist Max                     0.993613\n",
      "evaluation/env_infos/reward_dist Min                     4.60494e-32\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.044362\n",
      "evaluation/env_infos/final/reward_energy Std             0.0413011\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00322027\n",
      "evaluation/env_infos/final/reward_energy Min            -0.190079\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.222912\n",
      "evaluation/env_infos/initial/reward_energy Std           0.213851\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0125951\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.02969\n",
      "evaluation/env_infos/reward_energy Mean                 -0.063092\n",
      "evaluation/env_infos/reward_energy Std                   0.0859821\n",
      "evaluation/env_infos/reward_energy Max                  -0.00104715\n",
      "evaluation/env_infos/reward_energy Min                  -1.02969\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0707715\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.244738\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.698589\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.488588\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00167996\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0107914\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0304494\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0381739\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0376834\n",
      "evaluation/env_infos/end_effector_loc Std                0.154626\n",
      "evaluation/env_infos/end_effector_loc Max                0.698589\n",
      "evaluation/env_infos/end_effector_loc Min               -0.488588\n",
      "time/data storing (s)                                    0.00315414\n",
      "time/evaluation sampling (s)                             1.44009\n",
      "time/exploration sampling (s)                            0.13965\n",
      "time/logging (s)                                         0.0197898\n",
      "time/saving (s)                                          0.0289677\n",
      "time/training (s)                                       54.548\n",
      "time/epoch (s)                                          56.1796\n",
      "time/total (s)                                        4580.5\n",
      "Epoch                                                   87\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:30:42.224144 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 88 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000713256\n",
      "trainer/QF2 Loss                                         0.000946556\n",
      "trainer/Policy Loss                                      2.854\n",
      "trainer/Q1 Predictions Mean                             -1.0402\n",
      "trainer/Q1 Predictions Std                               0.95404\n",
      "trainer/Q1 Predictions Max                               1.12228\n",
      "trainer/Q1 Predictions Min                              -3.69971\n",
      "trainer/Q2 Predictions Mean                             -1.03471\n",
      "trainer/Q2 Predictions Std                               0.95488\n",
      "trainer/Q2 Predictions Max                               1.11517\n",
      "trainer/Q2 Predictions Min                              -3.68875\n",
      "trainer/Q Targets Mean                                  -1.04437\n",
      "trainer/Q Targets Std                                    0.957498\n",
      "trainer/Q Targets Max                                    1.15716\n",
      "trainer/Q Targets Min                                   -3.75101\n",
      "trainer/Log Pis Mean                                     1.82518\n",
      "trainer/Log Pis Std                                      1.5307\n",
      "trainer/Log Pis Max                                      5.01542\n",
      "trainer/Log Pis Min                                     -3.70905\n",
      "trainer/Policy mu Mean                                   0.0473441\n",
      "trainer/Policy mu Std                                    0.407791\n",
      "trainer/Policy mu Max                                    2.20331\n",
      "trainer/Policy mu Min                                   -2.30514\n",
      "trainer/Policy log std Mean                             -2.20436\n",
      "trainer/Policy log std Std                               0.671218\n",
      "trainer/Policy log std Max                              -0.0405939\n",
      "trainer/Policy log std Min                              -3.3227\n",
      "trainer/Alpha                                            0.022816\n",
      "trainer/Alpha Loss                                      -0.660891\n",
      "exploration/num steps total                           9900\n",
      "exploration/num paths total                            495\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.117967\n",
      "exploration/Rewards Std                                  0.0972992\n",
      "exploration/Rewards Max                                  0.0613822\n",
      "exploration/Rewards Min                                 -0.719589\n",
      "exploration/Returns Mean                                -2.35934\n",
      "exploration/Returns Std                                  1.22194\n",
      "exploration/Returns Max                                 -0.00465625\n",
      "exploration/Returns Min                                 -3.42246\n",
      "exploration/Actions Mean                                -0.00316595\n",
      "exploration/Actions Std                                  0.151526\n",
      "exploration/Actions Max                                  0.397889\n",
      "exploration/Actions Min                                 -0.900657\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.35934\n",
      "exploration/env_infos/final/reward_dist Mean             0.0628594\n",
      "exploration/env_infos/final/reward_dist Std              0.058378\n",
      "exploration/env_infos/final/reward_dist Max              0.15015\n",
      "exploration/env_infos/final/reward_dist Min              4.10023e-20\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00260435\n",
      "exploration/env_infos/initial/reward_dist Std            0.00458815\n",
      "exploration/env_infos/initial/reward_dist Max            0.0117664\n",
      "exploration/env_infos/initial/reward_dist Min            1.06785e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.124466\n",
      "exploration/env_infos/reward_dist Std                    0.168553\n",
      "exploration/env_infos/reward_dist Max                    0.740134\n",
      "exploration/env_infos/reward_dist Min                    4.10023e-20\n",
      "exploration/env_infos/final/reward_energy Mean          -0.15136\n",
      "exploration/env_infos/final/reward_energy Std            0.0512278\n",
      "exploration/env_infos/final/reward_energy Max           -0.0827304\n",
      "exploration/env_infos/final/reward_energy Min           -0.225133\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.392085\n",
      "exploration/env_infos/initial/reward_energy Std          0.427589\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0630191\n",
      "exploration/env_infos/initial/reward_energy Min         -1.22093\n",
      "exploration/env_infos/reward_energy Mean                -0.155471\n",
      "exploration/env_infos/reward_energy Std                  0.147543\n",
      "exploration/env_infos/reward_energy Max                 -0.0175455\n",
      "exploration/env_infos/reward_energy Min                 -1.22093\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.108977\n",
      "exploration/env_infos/final/end_effector_loc Std         0.307836\n",
      "exploration/env_infos/final/end_effector_loc Max         0.44749\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.594712\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.0115781\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0169308\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00295757\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0450329\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0840313\n",
      "exploration/env_infos/end_effector_loc Std               0.174826\n",
      "exploration/env_infos/end_effector_loc Max               0.44749\n",
      "exploration/env_infos/end_effector_loc Min              -0.594712\n",
      "evaluation/num steps total                           89000\n",
      "evaluation/num paths total                            4450\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0585888\n",
      "evaluation/Rewards Std                                   0.0703203\n",
      "evaluation/Rewards Max                                   0.127061\n",
      "evaluation/Rewards Min                                  -0.45308\n",
      "evaluation/Returns Mean                                 -1.17178\n",
      "evaluation/Returns Std                                   1.04895\n",
      "evaluation/Returns Max                                   1.83083\n",
      "evaluation/Returns Min                                  -3.29048\n",
      "evaluation/Actions Mean                                  0.00175714\n",
      "evaluation/Actions Std                                   0.0733458\n",
      "evaluation/Actions Max                                   0.668374\n",
      "evaluation/Actions Min                                  -0.448036\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.17178\n",
      "evaluation/env_infos/final/reward_dist Mean              0.173446\n",
      "evaluation/env_infos/final/reward_dist Std               0.266338\n",
      "evaluation/env_infos/final/reward_dist Max               0.999222\n",
      "evaluation/env_infos/final/reward_dist Min               3.16517e-18\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00684491\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0126979\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0512853\n",
      "evaluation/env_infos/initial/reward_dist Min             9.39962e-07\n",
      "evaluation/env_infos/reward_dist Mean                    0.153595\n",
      "evaluation/env_infos/reward_dist Std                     0.243147\n",
      "evaluation/env_infos/reward_dist Max                     0.999222\n",
      "evaluation/env_infos/reward_dist Min                     3.16517e-18\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0876882\n",
      "evaluation/env_infos/final/reward_energy Std             0.11302\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00258433\n",
      "evaluation/env_infos/final/reward_energy Min            -0.630141\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.232505\n",
      "evaluation/env_infos/initial/reward_energy Std           0.213519\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00914598\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.725212\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0619488\n",
      "evaluation/env_infos/reward_energy Std                   0.0832329\n",
      "evaluation/env_infos/reward_energy Max                  -0.000496725\n",
      "evaluation/env_infos/reward_energy Min                  -0.725212\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0474509\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.252252\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.531435\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.620855\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00325979\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.010674\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0334187\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0224018\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0292928\n",
      "evaluation/env_infos/end_effector_loc Std                0.158389\n",
      "evaluation/env_infos/end_effector_loc Max                0.531435\n",
      "evaluation/env_infos/end_effector_loc Min               -0.620855\n",
      "time/data storing (s)                                    0.00328978\n",
      "time/evaluation sampling (s)                             1.04944\n",
      "time/exploration sampling (s)                            0.167987\n",
      "time/logging (s)                                         0.0376045\n",
      "time/saving (s)                                          0.0439822\n",
      "time/training (s)                                       61.5074\n",
      "time/epoch (s)                                          62.8097\n",
      "time/total (s)                                        4644.45\n",
      "Epoch                                                   88\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:31:42.739108 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 89 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000661742\n",
      "trainer/QF2 Loss                                         0.000607599\n",
      "trainer/Policy Loss                                      3.14147\n",
      "trainer/Q1 Predictions Mean                             -1.11789\n",
      "trainer/Q1 Predictions Std                               0.877175\n",
      "trainer/Q1 Predictions Max                               1.19313\n",
      "trainer/Q1 Predictions Min                              -3.03903\n",
      "trainer/Q2 Predictions Mean                             -1.11696\n",
      "trainer/Q2 Predictions Std                               0.878045\n",
      "trainer/Q2 Predictions Max                               1.19455\n",
      "trainer/Q2 Predictions Min                              -3.04007\n",
      "trainer/Q Targets Mean                                  -1.12013\n",
      "trainer/Q Targets Std                                    0.882249\n",
      "trainer/Q Targets Max                                    1.20659\n",
      "trainer/Q Targets Min                                   -3.06234\n",
      "trainer/Log Pis Mean                                     2.03584\n",
      "trainer/Log Pis Std                                      1.33571\n",
      "trainer/Log Pis Max                                      4.46727\n",
      "trainer/Log Pis Min                                     -2.39148\n",
      "trainer/Policy mu Mean                                   0.0127224\n",
      "trainer/Policy mu Std                                    0.411846\n",
      "trainer/Policy mu Max                                    2.15832\n",
      "trainer/Policy mu Min                                   -2.48651\n",
      "trainer/Policy log std Mean                             -2.29916\n",
      "trainer/Policy log std Std                               0.636434\n",
      "trainer/Policy log std Max                              -0.265467\n",
      "trainer/Policy log std Min                              -3.27702\n",
      "trainer/Alpha                                            0.0228393\n",
      "trainer/Alpha Loss                                       0.135478\n",
      "exploration/num steps total                          10000\n",
      "exploration/num paths total                            500\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0561464\n",
      "exploration/Rewards Std                                  0.107047\n",
      "exploration/Rewards Max                                  0.15085\n",
      "exploration/Rewards Min                                 -0.300006\n",
      "exploration/Returns Mean                                -1.12293\n",
      "exploration/Returns Std                                  1.91971\n",
      "exploration/Returns Max                                  2.27685\n",
      "exploration/Returns Min                                 -3.14705\n",
      "exploration/Actions Mean                                -0.00717381\n",
      "exploration/Actions Std                                  0.124177\n",
      "exploration/Actions Max                                  0.43078\n",
      "exploration/Actions Min                                 -0.661325\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.12293\n",
      "exploration/env_infos/final/reward_dist Mean             0.0169231\n",
      "exploration/env_infos/final/reward_dist Std              0.0334529\n",
      "exploration/env_infos/final/reward_dist Max              0.0838278\n",
      "exploration/env_infos/final/reward_dist Min              1.07735e-17\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0103564\n",
      "exploration/env_infos/initial/reward_dist Std            0.015862\n",
      "exploration/env_infos/initial/reward_dist Max            0.0412912\n",
      "exploration/env_infos/initial/reward_dist Min            3.06136e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.145638\n",
      "exploration/env_infos/reward_dist Std                    0.290544\n",
      "exploration/env_infos/reward_dist Max                    0.953906\n",
      "exploration/env_infos/reward_dist Min                    1.07735e-17\n",
      "exploration/env_infos/final/reward_energy Mean          -0.122873\n",
      "exploration/env_infos/final/reward_energy Std            0.0596587\n",
      "exploration/env_infos/final/reward_energy Max           -0.059025\n",
      "exploration/env_infos/final/reward_energy Min           -0.212737\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.283125\n",
      "exploration/env_infos/initial/reward_energy Std          0.217481\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0480824\n",
      "exploration/env_infos/initial/reward_energy Min         -0.661942\n",
      "exploration/env_infos/reward_energy Mean                -0.132783\n",
      "exploration/env_infos/reward_energy Std                  0.115375\n",
      "exploration/env_infos/reward_energy Max                 -0.00545208\n",
      "exploration/env_infos/reward_energy Min                 -0.782788\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0390409\n",
      "exploration/env_infos/final/end_effector_loc Std         0.26013\n",
      "exploration/env_infos/final/end_effector_loc Max         0.548649\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.347223\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00457411\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0117643\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00857293\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0330662\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0162528\n",
      "exploration/env_infos/end_effector_loc Std               0.153439\n",
      "exploration/env_infos/end_effector_loc Max               0.548649\n",
      "exploration/env_infos/end_effector_loc Min              -0.347223\n",
      "evaluation/num steps total                           90000\n",
      "evaluation/num paths total                            4500\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0394548\n",
      "evaluation/Rewards Std                                   0.0750897\n",
      "evaluation/Rewards Max                                   0.169366\n",
      "evaluation/Rewards Min                                  -0.551534\n",
      "evaluation/Returns Mean                                 -0.789095\n",
      "evaluation/Returns Std                                   1.19663\n",
      "evaluation/Returns Max                                   1.68729\n",
      "evaluation/Returns Min                                  -2.82151\n",
      "evaluation/Actions Mean                                  0.00187652\n",
      "evaluation/Actions Std                                   0.0733587\n",
      "evaluation/Actions Max                                   0.840842\n",
      "evaluation/Actions Min                                  -0.720185\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.789095\n",
      "evaluation/env_infos/final/reward_dist Mean              0.221816\n",
      "evaluation/env_infos/final/reward_dist Std               0.281053\n",
      "evaluation/env_infos/final/reward_dist Max               0.912149\n",
      "evaluation/env_infos/final/reward_dist Min               2.19618e-12\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00697503\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0117158\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0662914\n",
      "evaluation/env_infos/initial/reward_dist Min             4.62636e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.213293\n",
      "evaluation/env_infos/reward_dist Std                     0.287505\n",
      "evaluation/env_infos/reward_dist Max                     0.996289\n",
      "evaluation/env_infos/reward_dist Min                     2.19618e-12\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0533888\n",
      "evaluation/env_infos/final/reward_energy Std             0.0544848\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0078962\n",
      "evaluation/env_infos/final/reward_energy Min            -0.234725\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.242182\n",
      "evaluation/env_infos/initial/reward_energy Std           0.220842\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00525646\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.909047\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0628743\n",
      "evaluation/env_infos/reward_energy Std                   0.0825642\n",
      "evaluation/env_infos/reward_energy Max                  -0.000219416\n",
      "evaluation/env_infos/reward_energy Min                  -0.909047\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0358187\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.23201\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.486271\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.559394\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00122375\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0115231\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0420421\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0360092\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0185866\n",
      "evaluation/env_infos/end_effector_loc Std                0.157884\n",
      "evaluation/env_infos/end_effector_loc Max                0.486271\n",
      "evaluation/env_infos/end_effector_loc Min               -0.559394\n",
      "time/data storing (s)                                    0.00317628\n",
      "time/evaluation sampling (s)                             1.3997\n",
      "time/exploration sampling (s)                            0.139922\n",
      "time/logging (s)                                         0.0244498\n",
      "time/saving (s)                                          0.0361482\n",
      "time/training (s)                                       57.3403\n",
      "time/epoch (s)                                          58.9437\n",
      "time/total (s)                                        4704.95\n",
      "Epoch                                                   89\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:32:45.227453 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 90 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000640028\n",
      "trainer/QF2 Loss                                         0.00113842\n",
      "trainer/Policy Loss                                      2.83636\n",
      "trainer/Q1 Predictions Mean                             -0.984102\n",
      "trainer/Q1 Predictions Std                               0.858803\n",
      "trainer/Q1 Predictions Max                               0.801073\n",
      "trainer/Q1 Predictions Min                              -2.88309\n",
      "trainer/Q2 Predictions Mean                             -0.974686\n",
      "trainer/Q2 Predictions Std                               0.847579\n",
      "trainer/Q2 Predictions Max                               0.757766\n",
      "trainer/Q2 Predictions Min                              -2.8244\n",
      "trainer/Q Targets Mean                                  -0.987131\n",
      "trainer/Q Targets Std                                    0.859702\n",
      "trainer/Q Targets Max                                    0.828441\n",
      "trainer/Q Targets Min                                   -2.92039\n",
      "trainer/Log Pis Mean                                     1.85798\n",
      "trainer/Log Pis Std                                      1.42351\n",
      "trainer/Log Pis Max                                      4.24953\n",
      "trainer/Log Pis Min                                     -4.17863\n",
      "trainer/Policy mu Mean                                  -0.0211141\n",
      "trainer/Policy mu Std                                    0.307442\n",
      "trainer/Policy mu Max                                    2.31925\n",
      "trainer/Policy mu Min                                   -1.68744\n",
      "trainer/Policy log std Mean                             -2.27155\n",
      "trainer/Policy log std Std                               0.598817\n",
      "trainer/Policy log std Max                              -0.0942147\n",
      "trainer/Policy log std Min                              -3.14763\n",
      "trainer/Alpha                                            0.0239412\n",
      "trainer/Alpha Loss                                      -0.529912\n",
      "exploration/num steps total                          10100\n",
      "exploration/num paths total                            505\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.106559\n",
      "exploration/Rewards Std                                  0.211285\n",
      "exploration/Rewards Max                                  0.1533\n",
      "exploration/Rewards Min                                 -0.963324\n",
      "exploration/Returns Mean                                -2.13119\n",
      "exploration/Returns Std                                  3.08466\n",
      "exploration/Returns Max                                  1.3905\n",
      "exploration/Returns Min                                 -7.73321\n",
      "exploration/Actions Mean                                 0.0245213\n",
      "exploration/Actions Std                                  0.170214\n",
      "exploration/Actions Max                                  0.60834\n",
      "exploration/Actions Min                                 -0.722439\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.13119\n",
      "exploration/env_infos/final/reward_dist Mean             0.240022\n",
      "exploration/env_infos/final/reward_dist Std              0.362824\n",
      "exploration/env_infos/final/reward_dist Max              0.948812\n",
      "exploration/env_infos/final/reward_dist Min              4.3996e-59\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00914037\n",
      "exploration/env_infos/initial/reward_dist Std            0.0139875\n",
      "exploration/env_infos/initial/reward_dist Max            0.0368588\n",
      "exploration/env_infos/initial/reward_dist Min            2.49888e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.235362\n",
      "exploration/env_infos/reward_dist Std                    0.279659\n",
      "exploration/env_infos/reward_dist Max                    0.979063\n",
      "exploration/env_infos/reward_dist Min                    7.70115e-61\n",
      "exploration/env_infos/final/reward_energy Mean          -0.180492\n",
      "exploration/env_infos/final/reward_energy Std            0.119609\n",
      "exploration/env_infos/final/reward_energy Max           -0.0493929\n",
      "exploration/env_infos/final/reward_energy Min           -0.401154\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.47752\n",
      "exploration/env_infos/initial/reward_energy Std          0.283066\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0116296\n",
      "exploration/env_infos/initial/reward_energy Min         -0.791655\n",
      "exploration/env_infos/reward_energy Mean                -0.18824\n",
      "exploration/env_infos/reward_energy Std                  0.153992\n",
      "exploration/env_infos/reward_energy Max                 -0.0110388\n",
      "exploration/env_infos/reward_energy Min                 -0.791655\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.205558\n",
      "exploration/env_infos/final/end_effector_loc Std         0.313639\n",
      "exploration/env_infos/final/end_effector_loc Max         1\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.0995369\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00377049\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0192606\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0285209\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0361219\n",
      "exploration/env_infos/end_effector_loc Mean              0.0861278\n",
      "exploration/env_infos/end_effector_loc Std               0.216575\n",
      "exploration/env_infos/end_effector_loc Max               1\n",
      "exploration/env_infos/end_effector_loc Min              -0.208321\n",
      "evaluation/num steps total                           91000\n",
      "evaluation/num paths total                            4550\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0375306\n",
      "evaluation/Rewards Std                                   0.0779658\n",
      "evaluation/Rewards Max                                   0.162847\n",
      "evaluation/Rewards Min                                  -0.38362\n",
      "evaluation/Returns Mean                                 -0.750612\n",
      "evaluation/Returns Std                                   1.24582\n",
      "evaluation/Returns Max                                   2.71314\n",
      "evaluation/Returns Min                                  -2.97882\n",
      "evaluation/Actions Mean                                  0.00120107\n",
      "evaluation/Actions Std                                   0.0802581\n",
      "evaluation/Actions Max                                   0.66231\n",
      "evaluation/Actions Min                                  -0.935465\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.750612\n",
      "evaluation/env_infos/final/reward_dist Mean              0.247381\n",
      "evaluation/env_infos/final/reward_dist Std               0.317342\n",
      "evaluation/env_infos/final/reward_dist Max               0.939502\n",
      "evaluation/env_infos/final/reward_dist Min               1.18724e-13\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0101378\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0179332\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0896233\n",
      "evaluation/env_infos/initial/reward_dist Min             1.68309e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.240723\n",
      "evaluation/env_infos/reward_dist Std                     0.302109\n",
      "evaluation/env_infos/reward_dist Max                     0.996951\n",
      "evaluation/env_infos/reward_dist Min                     1.18724e-13\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0598011\n",
      "evaluation/env_infos/final/reward_energy Std             0.0655014\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0053453\n",
      "evaluation/env_infos/final/reward_energy Min            -0.301835\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.279602\n",
      "evaluation/env_infos/initial/reward_energy Std           0.248965\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0106258\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.988779\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0660358\n",
      "evaluation/env_infos/reward_energy Std                   0.0923303\n",
      "evaluation/env_infos/reward_energy Max                  -0.000516442\n",
      "evaluation/env_infos/reward_energy Min                  -0.988779\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0069542\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.23478\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.57791\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.577618\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000544904\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0132251\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0331155\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0467733\n",
      "evaluation/env_infos/end_effector_loc Mean               0.000728956\n",
      "evaluation/env_infos/end_effector_loc Std                0.156388\n",
      "evaluation/env_infos/end_effector_loc Max                0.57791\n",
      "evaluation/env_infos/end_effector_loc Min               -0.577618\n",
      "time/data storing (s)                                    0.00332008\n",
      "time/evaluation sampling (s)                             1.09197\n",
      "time/exploration sampling (s)                            0.144858\n",
      "time/logging (s)                                         0.026303\n",
      "time/saving (s)                                          0.04399\n",
      "time/training (s)                                       59.8561\n",
      "time/epoch (s)                                          61.1666\n",
      "time/total (s)                                        4767.43\n",
      "Epoch                                                   90\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:33:59.795751 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 91 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00123428\n",
      "trainer/QF2 Loss                                         0.00085233\n",
      "trainer/Policy Loss                                      2.77402\n",
      "trainer/Q1 Predictions Mean                             -0.906798\n",
      "trainer/Q1 Predictions Std                               0.890154\n",
      "trainer/Q1 Predictions Max                               1.06924\n",
      "trainer/Q1 Predictions Min                              -3.41769\n",
      "trainer/Q2 Predictions Mean                             -0.903045\n",
      "trainer/Q2 Predictions Std                               0.892602\n",
      "trainer/Q2 Predictions Max                               1.09302\n",
      "trainer/Q2 Predictions Min                              -3.37066\n",
      "trainer/Q Targets Mean                                  -0.90397\n",
      "trainer/Q Targets Std                                    0.89054\n",
      "trainer/Q Targets Max                                    1.09966\n",
      "trainer/Q Targets Min                                   -3.38674\n",
      "trainer/Log Pis Mean                                     1.87763\n",
      "trainer/Log Pis Std                                      1.52987\n",
      "trainer/Log Pis Max                                      9.41667\n",
      "trainer/Log Pis Min                                     -6.19847\n",
      "trainer/Policy mu Mean                                   0.0032631\n",
      "trainer/Policy mu Std                                    0.341327\n",
      "trainer/Policy mu Max                                    2.13775\n",
      "trainer/Policy mu Min                                   -1.54728\n",
      "trainer/Policy log std Mean                             -2.23084\n",
      "trainer/Policy log std Std                               0.649008\n",
      "trainer/Policy log std Max                               1.11785\n",
      "trainer/Policy log std Min                              -3.22023\n",
      "trainer/Alpha                                            0.0232713\n",
      "trainer/Alpha Loss                                      -0.460019\n",
      "exploration/num steps total                          10200\n",
      "exploration/num paths total                            510\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0790683\n",
      "exploration/Rewards Std                                  0.0506442\n",
      "exploration/Rewards Max                                 -0.00658312\n",
      "exploration/Rewards Min                                 -0.261524\n",
      "exploration/Returns Mean                                -1.58137\n",
      "exploration/Returns Std                                  0.478011\n",
      "exploration/Returns Max                                 -0.897225\n",
      "exploration/Returns Min                                 -2.15701\n",
      "exploration/Actions Mean                                -0.00417638\n",
      "exploration/Actions Std                                  0.133492\n",
      "exploration/Actions Max                                  0.45813\n",
      "exploration/Actions Min                                 -0.880313\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.58137\n",
      "exploration/env_infos/final/reward_dist Mean             0.117311\n",
      "exploration/env_infos/final/reward_dist Std              0.187085\n",
      "exploration/env_infos/final/reward_dist Max              0.487298\n",
      "exploration/env_infos/final/reward_dist Min              1.2911e-08\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00467156\n",
      "exploration/env_infos/initial/reward_dist Std            0.00505045\n",
      "exploration/env_infos/initial/reward_dist Max            0.0114577\n",
      "exploration/env_infos/initial/reward_dist Min            1.49405e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.0451016\n",
      "exploration/env_infos/reward_dist Std                    0.0803597\n",
      "exploration/env_infos/reward_dist Max                    0.487298\n",
      "exploration/env_infos/reward_dist Min                    1.2911e-08\n",
      "exploration/env_infos/final/reward_energy Mean          -0.116114\n",
      "exploration/env_infos/final/reward_energy Std            0.0544181\n",
      "exploration/env_infos/final/reward_energy Max           -0.0143198\n",
      "exploration/env_infos/final/reward_energy Min           -0.161671\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.295228\n",
      "exploration/env_infos/initial/reward_energy Std          0.327942\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0518224\n",
      "exploration/env_infos/initial/reward_energy Min         -0.932794\n",
      "exploration/env_infos/reward_energy Mean                -0.134848\n",
      "exploration/env_infos/reward_energy Std                  0.132255\n",
      "exploration/env_infos/reward_energy Max                 -0.008866\n",
      "exploration/env_infos/reward_energy Min                 -0.932794\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.12443\n",
      "exploration/env_infos/final/end_effector_loc Std         0.137382\n",
      "exploration/env_infos/final/end_effector_loc Max         0.11283\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.340223\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00459428\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0149089\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0139562\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0440156\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0770302\n",
      "exploration/env_infos/end_effector_loc Std               0.131847\n",
      "exploration/env_infos/end_effector_loc Max               0.124984\n",
      "exploration/env_infos/end_effector_loc Min              -0.442337\n",
      "evaluation/num steps total                           92000\n",
      "evaluation/num paths total                            4600\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0562163\n",
      "evaluation/Rewards Std                                   0.0951759\n",
      "evaluation/Rewards Max                                   0.151251\n",
      "evaluation/Rewards Min                                  -0.563385\n",
      "evaluation/Returns Mean                                 -1.12433\n",
      "evaluation/Returns Std                                   1.51144\n",
      "evaluation/Returns Max                                   1.95285\n",
      "evaluation/Returns Min                                  -4.04166\n",
      "evaluation/Actions Mean                                 -0.00244832\n",
      "evaluation/Actions Std                                   0.103738\n",
      "evaluation/Actions Max                                   0.913266\n",
      "evaluation/Actions Min                                  -0.946562\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.12433\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0929317\n",
      "evaluation/env_infos/final/reward_dist Std               0.204088\n",
      "evaluation/env_infos/final/reward_dist Max               0.990081\n",
      "evaluation/env_infos/final/reward_dist Min               1.09003e-21\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00705997\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0140316\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0866816\n",
      "evaluation/env_infos/initial/reward_dist Min             2.25059e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.199076\n",
      "evaluation/env_infos/reward_dist Std                     0.281397\n",
      "evaluation/env_infos/reward_dist Max                     0.990081\n",
      "evaluation/env_infos/reward_dist Min                     1.09003e-21\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.063027\n",
      "evaluation/env_infos/final/reward_energy Std             0.0678059\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00729689\n",
      "evaluation/env_infos/final/reward_energy Min            -0.296924\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.386898\n",
      "evaluation/env_infos/initial/reward_energy Std           0.257223\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00744011\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.07643\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0887136\n",
      "evaluation/env_infos/reward_energy Std                   0.116897\n",
      "evaluation/env_infos/reward_energy Max                  -0.000436344\n",
      "evaluation/env_infos/reward_energy Min                  -1.07643\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.020869\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.278568\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.65079\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.599551\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00065268\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0164131\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0456633\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0473281\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.0100973\n",
      "evaluation/env_infos/end_effector_loc Std                0.190104\n",
      "evaluation/env_infos/end_effector_loc Max                0.65079\n",
      "evaluation/env_infos/end_effector_loc Min               -0.599551\n",
      "time/data storing (s)                                    0.00578928\n",
      "time/evaluation sampling (s)                             2.78377\n",
      "time/exploration sampling (s)                            0.461413\n",
      "time/logging (s)                                         0.0230301\n",
      "time/saving (s)                                          0.0390881\n",
      "time/training (s)                                       69.5847\n",
      "time/epoch (s)                                          72.8978\n",
      "time/total (s)                                        4842\n",
      "Epoch                                                   91\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:35:02.550017 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 92 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000881785\n",
      "trainer/QF2 Loss                                         0.000763298\n",
      "trainer/Policy Loss                                      3.13001\n",
      "trainer/Q1 Predictions Mean                             -1.04225\n",
      "trainer/Q1 Predictions Std                               0.92153\n",
      "trainer/Q1 Predictions Max                               1.11958\n",
      "trainer/Q1 Predictions Min                              -3.44071\n",
      "trainer/Q2 Predictions Mean                             -1.03701\n",
      "trainer/Q2 Predictions Std                               0.923219\n",
      "trainer/Q2 Predictions Max                               1.11549\n",
      "trainer/Q2 Predictions Min                              -3.45125\n",
      "trainer/Q Targets Mean                                  -1.03499\n",
      "trainer/Q Targets Std                                    0.930849\n",
      "trainer/Q Targets Max                                    1.13908\n",
      "trainer/Q Targets Min                                   -3.41586\n",
      "trainer/Log Pis Mean                                     2.09011\n",
      "trainer/Log Pis Std                                      1.33522\n",
      "trainer/Log Pis Max                                      4.28459\n",
      "trainer/Log Pis Min                                     -2.80409\n",
      "trainer/Policy mu Mean                                   0.0103411\n",
      "trainer/Policy mu Std                                    0.287976\n",
      "trainer/Policy mu Max                                    1.80597\n",
      "trainer/Policy mu Min                                   -1.43025\n",
      "trainer/Policy log std Mean                             -2.35636\n",
      "trainer/Policy log std Std                               0.597603\n",
      "trainer/Policy log std Max                              -0.468701\n",
      "trainer/Policy log std Min                              -3.19407\n",
      "trainer/Alpha                                            0.0229874\n",
      "trainer/Alpha Loss                                       0.34003\n",
      "exploration/num steps total                          10300\n",
      "exploration/num paths total                            515\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.070474\n",
      "exploration/Rewards Std                                  0.0577322\n",
      "exploration/Rewards Max                                  0.0465409\n",
      "exploration/Rewards Min                                 -0.249857\n",
      "exploration/Returns Mean                                -1.40948\n",
      "exploration/Returns Std                                  0.645698\n",
      "exploration/Returns Max                                 -0.88944\n",
      "exploration/Returns Min                                 -2.65808\n",
      "exploration/Actions Mean                                -0.00174995\n",
      "exploration/Actions Std                                  0.0904617\n",
      "exploration/Actions Max                                  0.27762\n",
      "exploration/Actions Min                                 -0.328585\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.40948\n",
      "exploration/env_infos/final/reward_dist Mean             0.272191\n",
      "exploration/env_infos/final/reward_dist Std              0.327954\n",
      "exploration/env_infos/final/reward_dist Max              0.778744\n",
      "exploration/env_infos/final/reward_dist Min              0.00208809\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00869101\n",
      "exploration/env_infos/initial/reward_dist Std            0.0128147\n",
      "exploration/env_infos/initial/reward_dist Max            0.0337604\n",
      "exploration/env_infos/initial/reward_dist Min            5.45648e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.153023\n",
      "exploration/env_infos/reward_dist Std                    0.228222\n",
      "exploration/env_infos/reward_dist Max                    0.946448\n",
      "exploration/env_infos/reward_dist Min                    5.45648e-06\n",
      "exploration/env_infos/final/reward_energy Mean          -0.174964\n",
      "exploration/env_infos/final/reward_energy Std            0.133364\n",
      "exploration/env_infos/final/reward_energy Max           -0.0463026\n",
      "exploration/env_infos/final/reward_energy Min           -0.428768\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.16898\n",
      "exploration/env_infos/initial/reward_energy Std          0.108237\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0724433\n",
      "exploration/env_infos/initial/reward_energy Min         -0.316997\n",
      "exploration/env_infos/reward_energy Mean                -0.103157\n",
      "exploration/env_infos/reward_energy Std                  0.0757057\n",
      "exploration/env_infos/reward_energy Max                 -0.0190504\n",
      "exploration/env_infos/reward_energy Min                 -0.428768\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.00691004\n",
      "exploration/env_infos/final/end_effector_loc Std         0.186741\n",
      "exploration/env_infos/final/end_effector_loc Max         0.256686\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.274634\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00187988\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00684127\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0129462\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.00914399\n",
      "exploration/env_infos/end_effector_loc Mean              0.0185676\n",
      "exploration/env_infos/end_effector_loc Std               0.122167\n",
      "exploration/env_infos/end_effector_loc Max               0.303053\n",
      "exploration/env_infos/end_effector_loc Min              -0.274634\n",
      "evaluation/num steps total                           93000\n",
      "evaluation/num paths total                            4650\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0471932\n",
      "evaluation/Rewards Std                                   0.0850357\n",
      "evaluation/Rewards Max                                   0.144433\n",
      "evaluation/Rewards Min                                  -0.635974\n",
      "evaluation/Returns Mean                                 -0.943864\n",
      "evaluation/Returns Std                                   1.21566\n",
      "evaluation/Returns Max                                   1.71129\n",
      "evaluation/Returns Min                                  -4.3987\n",
      "evaluation/Actions Mean                                 -0.00117127\n",
      "evaluation/Actions Std                                   0.0931722\n",
      "evaluation/Actions Max                                   0.833919\n",
      "evaluation/Actions Min                                  -0.872828\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.943864\n",
      "evaluation/env_infos/final/reward_dist Mean              0.213278\n",
      "evaluation/env_infos/final/reward_dist Std               0.303675\n",
      "evaluation/env_infos/final/reward_dist Max               0.969478\n",
      "evaluation/env_infos/final/reward_dist Min               9.06188e-28\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0141376\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0292777\n",
      "evaluation/env_infos/initial/reward_dist Max             0.14953\n",
      "evaluation/env_infos/initial/reward_dist Min             2.87403e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.20467\n",
      "evaluation/env_infos/reward_dist Std                     0.283211\n",
      "evaluation/env_infos/reward_dist Max                     0.996765\n",
      "evaluation/env_infos/reward_dist Min                     9.06188e-28\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0422569\n",
      "evaluation/env_infos/final/reward_energy Std             0.0367209\n",
      "evaluation/env_infos/final/reward_energy Max            -0.000947462\n",
      "evaluation/env_infos/final/reward_energy Min            -0.149969\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.324137\n",
      "evaluation/env_infos/initial/reward_energy Std           0.301542\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.00288932\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.20717\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0737012\n",
      "evaluation/env_infos/reward_energy Std                   0.109238\n",
      "evaluation/env_infos/reward_energy Max                  -0.000906137\n",
      "evaluation/env_infos/reward_energy Min                  -1.20717\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.00574169\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.26625\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.692232\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.771108\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000364004\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0156479\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.041696\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0436414\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00187669\n",
      "evaluation/env_infos/end_effector_loc Std                0.179506\n",
      "evaluation/env_infos/end_effector_loc Max                0.692232\n",
      "evaluation/env_infos/end_effector_loc Min               -0.771108\n",
      "time/data storing (s)                                    0.0031798\n",
      "time/evaluation sampling (s)                             1.24805\n",
      "time/exploration sampling (s)                            0.168132\n",
      "time/logging (s)                                         0.0370013\n",
      "time/saving (s)                                          0.0360527\n",
      "time/training (s)                                       59.8245\n",
      "time/epoch (s)                                          61.3169\n",
      "time/total (s)                                        4904.76\n",
      "Epoch                                                   92\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:36:06.094442 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 93 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000552864\r\n",
      "trainer/QF2 Loss                                         0.000750813\r\n",
      "trainer/Policy Loss                                      3.05719\r\n",
      "trainer/Q1 Predictions Mean                             -1.03226\r\n",
      "trainer/Q1 Predictions Std                               0.927364\r\n",
      "trainer/Q1 Predictions Max                               1.24941\r\n",
      "trainer/Q1 Predictions Min                              -3.33057\r\n",
      "trainer/Q2 Predictions Mean                             -1.03732\r\n",
      "trainer/Q2 Predictions Std                               0.92654\r\n",
      "trainer/Q2 Predictions Max                               1.2585\r\n",
      "trainer/Q2 Predictions Min                              -3.32075\r\n",
      "trainer/Q Targets Mean                                  -1.02803\r\n",
      "trainer/Q Targets Std                                    0.927139\r\n",
      "trainer/Q Targets Max                                    1.27629\r\n",
      "trainer/Q Targets Min                                   -3.34463\r\n",
      "trainer/Log Pis Mean                                     2.03596\r\n",
      "trainer/Log Pis Std                                      1.29991\r\n",
      "trainer/Log Pis Max                                      4.26897\r\n",
      "trainer/Log Pis Min                                     -3.0998\r\n",
      "trainer/Policy mu Mean                                  -0.00950598\r\n",
      "trainer/Policy mu Std                                    0.351936\r\n",
      "trainer/Policy mu Max                                    2.28394\r\n",
      "trainer/Policy mu Min                                   -1.84048\r\n",
      "trainer/Policy log std Mean                             -2.29362\r\n",
      "trainer/Policy log std Std                               0.646054\r\n",
      "trainer/Policy log std Max                              -0.123959\r\n",
      "trainer/Policy log std Min                              -3.19659\r\n",
      "trainer/Alpha                                            0.0244861\r\n",
      "trainer/Alpha Loss                                       0.133436\r\n",
      "exploration/num steps total                          10400\r\n",
      "exploration/num paths total                            520\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0474226\r\n",
      "exploration/Rewards Std                                  0.0751239\r\n",
      "exploration/Rewards Max                                  0.0642233\r\n",
      "exploration/Rewards Min                                 -0.232944\r\n",
      "exploration/Returns Mean                                -0.948451\r\n",
      "exploration/Returns Std                                  1.09127\r\n",
      "exploration/Returns Max                                  0.162239\r\n",
      "exploration/Returns Min                                 -2.98206\r\n",
      "exploration/Actions Mean                                -0.000756202\r\n",
      "exploration/Actions Std                                  0.143882\r\n",
      "exploration/Actions Max                                  0.557134\r\n",
      "exploration/Actions Min                                 -0.768123\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -0.948451\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.279471\r\n",
      "exploration/env_infos/final/reward_dist Std              0.198886\r\n",
      "exploration/env_infos/final/reward_dist Max              0.579468\r\n",
      "exploration/env_infos/final/reward_dist Min              1.35962e-09\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00176598\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0029237\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.00758152\r\n",
      "exploration/env_infos/initial/reward_dist Min            6.02475e-06\r\n",
      "exploration/env_infos/reward_dist Mean                   0.313747\r\n",
      "exploration/env_infos/reward_dist Std                    0.331051\r\n",
      "exploration/env_infos/reward_dist Max                    0.994138\r\n",
      "exploration/env_infos/reward_dist Min                    1.35962e-09\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.186314\r\n",
      "exploration/env_infos/final/reward_energy Std            0.107127\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0519045\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.378223\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.273843\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.272559\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0638855\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.77913\r\n",
      "exploration/env_infos/reward_energy Mean                -0.154393\r\n",
      "exploration/env_infos/reward_energy Std                  0.132545\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00433585\r\n",
      "exploration/env_infos/reward_energy Min                 -0.77913\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0196306\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.209946\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.279164\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.281417\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.0018457\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0135348\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0160664\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0384062\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.0166589\r\n",
      "exploration/env_infos/end_effector_loc Std               0.140372\r\n",
      "exploration/env_infos/end_effector_loc Max               0.296906\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.281417\r\n",
      "evaluation/num steps total                           94000\r\n",
      "evaluation/num paths total                            4700\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.041777\r\n",
      "evaluation/Rewards Std                                   0.0771634\r\n",
      "evaluation/Rewards Max                                   0.142204\r\n",
      "evaluation/Rewards Min                                  -0.439577\r\n",
      "evaluation/Returns Mean                                 -0.83554\r\n",
      "evaluation/Returns Std                                   1.14921\r\n",
      "evaluation/Returns Max                                   1.20556\r\n",
      "evaluation/Returns Min                                  -3.84012\r\n",
      "evaluation/Actions Mean                                  0.000223204\r\n",
      "evaluation/Actions Std                                   0.0739557\r\n",
      "evaluation/Actions Max                                   0.662821\r\n",
      "evaluation/Actions Min                                  -0.750245\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.83554\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.243994\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.300124\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.948336\r\n",
      "evaluation/env_infos/final/reward_dist Min               6.78386e-18\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00569732\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0100974\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0407956\r\n",
      "evaluation/env_infos/initial/reward_dist Min             1.26062e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.197057\r\n",
      "evaluation/env_infos/reward_dist Std                     0.285154\r\n",
      "evaluation/env_infos/reward_dist Max                     0.993238\r\n",
      "evaluation/env_infos/reward_dist Min                     6.78386e-18\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0403232\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.03679\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00255966\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.172251\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.21221\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.214899\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.014303\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.75515\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0571961\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0875648\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000656482\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.812033\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0063101\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.243653\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.700578\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.44369\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000201977\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.010676\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0301827\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0375122\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.000471489\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.154048\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.700578\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.44369\r\n",
      "time/data storing (s)                                    0.00328874\r\n",
      "time/evaluation sampling (s)                             1.33629\r\n",
      "time/exploration sampling (s)                            0.139918\r\n",
      "time/logging (s)                                         0.0207194\r\n",
      "time/saving (s)                                          0.029214\r\n",
      "time/training (s)                                       59.904\r\n",
      "time/epoch (s)                                          61.4334\r\n",
      "time/total (s)                                        4968.29\r\n",
      "Epoch                                                   93\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:37:08.130816 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 94 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000688004\r\n",
      "trainer/QF2 Loss                                         0.000717312\r\n",
      "trainer/Policy Loss                                      3.06034\r\n",
      "trainer/Q1 Predictions Mean                             -0.982229\r\n",
      "trainer/Q1 Predictions Std                               0.874711\r\n",
      "trainer/Q1 Predictions Max                               1.2721\r\n",
      "trainer/Q1 Predictions Min                              -3.23544\r\n",
      "trainer/Q2 Predictions Mean                             -0.973025\r\n",
      "trainer/Q2 Predictions Std                               0.871957\r\n",
      "trainer/Q2 Predictions Max                               1.28832\r\n",
      "trainer/Q2 Predictions Min                              -3.29083\r\n",
      "trainer/Q Targets Mean                                  -0.974871\r\n",
      "trainer/Q Targets Std                                    0.872212\r\n",
      "trainer/Q Targets Max                                    1.26761\r\n",
      "trainer/Q Targets Min                                   -3.25212\r\n",
      "trainer/Log Pis Mean                                     2.08728\r\n",
      "trainer/Log Pis Std                                      1.50328\r\n",
      "trainer/Log Pis Max                                      4.72616\r\n",
      "trainer/Log Pis Min                                     -4.13195\r\n",
      "trainer/Policy mu Mean                                   0.0239979\r\n",
      "trainer/Policy mu Std                                    0.342394\r\n",
      "trainer/Policy mu Max                                    2.52072\r\n",
      "trainer/Policy mu Min                                   -2.29813\r\n",
      "trainer/Policy log std Mean                             -2.39093\r\n",
      "trainer/Policy log std Std                               0.687371\r\n",
      "trainer/Policy log std Max                              -0.0266174\r\n",
      "trainer/Policy log std Min                              -3.29567\r\n",
      "trainer/Alpha                                            0.0238707\r\n",
      "trainer/Alpha Loss                                       0.325977\r\n",
      "exploration/num steps total                          10500\r\n",
      "exploration/num paths total                            525\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0745564\r\n",
      "exploration/Rewards Std                                  0.0731778\r\n",
      "exploration/Rewards Max                                  0.0799721\r\n",
      "exploration/Rewards Min                                 -0.332349\r\n",
      "exploration/Returns Mean                                -1.49113\r\n",
      "exploration/Returns Std                                  0.672955\r\n",
      "exploration/Returns Max                                 -0.819372\r\n",
      "exploration/Returns Min                                 -2.76734\r\n",
      "exploration/Actions Mean                                -0.00314385\r\n",
      "exploration/Actions Std                                  0.166792\r\n",
      "exploration/Actions Max                                  0.669795\r\n",
      "exploration/Actions Min                                 -0.663214\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -1.49113\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.118917\r\n",
      "exploration/env_infos/final/reward_dist Std              0.205142\r\n",
      "exploration/env_infos/final/reward_dist Max              0.527486\r\n",
      "exploration/env_infos/final/reward_dist Min              3.49265e-07\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00799545\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.00992791\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0273758\r\n",
      "exploration/env_infos/initial/reward_dist Min            0.000600852\r\n",
      "exploration/env_infos/reward_dist Mean                   0.269861\r\n",
      "exploration/env_infos/reward_dist Std                    0.304526\r\n",
      "exploration/env_infos/reward_dist Max                    0.999244\r\n",
      "exploration/env_infos/reward_dist Min                    3.49265e-07\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.110215\r\n",
      "exploration/env_infos/final/reward_energy Std            0.0768455\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0276589\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.248298\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.382988\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.269855\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0916288\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.721265\r\n",
      "exploration/env_infos/reward_energy Mean                -0.162365\r\n",
      "exploration/env_infos/reward_energy Std                  0.171163\r\n",
      "exploration/env_infos/reward_energy Max                 -0.0109869\r\n",
      "exploration/env_infos/reward_energy Min                 -0.851149\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0351003\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.252962\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.411933\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.369028\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.000762092\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0165468\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0334898\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0331607\r\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0133455\r\n",
      "exploration/env_infos/end_effector_loc Std               0.168502\r\n",
      "exploration/env_infos/end_effector_loc Max               0.411933\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.369028\r\n",
      "evaluation/num steps total                           95000\r\n",
      "evaluation/num paths total                            4750\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0551627\r\n",
      "evaluation/Rewards Std                                   0.0801043\r\n",
      "evaluation/Rewards Max                                   0.151601\r\n",
      "evaluation/Rewards Min                                  -0.481389\r\n",
      "evaluation/Returns Mean                                 -1.10325\r\n",
      "evaluation/Returns Std                                   1.14044\r\n",
      "evaluation/Returns Max                                   1.56293\r\n",
      "evaluation/Returns Min                                  -3.84451\r\n",
      "evaluation/Actions Mean                                  0.00349118\r\n",
      "evaluation/Actions Std                                   0.0815461\r\n",
      "evaluation/Actions Max                                   0.59823\r\n",
      "evaluation/Actions Min                                  -0.820185\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -1.10325\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.136655\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.241591\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.787572\r\n",
      "evaluation/env_infos/final/reward_dist Min               6.50086e-66\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00696124\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0129431\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0651122\r\n",
      "evaluation/env_infos/initial/reward_dist Min             2.6294e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.170711\r\n",
      "evaluation/env_infos/reward_dist Std                     0.271852\r\n",
      "evaluation/env_infos/reward_dist Max                     0.999358\r\n",
      "evaluation/env_infos/reward_dist Min                     6.50086e-66\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0459941\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0810337\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00257424\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.572593\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.255375\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.241591\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.000679267\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.01518\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0648959\r\n",
      "evaluation/env_infos/reward_energy Std                   0.095459\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.00066192\r\n",
      "evaluation/env_infos/reward_energy Min                  -1.01518\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0190783\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.280253\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          1\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.558225\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00185366\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0122899\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0299115\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0410092\r\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00182413\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.175094\r\n",
      "evaluation/env_infos/end_effector_loc Max                1\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.558225\r\n",
      "time/data storing (s)                                    0.00305937\r\n",
      "time/evaluation sampling (s)                             1.00254\r\n",
      "time/exploration sampling (s)                            0.168176\r\n",
      "time/logging (s)                                         0.0214288\r\n",
      "time/saving (s)                                          0.0301568\r\n",
      "time/training (s)                                       59.6287\r\n",
      "time/epoch (s)                                          60.8541\r\n",
      "time/total (s)                                        5030.33\r\n",
      "Epoch                                                   94\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:38:00.949973 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 95 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000758044\n",
      "trainer/QF2 Loss                                         0.00111567\n",
      "trainer/Policy Loss                                      2.8698\n",
      "trainer/Q1 Predictions Mean                             -0.940152\n",
      "trainer/Q1 Predictions Std                               0.885712\n",
      "trainer/Q1 Predictions Max                               1.19003\n",
      "trainer/Q1 Predictions Min                              -3.26018\n",
      "trainer/Q2 Predictions Mean                             -0.937716\n",
      "trainer/Q2 Predictions Std                               0.89208\n",
      "trainer/Q2 Predictions Max                               1.20477\n",
      "trainer/Q2 Predictions Min                              -3.26447\n",
      "trainer/Q Targets Mean                                  -0.943889\n",
      "trainer/Q Targets Std                                    0.892841\n",
      "trainer/Q Targets Max                                    1.17051\n",
      "trainer/Q Targets Min                                   -3.26833\n",
      "trainer/Log Pis Mean                                     1.93788\n",
      "trainer/Log Pis Std                                      1.44957\n",
      "trainer/Log Pis Max                                      4.32512\n",
      "trainer/Log Pis Min                                     -4.86589\n",
      "trainer/Policy mu Mean                                   0.00882974\n",
      "trainer/Policy mu Std                                    0.324555\n",
      "trainer/Policy mu Max                                    2.94644\n",
      "trainer/Policy mu Min                                   -1.5805\n",
      "trainer/Policy log std Mean                             -2.29702\n",
      "trainer/Policy log std Std                               0.636705\n",
      "trainer/Policy log std Max                               0.21114\n",
      "trainer/Policy log std Min                              -3.34425\n",
      "trainer/Alpha                                            0.0245923\n",
      "trainer/Alpha Loss                                      -0.230166\n",
      "exploration/num steps total                          10600\n",
      "exploration/num paths total                            530\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0627204\n",
      "exploration/Rewards Std                                  0.105086\n",
      "exploration/Rewards Max                                  0.13097\n",
      "exploration/Rewards Min                                 -0.345744\n",
      "exploration/Returns Mean                                -1.25441\n",
      "exploration/Returns Std                                  1.6589\n",
      "exploration/Returns Max                                  1.1709\n",
      "exploration/Returns Min                                 -3.80937\n",
      "exploration/Actions Mean                                -0.00101617\n",
      "exploration/Actions Std                                  0.212225\n",
      "exploration/Actions Max                                  0.834595\n",
      "exploration/Actions Min                                 -0.786222\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.25441\n",
      "exploration/env_infos/final/reward_dist Mean             0.267103\n",
      "exploration/env_infos/final/reward_dist Std              0.248473\n",
      "exploration/env_infos/final/reward_dist Max              0.721079\n",
      "exploration/env_infos/final/reward_dist Min              0.000264099\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0123459\n",
      "exploration/env_infos/initial/reward_dist Std            0.0112297\n",
      "exploration/env_infos/initial/reward_dist Max            0.0291787\n",
      "exploration/env_infos/initial/reward_dist Min            2.1233e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.224949\n",
      "exploration/env_infos/reward_dist Std                    0.277695\n",
      "exploration/env_infos/reward_dist Max                    0.994677\n",
      "exploration/env_infos/reward_dist Min                    2.1233e-05\n",
      "exploration/env_infos/final/reward_energy Mean          -0.17428\n",
      "exploration/env_infos/final/reward_energy Std            0.107365\n",
      "exploration/env_infos/final/reward_energy Max           -0.0334332\n",
      "exploration/env_infos/final/reward_energy Min           -0.335858\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.380144\n",
      "exploration/env_infos/initial/reward_energy Std          0.15034\n",
      "exploration/env_infos/initial/reward_energy Max         -0.121184\n",
      "exploration/env_infos/initial/reward_energy Min         -0.537139\n",
      "exploration/env_infos/reward_energy Mean                -0.235651\n",
      "exploration/env_infos/reward_energy Std                  0.185875\n",
      "exploration/env_infos/reward_energy Max                 -0.0220533\n",
      "exploration/env_infos/reward_energy Min                 -0.85963\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0543693\n",
      "exploration/env_infos/final/end_effector_loc Std         0.228366\n",
      "exploration/env_infos/final/end_effector_loc Max         0.453216\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.31111\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00406145\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0138706\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0217428\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0130313\n",
      "exploration/env_infos/end_effector_loc Mean              0.0610448\n",
      "exploration/env_infos/end_effector_loc Std               0.170153\n",
      "exploration/env_infos/end_effector_loc Max               0.477645\n",
      "exploration/env_infos/end_effector_loc Min              -0.31111\n",
      "evaluation/num steps total                           96000\n",
      "evaluation/num paths total                            4800\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0584013\n",
      "evaluation/Rewards Std                                   0.0963796\n",
      "evaluation/Rewards Max                                   0.185632\n",
      "evaluation/Rewards Min                                  -1.05927\n",
      "evaluation/Returns Mean                                 -1.16803\n",
      "evaluation/Returns Std                                   1.39482\n",
      "evaluation/Returns Max                                   2.27492\n",
      "evaluation/Returns Min                                  -5.34732\n",
      "evaluation/Actions Mean                                  0.00361708\n",
      "evaluation/Actions Std                                   0.0877271\n",
      "evaluation/Actions Max                                   0.827899\n",
      "evaluation/Actions Min                                  -0.77107\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.16803\n",
      "evaluation/env_infos/final/reward_dist Mean              0.121693\n",
      "evaluation/env_infos/final/reward_dist Std               0.200953\n",
      "evaluation/env_infos/final/reward_dist Max               0.809957\n",
      "evaluation/env_infos/final/reward_dist Min               1.32103e-76\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00487069\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00696911\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0228596\n",
      "evaluation/env_infos/initial/reward_dist Min             3.31288e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.190191\n",
      "evaluation/env_infos/reward_dist Std                     0.27434\n",
      "evaluation/env_infos/reward_dist Max                     0.997352\n",
      "evaluation/env_infos/reward_dist Min                     1.32103e-76\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0513059\n",
      "evaluation/env_infos/final/reward_energy Std             0.114378\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00146481\n",
      "evaluation/env_infos/final/reward_energy Min            -0.814317\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.263047\n",
      "evaluation/env_infos/initial/reward_energy Std           0.231751\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0151685\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.977458\n",
      "evaluation/env_infos/reward_energy Mean                 -0.06431\n",
      "evaluation/env_infos/reward_energy Std                   0.106219\n",
      "evaluation/env_infos/reward_energy Max                  -0.00146481\n",
      "evaluation/env_infos/reward_energy Min                  -1.16607\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0224897\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.305299\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.889316\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000159913\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0123936\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0312294\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0385535\n",
      "evaluation/env_infos/end_effector_loc Mean               0.00742768\n",
      "evaluation/env_infos/end_effector_loc Std                0.182973\n",
      "evaluation/env_infos/end_effector_loc Max                0.889316\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.00300627\n",
      "time/evaluation sampling (s)                             0.953054\n",
      "time/exploration sampling (s)                            0.120029\n",
      "time/logging (s)                                         0.0201578\n",
      "time/saving (s)                                          0.028441\n",
      "time/training (s)                                       50.4415\n",
      "time/epoch (s)                                          51.5662\n",
      "time/total (s)                                        5083.14\n",
      "Epoch                                                   95\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:38:52.855128 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 96 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                    2000\r\n",
      "trainer/QF1 Loss                                         0.000856302\r\n",
      "trainer/QF2 Loss                                         0.00066757\r\n",
      "trainer/Policy Loss                                      3.04197\r\n",
      "trainer/Q1 Predictions Mean                             -0.914488\r\n",
      "trainer/Q1 Predictions Std                               0.89333\r\n",
      "trainer/Q1 Predictions Max                               1.21323\r\n",
      "trainer/Q1 Predictions Min                              -3.05841\r\n",
      "trainer/Q2 Predictions Mean                             -0.922874\r\n",
      "trainer/Q2 Predictions Std                               0.894385\r\n",
      "trainer/Q2 Predictions Max                               1.17264\r\n",
      "trainer/Q2 Predictions Min                              -3.08942\r\n",
      "trainer/Q Targets Mean                                  -0.92274\r\n",
      "trainer/Q Targets Std                                    0.896049\r\n",
      "trainer/Q Targets Max                                    1.18823\r\n",
      "trainer/Q Targets Min                                   -3.14664\r\n",
      "trainer/Log Pis Mean                                     2.1362\r\n",
      "trainer/Log Pis Std                                      1.43315\r\n",
      "trainer/Log Pis Max                                      4.63741\r\n",
      "trainer/Log Pis Min                                     -4.19714\r\n",
      "trainer/Policy mu Mean                                   0.000297535\r\n",
      "trainer/Policy mu Std                                    0.280818\r\n",
      "trainer/Policy mu Max                                    2.38247\r\n",
      "trainer/Policy mu Min                                   -1.83082\r\n",
      "trainer/Policy log std Mean                             -2.38514\r\n",
      "trainer/Policy log std Std                               0.584461\r\n",
      "trainer/Policy log std Max                              -0.67978\r\n",
      "trainer/Policy log std Min                              -3.30892\r\n",
      "trainer/Alpha                                            0.0237794\r\n",
      "trainer/Alpha Loss                                       0.50932\r\n",
      "exploration/num steps total                          10700\r\n",
      "exploration/num paths total                            535\r\n",
      "exploration/path length Mean                            20\r\n",
      "exploration/path length Std                              0\r\n",
      "exploration/path length Max                             20\r\n",
      "exploration/path length Min                             20\r\n",
      "exploration/Rewards Mean                                -0.0418833\r\n",
      "exploration/Rewards Std                                  0.0852786\r\n",
      "exploration/Rewards Max                                  0.0890087\r\n",
      "exploration/Rewards Min                                 -0.379224\r\n",
      "exploration/Returns Mean                                -0.837666\r\n",
      "exploration/Returns Std                                  1.36166\r\n",
      "exploration/Returns Max                                  0.510004\r\n",
      "exploration/Returns Min                                 -2.82186\r\n",
      "exploration/Actions Mean                                 0.00518576\r\n",
      "exploration/Actions Std                                  0.120675\r\n",
      "exploration/Actions Max                                  0.456777\r\n",
      "exploration/Actions Min                                 -0.609062\r\n",
      "exploration/Num Paths                                    5\r\n",
      "exploration/Average Returns                             -0.837666\r\n",
      "exploration/env_infos/final/reward_dist Mean             0.207554\r\n",
      "exploration/env_infos/final/reward_dist Std              0.320219\r\n",
      "exploration/env_infos/final/reward_dist Max              0.829304\r\n",
      "exploration/env_infos/final/reward_dist Min              0.000199841\r\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0133204\r\n",
      "exploration/env_infos/initial/reward_dist Std            0.0165284\r\n",
      "exploration/env_infos/initial/reward_dist Max            0.0458353\r\n",
      "exploration/env_infos/initial/reward_dist Min            0.0029938\r\n",
      "exploration/env_infos/reward_dist Mean                   0.213549\r\n",
      "exploration/env_infos/reward_dist Std                    0.269068\r\n",
      "exploration/env_infos/reward_dist Max                    0.959488\r\n",
      "exploration/env_infos/reward_dist Min                    0.000199841\r\n",
      "exploration/env_infos/final/reward_energy Mean          -0.141519\r\n",
      "exploration/env_infos/final/reward_energy Std            0.108185\r\n",
      "exploration/env_infos/final/reward_energy Max           -0.0165354\r\n",
      "exploration/env_infos/final/reward_energy Min           -0.323003\r\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.35154\r\n",
      "exploration/env_infos/initial/reward_energy Std          0.177474\r\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0829441\r\n",
      "exploration/env_infos/initial/reward_energy Min         -0.61416\r\n",
      "exploration/env_infos/reward_energy Mean                -0.131261\r\n",
      "exploration/env_infos/reward_energy Std                  0.109312\r\n",
      "exploration/env_infos/reward_energy Max                 -0.00862862\r\n",
      "exploration/env_infos/reward_energy Min                 -0.61416\r\n",
      "exploration/env_infos/final/reward_safety Mean           0\r\n",
      "exploration/env_infos/final/reward_safety Std            0\r\n",
      "exploration/env_infos/final/reward_safety Max            0\r\n",
      "exploration/env_infos/final/reward_safety Min            0\r\n",
      "exploration/env_infos/initial/reward_safety Mean         0\r\n",
      "exploration/env_infos/initial/reward_safety Std          0\r\n",
      "exploration/env_infos/initial/reward_safety Max          0\r\n",
      "exploration/env_infos/initial/reward_safety Min          0\r\n",
      "exploration/env_infos/reward_safety Mean                 0\r\n",
      "exploration/env_infos/reward_safety Std                  0\r\n",
      "exploration/env_infos/reward_safety Max                  0\r\n",
      "exploration/env_infos/reward_safety Min                  0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0545678\r\n",
      "exploration/env_infos/final/end_effector_loc Std         0.146088\r\n",
      "exploration/env_infos/final/end_effector_loc Max         0.263877\r\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.152186\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00190476\r\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.013792\r\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0228388\r\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0304531\r\n",
      "exploration/env_infos/end_effector_loc Mean              0.0201514\r\n",
      "exploration/env_infos/end_effector_loc Std               0.124326\r\n",
      "exploration/env_infos/end_effector_loc Max               0.263877\r\n",
      "exploration/env_infos/end_effector_loc Min              -0.24456\r\n",
      "evaluation/num steps total                           97000\r\n",
      "evaluation/num paths total                            4850\r\n",
      "evaluation/path length Mean                             20\r\n",
      "evaluation/path length Std                               0\r\n",
      "evaluation/path length Max                              20\r\n",
      "evaluation/path length Min                              20\r\n",
      "evaluation/Rewards Mean                                 -0.0304878\r\n",
      "evaluation/Rewards Std                                   0.073711\r\n",
      "evaluation/Rewards Max                                   0.158272\r\n",
      "evaluation/Rewards Min                                  -0.30184\r\n",
      "evaluation/Returns Mean                                 -0.609756\r\n",
      "evaluation/Returns Std                                   1.1213\r\n",
      "evaluation/Returns Max                                   2.30432\r\n",
      "evaluation/Returns Min                                  -2.77697\r\n",
      "evaluation/Actions Mean                                  0.00240792\r\n",
      "evaluation/Actions Std                                   0.0757135\r\n",
      "evaluation/Actions Max                                   0.569313\r\n",
      "evaluation/Actions Min                                  -0.833519\r\n",
      "evaluation/Num Paths                                    50\r\n",
      "evaluation/Average Returns                              -0.609756\r\n",
      "evaluation/env_infos/final/reward_dist Mean              0.141798\r\n",
      "evaluation/env_infos/final/reward_dist Std               0.234228\r\n",
      "evaluation/env_infos/final/reward_dist Max               0.86428\r\n",
      "evaluation/env_infos/final/reward_dist Min               4.64787e-34\r\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0122134\r\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0214538\r\n",
      "evaluation/env_infos/initial/reward_dist Max             0.112471\r\n",
      "evaluation/env_infos/initial/reward_dist Min             2.29793e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                    0.197493\r\n",
      "evaluation/env_infos/reward_dist Std                     0.267954\r\n",
      "evaluation/env_infos/reward_dist Max                     0.997187\r\n",
      "evaluation/env_infos/reward_dist Min                     4.64787e-34\r\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.037745\r\n",
      "evaluation/env_infos/final/reward_energy Std             0.0335232\r\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00965169\r\n",
      "evaluation/env_infos/final/reward_energy Min            -0.190099\r\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.266814\r\n",
      "evaluation/env_infos/initial/reward_energy Std           0.211506\r\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.013866\r\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.944848\r\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0633629\r\n",
      "evaluation/env_infos/reward_energy Std                   0.0863817\r\n",
      "evaluation/env_infos/reward_energy Max                  -0.000306703\r\n",
      "evaluation/env_infos/reward_energy Min                  -0.944848\r\n",
      "evaluation/env_infos/final/reward_safety Mean            0\r\n",
      "evaluation/env_infos/final/reward_safety Std             0\r\n",
      "evaluation/env_infos/final/reward_safety Max             0\r\n",
      "evaluation/env_infos/final/reward_safety Min             0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\r\n",
      "evaluation/env_infos/initial/reward_safety Std           0\r\n",
      "evaluation/env_infos/initial/reward_safety Max           0\r\n",
      "evaluation/env_infos/initial/reward_safety Min           0\r\n",
      "evaluation/env_infos/reward_safety Mean                  0\r\n",
      "evaluation/env_infos/reward_safety Std                   0\r\n",
      "evaluation/env_infos/reward_safety Max                   0\r\n",
      "evaluation/env_infos/reward_safety Min                   0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0334023\r\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.25405\r\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.680096\r\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.740994\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000306194\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0120338\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0284656\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0416759\r\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0140236\r\n",
      "evaluation/env_infos/end_effector_loc Std                0.164969\r\n",
      "evaluation/env_infos/end_effector_loc Max                0.680096\r\n",
      "evaluation/env_infos/end_effector_loc Min               -0.740994\r\n",
      "time/data storing (s)                                    0.00284051\r\n",
      "time/evaluation sampling (s)                             0.950764\r\n",
      "time/exploration sampling (s)                            0.120982\r\n",
      "time/logging (s)                                         0.0209157\r\n",
      "time/saving (s)                                          0.0320069\r\n",
      "time/training (s)                                       49.5346\r\n",
      "time/epoch (s)                                          50.6621\r\n",
      "time/total (s)                                        5135.04\r\n",
      "Epoch                                                   96\r\n",
      "---------------------------------------------------  ---------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:39:47.092994 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 97 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000648741\n",
      "trainer/QF2 Loss                                         0.000595328\n",
      "trainer/Policy Loss                                      2.7864\n",
      "trainer/Q1 Predictions Mean                             -0.879615\n",
      "trainer/Q1 Predictions Std                               0.879684\n",
      "trainer/Q1 Predictions Max                               1.37787\n",
      "trainer/Q1 Predictions Min                              -3.29309\n",
      "trainer/Q2 Predictions Mean                             -0.88585\n",
      "trainer/Q2 Predictions Std                               0.873978\n",
      "trainer/Q2 Predictions Max                               1.36005\n",
      "trainer/Q2 Predictions Min                              -3.19472\n",
      "trainer/Q Targets Mean                                  -0.88556\n",
      "trainer/Q Targets Std                                    0.87643\n",
      "trainer/Q Targets Max                                    1.3235\n",
      "trainer/Q Targets Min                                   -3.1628\n",
      "trainer/Log Pis Mean                                     1.91316\n",
      "trainer/Log Pis Std                                      1.41629\n",
      "trainer/Log Pis Max                                      4.52847\n",
      "trainer/Log Pis Min                                     -2.21281\n",
      "trainer/Policy mu Mean                                   0.00313378\n",
      "trainer/Policy mu Std                                    0.326168\n",
      "trainer/Policy mu Max                                    1.96702\n",
      "trainer/Policy mu Min                                   -2.16815\n",
      "trainer/Policy log std Mean                             -2.28222\n",
      "trainer/Policy log std Std                               0.624367\n",
      "trainer/Policy log std Max                              -0.279602\n",
      "trainer/Policy log std Min                              -3.30772\n",
      "trainer/Alpha                                            0.0231315\n",
      "trainer/Alpha Loss                                      -0.327062\n",
      "exploration/num steps total                          10800\n",
      "exploration/num paths total                            540\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0781071\n",
      "exploration/Rewards Std                                  0.0709478\n",
      "exploration/Rewards Max                                  0.100983\n",
      "exploration/Rewards Min                                 -0.284782\n",
      "exploration/Returns Mean                                -1.56214\n",
      "exploration/Returns Std                                  0.550719\n",
      "exploration/Returns Max                                 -0.734329\n",
      "exploration/Returns Min                                 -2.39623\n",
      "exploration/Actions Mean                                 0.0132154\n",
      "exploration/Actions Std                                  0.137956\n",
      "exploration/Actions Max                                  0.471749\n",
      "exploration/Actions Min                                 -0.605127\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.56214\n",
      "exploration/env_infos/final/reward_dist Mean             0.00683041\n",
      "exploration/env_infos/final/reward_dist Std              0.0125174\n",
      "exploration/env_infos/final/reward_dist Max              0.031852\n",
      "exploration/env_infos/final/reward_dist Min              1.56785e-17\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00742935\n",
      "exploration/env_infos/initial/reward_dist Std            0.00694507\n",
      "exploration/env_infos/initial/reward_dist Max            0.0185226\n",
      "exploration/env_infos/initial/reward_dist Min            5.02147e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.133221\n",
      "exploration/env_infos/reward_dist Std                    0.168488\n",
      "exploration/env_infos/reward_dist Max                    0.585569\n",
      "exploration/env_infos/reward_dist Min                    1.56785e-17\n",
      "exploration/env_infos/final/reward_energy Mean          -0.149151\n",
      "exploration/env_infos/final/reward_energy Std            0.0601879\n",
      "exploration/env_infos/final/reward_energy Max           -0.0913021\n",
      "exploration/env_infos/final/reward_energy Min           -0.245859\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.355656\n",
      "exploration/env_infos/initial/reward_energy Std          0.142056\n",
      "exploration/env_infos/initial/reward_energy Max         -0.144299\n",
      "exploration/env_infos/initial/reward_energy Min         -0.500478\n",
      "exploration/env_infos/reward_energy Mean                -0.153926\n",
      "exploration/env_infos/reward_energy Std                  0.121324\n",
      "exploration/env_infos/reward_energy Max                 -0.00888746\n",
      "exploration/env_infos/reward_energy Min                 -0.62065\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.155983\n",
      "exploration/env_infos/final/end_effector_loc Std         0.22867\n",
      "exploration/env_infos/final/end_effector_loc Max         0.372841\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.39571\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00550856\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0123691\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0235874\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0161442\n",
      "exploration/env_infos/end_effector_loc Mean              0.0653447\n",
      "exploration/env_infos/end_effector_loc Std               0.160035\n",
      "exploration/env_infos/end_effector_loc Max               0.372841\n",
      "exploration/env_infos/end_effector_loc Min              -0.396622\n",
      "evaluation/num steps total                           98000\n",
      "evaluation/num paths total                            4900\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.0579093\n",
      "evaluation/Rewards Std                                   0.0724965\n",
      "evaluation/Rewards Max                                   0.149408\n",
      "evaluation/Rewards Min                                  -0.38742\n",
      "evaluation/Returns Mean                                 -1.15819\n",
      "evaluation/Returns Std                                   1.09349\n",
      "evaluation/Returns Max                                   1.4309\n",
      "evaluation/Returns Min                                  -4.1537\n",
      "evaluation/Actions Mean                                  0.00200762\n",
      "evaluation/Actions Std                                   0.0787153\n",
      "evaluation/Actions Max                                   0.584002\n",
      "evaluation/Actions Min                                  -0.696077\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -1.15819\n",
      "evaluation/env_infos/final/reward_dist Mean              0.105575\n",
      "evaluation/env_infos/final/reward_dist Std               0.221256\n",
      "evaluation/env_infos/final/reward_dist Max               0.891359\n",
      "evaluation/env_infos/final/reward_dist Min               2.12517e-25\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00739259\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0123257\n",
      "evaluation/env_infos/initial/reward_dist Max             0.048806\n",
      "evaluation/env_infos/initial/reward_dist Min             1.20632e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.191279\n",
      "evaluation/env_infos/reward_dist Std                     0.278368\n",
      "evaluation/env_infos/reward_dist Max                     0.995895\n",
      "evaluation/env_infos/reward_dist Min                     2.12517e-25\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0471072\n",
      "evaluation/env_infos/final/reward_energy Std             0.0496696\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00468534\n",
      "evaluation/env_infos/final/reward_energy Min            -0.308491\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.273471\n",
      "evaluation/env_infos/initial/reward_energy Std           0.208606\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0277949\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.908615\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0689311\n",
      "evaluation/env_infos/reward_energy Std                   0.0874572\n",
      "evaluation/env_infos/reward_energy Max                  -0.00178582\n",
      "evaluation/env_infos/reward_energy Min                  -0.908615\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0222133\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.287276\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.724901\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.693056\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.000456067\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.012152\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0292001\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0348038\n",
      "evaluation/env_infos/end_effector_loc Mean               0.00675316\n",
      "evaluation/env_infos/end_effector_loc Std                0.187713\n",
      "evaluation/env_infos/end_effector_loc Max                0.724901\n",
      "evaluation/env_infos/end_effector_loc Min               -0.693056\n",
      "time/data storing (s)                                    0.00304842\n",
      "time/evaluation sampling (s)                             1.46183\n",
      "time/exploration sampling (s)                            0.140613\n",
      "time/logging (s)                                         0.0208108\n",
      "time/saving (s)                                          0.0298506\n",
      "time/training (s)                                       51.2809\n",
      "time/epoch (s)                                          52.9371\n",
      "time/total (s)                                        5189.28\n",
      "Epoch                                                   97\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:40:41.421127 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 98 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.000979636\n",
      "trainer/QF2 Loss                                         0.00145386\n",
      "trainer/Policy Loss                                      2.72805\n",
      "trainer/Q1 Predictions Mean                             -0.879703\n",
      "trainer/Q1 Predictions Std                               0.897893\n",
      "trainer/Q1 Predictions Max                               1.43377\n",
      "trainer/Q1 Predictions Min                              -3.2412\n",
      "trainer/Q2 Predictions Mean                             -0.855599\n",
      "trainer/Q2 Predictions Std                               0.888868\n",
      "trainer/Q2 Predictions Max                               1.42271\n",
      "trainer/Q2 Predictions Min                              -3.21501\n",
      "trainer/Q Targets Mean                                  -0.876059\n",
      "trainer/Q Targets Std                                    0.894003\n",
      "trainer/Q Targets Max                                    1.39547\n",
      "trainer/Q Targets Min                                   -3.23343\n",
      "trainer/Log Pis Mean                                     1.86358\n",
      "trainer/Log Pis Std                                      1.45628\n",
      "trainer/Log Pis Max                                      4.58112\n",
      "trainer/Log Pis Min                                     -4.68844\n",
      "trainer/Policy mu Mean                                   0.0348339\n",
      "trainer/Policy mu Std                                    0.310976\n",
      "trainer/Policy mu Max                                    2.67831\n",
      "trainer/Policy mu Min                                   -1.41446\n",
      "trainer/Policy log std Mean                             -2.31506\n",
      "trainer/Policy log std Std                               0.600367\n",
      "trainer/Policy log std Max                              -0.0463334\n",
      "trainer/Policy log std Min                              -3.26277\n",
      "trainer/Alpha                                            0.0226434\n",
      "trainer/Alpha Loss                                      -0.516654\n",
      "exploration/num steps total                          10900\n",
      "exploration/num paths total                            545\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.0579444\n",
      "exploration/Rewards Std                                  0.100761\n",
      "exploration/Rewards Max                                  0.107177\n",
      "exploration/Rewards Min                                 -0.445149\n",
      "exploration/Returns Mean                                -1.15889\n",
      "exploration/Returns Std                                  1.56214\n",
      "exploration/Returns Max                                  0.804083\n",
      "exploration/Returns Min                                 -3.3982\n",
      "exploration/Actions Mean                                -0.000897653\n",
      "exploration/Actions Std                                  0.197299\n",
      "exploration/Actions Max                                  0.554897\n",
      "exploration/Actions Min                                 -0.5579\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -1.15889\n",
      "exploration/env_infos/final/reward_dist Mean             0.349541\n",
      "exploration/env_infos/final/reward_dist Std              0.304936\n",
      "exploration/env_infos/final/reward_dist Max              0.82714\n",
      "exploration/env_infos/final/reward_dist Min              0.0011835\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00429136\n",
      "exploration/env_infos/initial/reward_dist Std            0.00784501\n",
      "exploration/env_infos/initial/reward_dist Max            0.0199671\n",
      "exploration/env_infos/initial/reward_dist Min            9.34723e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.209032\n",
      "exploration/env_infos/reward_dist Std                    0.21722\n",
      "exploration/env_infos/reward_dist Max                    0.82714\n",
      "exploration/env_infos/reward_dist Min                    9.34723e-06\n",
      "exploration/env_infos/final/reward_energy Mean          -0.242154\n",
      "exploration/env_infos/final/reward_energy Std            0.0764205\n",
      "exploration/env_infos/final/reward_energy Max           -0.124625\n",
      "exploration/env_infos/final/reward_energy Min           -0.314558\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.341317\n",
      "exploration/env_infos/initial/reward_energy Std          0.188436\n",
      "exploration/env_infos/initial/reward_energy Max         -0.120804\n",
      "exploration/env_infos/initial/reward_energy Min         -0.632633\n",
      "exploration/env_infos/reward_energy Mean                -0.233495\n",
      "exploration/env_infos/reward_energy Std                  0.15276\n",
      "exploration/env_infos/reward_energy Max                 -0.0151134\n",
      "exploration/env_infos/reward_energy Min                 -0.645274\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0357071\n",
      "exploration/env_infos/final/end_effector_loc Std         0.210238\n",
      "exploration/env_infos/final/end_effector_loc Max         0.336163\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.275738\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000565689\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0137727\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0277095\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0223589\n",
      "exploration/env_infos/end_effector_loc Mean             -0.029667\n",
      "exploration/env_infos/end_effector_loc Std               0.16221\n",
      "exploration/env_infos/end_effector_loc Max               0.351197\n",
      "exploration/env_infos/end_effector_loc Min              -0.3309\n",
      "evaluation/num steps total                           99000\n",
      "evaluation/num paths total                            4950\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.042962\n",
      "evaluation/Rewards Std                                   0.071765\n",
      "evaluation/Rewards Max                                   0.169266\n",
      "evaluation/Rewards Min                                  -0.355064\n",
      "evaluation/Returns Mean                                 -0.859239\n",
      "evaluation/Returns Std                                   1.04224\n",
      "evaluation/Returns Max                                   1.67012\n",
      "evaluation/Returns Min                                  -2.91675\n",
      "evaluation/Actions Mean                                  0.00209217\n",
      "evaluation/Actions Std                                   0.0767482\n",
      "evaluation/Actions Max                                   0.707976\n",
      "evaluation/Actions Min                                  -0.680147\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -0.859239\n",
      "evaluation/env_infos/final/reward_dist Mean              0.145953\n",
      "evaluation/env_infos/final/reward_dist Std               0.25892\n",
      "evaluation/env_infos/final/reward_dist Max               0.957263\n",
      "evaluation/env_infos/final/reward_dist Min               3.09889e-19\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00595851\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0112068\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0520305\n",
      "evaluation/env_infos/initial/reward_dist Min             1.02967e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.179038\n",
      "evaluation/env_infos/reward_dist Std                     0.272862\n",
      "evaluation/env_infos/reward_dist Max                     0.985024\n",
      "evaluation/env_infos/reward_dist Min                     3.09889e-19\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.0514498\n",
      "evaluation/env_infos/final/reward_energy Std             0.0404716\n",
      "evaluation/env_infos/final/reward_energy Max            -0.00476627\n",
      "evaluation/env_infos/final/reward_energy Min            -0.183021\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.279691\n",
      "evaluation/env_infos/initial/reward_energy Std           0.213724\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0179589\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.780758\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0671767\n",
      "evaluation/env_infos/reward_energy Std                   0.0853031\n",
      "evaluation/env_infos/reward_energy Max                  -0.000705802\n",
      "evaluation/env_infos/reward_energy Min                  -0.780758\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0457971\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.251701\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.619819\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.525132\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00176154\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0123198\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0353988\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0340073\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0227388\n",
      "evaluation/env_infos/end_effector_loc Std                0.168486\n",
      "evaluation/env_infos/end_effector_loc Max                0.619819\n",
      "evaluation/env_infos/end_effector_loc Min               -0.525132\n",
      "time/data storing (s)                                    0.00315893\n",
      "time/evaluation sampling (s)                             1.06305\n",
      "time/exploration sampling (s)                            0.142227\n",
      "time/logging (s)                                         0.0215837\n",
      "time/saving (s)                                          0.0306853\n",
      "time/training (s)                                       51.7518\n",
      "time/epoch (s)                                          53.0125\n",
      "time/total (s)                                        5243.61\n",
      "Epoch                                                   98\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:41:39.166220 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 99 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                     2000\n",
      "trainer/QF1 Loss                                          0.000759883\n",
      "trainer/QF2 Loss                                          0.000806125\n",
      "trainer/Policy Loss                                       2.87161\n",
      "trainer/Q1 Predictions Mean                              -0.899691\n",
      "trainer/Q1 Predictions Std                                0.815776\n",
      "trainer/Q1 Predictions Max                                1.26907\n",
      "trainer/Q1 Predictions Min                               -2.85473\n",
      "trainer/Q2 Predictions Mean                              -0.894161\n",
      "trainer/Q2 Predictions Std                                0.815116\n",
      "trainer/Q2 Predictions Max                                1.28765\n",
      "trainer/Q2 Predictions Min                               -2.85111\n",
      "trainer/Q Targets Mean                                   -0.895059\n",
      "trainer/Q Targets Std                                     0.814831\n",
      "trainer/Q Targets Max                                     1.2531\n",
      "trainer/Q Targets Min                                    -2.80889\n",
      "trainer/Log Pis Mean                                      1.99265\n",
      "trainer/Log Pis Std                                       1.27234\n",
      "trainer/Log Pis Max                                       4.41099\n",
      "trainer/Log Pis Min                                      -1.37371\n",
      "trainer/Policy mu Mean                                   -0.0114338\n",
      "trainer/Policy mu Std                                     0.335486\n",
      "trainer/Policy mu Max                                     1.93696\n",
      "trainer/Policy mu Min                                    -2.46527\n",
      "trainer/Policy log std Mean                              -2.29367\n",
      "trainer/Policy log std Std                                0.581983\n",
      "trainer/Policy log std Max                               -0.357106\n",
      "trainer/Policy log std Min                               -3.18399\n",
      "trainer/Alpha                                             0.0234676\n",
      "trainer/Alpha Loss                                       -0.0275885\n",
      "exploration/num steps total                           11000\n",
      "exploration/num paths total                             550\n",
      "exploration/path length Mean                             20\n",
      "exploration/path length Std                               0\n",
      "exploration/path length Max                              20\n",
      "exploration/path length Min                              20\n",
      "exploration/Rewards Mean                                 -0.084779\n",
      "exploration/Rewards Std                                   0.0708489\n",
      "exploration/Rewards Max                                   0.114574\n",
      "exploration/Rewards Min                                  -0.21966\n",
      "exploration/Returns Mean                                 -1.69558\n",
      "exploration/Returns Std                                   1.11728\n",
      "exploration/Returns Max                                   0.419051\n",
      "exploration/Returns Min                                  -2.83494\n",
      "exploration/Actions Mean                                  0.00309165\n",
      "exploration/Actions Std                                   0.149925\n",
      "exploration/Actions Max                                   0.514514\n",
      "exploration/Actions Min                                  -0.800542\n",
      "exploration/Num Paths                                     5\n",
      "exploration/Average Returns                              -1.69558\n",
      "exploration/env_infos/final/reward_dist Mean              0.289366\n",
      "exploration/env_infos/final/reward_dist Std               0.359168\n",
      "exploration/env_infos/final/reward_dist Max               0.817577\n",
      "exploration/env_infos/final/reward_dist Min               1.10764e-15\n",
      "exploration/env_infos/initial/reward_dist Mean            0.00136419\n",
      "exploration/env_infos/initial/reward_dist Std             0.00237832\n",
      "exploration/env_infos/initial/reward_dist Max             0.00609587\n",
      "exploration/env_infos/initial/reward_dist Min             6.72222e-06\n",
      "exploration/env_infos/reward_dist Mean                    0.0600753\n",
      "exploration/env_infos/reward_dist Std                     0.154825\n",
      "exploration/env_infos/reward_dist Max                     0.817577\n",
      "exploration/env_infos/reward_dist Min                     1.10764e-15\n",
      "exploration/env_infos/final/reward_energy Mean           -0.141545\n",
      "exploration/env_infos/final/reward_energy Std             0.106899\n",
      "exploration/env_infos/final/reward_energy Max            -0.0178928\n",
      "exploration/env_infos/final/reward_energy Min            -0.340662\n",
      "exploration/env_infos/initial/reward_energy Mean         -0.188919\n",
      "exploration/env_infos/initial/reward_energy Std           0.118401\n",
      "exploration/env_infos/initial/reward_energy Max          -0.0357281\n",
      "exploration/env_infos/initial/reward_energy Min          -0.347112\n",
      "exploration/env_infos/reward_energy Mean                 -0.15483\n",
      "exploration/env_infos/reward_energy Std                   0.14492\n",
      "exploration/env_infos/reward_energy Max                  -0.0112202\n",
      "exploration/env_infos/reward_energy Min                  -0.829585\n",
      "exploration/env_infos/final/reward_safety Mean            0\n",
      "exploration/env_infos/final/reward_safety Std             0\n",
      "exploration/env_infos/final/reward_safety Max             0\n",
      "exploration/env_infos/final/reward_safety Min             0\n",
      "exploration/env_infos/initial/reward_safety Mean          0\n",
      "exploration/env_infos/initial/reward_safety Std           0\n",
      "exploration/env_infos/initial/reward_safety Max           0\n",
      "exploration/env_infos/initial/reward_safety Min           0\n",
      "exploration/env_infos/reward_safety Mean                  0\n",
      "exploration/env_infos/reward_safety Std                   0\n",
      "exploration/env_infos/reward_safety Max                   0\n",
      "exploration/env_infos/reward_safety Min                   0\n",
      "exploration/env_infos/final/end_effector_loc Mean         0.104239\n",
      "exploration/env_infos/final/end_effector_loc Std          0.27996\n",
      "exploration/env_infos/final/end_effector_loc Max          0.622515\n",
      "exploration/env_infos/final/end_effector_loc Min         -0.166346\n",
      "exploration/env_infos/initial/end_effector_loc Mean      -0.000658925\n",
      "exploration/env_infos/initial/end_effector_loc Std        0.00785508\n",
      "exploration/env_infos/initial/end_effector_loc Max        0.0151565\n",
      "exploration/env_infos/initial/end_effector_loc Min       -0.0163496\n",
      "exploration/env_infos/end_effector_loc Mean               0.0488755\n",
      "exploration/env_infos/end_effector_loc Std                0.177835\n",
      "exploration/env_infos/end_effector_loc Max                0.622515\n",
      "exploration/env_infos/end_effector_loc Min               -0.166346\n",
      "evaluation/num steps total                           100000\n",
      "evaluation/num paths total                             5000\n",
      "evaluation/path length Mean                              20\n",
      "evaluation/path length Std                                0\n",
      "evaluation/path length Max                               20\n",
      "evaluation/path length Min                               20\n",
      "evaluation/Rewards Mean                                  -0.0555603\n",
      "evaluation/Rewards Std                                    0.0788821\n",
      "evaluation/Rewards Max                                    0.148223\n",
      "evaluation/Rewards Min                                   -0.313124\n",
      "evaluation/Returns Mean                                  -1.11121\n",
      "evaluation/Returns Std                                    1.25351\n",
      "evaluation/Returns Max                                    2.11246\n",
      "evaluation/Returns Min                                   -3.47184\n",
      "evaluation/Actions Mean                                  -0.004054\n",
      "evaluation/Actions Std                                    0.0673975\n",
      "evaluation/Actions Max                                    0.498848\n",
      "evaluation/Actions Min                                   -0.46377\n",
      "evaluation/Num Paths                                     50\n",
      "evaluation/Average Returns                               -1.11121\n",
      "evaluation/env_infos/final/reward_dist Mean               0.15208\n",
      "evaluation/env_infos/final/reward_dist Std                0.220148\n",
      "evaluation/env_infos/final/reward_dist Max                0.778266\n",
      "evaluation/env_infos/final/reward_dist Min                5.51273e-45\n",
      "evaluation/env_infos/initial/reward_dist Mean             0.00657164\n",
      "evaluation/env_infos/initial/reward_dist Std              0.0129755\n",
      "evaluation/env_infos/initial/reward_dist Max              0.071424\n",
      "evaluation/env_infos/initial/reward_dist Min              1.89152e-06\n",
      "evaluation/env_infos/reward_dist Mean                     0.1936\n",
      "evaluation/env_infos/reward_dist Std                      0.281209\n",
      "evaluation/env_infos/reward_dist Max                      0.986701\n",
      "evaluation/env_infos/reward_dist Min                      7.94104e-47\n",
      "evaluation/env_infos/final/reward_energy Mean            -0.048012\n",
      "evaluation/env_infos/final/reward_energy Std              0.0507591\n",
      "evaluation/env_infos/final/reward_energy Max             -0.00544975\n",
      "evaluation/env_infos/final/reward_energy Min             -0.250308\n",
      "evaluation/env_infos/initial/reward_energy Mean          -0.20114\n",
      "evaluation/env_infos/initial/reward_energy Std            0.164995\n",
      "evaluation/env_infos/initial/reward_energy Max           -0.00614818\n",
      "evaluation/env_infos/initial/reward_energy Min           -0.569343\n",
      "evaluation/env_infos/reward_energy Mean                  -0.0603391\n",
      "evaluation/env_infos/reward_energy Std                    0.0740062\n",
      "evaluation/env_infos/reward_energy Max                   -0.000848076\n",
      "evaluation/env_infos/reward_energy Min                   -0.569343\n",
      "evaluation/env_infos/final/reward_safety Mean             0\n",
      "evaluation/env_infos/final/reward_safety Std              0\n",
      "evaluation/env_infos/final/reward_safety Max              0\n",
      "evaluation/env_infos/final/reward_safety Min              0\n",
      "evaluation/env_infos/initial/reward_safety Mean           0\n",
      "evaluation/env_infos/initial/reward_safety Std            0\n",
      "evaluation/env_infos/initial/reward_safety Max            0\n",
      "evaluation/env_infos/initial/reward_safety Min            0\n",
      "evaluation/env_infos/reward_safety Mean                   0\n",
      "evaluation/env_infos/reward_safety Std                    0\n",
      "evaluation/env_infos/reward_safety Max                    0\n",
      "evaluation/env_infos/reward_safety Min                    0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         -0.0113357\n",
      "evaluation/env_infos/final/end_effector_loc Std           0.271391\n",
      "evaluation/env_infos/final/end_effector_loc Max           0.506994\n",
      "evaluation/env_infos/final/end_effector_loc Min          -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean        0.000662461\n",
      "evaluation/env_infos/initial/end_effector_loc Std         0.009174\n",
      "evaluation/env_infos/initial/end_effector_loc Max         0.0249424\n",
      "evaluation/env_infos/initial/end_effector_loc Min        -0.0231885\n",
      "evaluation/env_infos/end_effector_loc Mean                0.000605874\n",
      "evaluation/env_infos/end_effector_loc Std                 0.17629\n",
      "evaluation/env_infos/end_effector_loc Max                 0.506994\n",
      "evaluation/env_infos/end_effector_loc Min                -1\n",
      "time/data storing (s)                                     0.00410688\n",
      "time/evaluation sampling (s)                              1.00906\n",
      "time/exploration sampling (s)                             0.138748\n",
      "time/logging (s)                                          0.0202418\n",
      "time/saving (s)                                           0.0306717\n",
      "time/training (s)                                        55.2412\n",
      "time/epoch (s)                                           56.444\n",
      "time/total (s)                                         5301.35\n",
      "Epoch                                                    99\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:42:46.526881 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 100 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                     2000\n",
      "trainer/QF1 Loss                                          0.000965132\n",
      "trainer/QF2 Loss                                          0.000832035\n",
      "trainer/Policy Loss                                       2.7737\n",
      "trainer/Q1 Predictions Mean                              -0.892382\n",
      "trainer/Q1 Predictions Std                                0.825243\n",
      "trainer/Q1 Predictions Max                                0.66315\n",
      "trainer/Q1 Predictions Min                               -3.44694\n",
      "trainer/Q2 Predictions Mean                              -0.878037\n",
      "trainer/Q2 Predictions Std                                0.822492\n",
      "trainer/Q2 Predictions Max                                0.685681\n",
      "trainer/Q2 Predictions Min                               -3.40979\n",
      "trainer/Q Targets Mean                                   -0.881994\n",
      "trainer/Q Targets Std                                     0.827245\n",
      "trainer/Q Targets Max                                     0.677497\n",
      "trainer/Q Targets Min                                    -3.4456\n",
      "trainer/Log Pis Mean                                      1.89815\n",
      "trainer/Log Pis Std                                       1.51611\n",
      "trainer/Log Pis Max                                       4.4092\n",
      "trainer/Log Pis Min                                      -4.62687\n",
      "trainer/Policy mu Mean                                   -0.00790042\n",
      "trainer/Policy mu Std                                     0.337708\n",
      "trainer/Policy mu Max                                     2.69051\n",
      "trainer/Policy mu Min                                    -2.10549\n",
      "trainer/Policy log std Mean                              -2.3199\n",
      "trainer/Policy log std Std                                0.615579\n",
      "trainer/Policy log std Max                                0.161183\n",
      "trainer/Policy log std Min                               -3.25007\n",
      "trainer/Alpha                                             0.0221412\n",
      "trainer/Alpha Loss                                       -0.388084\n",
      "exploration/num steps total                           11100\n",
      "exploration/num paths total                             555\n",
      "exploration/path length Mean                             20\n",
      "exploration/path length Std                               0\n",
      "exploration/path length Max                              20\n",
      "exploration/path length Min                              20\n",
      "exploration/Rewards Mean                                 -0.0918739\n",
      "exploration/Rewards Std                                   0.111288\n",
      "exploration/Rewards Max                                   0.106438\n",
      "exploration/Rewards Min                                  -0.328153\n",
      "exploration/Returns Mean                                 -1.83748\n",
      "exploration/Returns Std                                   1.94446\n",
      "exploration/Returns Max                                   0.591949\n",
      "exploration/Returns Min                                  -4.9831\n",
      "exploration/Actions Mean                                 -0.00564794\n",
      "exploration/Actions Std                                   0.136297\n",
      "exploration/Actions Max                                   0.623421\n",
      "exploration/Actions Min                                  -0.341401\n",
      "exploration/Num Paths                                     5\n",
      "exploration/Average Returns                              -1.83748\n",
      "exploration/env_infos/final/reward_dist Mean              0.135405\n",
      "exploration/env_infos/final/reward_dist Std               0.267158\n",
      "exploration/env_infos/final/reward_dist Max               0.669691\n",
      "exploration/env_infos/final/reward_dist Min               2.72157e-26\n",
      "exploration/env_infos/initial/reward_dist Mean            0.00843529\n",
      "exploration/env_infos/initial/reward_dist Std             0.00507658\n",
      "exploration/env_infos/initial/reward_dist Max             0.0158763\n",
      "exploration/env_infos/initial/reward_dist Min             1.57802e-05\n",
      "exploration/env_infos/reward_dist Mean                    0.199153\n",
      "exploration/env_infos/reward_dist Std                     0.270742\n",
      "exploration/env_infos/reward_dist Max                     0.990409\n",
      "exploration/env_infos/reward_dist Min                     2.72157e-26\n",
      "exploration/env_infos/final/reward_energy Mean           -0.151234\n",
      "exploration/env_infos/final/reward_energy Std             0.0398601\n",
      "exploration/env_infos/final/reward_energy Max            -0.0915336\n",
      "exploration/env_infos/final/reward_energy Min            -0.208651\n",
      "exploration/env_infos/initial/reward_energy Mean         -0.370278\n",
      "exploration/env_infos/initial/reward_energy Std           0.180914\n",
      "exploration/env_infos/initial/reward_energy Max          -0.0525186\n",
      "exploration/env_infos/initial/reward_energy Min          -0.543474\n",
      "exploration/env_infos/reward_energy Mean                 -0.156587\n",
      "exploration/env_infos/reward_energy Std                   0.112686\n",
      "exploration/env_infos/reward_energy Max                  -0.00823525\n",
      "exploration/env_infos/reward_energy Min                  -0.628572\n",
      "exploration/env_infos/final/reward_safety Mean            0\n",
      "exploration/env_infos/final/reward_safety Std             0\n",
      "exploration/env_infos/final/reward_safety Max             0\n",
      "exploration/env_infos/final/reward_safety Min             0\n",
      "exploration/env_infos/initial/reward_safety Mean          0\n",
      "exploration/env_infos/initial/reward_safety Std           0\n",
      "exploration/env_infos/initial/reward_safety Max           0\n",
      "exploration/env_infos/initial/reward_safety Min           0\n",
      "exploration/env_infos/reward_safety Mean                  0\n",
      "exploration/env_infos/reward_safety Std                   0\n",
      "exploration/env_infos/reward_safety Max                   0\n",
      "exploration/env_infos/reward_safety Min                   0\n",
      "exploration/env_infos/final/end_effector_loc Mean        -0.0280966\n",
      "exploration/env_infos/final/end_effector_loc Std          0.33285\n",
      "exploration/env_infos/final/end_effector_loc Max          0.572782\n",
      "exploration/env_infos/final/end_effector_loc Min         -0.569061\n",
      "exploration/env_infos/initial/end_effector_loc Mean       0.00356501\n",
      "exploration/env_infos/initial/end_effector_loc Std        0.0141275\n",
      "exploration/env_infos/initial/end_effector_loc Max        0.0266993\n",
      "exploration/env_infos/initial/end_effector_loc Min       -0.0123898\n",
      "exploration/env_infos/end_effector_loc Mean               0.0067772\n",
      "exploration/env_infos/end_effector_loc Std                0.224733\n",
      "exploration/env_infos/end_effector_loc Max                0.572782\n",
      "exploration/env_infos/end_effector_loc Min               -0.569061\n",
      "evaluation/num steps total                           101000\n",
      "evaluation/num paths total                             5050\n",
      "evaluation/path length Mean                              20\n",
      "evaluation/path length Std                                0\n",
      "evaluation/path length Max                               20\n",
      "evaluation/path length Min                               20\n",
      "evaluation/Rewards Mean                                  -0.0440986\n",
      "evaluation/Rewards Std                                    0.078739\n",
      "evaluation/Rewards Max                                    0.1403\n",
      "evaluation/Rewards Min                                   -0.359195\n",
      "evaluation/Returns Mean                                  -0.881971\n",
      "evaluation/Returns Std                                    1.21911\n",
      "evaluation/Returns Max                                    1.93593\n",
      "evaluation/Returns Min                                   -3.69568\n",
      "evaluation/Actions Mean                                  -0.00303259\n",
      "evaluation/Actions Std                                    0.0772697\n",
      "evaluation/Actions Max                                    0.651846\n",
      "evaluation/Actions Min                                   -0.731086\n",
      "evaluation/Num Paths                                     50\n",
      "evaluation/Average Returns                               -0.881971\n",
      "evaluation/env_infos/final/reward_dist Mean               0.183446\n",
      "evaluation/env_infos/final/reward_dist Std                0.275394\n",
      "evaluation/env_infos/final/reward_dist Max                0.980205\n",
      "evaluation/env_infos/final/reward_dist Min                2.69479e-30\n",
      "evaluation/env_infos/initial/reward_dist Mean             0.00631203\n",
      "evaluation/env_infos/initial/reward_dist Std              0.00919806\n",
      "evaluation/env_infos/initial/reward_dist Max              0.0303489\n",
      "evaluation/env_infos/initial/reward_dist Min              1.36774e-06\n",
      "evaluation/env_infos/reward_dist Mean                     0.207551\n",
      "evaluation/env_infos/reward_dist Std                      0.285771\n",
      "evaluation/env_infos/reward_dist Max                      0.999211\n",
      "evaluation/env_infos/reward_dist Min                      2.69479e-30\n",
      "evaluation/env_infos/final/reward_energy Mean            -0.0621978\n",
      "evaluation/env_infos/final/reward_energy Std              0.0706542\n",
      "evaluation/env_infos/final/reward_energy Max             -0.00438076\n",
      "evaluation/env_infos/final/reward_energy Min             -0.401042\n",
      "evaluation/env_infos/initial/reward_energy Mean          -0.266587\n",
      "evaluation/env_infos/initial/reward_energy Std            0.232699\n",
      "evaluation/env_infos/initial/reward_energy Max           -0.0135708\n",
      "evaluation/env_infos/initial/reward_energy Min           -0.877776\n",
      "evaluation/env_infos/reward_energy Mean                  -0.0656029\n",
      "evaluation/env_infos/reward_energy Std                    0.0874977\n",
      "evaluation/env_infos/reward_energy Max                   -0.000565208\n",
      "evaluation/env_infos/reward_energy Min                   -0.877776\n",
      "evaluation/env_infos/final/reward_safety Mean             0\n",
      "evaluation/env_infos/final/reward_safety Std              0\n",
      "evaluation/env_infos/final/reward_safety Max              0\n",
      "evaluation/env_infos/final/reward_safety Min              0\n",
      "evaluation/env_infos/initial/reward_safety Mean           0\n",
      "evaluation/env_infos/initial/reward_safety Std            0\n",
      "evaluation/env_infos/initial/reward_safety Max            0\n",
      "evaluation/env_infos/initial/reward_safety Min            0\n",
      "evaluation/env_infos/reward_safety Mean                   0\n",
      "evaluation/env_infos/reward_safety Std                    0\n",
      "evaluation/env_infos/reward_safety Max                    0\n",
      "evaluation/env_infos/reward_safety Min                    0\n",
      "evaluation/env_infos/final/end_effector_loc Mean          0.0134234\n",
      "evaluation/env_infos/final/end_effector_loc Std           0.263994\n",
      "evaluation/env_infos/final/end_effector_loc Max           0.590147\n",
      "evaluation/env_infos/final/end_effector_loc Min          -0.726812\n",
      "evaluation/env_infos/initial/end_effector_loc Mean        0.00209599\n",
      "evaluation/env_infos/initial/end_effector_loc Std         0.012334\n",
      "evaluation/env_infos/initial/end_effector_loc Max         0.0325923\n",
      "evaluation/env_infos/initial/end_effector_loc Min        -0.0365543\n",
      "evaluation/env_infos/end_effector_loc Mean                0.0172342\n",
      "evaluation/env_infos/end_effector_loc Std                 0.169618\n",
      "evaluation/env_infos/end_effector_loc Max                 0.590147\n",
      "evaluation/env_infos/end_effector_loc Min                -0.726812\n",
      "time/data storing (s)                                     0.00346009\n",
      "time/evaluation sampling (s)                              0.965123\n",
      "time/exploration sampling (s)                             0.221179\n",
      "time/logging (s)                                          0.0198618\n",
      "time/saving (s)                                           0.0582632\n",
      "time/training (s)                                        64.792\n",
      "time/epoch (s)                                           66.0599\n",
      "time/total (s)                                         5368.71\n",
      "Epoch                                                   100\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:43:41.394369 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 101 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                     2000\n",
      "trainer/QF1 Loss                                          0.000842666\n",
      "trainer/QF2 Loss                                          0.000706956\n",
      "trainer/Policy Loss                                       2.65308\n",
      "trainer/Q1 Predictions Mean                              -0.846738\n",
      "trainer/Q1 Predictions Std                                0.822736\n",
      "trainer/Q1 Predictions Max                                1.02256\n",
      "trainer/Q1 Predictions Min                               -3.25493\n",
      "trainer/Q2 Predictions Mean                              -0.832326\n",
      "trainer/Q2 Predictions Std                                0.822987\n",
      "trainer/Q2 Predictions Max                                1.0384\n",
      "trainer/Q2 Predictions Min                               -3.19616\n",
      "trainer/Q Targets Mean                                   -0.835337\n",
      "trainer/Q Targets Std                                     0.817954\n",
      "trainer/Q Targets Max                                     1.00351\n",
      "trainer/Q Targets Min                                    -3.27918\n",
      "trainer/Log Pis Mean                                      1.82776\n",
      "trainer/Log Pis Std                                       1.41824\n",
      "trainer/Log Pis Max                                       5.47241\n",
      "trainer/Log Pis Min                                      -5.6143\n",
      "trainer/Policy mu Mean                                   -0.0673202\n",
      "trainer/Policy mu Std                                     0.434096\n",
      "trainer/Policy mu Max                                     2.91415\n",
      "trainer/Policy mu Min                                    -2.33324\n",
      "trainer/Policy log std Mean                              -2.19868\n",
      "trainer/Policy log std Std                                0.63582\n",
      "trainer/Policy log std Max                               -0.166\n",
      "trainer/Policy log std Min                               -3.14803\n",
      "trainer/Alpha                                             0.0224006\n",
      "trainer/Alpha Loss                                       -0.654293\n",
      "exploration/num steps total                           11200\n",
      "exploration/num paths total                             560\n",
      "exploration/path length Mean                             20\n",
      "exploration/path length Std                               0\n",
      "exploration/path length Max                              20\n",
      "exploration/path length Min                              20\n",
      "exploration/Rewards Mean                                 -0.0927794\n",
      "exploration/Rewards Std                                   0.0504436\n",
      "exploration/Rewards Max                                   0.00652409\n",
      "exploration/Rewards Min                                  -0.296386\n",
      "exploration/Returns Mean                                 -1.85559\n",
      "exploration/Returns Std                                   0.534007\n",
      "exploration/Returns Max                                  -0.892849\n",
      "exploration/Returns Min                                  -2.42279\n",
      "exploration/Actions Mean                                 -0.00311189\n",
      "exploration/Actions Std                                   0.0876936\n",
      "exploration/Actions Max                                   0.314904\n",
      "exploration/Actions Min                                  -0.344485\n",
      "exploration/Num Paths                                     5\n",
      "exploration/Average Returns                              -1.85559\n",
      "exploration/env_infos/final/reward_dist Mean              0.146952\n",
      "exploration/env_infos/final/reward_dist Std               0.241435\n",
      "exploration/env_infos/final/reward_dist Max               0.623768\n",
      "exploration/env_infos/final/reward_dist Min               9.26024e-11\n",
      "exploration/env_infos/initial/reward_dist Mean            0.00551921\n",
      "exploration/env_infos/initial/reward_dist Std             0.0105186\n",
      "exploration/env_infos/initial/reward_dist Max             0.0265482\n",
      "exploration/env_infos/initial/reward_dist Min             6.78205e-06\n",
      "exploration/env_infos/reward_dist Mean                    0.130518\n",
      "exploration/env_infos/reward_dist Std                     0.195019\n",
      "exploration/env_infos/reward_dist Max                     0.821329\n",
      "exploration/env_infos/reward_dist Min                     9.26024e-11\n",
      "exploration/env_infos/final/reward_energy Mean           -0.124525\n",
      "exploration/env_infos/final/reward_energy Std             0.0680052\n",
      "exploration/env_infos/final/reward_energy Max            -0.0571518\n",
      "exploration/env_infos/final/reward_energy Min            -0.251905\n",
      "exploration/env_infos/initial/reward_energy Mean         -0.149971\n",
      "exploration/env_infos/initial/reward_energy Std           0.116008\n",
      "exploration/env_infos/initial/reward_energy Max          -0.0126822\n",
      "exploration/env_infos/initial/reward_energy Min          -0.362151\n",
      "exploration/env_infos/reward_energy Mean                 -0.100582\n",
      "exploration/env_infos/reward_energy Std                   0.0726834\n",
      "exploration/env_infos/reward_energy Max                  -0.00650984\n",
      "exploration/env_infos/reward_energy Min                  -0.362151\n",
      "exploration/env_infos/final/reward_safety Mean            0\n",
      "exploration/env_infos/final/reward_safety Std             0\n",
      "exploration/env_infos/final/reward_safety Max             0\n",
      "exploration/env_infos/final/reward_safety Min             0\n",
      "exploration/env_infos/initial/reward_safety Mean          0\n",
      "exploration/env_infos/initial/reward_safety Std           0\n",
      "exploration/env_infos/initial/reward_safety Max           0\n",
      "exploration/env_infos/initial/reward_safety Min           0\n",
      "exploration/env_infos/reward_safety Mean                  0\n",
      "exploration/env_infos/reward_safety Std                   0\n",
      "exploration/env_infos/reward_safety Max                   0\n",
      "exploration/env_infos/reward_safety Min                   0\n",
      "exploration/env_infos/final/end_effector_loc Mean        -0.0177605\n",
      "exploration/env_infos/final/end_effector_loc Std          0.252155\n",
      "exploration/env_infos/final/end_effector_loc Max          0.245131\n",
      "exploration/env_infos/final/end_effector_loc Min         -0.591476\n",
      "exploration/env_infos/initial/end_effector_loc Mean       0.00104024\n",
      "exploration/env_infos/initial/end_effector_loc Std        0.00662226\n",
      "exploration/env_infos/initial/end_effector_loc Max        0.0157452\n",
      "exploration/env_infos/initial/end_effector_loc Min       -0.00894276\n",
      "exploration/env_infos/end_effector_loc Mean              -0.000615486\n",
      "exploration/env_infos/end_effector_loc Std                0.151802\n",
      "exploration/env_infos/end_effector_loc Max                0.245131\n",
      "exploration/env_infos/end_effector_loc Min               -0.591476\n",
      "evaluation/num steps total                           102000\n",
      "evaluation/num paths total                             5100\n",
      "evaluation/path length Mean                              20\n",
      "evaluation/path length Std                                0\n",
      "evaluation/path length Max                               20\n",
      "evaluation/path length Min                               20\n",
      "evaluation/Rewards Mean                                  -0.0526571\n",
      "evaluation/Rewards Std                                    0.0646748\n",
      "evaluation/Rewards Max                                    0.108332\n",
      "evaluation/Rewards Min                                   -0.351516\n",
      "evaluation/Returns Mean                                  -1.05314\n",
      "evaluation/Returns Std                                    0.917056\n",
      "evaluation/Returns Max                                    1.46696\n",
      "evaluation/Returns Min                                   -2.81958\n",
      "evaluation/Actions Mean                                  -0.00529276\n",
      "evaluation/Actions Std                                    0.0783711\n",
      "evaluation/Actions Max                                    0.576357\n",
      "evaluation/Actions Min                                   -0.699459\n",
      "evaluation/Num Paths                                     50\n",
      "evaluation/Average Returns                               -1.05314\n",
      "evaluation/env_infos/final/reward_dist Mean               0.0975101\n",
      "evaluation/env_infos/final/reward_dist Std                0.191259\n",
      "evaluation/env_infos/final/reward_dist Max                0.943251\n",
      "evaluation/env_infos/final/reward_dist Min                8.88527e-15\n",
      "evaluation/env_infos/initial/reward_dist Mean             0.00705038\n",
      "evaluation/env_infos/initial/reward_dist Std              0.0126466\n",
      "evaluation/env_infos/initial/reward_dist Max              0.0578099\n",
      "evaluation/env_infos/initial/reward_dist Min              1.71274e-06\n",
      "evaluation/env_infos/reward_dist Mean                     0.159037\n",
      "evaluation/env_infos/reward_dist Std                      0.252717\n",
      "evaluation/env_infos/reward_dist Max                      0.994666\n",
      "evaluation/env_infos/reward_dist Min                      8.88527e-15\n",
      "evaluation/env_infos/final/reward_energy Mean            -0.0393576\n",
      "evaluation/env_infos/final/reward_energy Std              0.0304468\n",
      "evaluation/env_infos/final/reward_energy Max             -0.00625994\n",
      "evaluation/env_infos/final/reward_energy Min             -0.150157\n",
      "evaluation/env_infos/initial/reward_energy Mean          -0.271166\n",
      "evaluation/env_infos/initial/reward_energy Std            0.239619\n",
      "evaluation/env_infos/initial/reward_energy Max           -0.00557138\n",
      "evaluation/env_infos/initial/reward_energy Min           -0.874347\n",
      "evaluation/env_infos/reward_energy Mean                  -0.0628858\n",
      "evaluation/env_infos/reward_energy Std                    0.0915722\n",
      "evaluation/env_infos/reward_energy Max                   -0.000700736\n",
      "evaluation/env_infos/reward_energy Min                   -0.874347\n",
      "evaluation/env_infos/final/reward_safety Mean             0\n",
      "evaluation/env_infos/final/reward_safety Std              0\n",
      "evaluation/env_infos/final/reward_safety Max              0\n",
      "evaluation/env_infos/final/reward_safety Min              0\n",
      "evaluation/env_infos/initial/reward_safety Mean           0\n",
      "evaluation/env_infos/initial/reward_safety Std            0\n",
      "evaluation/env_infos/initial/reward_safety Max            0\n",
      "evaluation/env_infos/initial/reward_safety Min            0\n",
      "evaluation/env_infos/reward_safety Mean                   0\n",
      "evaluation/env_infos/reward_safety Std                    0\n",
      "evaluation/env_infos/reward_safety Max                    0\n",
      "evaluation/env_infos/reward_safety Min                    0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         -0.0703629\n",
      "evaluation/env_infos/final/end_effector_loc Std           0.241651\n",
      "evaluation/env_infos/final/end_effector_loc Max           0.472421\n",
      "evaluation/env_infos/final/end_effector_loc Min          -0.513739\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       -0.00301239\n",
      "evaluation/env_infos/initial/end_effector_loc Std         0.0124343\n",
      "evaluation/env_infos/initial/end_effector_loc Max         0.0288179\n",
      "evaluation/env_infos/initial/end_effector_loc Min        -0.0349729\n",
      "evaluation/env_infos/end_effector_loc Mean               -0.0371945\n",
      "evaluation/env_infos/end_effector_loc Std                 0.162057\n",
      "evaluation/env_infos/end_effector_loc Max                 0.472421\n",
      "evaluation/env_infos/end_effector_loc Min                -0.513739\n",
      "time/data storing (s)                                     0.00309288\n",
      "time/evaluation sampling (s)                              1.00932\n",
      "time/exploration sampling (s)                             0.157396\n",
      "time/logging (s)                                          0.0192601\n",
      "time/saving (s)                                           0.0292762\n",
      "time/training (s)                                        52.3823\n",
      "time/epoch (s)                                           53.6007\n",
      "time/total (s)                                         5423.57\n",
      "Epoch                                                   101\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:44:37.826859 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 102 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                     2000\n",
      "trainer/QF1 Loss                                          0.00095283\n",
      "trainer/QF2 Loss                                          0.00101725\n",
      "trainer/Policy Loss                                       2.88916\n",
      "trainer/Q1 Predictions Mean                              -0.928115\n",
      "trainer/Q1 Predictions Std                                0.861269\n",
      "trainer/Q1 Predictions Max                                1.17037\n",
      "trainer/Q1 Predictions Min                               -3.36699\n",
      "trainer/Q2 Predictions Mean                              -0.917516\n",
      "trainer/Q2 Predictions Std                                0.857977\n",
      "trainer/Q2 Predictions Max                                1.16507\n",
      "trainer/Q2 Predictions Min                               -3.3419\n",
      "trainer/Q Targets Mean                                   -0.912938\n",
      "trainer/Q Targets Std                                     0.860622\n",
      "trainer/Q Targets Max                                     1.15555\n",
      "trainer/Q Targets Min                                    -3.35233\n",
      "trainer/Log Pis Mean                                      1.972\n",
      "trainer/Log Pis Std                                       1.41601\n",
      "trainer/Log Pis Max                                       7.41907\n",
      "trainer/Log Pis Min                                      -3.99575\n",
      "trainer/Policy mu Mean                                   -0.0123032\n",
      "trainer/Policy mu Std                                     0.347855\n",
      "trainer/Policy mu Max                                     2.59567\n",
      "trainer/Policy mu Min                                    -2.71349\n",
      "trainer/Policy log std Mean                              -2.27937\n",
      "trainer/Policy log std Std                                0.582713\n",
      "trainer/Policy log std Max                               -0.36304\n",
      "trainer/Policy log std Min                               -3.37847\n",
      "trainer/Alpha                                             0.0221824\n",
      "trainer/Alpha Loss                                       -0.106611\n",
      "exploration/num steps total                           11300\n",
      "exploration/num paths total                             565\n",
      "exploration/path length Mean                             20\n",
      "exploration/path length Std                               0\n",
      "exploration/path length Max                              20\n",
      "exploration/path length Min                              20\n",
      "exploration/Rewards Mean                                 -0.0957486\n",
      "exploration/Rewards Std                                   0.0731281\n",
      "exploration/Rewards Max                                   0.0536658\n",
      "exploration/Rewards Min                                  -0.396291\n",
      "exploration/Returns Mean                                 -1.91497\n",
      "exploration/Returns Std                                   0.616706\n",
      "exploration/Returns Max                                  -0.8395\n",
      "exploration/Returns Min                                  -2.49704\n",
      "exploration/Actions Mean                                 -0.00837462\n",
      "exploration/Actions Std                                   0.182547\n",
      "exploration/Actions Max                                   0.538875\n",
      "exploration/Actions Min                                  -0.889466\n",
      "exploration/Num Paths                                     5\n",
      "exploration/Average Returns                              -1.91497\n",
      "exploration/env_infos/final/reward_dist Mean              0.0615849\n",
      "exploration/env_infos/final/reward_dist Std               0.0658132\n",
      "exploration/env_infos/final/reward_dist Max               0.169971\n",
      "exploration/env_infos/final/reward_dist Min               1.98486e-09\n",
      "exploration/env_infos/initial/reward_dist Mean            0.000675147\n",
      "exploration/env_infos/initial/reward_dist Std             0.0011573\n",
      "exploration/env_infos/initial/reward_dist Max             0.00298608\n",
      "exploration/env_infos/initial/reward_dist Min             1.74035e-05\n",
      "exploration/env_infos/reward_dist Mean                    0.205769\n",
      "exploration/env_infos/reward_dist Std                     0.287972\n",
      "exploration/env_infos/reward_dist Max                     0.953863\n",
      "exploration/env_infos/reward_dist Min                     1.98486e-09\n",
      "exploration/env_infos/final/reward_energy Mean           -0.129846\n",
      "exploration/env_infos/final/reward_energy Std             0.0694875\n",
      "exploration/env_infos/final/reward_energy Max            -0.0552969\n",
      "exploration/env_infos/final/reward_energy Min            -0.257208\n",
      "exploration/env_infos/initial/reward_energy Mean         -0.558301\n",
      "exploration/env_infos/initial/reward_energy Std           0.221084\n",
      "exploration/env_infos/initial/reward_energy Max          -0.221394\n",
      "exploration/env_infos/initial/reward_energy Min          -0.896116\n",
      "exploration/env_infos/reward_energy Mean                 -0.1959\n",
      "exploration/env_infos/reward_energy Std                   0.168554\n",
      "exploration/env_infos/reward_energy Max                  -0.0184877\n",
      "exploration/env_infos/reward_energy Min                  -0.896116\n",
      "exploration/env_infos/final/reward_safety Mean            0\n",
      "exploration/env_infos/final/reward_safety Std             0\n",
      "exploration/env_infos/final/reward_safety Max             0\n",
      "exploration/env_infos/final/reward_safety Min             0\n",
      "exploration/env_infos/initial/reward_safety Mean          0\n",
      "exploration/env_infos/initial/reward_safety Std           0\n",
      "exploration/env_infos/initial/reward_safety Max           0\n",
      "exploration/env_infos/initial/reward_safety Min           0\n",
      "exploration/env_infos/reward_safety Mean                  0\n",
      "exploration/env_infos/reward_safety Std                   0\n",
      "exploration/env_infos/reward_safety Max                   0\n",
      "exploration/env_infos/reward_safety Min                   0\n",
      "exploration/env_infos/final/end_effector_loc Mean        -0.17887\n",
      "exploration/env_infos/final/end_effector_loc Std          0.214869\n",
      "exploration/env_infos/final/end_effector_loc Max          0.294698\n",
      "exploration/env_infos/final/end_effector_loc Min         -0.529969\n",
      "exploration/env_infos/initial/end_effector_loc Mean      -0.0139184\n",
      "exploration/env_infos/initial/end_effector_loc Std        0.0160312\n",
      "exploration/env_infos/initial/end_effector_loc Max        0.0182207\n",
      "exploration/env_infos/initial/end_effector_loc Min       -0.0444733\n",
      "exploration/env_infos/end_effector_loc Mean              -0.113498\n",
      "exploration/env_infos/end_effector_loc Std                0.174359\n",
      "exploration/env_infos/end_effector_loc Max                0.360482\n",
      "exploration/env_infos/end_effector_loc Min               -0.529969\n",
      "evaluation/num steps total                           103000\n",
      "evaluation/num paths total                             5150\n",
      "evaluation/path length Mean                              20\n",
      "evaluation/path length Std                                0\n",
      "evaluation/path length Max                               20\n",
      "evaluation/path length Min                               20\n",
      "evaluation/Rewards Mean                                  -0.0466781\n",
      "evaluation/Rewards Std                                    0.073357\n",
      "evaluation/Rewards Max                                    0.139446\n",
      "evaluation/Rewards Min                                   -0.393711\n",
      "evaluation/Returns Mean                                  -0.933562\n",
      "evaluation/Returns Std                                    1.02282\n",
      "evaluation/Returns Max                                    1.57506\n",
      "evaluation/Returns Min                                   -3.05075\n",
      "evaluation/Actions Mean                                  -0.00429986\n",
      "evaluation/Actions Std                                    0.071498\n",
      "evaluation/Actions Max                                    0.7184\n",
      "evaluation/Actions Min                                   -0.559091\n",
      "evaluation/Num Paths                                     50\n",
      "evaluation/Average Returns                               -0.933562\n",
      "evaluation/env_infos/final/reward_dist Mean               0.172228\n",
      "evaluation/env_infos/final/reward_dist Std                0.256631\n",
      "evaluation/env_infos/final/reward_dist Max                0.987915\n",
      "evaluation/env_infos/final/reward_dist Min                1.62245e-34\n",
      "evaluation/env_infos/initial/reward_dist Mean             0.00523466\n",
      "evaluation/env_infos/initial/reward_dist Std              0.00994287\n",
      "evaluation/env_infos/initial/reward_dist Max              0.043102\n",
      "evaluation/env_infos/initial/reward_dist Min              8.94657e-07\n",
      "evaluation/env_infos/reward_dist Mean                     0.194409\n",
      "evaluation/env_infos/reward_dist Std                      0.286892\n",
      "evaluation/env_infos/reward_dist Max                      0.998329\n",
      "evaluation/env_infos/reward_dist Min                      1.62245e-34\n",
      "evaluation/env_infos/final/reward_energy Mean            -0.0409172\n",
      "evaluation/env_infos/final/reward_energy Std              0.0302903\n",
      "evaluation/env_infos/final/reward_energy Max             -0.00494588\n",
      "evaluation/env_infos/final/reward_energy Min             -0.135811\n",
      "evaluation/env_infos/initial/reward_energy Mean          -0.267871\n",
      "evaluation/env_infos/initial/reward_energy Std            0.202548\n",
      "evaluation/env_infos/initial/reward_energy Max           -0.0128001\n",
      "evaluation/env_infos/initial/reward_energy Min           -0.742209\n",
      "evaluation/env_infos/reward_energy Mean                  -0.0616684\n",
      "evaluation/env_infos/reward_energy Std                    0.0803611\n",
      "evaluation/env_infos/reward_energy Max                   -0.00130771\n",
      "evaluation/env_infos/reward_energy Min                   -0.742209\n",
      "evaluation/env_infos/final/reward_safety Mean             0\n",
      "evaluation/env_infos/final/reward_safety Std              0\n",
      "evaluation/env_infos/final/reward_safety Max              0\n",
      "evaluation/env_infos/final/reward_safety Min              0\n",
      "evaluation/env_infos/initial/reward_safety Mean           0\n",
      "evaluation/env_infos/initial/reward_safety Std            0\n",
      "evaluation/env_infos/initial/reward_safety Max            0\n",
      "evaluation/env_infos/initial/reward_safety Min            0\n",
      "evaluation/env_infos/reward_safety Mean                   0\n",
      "evaluation/env_infos/reward_safety Std                    0\n",
      "evaluation/env_infos/reward_safety Max                    0\n",
      "evaluation/env_infos/reward_safety Min                    0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         -0.0631596\n",
      "evaluation/env_infos/final/end_effector_loc Std           0.265385\n",
      "evaluation/env_infos/final/end_effector_loc Max           0.738643\n",
      "evaluation/env_infos/final/end_effector_loc Min          -0.647502\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       -0.00176277\n",
      "evaluation/env_infos/initial/end_effector_loc Std         0.0117417\n",
      "evaluation/env_infos/initial/end_effector_loc Max         0.03592\n",
      "evaluation/env_infos/initial/end_effector_loc Min        -0.0279545\n",
      "evaluation/env_infos/end_effector_loc Mean               -0.0323495\n",
      "evaluation/env_infos/end_effector_loc Std                 0.177442\n",
      "evaluation/env_infos/end_effector_loc Max                 0.738643\n",
      "evaluation/env_infos/end_effector_loc Min                -0.647502\n",
      "time/data storing (s)                                     0.0029822\n",
      "time/evaluation sampling (s)                              0.954743\n",
      "time/exploration sampling (s)                             0.13053\n",
      "time/logging (s)                                          0.0218389\n",
      "time/saving (s)                                           0.0333646\n",
      "time/training (s)                                        54.0088\n",
      "time/epoch (s)                                           55.1522\n",
      "time/total (s)                                         5480\n",
      "Epoch                                                   102\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:45:36.519636 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 103 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                     2000\n",
      "trainer/QF1 Loss                                          0.000824345\n",
      "trainer/QF2 Loss                                          0.000677397\n",
      "trainer/Policy Loss                                       2.94776\n",
      "trainer/Q1 Predictions Mean                              -0.960783\n",
      "trainer/Q1 Predictions Std                                0.829129\n",
      "trainer/Q1 Predictions Max                                0.858973\n",
      "trainer/Q1 Predictions Min                               -3.32308\n",
      "trainer/Q2 Predictions Mean                              -0.969136\n",
      "trainer/Q2 Predictions Std                                0.828513\n",
      "trainer/Q2 Predictions Max                                0.82694\n",
      "trainer/Q2 Predictions Min                               -3.33085\n",
      "trainer/Q Targets Mean                                   -0.968778\n",
      "trainer/Q Targets Std                                     0.827138\n",
      "trainer/Q Targets Max                                     0.797073\n",
      "trainer/Q Targets Min                                    -3.32213\n",
      "trainer/Log Pis Mean                                      1.99104\n",
      "trainer/Log Pis Std                                       1.44377\n",
      "trainer/Log Pis Max                                       6.61912\n",
      "trainer/Log Pis Min                                      -5.22558\n",
      "trainer/Policy mu Mean                                   -0.0158795\n",
      "trainer/Policy mu Std                                     0.382014\n",
      "trainer/Policy mu Max                                     2.25528\n",
      "trainer/Policy mu Min                                    -2.41662\n",
      "trainer/Policy log std Mean                              -2.2914\n",
      "trainer/Policy log std Std                                0.56711\n",
      "trainer/Policy log std Max                               -0.213232\n",
      "trainer/Policy log std Min                               -3.26101\n",
      "trainer/Alpha                                             0.023613\n",
      "trainer/Alpha Loss                                       -0.0335615\n",
      "exploration/num steps total                           11400\n",
      "exploration/num paths total                             570\n",
      "exploration/path length Mean                             20\n",
      "exploration/path length Std                               0\n",
      "exploration/path length Max                              20\n",
      "exploration/path length Min                              20\n",
      "exploration/Rewards Mean                                 -0.0402387\n",
      "exploration/Rewards Std                                   0.0841851\n",
      "exploration/Rewards Max                                   0.126852\n",
      "exploration/Rewards Min                                  -0.261375\n",
      "exploration/Returns Mean                                 -0.804774\n",
      "exploration/Returns Std                                   1.37466\n",
      "exploration/Returns Max                                   1.19196\n",
      "exploration/Returns Min                                  -2.39145\n",
      "exploration/Actions Mean                                 -0.00161758\n",
      "exploration/Actions Std                                   0.151496\n",
      "exploration/Actions Max                                   0.543148\n",
      "exploration/Actions Min                                  -0.642051\n",
      "exploration/Num Paths                                     5\n",
      "exploration/Average Returns                              -0.804774\n",
      "exploration/env_infos/final/reward_dist Mean              0.282671\n",
      "exploration/env_infos/final/reward_dist Std               0.257726\n",
      "exploration/env_infos/final/reward_dist Max               0.637568\n",
      "exploration/env_infos/final/reward_dist Min               0.000446724\n",
      "exploration/env_infos/initial/reward_dist Mean            0.00819639\n",
      "exploration/env_infos/initial/reward_dist Std             0.0154928\n",
      "exploration/env_infos/initial/reward_dist Max             0.0391769\n",
      "exploration/env_infos/initial/reward_dist Min             0.000132268\n",
      "exploration/env_infos/reward_dist Mean                    0.234207\n",
      "exploration/env_infos/reward_dist Std                     0.245865\n",
      "exploration/env_infos/reward_dist Max                     0.900927\n",
      "exploration/env_infos/reward_dist Min                     0.000125389\n",
      "exploration/env_infos/final/reward_energy Mean           -0.0705908\n",
      "exploration/env_infos/final/reward_energy Std             0.0195685\n",
      "exploration/env_infos/final/reward_energy Max            -0.0495458\n",
      "exploration/env_infos/final/reward_energy Min            -0.104214\n",
      "exploration/env_infos/initial/reward_energy Mean         -0.368139\n",
      "exploration/env_infos/initial/reward_energy Std           0.258362\n",
      "exploration/env_infos/initial/reward_energy Max          -0.0501347\n",
      "exploration/env_infos/initial/reward_energy Min          -0.783621\n",
      "exploration/env_infos/reward_energy Mean                 -0.162935\n",
      "exploration/env_infos/reward_energy Std                   0.139138\n",
      "exploration/env_infos/reward_energy Max                  -0.011021\n",
      "exploration/env_infos/reward_energy Min                  -0.783621\n",
      "exploration/env_infos/final/reward_safety Mean            0\n",
      "exploration/env_infos/final/reward_safety Std             0\n",
      "exploration/env_infos/final/reward_safety Max             0\n",
      "exploration/env_infos/final/reward_safety Min             0\n",
      "exploration/env_infos/initial/reward_safety Mean          0\n",
      "exploration/env_infos/initial/reward_safety Std           0\n",
      "exploration/env_infos/initial/reward_safety Max           0\n",
      "exploration/env_infos/initial/reward_safety Min           0\n",
      "exploration/env_infos/reward_safety Mean                  0\n",
      "exploration/env_infos/reward_safety Std                   0\n",
      "exploration/env_infos/reward_safety Max                   0\n",
      "exploration/env_infos/reward_safety Min                   0\n",
      "exploration/env_infos/final/end_effector_loc Mean         0.0118073\n",
      "exploration/env_infos/final/end_effector_loc Std          0.186928\n",
      "exploration/env_infos/final/end_effector_loc Max          0.371014\n",
      "exploration/env_infos/final/end_effector_loc Min         -0.205578\n",
      "exploration/env_infos/initial/end_effector_loc Mean      -0.00173028\n",
      "exploration/env_infos/initial/end_effector_loc Std        0.0158067\n",
      "exploration/env_infos/initial/end_effector_loc Max        0.0224629\n",
      "exploration/env_infos/initial/end_effector_loc Min       -0.0321025\n",
      "exploration/env_infos/end_effector_loc Mean               0.000632241\n",
      "exploration/env_infos/end_effector_loc Std                0.138654\n",
      "exploration/env_infos/end_effector_loc Max                0.371014\n",
      "exploration/env_infos/end_effector_loc Min               -0.232606\n",
      "evaluation/num steps total                           104000\n",
      "evaluation/num paths total                             5200\n",
      "evaluation/path length Mean                              20\n",
      "evaluation/path length Std                                0\n",
      "evaluation/path length Max                               20\n",
      "evaluation/path length Min                               20\n",
      "evaluation/Rewards Mean                                  -0.0414531\n",
      "evaluation/Rewards Std                                    0.0735212\n",
      "evaluation/Rewards Max                                    0.156656\n",
      "evaluation/Rewards Min                                   -0.384468\n",
      "evaluation/Returns Mean                                  -0.829062\n",
      "evaluation/Returns Std                                    1.09272\n",
      "evaluation/Returns Max                                    2.48223\n",
      "evaluation/Returns Min                                   -3.32843\n",
      "evaluation/Actions Mean                                  -0.0036834\n",
      "evaluation/Actions Std                                    0.0865659\n",
      "evaluation/Actions Max                                    0.612761\n",
      "evaluation/Actions Min                                   -0.853445\n",
      "evaluation/Num Paths                                     50\n",
      "evaluation/Average Returns                               -0.829062\n",
      "evaluation/env_infos/final/reward_dist Mean               0.212789\n",
      "evaluation/env_infos/final/reward_dist Std                0.287372\n",
      "evaluation/env_infos/final/reward_dist Max                0.95901\n",
      "evaluation/env_infos/final/reward_dist Min                8.40184e-22\n",
      "evaluation/env_infos/initial/reward_dist Mean             0.00844329\n",
      "evaluation/env_infos/initial/reward_dist Std              0.0126097\n",
      "evaluation/env_infos/initial/reward_dist Max              0.0534185\n",
      "evaluation/env_infos/initial/reward_dist Min              1.88023e-06\n",
      "evaluation/env_infos/reward_dist Mean                     0.257335\n",
      "evaluation/env_infos/reward_dist Std                      0.301093\n",
      "evaluation/env_infos/reward_dist Max                      0.999928\n",
      "evaluation/env_infos/reward_dist Min                      8.40184e-22\n",
      "evaluation/env_infos/final/reward_energy Mean            -0.0419911\n",
      "evaluation/env_infos/final/reward_energy Std              0.0313468\n",
      "evaluation/env_infos/final/reward_energy Max             -0.0022196\n",
      "evaluation/env_infos/final/reward_energy Min             -0.151479\n",
      "evaluation/env_infos/initial/reward_energy Mean          -0.310078\n",
      "evaluation/env_infos/initial/reward_energy Std            0.238557\n",
      "evaluation/env_infos/initial/reward_energy Max           -0.00670894\n",
      "evaluation/env_infos/initial/reward_energy Min           -0.869855\n",
      "evaluation/env_infos/reward_energy Mean                  -0.0747313\n",
      "evaluation/env_infos/reward_energy Std                    0.0971065\n",
      "evaluation/env_infos/reward_energy Max                   -0.000275609\n",
      "evaluation/env_infos/reward_energy Min                   -0.869855\n",
      "evaluation/env_infos/final/reward_safety Mean             0\n",
      "evaluation/env_infos/final/reward_safety Std              0\n",
      "evaluation/env_infos/final/reward_safety Max              0\n",
      "evaluation/env_infos/final/reward_safety Min              0\n",
      "evaluation/env_infos/initial/reward_safety Mean           0\n",
      "evaluation/env_infos/initial/reward_safety Std            0\n",
      "evaluation/env_infos/initial/reward_safety Max            0\n",
      "evaluation/env_infos/initial/reward_safety Min            0\n",
      "evaluation/env_infos/reward_safety Mean                   0\n",
      "evaluation/env_infos/reward_safety Std                    0\n",
      "evaluation/env_infos/reward_safety Max                    0\n",
      "evaluation/env_infos/reward_safety Min                    0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         -0.0734175\n",
      "evaluation/env_infos/final/end_effector_loc Std           0.250356\n",
      "evaluation/env_infos/final/end_effector_loc Max           0.541969\n",
      "evaluation/env_infos/final/end_effector_loc Min          -0.633809\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       -0.00317033\n",
      "evaluation/env_infos/initial/end_effector_loc Std         0.0134637\n",
      "evaluation/env_infos/initial/end_effector_loc Max         0.030638\n",
      "evaluation/env_infos/initial/end_effector_loc Min        -0.0426722\n",
      "evaluation/env_infos/end_effector_loc Mean               -0.0420601\n",
      "evaluation/env_infos/end_effector_loc Std                 0.173907\n",
      "evaluation/env_infos/end_effector_loc Max                 0.541969\n",
      "evaluation/env_infos/end_effector_loc Min                -0.633809\n",
      "time/data storing (s)                                     0.00330015\n",
      "time/evaluation sampling (s)                              1.50796\n",
      "time/exploration sampling (s)                             0.143484\n",
      "time/logging (s)                                          0.0241798\n",
      "time/saving (s)                                           0.0334793\n",
      "time/training (s)                                        55.3235\n",
      "time/epoch (s)                                           57.0359\n",
      "time/total (s)                                         5538.7\n",
      "Epoch                                                   103\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:46:35.303858 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 104 finished\n",
      "---------------------------------------------------  -----------------\n",
      "replay_buffer/size                                     2000\n",
      "trainer/QF1 Loss                                          0.000725418\n",
      "trainer/QF2 Loss                                          0.00105655\n",
      "trainer/Policy Loss                                       2.78945\n",
      "trainer/Q1 Predictions Mean                              -0.867094\n",
      "trainer/Q1 Predictions Std                                0.794872\n",
      "trainer/Q1 Predictions Max                                1.15371\n",
      "trainer/Q1 Predictions Min                               -3.21693\n",
      "trainer/Q2 Predictions Mean                              -0.883217\n",
      "trainer/Q2 Predictions Std                                0.801176\n",
      "trainer/Q2 Predictions Max                                1.14736\n",
      "trainer/Q2 Predictions Min                               -3.25833\n",
      "trainer/Q Targets Mean                                   -0.873245\n",
      "trainer/Q Targets Std                                     0.79416\n",
      "trainer/Q Targets Max                                     1.18959\n",
      "trainer/Q Targets Min                                    -3.2067\n",
      "trainer/Log Pis Mean                                      1.91884\n",
      "trainer/Log Pis Std                                       1.43588\n",
      "trainer/Log Pis Max                                       4.44715\n",
      "trainer/Log Pis Min                                      -3.02206\n",
      "trainer/Policy mu Mean                                   -0.00368161\n",
      "trainer/Policy mu Std                                     0.225706\n",
      "trainer/Policy mu Max                                     1.21268\n",
      "trainer/Policy mu Min                                    -1.81532\n",
      "trainer/Policy log std Mean                              -2.33774\n",
      "trainer/Policy log std Std                                0.550454\n",
      "trainer/Policy log std Max                               -0.462317\n",
      "trainer/Policy log std Min                               -3.3659\n",
      "trainer/Alpha                                             0.022526\n",
      "trainer/Alpha Loss                                       -0.307749\n",
      "exploration/num steps total                           11500\n",
      "exploration/num paths total                             575\n",
      "exploration/path length Mean                             20\n",
      "exploration/path length Std                               0\n",
      "exploration/path length Max                              20\n",
      "exploration/path length Min                              20\n",
      "exploration/Rewards Mean                                 -0.123959\n",
      "exploration/Rewards Std                                   0.130299\n",
      "exploration/Rewards Max                                  -0.004973\n",
      "exploration/Rewards Min                                  -0.822098\n",
      "exploration/Returns Mean                                 -2.47918\n",
      "exploration/Returns Std                                   1.3859\n",
      "exploration/Returns Max                                  -0.816851\n",
      "exploration/Returns Min                                  -5.00219\n",
      "exploration/Actions Mean                                  0.0211013\n",
      "exploration/Actions Std                                   0.193474\n",
      "exploration/Actions Max                                   0.809261\n",
      "exploration/Actions Min                                  -0.48602\n",
      "exploration/Num Paths                                     5\n",
      "exploration/Average Returns                              -2.47918\n",
      "exploration/env_infos/final/reward_dist Mean              0.15277\n",
      "exploration/env_infos/final/reward_dist Std               0.273463\n",
      "exploration/env_infos/final/reward_dist Max               0.697281\n",
      "exploration/env_infos/final/reward_dist Min               1.83929e-87\n",
      "exploration/env_infos/initial/reward_dist Mean            0.00566196\n",
      "exploration/env_infos/initial/reward_dist Std             0.00710339\n",
      "exploration/env_infos/initial/reward_dist Max             0.0188879\n",
      "exploration/env_infos/initial/reward_dist Min             6.08735e-05\n",
      "exploration/env_infos/reward_dist Mean                    0.143406\n",
      "exploration/env_infos/reward_dist Std                     0.260121\n",
      "exploration/env_infos/reward_dist Max                     0.962965\n",
      "exploration/env_infos/reward_dist Min                     1.83929e-87\n",
      "exploration/env_infos/final/reward_energy Mean           -0.372754\n",
      "exploration/env_infos/final/reward_energy Std             0.270342\n",
      "exploration/env_infos/final/reward_energy Max            -0.123008\n",
      "exploration/env_infos/final/reward_energy Min            -0.869417\n",
      "exploration/env_infos/initial/reward_energy Mean         -0.400113\n",
      "exploration/env_infos/initial/reward_energy Std           0.274885\n",
      "exploration/env_infos/initial/reward_energy Max          -0.0936741\n",
      "exploration/env_infos/initial/reward_energy Min          -0.866402\n",
      "exploration/env_infos/reward_energy Mean                 -0.210033\n",
      "exploration/env_infos/reward_energy Std                   0.177879\n",
      "exploration/env_infos/reward_energy Max                  -0.0214269\n",
      "exploration/env_infos/reward_energy Min                  -0.869417\n",
      "exploration/env_infos/final/reward_safety Mean            0\n",
      "exploration/env_infos/final/reward_safety Std             0\n",
      "exploration/env_infos/final/reward_safety Max             0\n",
      "exploration/env_infos/final/reward_safety Min             0\n",
      "exploration/env_infos/initial/reward_safety Mean          0\n",
      "exploration/env_infos/initial/reward_safety Std           0\n",
      "exploration/env_infos/initial/reward_safety Max           0\n",
      "exploration/env_infos/initial/reward_safety Min           0\n",
      "exploration/env_infos/reward_safety Mean                  0\n",
      "exploration/env_infos/reward_safety Std                   0\n",
      "exploration/env_infos/reward_safety Max                   0\n",
      "exploration/env_infos/reward_safety Min                   0\n",
      "exploration/env_infos/final/end_effector_loc Mean         0.215389\n",
      "exploration/env_infos/final/end_effector_loc Std          0.426642\n",
      "exploration/env_infos/final/end_effector_loc Max          0.963622\n",
      "exploration/env_infos/final/end_effector_loc Min         -0.613123\n",
      "exploration/env_infos/initial/end_effector_loc Mean       0.0041234\n",
      "exploration/env_infos/initial/end_effector_loc Std        0.0166602\n",
      "exploration/env_infos/initial/end_effector_loc Max        0.0358621\n",
      "exploration/env_infos/initial/end_effector_loc Min       -0.024301\n",
      "exploration/env_infos/end_effector_loc Mean               0.104388\n",
      "exploration/env_infos/end_effector_loc Std                0.235537\n",
      "exploration/env_infos/end_effector_loc Max                0.963622\n",
      "exploration/env_infos/end_effector_loc Min               -0.613123\n",
      "evaluation/num steps total                           105000\n",
      "evaluation/num paths total                             5250\n",
      "evaluation/path length Mean                              20\n",
      "evaluation/path length Std                                0\n",
      "evaluation/path length Max                               20\n",
      "evaluation/path length Min                               20\n",
      "evaluation/Rewards Mean                                  -0.0542931\n",
      "evaluation/Rewards Std                                    0.104486\n",
      "evaluation/Rewards Max                                    0.149601\n",
      "evaluation/Rewards Min                                   -1.23319\n",
      "evaluation/Returns Mean                                  -1.08586\n",
      "evaluation/Returns Std                                    1.46324\n",
      "evaluation/Returns Max                                    2.01005\n",
      "evaluation/Returns Min                                   -5.25815\n",
      "evaluation/Actions Mean                                   0.00136236\n",
      "evaluation/Actions Std                                    0.0742862\n",
      "evaluation/Actions Max                                    0.541288\n",
      "evaluation/Actions Min                                   -0.826373\n",
      "evaluation/Num Paths                                     50\n",
      "evaluation/Average Returns                               -1.08586\n",
      "evaluation/env_infos/final/reward_dist Mean               0.18625\n",
      "evaluation/env_infos/final/reward_dist Std                0.274765\n",
      "evaluation/env_infos/final/reward_dist Max                0.915601\n",
      "evaluation/env_infos/final/reward_dist Min                3.11061e-101\n",
      "evaluation/env_infos/initial/reward_dist Mean             0.00594715\n",
      "evaluation/env_infos/initial/reward_dist Std              0.011661\n",
      "evaluation/env_infos/initial/reward_dist Max              0.0545839\n",
      "evaluation/env_infos/initial/reward_dist Min              9.20871e-07\n",
      "evaluation/env_infos/reward_dist Mean                     0.222515\n",
      "evaluation/env_infos/reward_dist Std                      0.283808\n",
      "evaluation/env_infos/reward_dist Max                      0.996667\n",
      "evaluation/env_infos/reward_dist Min                      3.11061e-101\n",
      "evaluation/env_infos/final/reward_energy Mean            -0.0624653\n",
      "evaluation/env_infos/final/reward_energy Std              0.110782\n",
      "evaluation/env_infos/final/reward_energy Max             -0.00156588\n",
      "evaluation/env_infos/final/reward_energy Min             -0.736098\n",
      "evaluation/env_infos/initial/reward_energy Mean          -0.234253\n",
      "evaluation/env_infos/initial/reward_energy Std            0.159281\n",
      "evaluation/env_infos/initial/reward_energy Max           -0.0322141\n",
      "evaluation/env_infos/initial/reward_energy Min           -0.82763\n",
      "evaluation/env_infos/reward_energy Mean                  -0.0691473\n",
      "evaluation/env_infos/reward_energy Std                    0.0791154\n",
      "evaluation/env_infos/reward_energy Max                   -0.00156588\n",
      "evaluation/env_infos/reward_energy Min                   -0.82763\n",
      "evaluation/env_infos/final/reward_safety Mean             0\n",
      "evaluation/env_infos/final/reward_safety Std              0\n",
      "evaluation/env_infos/final/reward_safety Max              0\n",
      "evaluation/env_infos/final/reward_safety Min              0\n",
      "evaluation/env_infos/initial/reward_safety Mean           0\n",
      "evaluation/env_infos/initial/reward_safety Std            0\n",
      "evaluation/env_infos/initial/reward_safety Max            0\n",
      "evaluation/env_infos/initial/reward_safety Min            0\n",
      "evaluation/env_infos/reward_safety Mean                   0\n",
      "evaluation/env_infos/reward_safety Std                    0\n",
      "evaluation/env_infos/reward_safety Max                    0\n",
      "evaluation/env_infos/reward_safety Min                    0\n",
      "evaluation/env_infos/final/end_effector_loc Mean          0.0001255\n",
      "evaluation/env_infos/final/end_effector_loc Std           0.298893\n",
      "evaluation/env_infos/final/end_effector_loc Max           1\n",
      "evaluation/env_infos/final/end_effector_loc Min          -0.618522\n",
      "evaluation/env_infos/initial/end_effector_loc Mean        0.000169309\n",
      "evaluation/env_infos/initial/end_effector_loc Std         0.0100139\n",
      "evaluation/env_infos/initial/end_effector_loc Max         0.0215615\n",
      "evaluation/env_infos/initial/end_effector_loc Min        -0.0413186\n",
      "evaluation/env_infos/end_effector_loc Mean                0.000402566\n",
      "evaluation/env_infos/end_effector_loc Std                 0.180223\n",
      "evaluation/env_infos/end_effector_loc Max                 1\n",
      "evaluation/env_infos/end_effector_loc Min                -0.618522\n",
      "time/data storing (s)                                     0.00322779\n",
      "time/evaluation sampling (s)                              1.17763\n",
      "time/exploration sampling (s)                             0.157531\n",
      "time/logging (s)                                          0.0202859\n",
      "time/saving (s)                                           0.0276202\n",
      "time/training (s)                                        55.9421\n",
      "time/epoch (s)                                           57.3284\n",
      "time/total (s)                                         5597.47\n",
      "Epoch                                                   104\n",
      "---------------------------------------------------  -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:47:33.707449 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 105 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                     2000\n",
      "trainer/QF1 Loss                                          0.000729442\n",
      "trainer/QF2 Loss                                          0.00107311\n",
      "trainer/Policy Loss                                       3.03717\n",
      "trainer/Q1 Predictions Mean                              -0.948576\n",
      "trainer/Q1 Predictions Std                                0.828836\n",
      "trainer/Q1 Predictions Max                                1.0788\n",
      "trainer/Q1 Predictions Min                               -3.01877\n",
      "trainer/Q2 Predictions Mean                              -0.937977\n",
      "trainer/Q2 Predictions Std                                0.828189\n",
      "trainer/Q2 Predictions Max                                1.10519\n",
      "trainer/Q2 Predictions Min                               -2.97661\n",
      "trainer/Q Targets Mean                                   -0.95086\n",
      "trainer/Q Targets Std                                     0.831094\n",
      "trainer/Q Targets Max                                     1.09514\n",
      "trainer/Q Targets Min                                    -3.06836\n",
      "trainer/Log Pis Mean                                      2.0991\n",
      "trainer/Log Pis Std                                       1.30956\n",
      "trainer/Log Pis Max                                       4.52102\n",
      "trainer/Log Pis Min                                      -2.41671\n",
      "trainer/Policy mu Mean                                   -0.0398142\n",
      "trainer/Policy mu Std                                     0.319034\n",
      "trainer/Policy mu Max                                     2.00936\n",
      "trainer/Policy mu Min                                    -2.16757\n",
      "trainer/Policy log std Mean                              -2.31686\n",
      "trainer/Policy log std Std                                0.565935\n",
      "trainer/Policy log std Max                               -0.322351\n",
      "trainer/Policy log std Min                               -3.23261\n",
      "trainer/Alpha                                             0.0225753\n",
      "trainer/Alpha Loss                                        0.375643\n",
      "exploration/num steps total                           11600\n",
      "exploration/num paths total                             580\n",
      "exploration/path length Mean                             20\n",
      "exploration/path length Std                               0\n",
      "exploration/path length Max                              20\n",
      "exploration/path length Min                              20\n",
      "exploration/Rewards Mean                                 -0.122431\n",
      "exploration/Rewards Std                                   0.10393\n",
      "exploration/Rewards Max                                   0.0941221\n",
      "exploration/Rewards Min                                  -0.490137\n",
      "exploration/Returns Mean                                 -2.44862\n",
      "exploration/Returns Std                                   1.02065\n",
      "exploration/Returns Max                                  -1.35637\n",
      "exploration/Returns Min                                  -3.94194\n",
      "exploration/Actions Mean                                 -0.00239779\n",
      "exploration/Actions Std                                   0.100331\n",
      "exploration/Actions Max                                   0.349037\n",
      "exploration/Actions Min                                  -0.333528\n",
      "exploration/Num Paths                                     5\n",
      "exploration/Average Returns                              -2.44862\n",
      "exploration/env_infos/final/reward_dist Mean              0.166647\n",
      "exploration/env_infos/final/reward_dist Std               0.201767\n",
      "exploration/env_infos/final/reward_dist Max               0.487189\n",
      "exploration/env_infos/final/reward_dist Min               8.70587e-36\n",
      "exploration/env_infos/initial/reward_dist Mean            0.000464012\n",
      "exploration/env_infos/initial/reward_dist Std             0.000771912\n",
      "exploration/env_infos/initial/reward_dist Max             0.00198751\n",
      "exploration/env_infos/initial/reward_dist Min             8.16621e-07\n",
      "exploration/env_infos/reward_dist Mean                    0.163852\n",
      "exploration/env_infos/reward_dist Std                     0.282613\n",
      "exploration/env_infos/reward_dist Max                     0.991785\n",
      "exploration/env_infos/reward_dist Min                     8.70587e-36\n",
      "exploration/env_infos/final/reward_energy Mean           -0.0740407\n",
      "exploration/env_infos/final/reward_energy Std             0.0374403\n",
      "exploration/env_infos/final/reward_energy Max            -0.0251376\n",
      "exploration/env_infos/final/reward_energy Min            -0.123108\n",
      "exploration/env_infos/initial/reward_energy Mean         -0.142283\n",
      "exploration/env_infos/initial/reward_energy Std           0.0894278\n",
      "exploration/env_infos/initial/reward_energy Max          -0.0427217\n",
      "exploration/env_infos/initial/reward_energy Min          -0.306084\n",
      "exploration/env_infos/reward_energy Mean                 -0.117533\n",
      "exploration/env_infos/reward_energy Std                   0.0795616\n",
      "exploration/env_infos/reward_energy Max                  -0.0210184\n",
      "exploration/env_infos/reward_energy Min                  -0.383098\n",
      "exploration/env_infos/final/reward_safety Mean            0\n",
      "exploration/env_infos/final/reward_safety Std             0\n",
      "exploration/env_infos/final/reward_safety Max             0\n",
      "exploration/env_infos/final/reward_safety Min             0\n",
      "exploration/env_infos/initial/reward_safety Mean          0\n",
      "exploration/env_infos/initial/reward_safety Std           0\n",
      "exploration/env_infos/initial/reward_safety Max           0\n",
      "exploration/env_infos/initial/reward_safety Min           0\n",
      "exploration/env_infos/reward_safety Mean                  0\n",
      "exploration/env_infos/reward_safety Std                   0\n",
      "exploration/env_infos/reward_safety Max                   0\n",
      "exploration/env_infos/reward_safety Min                   0\n",
      "exploration/env_infos/final/end_effector_loc Mean        -0.0268197\n",
      "exploration/env_infos/final/end_effector_loc Std          0.371664\n",
      "exploration/env_infos/final/end_effector_loc Max          0.572524\n",
      "exploration/env_infos/final/end_effector_loc Min         -0.650266\n",
      "exploration/env_infos/initial/end_effector_loc Mean      -0.00275041\n",
      "exploration/env_infos/initial/end_effector_loc Std        0.00526663\n",
      "exploration/env_infos/initial/end_effector_loc Max        0.00670424\n",
      "exploration/env_infos/initial/end_effector_loc Min       -0.0137576\n",
      "exploration/env_infos/end_effector_loc Mean              -0.011614\n",
      "exploration/env_infos/end_effector_loc Std                0.212402\n",
      "exploration/env_infos/end_effector_loc Max                0.572524\n",
      "exploration/env_infos/end_effector_loc Min               -0.650266\n",
      "evaluation/num steps total                           106000\n",
      "evaluation/num paths total                             5300\n",
      "evaluation/path length Mean                              20\n",
      "evaluation/path length Std                                0\n",
      "evaluation/path length Max                               20\n",
      "evaluation/path length Min                               20\n",
      "evaluation/Rewards Mean                                  -0.0417961\n",
      "evaluation/Rewards Std                                    0.0734265\n",
      "evaluation/Rewards Max                                    0.150211\n",
      "evaluation/Rewards Min                                   -0.478307\n",
      "evaluation/Returns Mean                                  -0.835922\n",
      "evaluation/Returns Std                                    1.07687\n",
      "evaluation/Returns Max                                    1.53134\n",
      "evaluation/Returns Min                                   -3.25153\n",
      "evaluation/Actions Mean                                  -0.000639023\n",
      "evaluation/Actions Std                                    0.0784005\n",
      "evaluation/Actions Max                                    0.606235\n",
      "evaluation/Actions Min                                   -0.904948\n",
      "evaluation/Num Paths                                     50\n",
      "evaluation/Average Returns                               -0.835922\n",
      "evaluation/env_infos/final/reward_dist Mean               0.201866\n",
      "evaluation/env_infos/final/reward_dist Std                0.283067\n",
      "evaluation/env_infos/final/reward_dist Max                0.88861\n",
      "evaluation/env_infos/final/reward_dist Min                5.31446e-17\n",
      "evaluation/env_infos/initial/reward_dist Mean             0.00877254\n",
      "evaluation/env_infos/initial/reward_dist Std              0.0170881\n",
      "evaluation/env_infos/initial/reward_dist Max              0.0688706\n",
      "evaluation/env_infos/initial/reward_dist Min              3.09937e-06\n",
      "evaluation/env_infos/reward_dist Mean                     0.179843\n",
      "evaluation/env_infos/reward_dist Std                      0.265794\n",
      "evaluation/env_infos/reward_dist Max                      0.989055\n",
      "evaluation/env_infos/reward_dist Min                      5.31446e-17\n",
      "evaluation/env_infos/final/reward_energy Mean            -0.0527525\n",
      "evaluation/env_infos/final/reward_energy Std              0.0633652\n",
      "evaluation/env_infos/final/reward_energy Max             -0.00707472\n",
      "evaluation/env_infos/final/reward_energy Min             -0.424765\n",
      "evaluation/env_infos/initial/reward_energy Mean          -0.264131\n",
      "evaluation/env_infos/initial/reward_energy Std            0.230102\n",
      "evaluation/env_infos/initial/reward_energy Max           -0.00497903\n",
      "evaluation/env_infos/initial/reward_energy Min           -0.91895\n",
      "evaluation/env_infos/reward_energy Mean                  -0.0666434\n",
      "evaluation/env_infos/reward_energy Std                    0.0886157\n",
      "evaluation/env_infos/reward_energy Max                   -0.000493674\n",
      "evaluation/env_infos/reward_energy Min                   -0.91895\n",
      "evaluation/env_infos/final/reward_safety Mean             0\n",
      "evaluation/env_infos/final/reward_safety Std              0\n",
      "evaluation/env_infos/final/reward_safety Max              0\n",
      "evaluation/env_infos/final/reward_safety Min              0\n",
      "evaluation/env_infos/initial/reward_safety Mean           0\n",
      "evaluation/env_infos/initial/reward_safety Std            0\n",
      "evaluation/env_infos/initial/reward_safety Max            0\n",
      "evaluation/env_infos/initial/reward_safety Min            0\n",
      "evaluation/env_infos/reward_safety Mean                   0\n",
      "evaluation/env_infos/reward_safety Std                    0\n",
      "evaluation/env_infos/reward_safety Max                    0\n",
      "evaluation/env_infos/reward_safety Min                    0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         -0.0202136\n",
      "evaluation/env_infos/final/end_effector_loc Std           0.235277\n",
      "evaluation/env_infos/final/end_effector_loc Max           0.584801\n",
      "evaluation/env_infos/final/end_effector_loc Min          -0.516547\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       -0.000839864\n",
      "evaluation/env_infos/initial/end_effector_loc Std         0.0123566\n",
      "evaluation/env_infos/initial/end_effector_loc Max         0.0303117\n",
      "evaluation/env_infos/initial/end_effector_loc Min        -0.0452474\n",
      "evaluation/env_infos/end_effector_loc Mean               -0.0117293\n",
      "evaluation/env_infos/end_effector_loc Std                 0.158323\n",
      "evaluation/env_infos/end_effector_loc Max                 0.584801\n",
      "evaluation/env_infos/end_effector_loc Min                -0.516547\n",
      "time/data storing (s)                                     0.00320163\n",
      "time/evaluation sampling (s)                              1.17269\n",
      "time/exploration sampling (s)                             0.146925\n",
      "time/logging (s)                                          0.0206106\n",
      "time/saving (s)                                           0.0289517\n",
      "time/training (s)                                        55.6508\n",
      "time/epoch (s)                                           57.0232\n",
      "time/total (s)                                         5655.88\n",
      "Epoch                                                   105\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:48:27.386683 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 106 finished\r\n",
      "---------------------------------------------------  ----------------\r\n",
      "replay_buffer/size                                     2000\r\n",
      "trainer/QF1 Loss                                          0.000910799\r\n",
      "trainer/QF2 Loss                                          0.00096429\r\n",
      "trainer/Policy Loss                                       2.9587\r\n",
      "trainer/Q1 Predictions Mean                              -0.987218\r\n",
      "trainer/Q1 Predictions Std                                0.810095\r\n",
      "trainer/Q1 Predictions Max                                1.16228\r\n",
      "trainer/Q1 Predictions Min                               -3.07917\r\n",
      "trainer/Q2 Predictions Mean                              -0.987788\r\n",
      "trainer/Q2 Predictions Std                                0.808381\r\n",
      "trainer/Q2 Predictions Max                                1.17227\r\n",
      "trainer/Q2 Predictions Min                               -3.12159\r\n",
      "trainer/Q Targets Mean                                   -0.985199\r\n",
      "trainer/Q Targets Std                                     0.813847\r\n",
      "trainer/Q Targets Max                                     1.22753\r\n",
      "trainer/Q Targets Min                                    -3.10712\r\n",
      "trainer/Log Pis Mean                                      1.99003\r\n",
      "trainer/Log Pis Std                                       1.37642\r\n",
      "trainer/Log Pis Max                                       6.35413\r\n",
      "trainer/Log Pis Min                                      -4.04508\r\n",
      "trainer/Policy mu Mean                                   -0.0228919\r\n",
      "trainer/Policy mu Std                                     0.335093\r\n",
      "trainer/Policy mu Max                                     1.8992\r\n",
      "trainer/Policy mu Min                                    -2.47414\r\n",
      "trainer/Policy log std Mean                              -2.30427\r\n",
      "trainer/Policy log std Std                                0.59494\r\n",
      "trainer/Policy log std Max                               -0.281194\r\n",
      "trainer/Policy log std Min                               -3.36808\r\n",
      "trainer/Alpha                                             0.0230919\r\n",
      "trainer/Alpha Loss                                       -0.0375738\r\n",
      "exploration/num steps total                           11700\r\n",
      "exploration/num paths total                             585\r\n",
      "exploration/path length Mean                             20\r\n",
      "exploration/path length Std                               0\r\n",
      "exploration/path length Max                              20\r\n",
      "exploration/path length Min                              20\r\n",
      "exploration/Rewards Mean                                 -0.0926117\r\n",
      "exploration/Rewards Std                                   0.0993617\r\n",
      "exploration/Rewards Max                                   0.164075\r\n",
      "exploration/Rewards Min                                  -0.338127\r\n",
      "exploration/Returns Mean                                 -1.85223\r\n",
      "exploration/Returns Std                                   1.69177\r\n",
      "exploration/Returns Max                                   0.935105\r\n",
      "exploration/Returns Min                                  -3.82528\r\n",
      "exploration/Actions Mean                                 -0.00522957\r\n",
      "exploration/Actions Std                                   0.122706\r\n",
      "exploration/Actions Max                                   0.510138\r\n",
      "exploration/Actions Min                                  -0.651233\r\n",
      "exploration/Num Paths                                     5\r\n",
      "exploration/Average Returns                              -1.85223\r\n",
      "exploration/env_infos/final/reward_dist Mean              0.282595\r\n",
      "exploration/env_infos/final/reward_dist Std               0.359043\r\n",
      "exploration/env_infos/final/reward_dist Max               0.860791\r\n",
      "exploration/env_infos/final/reward_dist Min               1.27265e-22\r\n",
      "exploration/env_infos/initial/reward_dist Mean            0.00967069\r\n",
      "exploration/env_infos/initial/reward_dist Std             0.0168968\r\n",
      "exploration/env_infos/initial/reward_dist Max             0.0434524\r\n",
      "exploration/env_infos/initial/reward_dist Min             0.000423033\r\n",
      "exploration/env_infos/reward_dist Mean                    0.135074\r\n",
      "exploration/env_infos/reward_dist Std                     0.235793\r\n",
      "exploration/env_infos/reward_dist Max                     0.995026\r\n",
      "exploration/env_infos/reward_dist Min                     1.27265e-22\r\n",
      "exploration/env_infos/final/reward_energy Mean           -0.14264\r\n",
      "exploration/env_infos/final/reward_energy Std             0.0707213\r\n",
      "exploration/env_infos/final/reward_energy Max            -0.0245482\r\n",
      "exploration/env_infos/final/reward_energy Min            -0.235164\r\n",
      "exploration/env_infos/initial/reward_energy Mean         -0.200553\r\n",
      "exploration/env_infos/initial/reward_energy Std           0.0712855\r\n",
      "exploration/env_infos/initial/reward_energy Max          -0.109593\r\n",
      "exploration/env_infos/initial/reward_energy Min          -0.285966\r\n",
      "exploration/env_infos/reward_energy Mean                 -0.134961\r\n",
      "exploration/env_infos/reward_energy Std                   0.109332\r\n",
      "exploration/env_infos/reward_energy Max                  -0.00247481\r\n",
      "exploration/env_infos/reward_energy Min                  -0.651325\r\n",
      "exploration/env_infos/final/reward_safety Mean            0\r\n",
      "exploration/env_infos/final/reward_safety Std             0\r\n",
      "exploration/env_infos/final/reward_safety Max             0\r\n",
      "exploration/env_infos/final/reward_safety Min             0\r\n",
      "exploration/env_infos/initial/reward_safety Mean          0\r\n",
      "exploration/env_infos/initial/reward_safety Std           0\r\n",
      "exploration/env_infos/initial/reward_safety Max           0\r\n",
      "exploration/env_infos/initial/reward_safety Min           0\r\n",
      "exploration/env_infos/reward_safety Mean                  0\r\n",
      "exploration/env_infos/reward_safety Std                   0\r\n",
      "exploration/env_infos/reward_safety Max                   0\r\n",
      "exploration/env_infos/reward_safety Min                   0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        -0.126674\r\n",
      "exploration/env_infos/final/end_effector_loc Std          0.27405\r\n",
      "exploration/env_infos/final/end_effector_loc Max          0.236216\r\n",
      "exploration/env_infos/final/end_effector_loc Min         -0.766657\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      -0.00214762\r\n",
      "exploration/env_infos/initial/end_effector_loc Std        0.00721227\r\n",
      "exploration/env_infos/initial/end_effector_loc Max        0.00700883\r\n",
      "exploration/env_infos/initial/end_effector_loc Min       -0.0131134\r\n",
      "exploration/env_infos/end_effector_loc Mean              -0.0720835\r\n",
      "exploration/env_infos/end_effector_loc Std                0.16897\r\n",
      "exploration/env_infos/end_effector_loc Max                0.236216\r\n",
      "exploration/env_infos/end_effector_loc Min               -0.766657\r\n",
      "evaluation/num steps total                           107000\r\n",
      "evaluation/num paths total                             5350\r\n",
      "evaluation/path length Mean                              20\r\n",
      "evaluation/path length Std                                0\r\n",
      "evaluation/path length Max                               20\r\n",
      "evaluation/path length Min                               20\r\n",
      "evaluation/Rewards Mean                                  -0.0245087\r\n",
      "evaluation/Rewards Std                                    0.0825388\r\n",
      "evaluation/Rewards Max                                    0.163418\r\n",
      "evaluation/Rewards Min                                   -0.394228\r\n",
      "evaluation/Returns Mean                                  -0.490174\r\n",
      "evaluation/Returns Std                                    1.17395\r\n",
      "evaluation/Returns Max                                    2.12156\r\n",
      "evaluation/Returns Min                                   -2.43619\r\n",
      "evaluation/Actions Mean                                  -0.0030413\r\n",
      "evaluation/Actions Std                                    0.0662166\r\n",
      "evaluation/Actions Max                                    0.602375\r\n",
      "evaluation/Actions Min                                   -0.463184\r\n",
      "evaluation/Num Paths                                     50\r\n",
      "evaluation/Average Returns                               -0.490174\r\n",
      "evaluation/env_infos/final/reward_dist Mean               0.184829\r\n",
      "evaluation/env_infos/final/reward_dist Std                0.262018\r\n",
      "evaluation/env_infos/final/reward_dist Max                0.962789\r\n",
      "evaluation/env_infos/final/reward_dist Min                1.17804e-22\r\n",
      "evaluation/env_infos/initial/reward_dist Mean             0.0049755\r\n",
      "evaluation/env_infos/initial/reward_dist Std              0.0100465\r\n",
      "evaluation/env_infos/initial/reward_dist Max              0.0550762\r\n",
      "evaluation/env_infos/initial/reward_dist Min              9.65584e-07\r\n",
      "evaluation/env_infos/reward_dist Mean                     0.244146\r\n",
      "evaluation/env_infos/reward_dist Std                      0.301324\r\n",
      "evaluation/env_infos/reward_dist Max                      0.989057\r\n",
      "evaluation/env_infos/reward_dist Min                      1.17804e-22\r\n",
      "evaluation/env_infos/final/reward_energy Mean            -0.0512207\r\n",
      "evaluation/env_infos/final/reward_energy Std              0.0409013\r\n",
      "evaluation/env_infos/final/reward_energy Max             -0.00281267\r\n",
      "evaluation/env_infos/final/reward_energy Min             -0.18454\r\n",
      "evaluation/env_infos/initial/reward_energy Mean          -0.23757\r\n",
      "evaluation/env_infos/initial/reward_energy Std            0.165305\r\n",
      "evaluation/env_infos/initial/reward_energy Max           -0.0129215\r\n",
      "evaluation/env_infos/initial/reward_energy Min           -0.710333\r\n",
      "evaluation/env_infos/reward_energy Mean                  -0.0613484\r\n",
      "evaluation/env_infos/reward_energy Std                    0.0708813\r\n",
      "evaluation/env_infos/reward_energy Max                   -0.000366379\r\n",
      "evaluation/env_infos/reward_energy Min                   -0.710333\r\n",
      "evaluation/env_infos/final/reward_safety Mean             0\r\n",
      "evaluation/env_infos/final/reward_safety Std              0\r\n",
      "evaluation/env_infos/final/reward_safety Max              0\r\n",
      "evaluation/env_infos/final/reward_safety Min              0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean           0\r\n",
      "evaluation/env_infos/initial/reward_safety Std            0\r\n",
      "evaluation/env_infos/initial/reward_safety Max            0\r\n",
      "evaluation/env_infos/initial/reward_safety Min            0\r\n",
      "evaluation/env_infos/reward_safety Mean                   0\r\n",
      "evaluation/env_infos/reward_safety Std                    0\r\n",
      "evaluation/env_infos/reward_safety Max                    0\r\n",
      "evaluation/env_infos/reward_safety Min                    0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         -0.0712727\r\n",
      "evaluation/env_infos/final/end_effector_loc Std           0.247945\r\n",
      "evaluation/env_infos/final/end_effector_loc Max           0.472253\r\n",
      "evaluation/env_infos/final/end_effector_loc Min          -0.72336\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       -0.000479196\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std         0.0102214\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max         0.0301188\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min        -0.0231592\r\n",
      "evaluation/env_infos/end_effector_loc Mean               -0.0335302\r\n",
      "evaluation/env_infos/end_effector_loc Std                 0.165279\r\n",
      "evaluation/env_infos/end_effector_loc Max                 0.472253\r\n",
      "evaluation/env_infos/end_effector_loc Min                -0.72336\r\n",
      "time/data storing (s)                                     0.00302312\r\n",
      "time/evaluation sampling (s)                              0.992068\r\n",
      "time/exploration sampling (s)                             0.125795\r\n",
      "time/logging (s)                                          0.0205859\r\n",
      "time/saving (s)                                           0.031402\r\n",
      "time/training (s)                                        51.2135\r\n",
      "time/epoch (s)                                           52.3863\r\n",
      "time/total (s)                                         5709.55\r\n",
      "Epoch                                                   106\r\n",
      "---------------------------------------------------  ----------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:49:23.701404 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 107 finished\r\n",
      "---------------------------------------------------  ----------------\r\n",
      "replay_buffer/size                                     2000\r\n",
      "trainer/QF1 Loss                                          0.00115169\r\n",
      "trainer/QF2 Loss                                          0.00140755\r\n",
      "trainer/Policy Loss                                       2.99921\r\n",
      "trainer/Q1 Predictions Mean                              -0.889727\r\n",
      "trainer/Q1 Predictions Std                                0.802804\r\n",
      "trainer/Q1 Predictions Max                                0.754837\r\n",
      "trainer/Q1 Predictions Min                               -3.166\r\n",
      "trainer/Q2 Predictions Mean                              -0.888488\r\n",
      "trainer/Q2 Predictions Std                                0.800225\r\n",
      "trainer/Q2 Predictions Max                                0.749775\r\n",
      "trainer/Q2 Predictions Min                               -3.13668\r\n",
      "trainer/Q Targets Mean                                   -0.896153\r\n",
      "trainer/Q Targets Std                                     0.811065\r\n",
      "trainer/Q Targets Max                                     0.787552\r\n",
      "trainer/Q Targets Min                                    -3.16558\r\n",
      "trainer/Log Pis Mean                                      2.11199\r\n",
      "trainer/Log Pis Std                                       1.3191\r\n",
      "trainer/Log Pis Max                                       4.75371\r\n",
      "trainer/Log Pis Min                                      -2.64353\r\n",
      "trainer/Policy mu Mean                                   -0.0189385\r\n",
      "trainer/Policy mu Std                                     0.312812\r\n",
      "trainer/Policy mu Max                                     1.85514\r\n",
      "trainer/Policy mu Min                                    -1.98728\r\n",
      "trainer/Policy log std Mean                              -2.38233\r\n",
      "trainer/Policy log std Std                                0.571152\r\n",
      "trainer/Policy log std Max                               -0.0723205\r\n",
      "trainer/Policy log std Min                               -3.32117\r\n",
      "trainer/Alpha                                             0.0230121\r\n",
      "trainer/Alpha Loss                                        0.42275\r\n",
      "exploration/num steps total                           11800\r\n",
      "exploration/num paths total                             590\r\n",
      "exploration/path length Mean                             20\r\n",
      "exploration/path length Std                               0\r\n",
      "exploration/path length Max                              20\r\n",
      "exploration/path length Min                              20\r\n",
      "exploration/Rewards Mean                                 -0.0718029\r\n",
      "exploration/Rewards Std                                   0.104598\r\n",
      "exploration/Rewards Max                                   0.136035\r\n",
      "exploration/Rewards Min                                  -0.336208\r\n",
      "exploration/Returns Mean                                 -1.43606\r\n",
      "exploration/Returns Std                                   1.67647\r\n",
      "exploration/Returns Max                                   0.694809\r\n",
      "exploration/Returns Min                                  -3.31094\r\n",
      "exploration/Actions Mean                                 -0.00800893\r\n",
      "exploration/Actions Std                                   0.129361\r\n",
      "exploration/Actions Max                                   0.511475\r\n",
      "exploration/Actions Min                                  -0.573373\r\n",
      "exploration/Num Paths                                     5\r\n",
      "exploration/Average Returns                              -1.43606\r\n",
      "exploration/env_infos/final/reward_dist Mean              0.0135348\r\n",
      "exploration/env_infos/final/reward_dist Std               0.0110831\r\n",
      "exploration/env_infos/final/reward_dist Max               0.0240923\r\n",
      "exploration/env_infos/final/reward_dist Min               1.62185e-08\r\n",
      "exploration/env_infos/initial/reward_dist Mean            0.00203715\r\n",
      "exploration/env_infos/initial/reward_dist Std             0.00349351\r\n",
      "exploration/env_infos/initial/reward_dist Max             0.00901154\r\n",
      "exploration/env_infos/initial/reward_dist Min             1.55745e-05\r\n",
      "exploration/env_infos/reward_dist Mean                    0.201871\r\n",
      "exploration/env_infos/reward_dist Std                     0.24048\r\n",
      "exploration/env_infos/reward_dist Max                     0.856126\r\n",
      "exploration/env_infos/reward_dist Min                     1.62185e-08\r\n",
      "exploration/env_infos/final/reward_energy Mean           -0.0905191\r\n",
      "exploration/env_infos/final/reward_energy Std             0.0564856\r\n",
      "exploration/env_infos/final/reward_energy Max            -0.00988332\r\n",
      "exploration/env_infos/final/reward_energy Min            -0.158235\r\n",
      "exploration/env_infos/initial/reward_energy Mean         -0.30688\r\n",
      "exploration/env_infos/initial/reward_energy Std           0.145304\r\n",
      "exploration/env_infos/initial/reward_energy Max          -0.191617\r\n",
      "exploration/env_infos/initial/reward_energy Min          -0.587676\r\n",
      "exploration/env_infos/reward_energy Mean                 -0.150178\r\n",
      "exploration/env_infos/reward_energy Std                   0.105088\r\n",
      "exploration/env_infos/reward_energy Max                  -0.00748321\r\n",
      "exploration/env_infos/reward_energy Min                  -0.587676\r\n",
      "exploration/env_infos/final/reward_safety Mean            0\r\n",
      "exploration/env_infos/final/reward_safety Std             0\r\n",
      "exploration/env_infos/final/reward_safety Max             0\r\n",
      "exploration/env_infos/final/reward_safety Min             0\r\n",
      "exploration/env_infos/initial/reward_safety Mean          0\r\n",
      "exploration/env_infos/initial/reward_safety Std           0\r\n",
      "exploration/env_infos/initial/reward_safety Max           0\r\n",
      "exploration/env_infos/initial/reward_safety Min           0\r\n",
      "exploration/env_infos/reward_safety Mean                  0\r\n",
      "exploration/env_infos/reward_safety Std                   0\r\n",
      "exploration/env_infos/reward_safety Max                   0\r\n",
      "exploration/env_infos/reward_safety Min                   0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean        -0.149807\r\n",
      "exploration/env_infos/final/end_effector_loc Std          0.248547\r\n",
      "exploration/env_infos/final/end_effector_loc Max          0.348817\r\n",
      "exploration/env_infos/final/end_effector_loc Min         -0.551777\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      -0.00184701\r\n",
      "exploration/env_infos/initial/end_effector_loc Std        0.0118617\r\n",
      "exploration/env_infos/initial/end_effector_loc Max        0.00920769\r\n",
      "exploration/env_infos/initial/end_effector_loc Min       -0.0286687\r\n",
      "exploration/env_infos/end_effector_loc Mean              -0.0752822\r\n",
      "exploration/env_infos/end_effector_loc Std                0.169168\r\n",
      "exploration/env_infos/end_effector_loc Max                0.348817\r\n",
      "exploration/env_infos/end_effector_loc Min               -0.551777\r\n",
      "evaluation/num steps total                           108000\r\n",
      "evaluation/num paths total                             5400\r\n",
      "evaluation/path length Mean                              20\r\n",
      "evaluation/path length Std                                0\r\n",
      "evaluation/path length Max                               20\r\n",
      "evaluation/path length Min                               20\r\n",
      "evaluation/Rewards Mean                                  -0.0401879\r\n",
      "evaluation/Rewards Std                                    0.0710846\r\n",
      "evaluation/Rewards Max                                    0.134962\r\n",
      "evaluation/Rewards Min                                   -0.287462\r\n",
      "evaluation/Returns Mean                                  -0.803758\r\n",
      "evaluation/Returns Std                                    1.06835\r\n",
      "evaluation/Returns Max                                    1.4024\r\n",
      "evaluation/Returns Min                                   -3.18294\r\n",
      "evaluation/Actions Mean                                   0.000987379\r\n",
      "evaluation/Actions Std                                    0.0652057\r\n",
      "evaluation/Actions Max                                    0.542108\r\n",
      "evaluation/Actions Min                                   -0.598412\r\n",
      "evaluation/Num Paths                                     50\r\n",
      "evaluation/Average Returns                               -0.803758\r\n",
      "evaluation/env_infos/final/reward_dist Mean               0.256618\r\n",
      "evaluation/env_infos/final/reward_dist Std                0.320241\r\n",
      "evaluation/env_infos/final/reward_dist Max                0.993763\r\n",
      "evaluation/env_infos/final/reward_dist Min                5.49612e-11\r\n",
      "evaluation/env_infos/initial/reward_dist Mean             0.0040527\r\n",
      "evaluation/env_infos/initial/reward_dist Std              0.00727677\r\n",
      "evaluation/env_infos/initial/reward_dist Max              0.0294905\r\n",
      "evaluation/env_infos/initial/reward_dist Min              9.27452e-07\r\n",
      "evaluation/env_infos/reward_dist Mean                     0.203555\r\n",
      "evaluation/env_infos/reward_dist Std                      0.279397\r\n",
      "evaluation/env_infos/reward_dist Max                      0.993763\r\n",
      "evaluation/env_infos/reward_dist Min                      5.49612e-11\r\n",
      "evaluation/env_infos/final/reward_energy Mean            -0.0393536\r\n",
      "evaluation/env_infos/final/reward_energy Std              0.0266639\r\n",
      "evaluation/env_infos/final/reward_energy Max             -0.00349749\r\n",
      "evaluation/env_infos/final/reward_energy Min             -0.142686\r\n",
      "evaluation/env_infos/initial/reward_energy Mean          -0.222386\r\n",
      "evaluation/env_infos/initial/reward_energy Std            0.201254\r\n",
      "evaluation/env_infos/initial/reward_energy Max           -0.0108519\r\n",
      "evaluation/env_infos/initial/reward_energy Min           -0.783152\r\n",
      "evaluation/env_infos/reward_energy Mean                  -0.0542424\r\n",
      "evaluation/env_infos/reward_energy Std                    0.0745873\r\n",
      "evaluation/env_infos/reward_energy Max                   -0.000813258\r\n",
      "evaluation/env_infos/reward_energy Min                   -0.783152\r\n",
      "evaluation/env_infos/final/reward_safety Mean             0\r\n",
      "evaluation/env_infos/final/reward_safety Std              0\r\n",
      "evaluation/env_infos/final/reward_safety Max              0\r\n",
      "evaluation/env_infos/final/reward_safety Min              0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean           0\r\n",
      "evaluation/env_infos/initial/reward_safety Std            0\r\n",
      "evaluation/env_infos/initial/reward_safety Max            0\r\n",
      "evaluation/env_infos/initial/reward_safety Min            0\r\n",
      "evaluation/env_infos/reward_safety Mean                   0\r\n",
      "evaluation/env_infos/reward_safety Std                    0\r\n",
      "evaluation/env_infos/reward_safety Max                    0\r\n",
      "evaluation/env_infos/reward_safety Min                    0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean          0.0248473\r\n",
      "evaluation/env_infos/final/end_effector_loc Std           0.22781\r\n",
      "evaluation/env_infos/final/end_effector_loc Max           0.473132\r\n",
      "evaluation/env_infos/final/end_effector_loc Min          -0.548087\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean        0.000785391\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std         0.010575\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max         0.0271054\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min        -0.0299206\r\n",
      "evaluation/env_infos/end_effector_loc Mean                0.0150168\r\n",
      "evaluation/env_infos/end_effector_loc Std                 0.149785\r\n",
      "evaluation/env_infos/end_effector_loc Max                 0.473132\r\n",
      "evaluation/env_infos/end_effector_loc Min                -0.548087\r\n",
      "time/data storing (s)                                     0.00310516\r\n",
      "time/evaluation sampling (s)                              1.02661\r\n",
      "time/exploration sampling (s)                             0.143373\r\n",
      "time/logging (s)                                          0.0223987\r\n",
      "time/saving (s)                                           0.0317302\r\n",
      "time/training (s)                                        53.7142\r\n",
      "time/epoch (s)                                           54.9415\r\n",
      "time/total (s)                                         5765.87\r\n",
      "Epoch                                                   107\r\n",
      "---------------------------------------------------  ----------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:50:21.788548 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 108 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                     2000\n",
      "trainer/QF1 Loss                                          0.000882295\n",
      "trainer/QF2 Loss                                          0.000771717\n",
      "trainer/Policy Loss                                       2.98846\n",
      "trainer/Q1 Predictions Mean                              -0.944211\n",
      "trainer/Q1 Predictions Std                                0.870756\n",
      "trainer/Q1 Predictions Max                                0.66897\n",
      "trainer/Q1 Predictions Min                               -3.37345\n",
      "trainer/Q2 Predictions Mean                              -0.940628\n",
      "trainer/Q2 Predictions Std                                0.873525\n",
      "trainer/Q2 Predictions Max                                0.666524\n",
      "trainer/Q2 Predictions Min                               -3.37526\n",
      "trainer/Q Targets Mean                                   -0.937577\n",
      "trainer/Q Targets Std                                     0.866261\n",
      "trainer/Q Targets Max                                     0.67071\n",
      "trainer/Q Targets Min                                    -3.36511\n",
      "trainer/Log Pis Mean                                      2.05929\n",
      "trainer/Log Pis Std                                       1.27849\n",
      "trainer/Log Pis Max                                       4.76775\n",
      "trainer/Log Pis Min                                      -2.42952\n",
      "trainer/Policy mu Mean                                    0.0282869\n",
      "trainer/Policy mu Std                                     0.278695\n",
      "trainer/Policy mu Max                                     2.10193\n",
      "trainer/Policy mu Min                                    -1.38888\n",
      "trainer/Policy log std Mean                              -2.33017\n",
      "trainer/Policy log std Std                                0.525954\n",
      "trainer/Policy log std Max                               -0.674355\n",
      "trainer/Policy log std Min                               -3.2717\n",
      "trainer/Alpha                                             0.0236292\n",
      "trainer/Alpha Loss                                        0.222026\n",
      "exploration/num steps total                           11900\n",
      "exploration/num paths total                             595\n",
      "exploration/path length Mean                             20\n",
      "exploration/path length Std                               0\n",
      "exploration/path length Max                              20\n",
      "exploration/path length Min                              20\n",
      "exploration/Rewards Mean                                 -0.0764736\n",
      "exploration/Rewards Std                                   0.0504706\n",
      "exploration/Rewards Max                                   0.0370679\n",
      "exploration/Rewards Min                                  -0.236115\n",
      "exploration/Returns Mean                                 -1.52947\n",
      "exploration/Returns Std                                   0.434943\n",
      "exploration/Returns Max                                  -0.848799\n",
      "exploration/Returns Min                                  -2.1376\n",
      "exploration/Actions Mean                                 -0.00721185\n",
      "exploration/Actions Std                                   0.0929895\n",
      "exploration/Actions Max                                   0.28849\n",
      "exploration/Actions Min                                  -0.347379\n",
      "exploration/Num Paths                                     5\n",
      "exploration/Average Returns                              -1.52947\n",
      "exploration/env_infos/final/reward_dist Mean              0.0910357\n",
      "exploration/env_infos/final/reward_dist Std               0.1054\n",
      "exploration/env_infos/final/reward_dist Max               0.248933\n",
      "exploration/env_infos/final/reward_dist Min               1.39227e-07\n",
      "exploration/env_infos/initial/reward_dist Mean            0.0139074\n",
      "exploration/env_infos/initial/reward_dist Std             0.0173716\n",
      "exploration/env_infos/initial/reward_dist Max             0.0459977\n",
      "exploration/env_infos/initial/reward_dist Min             2.13278e-05\n",
      "exploration/env_infos/reward_dist Mean                    0.107754\n",
      "exploration/env_infos/reward_dist Std                     0.185865\n",
      "exploration/env_infos/reward_dist Max                     0.78308\n",
      "exploration/env_infos/reward_dist Min                     1.39227e-07\n",
      "exploration/env_infos/final/reward_energy Mean           -0.120912\n",
      "exploration/env_infos/final/reward_energy Std             0.0798174\n",
      "exploration/env_infos/final/reward_energy Max            -0.0648763\n",
      "exploration/env_infos/final/reward_energy Min            -0.276721\n",
      "exploration/env_infos/initial/reward_energy Mean         -0.2123\n",
      "exploration/env_infos/initial/reward_energy Std           0.159163\n",
      "exploration/env_infos/initial/reward_energy Max          -0.010538\n",
      "exploration/env_infos/initial/reward_energy Min          -0.418219\n",
      "exploration/env_infos/reward_energy Mean                 -0.106789\n",
      "exploration/env_infos/reward_energy Std                   0.077423\n",
      "exploration/env_infos/reward_energy Max                  -0.00722027\n",
      "exploration/env_infos/reward_energy Min                  -0.418219\n",
      "exploration/env_infos/final/reward_safety Mean            0\n",
      "exploration/env_infos/final/reward_safety Std             0\n",
      "exploration/env_infos/final/reward_safety Max             0\n",
      "exploration/env_infos/final/reward_safety Min             0\n",
      "exploration/env_infos/initial/reward_safety Mean          0\n",
      "exploration/env_infos/initial/reward_safety Std           0\n",
      "exploration/env_infos/initial/reward_safety Max           0\n",
      "exploration/env_infos/initial/reward_safety Min           0\n",
      "exploration/env_infos/reward_safety Mean                  0\n",
      "exploration/env_infos/reward_safety Std                   0\n",
      "exploration/env_infos/reward_safety Max                   0\n",
      "exploration/env_infos/reward_safety Min                   0\n",
      "exploration/env_infos/final/end_effector_loc Mean        -0.122421\n",
      "exploration/env_infos/final/end_effector_loc Std          0.192168\n",
      "exploration/env_infos/final/end_effector_loc Max          0.159867\n",
      "exploration/env_infos/final/end_effector_loc Min         -0.395596\n",
      "exploration/env_infos/initial/end_effector_loc Mean      -0.00593544\n",
      "exploration/env_infos/initial/end_effector_loc Std        0.00726467\n",
      "exploration/env_infos/initial/end_effector_loc Max        0.00238091\n",
      "exploration/env_infos/initial/end_effector_loc Min       -0.017369\n",
      "exploration/env_infos/end_effector_loc Mean              -0.0642038\n",
      "exploration/env_infos/end_effector_loc Std                0.135782\n",
      "exploration/env_infos/end_effector_loc Max                0.159867\n",
      "exploration/env_infos/end_effector_loc Min               -0.399294\n",
      "evaluation/num steps total                           109000\n",
      "evaluation/num paths total                             5450\n",
      "evaluation/path length Mean                              20\n",
      "evaluation/path length Std                                0\n",
      "evaluation/path length Max                               20\n",
      "evaluation/path length Min                               20\n",
      "evaluation/Rewards Mean                                  -0.0336651\n",
      "evaluation/Rewards Std                                    0.0735198\n",
      "evaluation/Rewards Max                                    0.158015\n",
      "evaluation/Rewards Min                                   -0.31967\n",
      "evaluation/Returns Mean                                  -0.673301\n",
      "evaluation/Returns Std                                    1.08949\n",
      "evaluation/Returns Max                                    1.94211\n",
      "evaluation/Returns Min                                   -2.8014\n",
      "evaluation/Actions Mean                                  -0.00140109\n",
      "evaluation/Actions Std                                    0.0667547\n",
      "evaluation/Actions Max                                    0.570683\n",
      "evaluation/Actions Min                                   -0.514782\n",
      "evaluation/Num Paths                                     50\n",
      "evaluation/Average Returns                               -0.673301\n",
      "evaluation/env_infos/final/reward_dist Mean               0.169075\n",
      "evaluation/env_infos/final/reward_dist Std                0.239388\n",
      "evaluation/env_infos/final/reward_dist Max                0.771477\n",
      "evaluation/env_infos/final/reward_dist Min                9.73135e-13\n",
      "evaluation/env_infos/initial/reward_dist Mean             0.00700929\n",
      "evaluation/env_infos/initial/reward_dist Std              0.0128225\n",
      "evaluation/env_infos/initial/reward_dist Max              0.0701578\n",
      "evaluation/env_infos/initial/reward_dist Min              1.08137e-06\n",
      "evaluation/env_infos/reward_dist Mean                     0.238986\n",
      "evaluation/env_infos/reward_dist Std                      0.279872\n",
      "evaluation/env_infos/reward_dist Max                      0.999945\n",
      "evaluation/env_infos/reward_dist Min                      9.73135e-13\n",
      "evaluation/env_infos/final/reward_energy Mean            -0.0432221\n",
      "evaluation/env_infos/final/reward_energy Std              0.0472931\n",
      "evaluation/env_infos/final/reward_energy Max             -0.00348305\n",
      "evaluation/env_infos/final/reward_energy Min             -0.294464\n",
      "evaluation/env_infos/initial/reward_energy Mean          -0.233118\n",
      "evaluation/env_infos/initial/reward_energy Std            0.160341\n",
      "evaluation/env_infos/initial/reward_energy Max           -0.0326693\n",
      "evaluation/env_infos/initial/reward_energy Min           -0.573368\n",
      "evaluation/env_infos/reward_energy Mean                  -0.0600319\n",
      "evaluation/env_infos/reward_energy Std                    0.0728866\n",
      "evaluation/env_infos/reward_energy Max                   -0.0011545\n",
      "evaluation/env_infos/reward_energy Min                   -0.573368\n",
      "evaluation/env_infos/final/reward_safety Mean             0\n",
      "evaluation/env_infos/final/reward_safety Std              0\n",
      "evaluation/env_infos/final/reward_safety Max              0\n",
      "evaluation/env_infos/final/reward_safety Min              0\n",
      "evaluation/env_infos/initial/reward_safety Mean           0\n",
      "evaluation/env_infos/initial/reward_safety Std            0\n",
      "evaluation/env_infos/initial/reward_safety Max            0\n",
      "evaluation/env_infos/initial/reward_safety Min            0\n",
      "evaluation/env_infos/reward_safety Mean                   0\n",
      "evaluation/env_infos/reward_safety Std                    0\n",
      "evaluation/env_infos/reward_safety Max                    0\n",
      "evaluation/env_infos/reward_safety Min                    0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         -0.0131059\n",
      "evaluation/env_infos/final/end_effector_loc Std           0.262874\n",
      "evaluation/env_infos/final/end_effector_loc Max           0.516902\n",
      "evaluation/env_infos/final/end_effector_loc Min          -0.611907\n",
      "evaluation/env_infos/initial/end_effector_loc Mean        0.000944458\n",
      "evaluation/env_infos/initial/end_effector_loc Std         0.00995865\n",
      "evaluation/env_infos/initial/end_effector_loc Max         0.0285342\n",
      "evaluation/env_infos/initial/end_effector_loc Min        -0.0257391\n",
      "evaluation/env_infos/end_effector_loc Mean               -0.000881708\n",
      "evaluation/env_infos/end_effector_loc Std                 0.168043\n",
      "evaluation/env_infos/end_effector_loc Max                 0.516902\n",
      "evaluation/env_infos/end_effector_loc Min                -0.611907\n",
      "time/data storing (s)                                     0.00311485\n",
      "time/evaluation sampling (s)                              1.11357\n",
      "time/exploration sampling (s)                             0.13069\n",
      "time/logging (s)                                          0.0244846\n",
      "time/saving (s)                                           0.0360929\n",
      "time/training (s)                                        55.2827\n",
      "time/epoch (s)                                           56.5906\n",
      "time/total (s)                                         5823.95\n",
      "Epoch                                                   108\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:51:19.506173 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 109 finished\r\n",
      "---------------------------------------------------  ----------------\r\n",
      "replay_buffer/size                                     2000\r\n",
      "trainer/QF1 Loss                                          0.00061819\r\n",
      "trainer/QF2 Loss                                          0.00145868\r\n",
      "trainer/Policy Loss                                       2.79911\r\n",
      "trainer/Q1 Predictions Mean                              -0.955331\r\n",
      "trainer/Q1 Predictions Std                                0.827693\r\n",
      "trainer/Q1 Predictions Max                                0.830229\r\n",
      "trainer/Q1 Predictions Min                               -3.27995\r\n",
      "trainer/Q2 Predictions Mean                              -0.949492\r\n",
      "trainer/Q2 Predictions Std                                0.830917\r\n",
      "trainer/Q2 Predictions Max                                0.81324\r\n",
      "trainer/Q2 Predictions Min                               -3.29096\r\n",
      "trainer/Q Targets Mean                                   -0.95497\r\n",
      "trainer/Q Targets Std                                     0.831056\r\n",
      "trainer/Q Targets Max                                     0.805168\r\n",
      "trainer/Q Targets Min                                    -3.27209\r\n",
      "trainer/Log Pis Mean                                      1.84711\r\n",
      "trainer/Log Pis Std                                       1.36213\r\n",
      "trainer/Log Pis Max                                       6.1064\r\n",
      "trainer/Log Pis Min                                      -2.86255\r\n",
      "trainer/Policy mu Mean                                    0.0291241\r\n",
      "trainer/Policy mu Std                                     0.290379\r\n",
      "trainer/Policy mu Max                                     2.1641\r\n",
      "trainer/Policy mu Min                                    -1.97051\r\n",
      "trainer/Policy log std Mean                              -2.27679\r\n",
      "trainer/Policy log std Std                                0.510066\r\n",
      "trainer/Policy log std Max                                0.332454\r\n",
      "trainer/Policy log std Min                               -3.20674\r\n",
      "trainer/Alpha                                             0.0235319\r\n",
      "trainer/Alpha Loss                                       -0.572916\r\n",
      "exploration/num steps total                           12000\r\n",
      "exploration/num paths total                             600\r\n",
      "exploration/path length Mean                             20\r\n",
      "exploration/path length Std                               0\r\n",
      "exploration/path length Max                              20\r\n",
      "exploration/path length Min                              20\r\n",
      "exploration/Rewards Mean                                 -0.125338\r\n",
      "exploration/Rewards Std                                   0.0670743\r\n",
      "exploration/Rewards Max                                   0.0140541\r\n",
      "exploration/Rewards Min                                  -0.418222\r\n",
      "exploration/Returns Mean                                 -2.50676\r\n",
      "exploration/Returns Std                                   0.601199\r\n",
      "exploration/Returns Max                                  -1.3511\r\n",
      "exploration/Returns Min                                  -3.02255\r\n",
      "exploration/Actions Mean                                  0.00258541\r\n",
      "exploration/Actions Std                                   0.187733\r\n",
      "exploration/Actions Max                                   0.647082\r\n",
      "exploration/Actions Min                                  -0.766726\r\n",
      "exploration/Num Paths                                     5\r\n",
      "exploration/Average Returns                              -2.50676\r\n",
      "exploration/env_infos/final/reward_dist Mean              0.000544239\r\n",
      "exploration/env_infos/final/reward_dist Std               0.00059577\r\n",
      "exploration/env_infos/final/reward_dist Max               0.00145935\r\n",
      "exploration/env_infos/final/reward_dist Min               1.65763e-06\r\n",
      "exploration/env_infos/initial/reward_dist Mean            0.00409268\r\n",
      "exploration/env_infos/initial/reward_dist Std             0.00387986\r\n",
      "exploration/env_infos/initial/reward_dist Max             0.0091113\r\n",
      "exploration/env_infos/initial/reward_dist Min             1.06299e-05\r\n",
      "exploration/env_infos/reward_dist Mean                    0.0915337\r\n",
      "exploration/env_infos/reward_dist Std                     0.201888\r\n",
      "exploration/env_infos/reward_dist Max                     0.971191\r\n",
      "exploration/env_infos/reward_dist Min                     1.91201e-17\r\n",
      "exploration/env_infos/final/reward_energy Mean           -0.227893\r\n",
      "exploration/env_infos/final/reward_energy Std             0.195147\r\n",
      "exploration/env_infos/final/reward_energy Max            -0.0318001\r\n",
      "exploration/env_infos/final/reward_energy Min            -0.58887\r\n",
      "exploration/env_infos/initial/reward_energy Mean         -0.531663\r\n",
      "exploration/env_infos/initial/reward_energy Std           0.257806\r\n",
      "exploration/env_infos/initial/reward_energy Max          -0.15251\r\n",
      "exploration/env_infos/initial/reward_energy Min          -0.824183\r\n",
      "exploration/env_infos/reward_energy Mean                 -0.203274\r\n",
      "exploration/env_infos/reward_energy Std                   0.170823\r\n",
      "exploration/env_infos/reward_energy Max                  -0.00307576\r\n",
      "exploration/env_infos/reward_energy Min                  -0.93262\r\n",
      "exploration/env_infos/final/reward_safety Mean            0\r\n",
      "exploration/env_infos/final/reward_safety Std             0\r\n",
      "exploration/env_infos/final/reward_safety Max             0\r\n",
      "exploration/env_infos/final/reward_safety Min             0\r\n",
      "exploration/env_infos/initial/reward_safety Mean          0\r\n",
      "exploration/env_infos/initial/reward_safety Std           0\r\n",
      "exploration/env_infos/initial/reward_safety Max           0\r\n",
      "exploration/env_infos/initial/reward_safety Min           0\r\n",
      "exploration/env_infos/reward_safety Mean                  0\r\n",
      "exploration/env_infos/reward_safety Std                   0\r\n",
      "exploration/env_infos/reward_safety Max                   0\r\n",
      "exploration/env_infos/reward_safety Min                   0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean         0.0126984\r\n",
      "exploration/env_infos/final/end_effector_loc Std          0.29792\r\n",
      "exploration/env_infos/final/end_effector_loc Max          0.373592\r\n",
      "exploration/env_infos/final/end_effector_loc Min         -0.438987\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean      -0.000899297\r\n",
      "exploration/env_infos/initial/end_effector_loc Std        0.0208711\r\n",
      "exploration/env_infos/initial/end_effector_loc Max        0.0323541\r\n",
      "exploration/env_infos/initial/end_effector_loc Min       -0.0293409\r\n",
      "exploration/env_infos/end_effector_loc Mean               0.00875312\r\n",
      "exploration/env_infos/end_effector_loc Std                0.226624\r\n",
      "exploration/env_infos/end_effector_loc Max                0.548021\r\n",
      "exploration/env_infos/end_effector_loc Min               -0.438987\r\n",
      "evaluation/num steps total                           110000\r\n",
      "evaluation/num paths total                             5500\r\n",
      "evaluation/path length Mean                              20\r\n",
      "evaluation/path length Std                                0\r\n",
      "evaluation/path length Max                               20\r\n",
      "evaluation/path length Min                               20\r\n",
      "evaluation/Rewards Mean                                  -0.0419025\r\n",
      "evaluation/Rewards Std                                    0.0747586\r\n",
      "evaluation/Rewards Max                                    0.169408\r\n",
      "evaluation/Rewards Min                                   -0.449084\r\n",
      "evaluation/Returns Mean                                  -0.838049\r\n",
      "evaluation/Returns Std                                    1.08029\r\n",
      "evaluation/Returns Max                                    2.51188\r\n",
      "evaluation/Returns Min                                   -3.7841\r\n",
      "evaluation/Actions Mean                                  -0.00371\r\n",
      "evaluation/Actions Std                                    0.0818154\r\n",
      "evaluation/Actions Max                                    0.673934\r\n",
      "evaluation/Actions Min                                   -0.689218\r\n",
      "evaluation/Num Paths                                     50\r\n",
      "evaluation/Average Returns                               -0.838049\r\n",
      "evaluation/env_infos/final/reward_dist Mean               0.247174\r\n",
      "evaluation/env_infos/final/reward_dist Std                0.306899\r\n",
      "evaluation/env_infos/final/reward_dist Max                0.982775\r\n",
      "evaluation/env_infos/final/reward_dist Min                5.33598e-11\r\n",
      "evaluation/env_infos/initial/reward_dist Mean             0.00426848\r\n",
      "evaluation/env_infos/initial/reward_dist Std              0.0143057\r\n",
      "evaluation/env_infos/initial/reward_dist Max              0.0996254\r\n",
      "evaluation/env_infos/initial/reward_dist Min              1.78588e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                     0.219397\r\n",
      "evaluation/env_infos/reward_dist Std                      0.291584\r\n",
      "evaluation/env_infos/reward_dist Max                      0.999501\r\n",
      "evaluation/env_infos/reward_dist Min                      5.33598e-11\r\n",
      "evaluation/env_infos/final/reward_energy Mean            -0.034353\r\n",
      "evaluation/env_infos/final/reward_energy Std              0.0283986\r\n",
      "evaluation/env_infos/final/reward_energy Max             -0.00187464\r\n",
      "evaluation/env_infos/final/reward_energy Min             -0.151001\r\n",
      "evaluation/env_infos/initial/reward_energy Mean          -0.293499\r\n",
      "evaluation/env_infos/initial/reward_energy Std            0.250048\r\n",
      "evaluation/env_infos/initial/reward_energy Max           -0.00779318\r\n",
      "evaluation/env_infos/initial/reward_energy Min           -0.816139\r\n",
      "evaluation/env_infos/reward_energy Mean                  -0.0661679\r\n",
      "evaluation/env_infos/reward_energy Std                    0.0950624\r\n",
      "evaluation/env_infos/reward_energy Max                   -0.00187464\r\n",
      "evaluation/env_infos/reward_energy Min                   -0.816139\r\n",
      "evaluation/env_infos/final/reward_safety Mean             0\r\n",
      "evaluation/env_infos/final/reward_safety Std              0\r\n",
      "evaluation/env_infos/final/reward_safety Max              0\r\n",
      "evaluation/env_infos/final/reward_safety Min              0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean           0\r\n",
      "evaluation/env_infos/initial/reward_safety Std            0\r\n",
      "evaluation/env_infos/initial/reward_safety Max            0\r\n",
      "evaluation/env_infos/initial/reward_safety Min            0\r\n",
      "evaluation/env_infos/reward_safety Mean                   0\r\n",
      "evaluation/env_infos/reward_safety Std                    0\r\n",
      "evaluation/env_infos/reward_safety Max                    0\r\n",
      "evaluation/env_infos/reward_safety Min                    0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean         -0.0320247\r\n",
      "evaluation/env_infos/final/end_effector_loc Std           0.237051\r\n",
      "evaluation/env_infos/final/end_effector_loc Max           0.58899\r\n",
      "evaluation/env_infos/final/end_effector_loc Min          -0.605898\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       -0.000467557\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std         0.013624\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max         0.0336967\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min        -0.0344609\r\n",
      "evaluation/env_infos/end_effector_loc Mean               -0.00969896\r\n",
      "evaluation/env_infos/end_effector_loc Std                 0.159868\r\n",
      "evaluation/env_infos/end_effector_loc Max                 0.58899\r\n",
      "evaluation/env_infos/end_effector_loc Min                -0.605898\r\n",
      "time/data storing (s)                                     0.00306007\r\n",
      "time/evaluation sampling (s)                              1.46387\r\n",
      "time/exploration sampling (s)                             0.134771\r\n",
      "time/logging (s)                                          0.019981\r\n",
      "time/saving (s)                                           0.0278038\r\n",
      "time/training (s)                                        54.3348\r\n",
      "time/epoch (s)                                           55.9843\r\n",
      "time/total (s)                                         5881.66\r\n",
      "Epoch                                                   109\r\n",
      "---------------------------------------------------  ----------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:52:16.319483 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 110 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                     2000\n",
      "trainer/QF1 Loss                                          0.000725168\n",
      "trainer/QF2 Loss                                          0.000838748\n",
      "trainer/Policy Loss                                       2.88322\n",
      "trainer/Q1 Predictions Mean                              -0.981821\n",
      "trainer/Q1 Predictions Std                                0.890702\n",
      "trainer/Q1 Predictions Max                                0.756833\n",
      "trainer/Q1 Predictions Min                               -3.24852\n",
      "trainer/Q2 Predictions Mean                              -0.977548\n",
      "trainer/Q2 Predictions Std                                0.890151\n",
      "trainer/Q2 Predictions Max                                0.747389\n",
      "trainer/Q2 Predictions Min                               -3.27438\n",
      "trainer/Q Targets Mean                                   -0.979721\n",
      "trainer/Q Targets Std                                     0.896493\n",
      "trainer/Q Targets Max                                     0.736359\n",
      "trainer/Q Targets Min                                    -3.36485\n",
      "trainer/Log Pis Mean                                      1.91441\n",
      "trainer/Log Pis Std                                       1.27966\n",
      "trainer/Log Pis Max                                       4.81413\n",
      "trainer/Log Pis Min                                      -2.64157\n",
      "trainer/Policy mu Mean                                   -0.0024128\n",
      "trainer/Policy mu Std                                     0.367272\n",
      "trainer/Policy mu Max                                     2.66395\n",
      "trainer/Policy mu Min                                    -2.43033\n",
      "trainer/Policy log std Mean                              -2.28853\n",
      "trainer/Policy log std Std                                0.54836\n",
      "trainer/Policy log std Max                               -0.394177\n",
      "trainer/Policy log std Min                               -3.24937\n",
      "trainer/Alpha                                             0.0231921\n",
      "trainer/Alpha Loss                                       -0.322166\n",
      "exploration/num steps total                           12100\n",
      "exploration/num paths total                             605\n",
      "exploration/path length Mean                             20\n",
      "exploration/path length Std                               0\n",
      "exploration/path length Max                              20\n",
      "exploration/path length Min                              20\n",
      "exploration/Rewards Mean                                 -0.0546697\n",
      "exploration/Rewards Std                                   0.0657899\n",
      "exploration/Rewards Max                                   0.0952927\n",
      "exploration/Rewards Min                                  -0.260906\n",
      "exploration/Returns Mean                                 -1.09339\n",
      "exploration/Returns Std                                   0.737721\n",
      "exploration/Returns Max                                   0.0834409\n",
      "exploration/Returns Min                                  -2.15252\n",
      "exploration/Actions Mean                                 -0.0111532\n",
      "exploration/Actions Std                                   0.151305\n",
      "exploration/Actions Max                                   0.475556\n",
      "exploration/Actions Min                                  -0.472896\n",
      "exploration/Num Paths                                     5\n",
      "exploration/Average Returns                              -1.09339\n",
      "exploration/env_infos/final/reward_dist Mean              0.370208\n",
      "exploration/env_infos/final/reward_dist Std               0.353975\n",
      "exploration/env_infos/final/reward_dist Max               0.842826\n",
      "exploration/env_infos/final/reward_dist Min               0.00328122\n",
      "exploration/env_infos/initial/reward_dist Mean            0.00436992\n",
      "exploration/env_infos/initial/reward_dist Std             0.00564656\n",
      "exploration/env_infos/initial/reward_dist Max             0.0141288\n",
      "exploration/env_infos/initial/reward_dist Min             2.96889e-06\n",
      "exploration/env_infos/reward_dist Mean                    0.213581\n",
      "exploration/env_infos/reward_dist Std                     0.260681\n",
      "exploration/env_infos/reward_dist Max                     0.935964\n",
      "exploration/env_infos/reward_dist Min                     2.96889e-06\n",
      "exploration/env_infos/final/reward_energy Mean           -0.170321\n",
      "exploration/env_infos/final/reward_energy Std             0.143678\n",
      "exploration/env_infos/final/reward_energy Max            -0.0255689\n",
      "exploration/env_infos/final/reward_energy Min            -0.443248\n",
      "exploration/env_infos/initial/reward_energy Mean         -0.308272\n",
      "exploration/env_infos/initial/reward_energy Std           0.10906\n",
      "exploration/env_infos/initial/reward_energy Max          -0.114038\n",
      "exploration/env_infos/initial/reward_energy Min          -0.419664\n",
      "exploration/env_infos/reward_energy Mean                 -0.183631\n",
      "exploration/env_infos/reward_energy Std                   0.110973\n",
      "exploration/env_infos/reward_energy Max                  -0.0255689\n",
      "exploration/env_infos/reward_energy Min                  -0.51861\n",
      "exploration/env_infos/final/reward_safety Mean            0\n",
      "exploration/env_infos/final/reward_safety Std             0\n",
      "exploration/env_infos/final/reward_safety Max             0\n",
      "exploration/env_infos/final/reward_safety Min             0\n",
      "exploration/env_infos/initial/reward_safety Mean          0\n",
      "exploration/env_infos/initial/reward_safety Std           0\n",
      "exploration/env_infos/initial/reward_safety Max           0\n",
      "exploration/env_infos/initial/reward_safety Min           0\n",
      "exploration/env_infos/reward_safety Mean                  0\n",
      "exploration/env_infos/reward_safety Std                   0\n",
      "exploration/env_infos/reward_safety Max                   0\n",
      "exploration/env_infos/reward_safety Min                   0\n",
      "exploration/env_infos/final/end_effector_loc Mean         0.0143413\n",
      "exploration/env_infos/final/end_effector_loc Std          0.18148\n",
      "exploration/env_infos/final/end_effector_loc Max          0.335191\n",
      "exploration/env_infos/final/end_effector_loc Min         -0.334383\n",
      "exploration/env_infos/initial/end_effector_loc Mean       0.00410865\n",
      "exploration/env_infos/initial/end_effector_loc Std        0.0108063\n",
      "exploration/env_infos/initial/end_effector_loc Max        0.0203906\n",
      "exploration/env_infos/initial/end_effector_loc Min       -0.0120209\n",
      "exploration/env_infos/end_effector_loc Mean               0.028701\n",
      "exploration/env_infos/end_effector_loc Std                0.139451\n",
      "exploration/env_infos/end_effector_loc Max                0.335191\n",
      "exploration/env_infos/end_effector_loc Min               -0.334383\n",
      "evaluation/num steps total                           111000\n",
      "evaluation/num paths total                             5550\n",
      "evaluation/path length Mean                              20\n",
      "evaluation/path length Std                                0\n",
      "evaluation/path length Max                               20\n",
      "evaluation/path length Min                               20\n",
      "evaluation/Rewards Mean                                  -0.0367034\n",
      "evaluation/Rewards Std                                    0.0889898\n",
      "evaluation/Rewards Max                                    0.163744\n",
      "evaluation/Rewards Min                                   -0.574987\n",
      "evaluation/Returns Mean                                  -0.734068\n",
      "evaluation/Returns Std                                    1.4379\n",
      "evaluation/Returns Max                                    2.33142\n",
      "evaluation/Returns Min                                   -4.49868\n",
      "evaluation/Actions Mean                                   0.000701666\n",
      "evaluation/Actions Std                                    0.0783914\n",
      "evaluation/Actions Max                                    0.60364\n",
      "evaluation/Actions Min                                   -0.578218\n",
      "evaluation/Num Paths                                     50\n",
      "evaluation/Average Returns                               -0.734068\n",
      "evaluation/env_infos/final/reward_dist Mean               0.208952\n",
      "evaluation/env_infos/final/reward_dist Std                0.289876\n",
      "evaluation/env_infos/final/reward_dist Max                0.946115\n",
      "evaluation/env_infos/final/reward_dist Min                1.14203e-28\n",
      "evaluation/env_infos/initial/reward_dist Mean             0.00915807\n",
      "evaluation/env_infos/initial/reward_dist Std              0.0192351\n",
      "evaluation/env_infos/initial/reward_dist Max              0.104052\n",
      "evaluation/env_infos/initial/reward_dist Min              1.88204e-06\n",
      "evaluation/env_infos/reward_dist Mean                     0.214273\n",
      "evaluation/env_infos/reward_dist Std                      0.291011\n",
      "evaluation/env_infos/reward_dist Max                      0.984632\n",
      "evaluation/env_infos/reward_dist Min                      1.14203e-28\n",
      "evaluation/env_infos/final/reward_energy Mean            -0.0478726\n",
      "evaluation/env_infos/final/reward_energy Std              0.0621034\n",
      "evaluation/env_infos/final/reward_energy Max             -0.00203086\n",
      "evaluation/env_infos/final/reward_energy Min             -0.43337\n",
      "evaluation/env_infos/initial/reward_energy Mean          -0.281913\n",
      "evaluation/env_infos/initial/reward_energy Std            0.206201\n",
      "evaluation/env_infos/initial/reward_energy Max           -0.0232736\n",
      "evaluation/env_infos/initial/reward_energy Min           -0.739935\n",
      "evaluation/env_infos/reward_energy Mean                  -0.070678\n",
      "evaluation/env_infos/reward_energy Std                    0.0854168\n",
      "evaluation/env_infos/reward_energy Max                   -0.00136556\n",
      "evaluation/env_infos/reward_energy Min                   -0.739935\n",
      "evaluation/env_infos/final/reward_safety Mean             0\n",
      "evaluation/env_infos/final/reward_safety Std              0\n",
      "evaluation/env_infos/final/reward_safety Max              0\n",
      "evaluation/env_infos/final/reward_safety Min              0\n",
      "evaluation/env_infos/initial/reward_safety Mean           0\n",
      "evaluation/env_infos/initial/reward_safety Std            0\n",
      "evaluation/env_infos/initial/reward_safety Max            0\n",
      "evaluation/env_infos/initial/reward_safety Min            0\n",
      "evaluation/env_infos/reward_safety Mean                   0\n",
      "evaluation/env_infos/reward_safety Std                    0\n",
      "evaluation/env_infos/reward_safety Max                    0\n",
      "evaluation/env_infos/reward_safety Min                    0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         -0.00822931\n",
      "evaluation/env_infos/final/end_effector_loc Std           0.249572\n",
      "evaluation/env_infos/final/end_effector_loc Max           0.555125\n",
      "evaluation/env_infos/final/end_effector_loc Min          -0.488165\n",
      "evaluation/env_infos/initial/end_effector_loc Mean        0.000487138\n",
      "evaluation/env_infos/initial/end_effector_loc Std         0.0123392\n",
      "evaluation/env_infos/initial/end_effector_loc Max         0.030182\n",
      "evaluation/env_infos/initial/end_effector_loc Min        -0.0289109\n",
      "evaluation/env_infos/end_effector_loc Mean               -0.00344875\n",
      "evaluation/env_infos/end_effector_loc Std                 0.173548\n",
      "evaluation/env_infos/end_effector_loc Max                 0.555125\n",
      "evaluation/env_infos/end_effector_loc Min                -0.488165\n",
      "time/data storing (s)                                     0.00315959\n",
      "time/evaluation sampling (s)                              1.06734\n",
      "time/exploration sampling (s)                             0.137065\n",
      "time/logging (s)                                          0.0211702\n",
      "time/saving (s)                                           0.0300719\n",
      "time/training (s)                                        54.1551\n",
      "time/epoch (s)                                           55.4139\n",
      "time/total (s)                                         5938.47\n",
      "Epoch                                                   110\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:53:19.353916 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 111 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                     2000\n",
      "trainer/QF1 Loss                                          0.000562897\n",
      "trainer/QF2 Loss                                          0.000608305\n",
      "trainer/Policy Loss                                       2.7512\n",
      "trainer/Q1 Predictions Mean                              -0.887528\n",
      "trainer/Q1 Predictions Std                                0.820232\n",
      "trainer/Q1 Predictions Max                                0.816126\n",
      "trainer/Q1 Predictions Min                               -3.23333\n",
      "trainer/Q2 Predictions Mean                              -0.888821\n",
      "trainer/Q2 Predictions Std                                0.819112\n",
      "trainer/Q2 Predictions Max                                0.818924\n",
      "trainer/Q2 Predictions Min                               -3.22179\n",
      "trainer/Q Targets Mean                                   -0.891039\n",
      "trainer/Q Targets Std                                     0.817861\n",
      "trainer/Q Targets Max                                     0.818813\n",
      "trainer/Q Targets Min                                    -3.26373\n",
      "trainer/Log Pis Mean                                      1.8729\n",
      "trainer/Log Pis Std                                       1.25967\n",
      "trainer/Log Pis Max                                       4.40565\n",
      "trainer/Log Pis Min                                      -3.43354\n",
      "trainer/Policy mu Mean                                    0.0220885\n",
      "trainer/Policy mu Std                                     0.252285\n",
      "trainer/Policy mu Max                                     1.7564\n",
      "trainer/Policy mu Min                                    -1.29432\n",
      "trainer/Policy log std Mean                              -2.29162\n",
      "trainer/Policy log std Std                                0.512493\n",
      "trainer/Policy log std Max                               -0.381812\n",
      "trainer/Policy log std Min                               -3.2328\n",
      "trainer/Alpha                                             0.0226097\n",
      "trainer/Alpha Loss                                       -0.481382\n",
      "exploration/num steps total                           12200\n",
      "exploration/num paths total                             610\n",
      "exploration/path length Mean                             20\n",
      "exploration/path length Std                               0\n",
      "exploration/path length Max                              20\n",
      "exploration/path length Min                              20\n",
      "exploration/Rewards Mean                                 -0.0368357\n",
      "exploration/Rewards Std                                   0.105715\n",
      "exploration/Rewards Max                                   0.0987497\n",
      "exploration/Rewards Min                                  -0.310898\n",
      "exploration/Returns Mean                                 -0.736714\n",
      "exploration/Returns Std                                   1.83565\n",
      "exploration/Returns Max                                   0.822503\n",
      "exploration/Returns Min                                  -4.20492\n",
      "exploration/Actions Mean                                 -0.0101302\n",
      "exploration/Actions Std                                   0.174799\n",
      "exploration/Actions Max                                   0.817867\n",
      "exploration/Actions Min                                  -0.638037\n",
      "exploration/Num Paths                                     5\n",
      "exploration/Average Returns                              -0.736714\n",
      "exploration/env_infos/final/reward_dist Mean              0.223692\n",
      "exploration/env_infos/final/reward_dist Std               0.211722\n",
      "exploration/env_infos/final/reward_dist Max               0.556364\n",
      "exploration/env_infos/final/reward_dist Min               2.47976e-05\n",
      "exploration/env_infos/initial/reward_dist Mean            0.0179267\n",
      "exploration/env_infos/initial/reward_dist Std             0.0231584\n",
      "exploration/env_infos/initial/reward_dist Max             0.058169\n",
      "exploration/env_infos/initial/reward_dist Min             3.93511e-05\n",
      "exploration/env_infos/reward_dist Mean                    0.28437\n",
      "exploration/env_infos/reward_dist Std                     0.261221\n",
      "exploration/env_infos/reward_dist Max                     0.993974\n",
      "exploration/env_infos/reward_dist Min                     2.47976e-05\n",
      "exploration/env_infos/final/reward_energy Mean           -0.162983\n",
      "exploration/env_infos/final/reward_energy Std             0.0863896\n",
      "exploration/env_infos/final/reward_energy Max            -0.0341602\n",
      "exploration/env_infos/final/reward_energy Min            -0.304411\n",
      "exploration/env_infos/initial/reward_energy Mean         -0.529653\n",
      "exploration/env_infos/initial/reward_energy Std           0.202356\n",
      "exploration/env_infos/initial/reward_energy Max          -0.237883\n",
      "exploration/env_infos/initial/reward_energy Min          -0.857695\n",
      "exploration/env_infos/reward_energy Mean                 -0.196644\n",
      "exploration/env_infos/reward_energy Std                   0.150485\n",
      "exploration/env_infos/reward_energy Max                  -0.0137054\n",
      "exploration/env_infos/reward_energy Min                  -0.857695\n",
      "exploration/env_infos/final/reward_safety Mean            0\n",
      "exploration/env_infos/final/reward_safety Std             0\n",
      "exploration/env_infos/final/reward_safety Max             0\n",
      "exploration/env_infos/final/reward_safety Min             0\n",
      "exploration/env_infos/initial/reward_safety Mean          0\n",
      "exploration/env_infos/initial/reward_safety Std           0\n",
      "exploration/env_infos/initial/reward_safety Max           0\n",
      "exploration/env_infos/initial/reward_safety Min           0\n",
      "exploration/env_infos/reward_safety Mean                  0\n",
      "exploration/env_infos/reward_safety Std                   0\n",
      "exploration/env_infos/reward_safety Max                   0\n",
      "exploration/env_infos/reward_safety Min                   0\n",
      "exploration/env_infos/final/end_effector_loc Mean        -0.0399047\n",
      "exploration/env_infos/final/end_effector_loc Std          0.222056\n",
      "exploration/env_infos/final/end_effector_loc Max          0.300127\n",
      "exploration/env_infos/final/end_effector_loc Min         -0.321043\n",
      "exploration/env_infos/initial/end_effector_loc Mean       0.00611128\n",
      "exploration/env_infos/initial/end_effector_loc Std        0.019092\n",
      "exploration/env_infos/initial/end_effector_loc Max        0.0408933\n",
      "exploration/env_infos/initial/end_effector_loc Min       -0.0181558\n",
      "exploration/env_infos/end_effector_loc Mean              -0.000972946\n",
      "exploration/env_infos/end_effector_loc Std                0.173978\n",
      "exploration/env_infos/end_effector_loc Max                0.300127\n",
      "exploration/env_infos/end_effector_loc Min               -0.321043\n",
      "evaluation/num steps total                           112000\n",
      "evaluation/num paths total                             5600\n",
      "evaluation/path length Mean                              20\n",
      "evaluation/path length Std                                0\n",
      "evaluation/path length Max                               20\n",
      "evaluation/path length Min                               20\n",
      "evaluation/Rewards Mean                                  -0.0478263\n",
      "evaluation/Rewards Std                                    0.0720962\n",
      "evaluation/Rewards Max                                    0.125706\n",
      "evaluation/Rewards Min                                   -0.306227\n",
      "evaluation/Returns Mean                                  -0.956527\n",
      "evaluation/Returns Std                                    1.15984\n",
      "evaluation/Returns Max                                    1.33584\n",
      "evaluation/Returns Min                                   -3.26293\n",
      "evaluation/Actions Mean                                   0.00187217\n",
      "evaluation/Actions Std                                    0.07595\n",
      "evaluation/Actions Max                                    0.573153\n",
      "evaluation/Actions Min                                   -0.785026\n",
      "evaluation/Num Paths                                     50\n",
      "evaluation/Average Returns                               -0.956527\n",
      "evaluation/env_infos/final/reward_dist Mean               0.161806\n",
      "evaluation/env_infos/final/reward_dist Std                0.237017\n",
      "evaluation/env_infos/final/reward_dist Max                0.924919\n",
      "evaluation/env_infos/final/reward_dist Min                5.75852e-13\n",
      "evaluation/env_infos/initial/reward_dist Mean             0.00728584\n",
      "evaluation/env_infos/initial/reward_dist Std              0.0122061\n",
      "evaluation/env_infos/initial/reward_dist Max              0.0443252\n",
      "evaluation/env_infos/initial/reward_dist Min              1.49984e-06\n",
      "evaluation/env_infos/reward_dist Mean                     0.234717\n",
      "evaluation/env_infos/reward_dist Std                      0.286089\n",
      "evaluation/env_infos/reward_dist Max                      0.995975\n",
      "evaluation/env_infos/reward_dist Min                      5.75852e-13\n",
      "evaluation/env_infos/final/reward_energy Mean            -0.050571\n",
      "evaluation/env_infos/final/reward_energy Std              0.0462988\n",
      "evaluation/env_infos/final/reward_energy Max             -0.00867265\n",
      "evaluation/env_infos/final/reward_energy Min             -0.248722\n",
      "evaluation/env_infos/initial/reward_energy Mean          -0.281085\n",
      "evaluation/env_infos/initial/reward_energy Std            0.227776\n",
      "evaluation/env_infos/initial/reward_energy Max           -0.00919197\n",
      "evaluation/env_infos/initial/reward_energy Min           -0.786673\n",
      "evaluation/env_infos/reward_energy Mean                  -0.0663783\n",
      "evaluation/env_infos/reward_energy Std                    0.0844851\n",
      "evaluation/env_infos/reward_energy Max                   -0.00216989\n",
      "evaluation/env_infos/reward_energy Min                   -0.786673\n",
      "evaluation/env_infos/final/reward_safety Mean             0\n",
      "evaluation/env_infos/final/reward_safety Std              0\n",
      "evaluation/env_infos/final/reward_safety Max              0\n",
      "evaluation/env_infos/final/reward_safety Min              0\n",
      "evaluation/env_infos/initial/reward_safety Mean           0\n",
      "evaluation/env_infos/initial/reward_safety Std            0\n",
      "evaluation/env_infos/initial/reward_safety Max            0\n",
      "evaluation/env_infos/initial/reward_safety Min            0\n",
      "evaluation/env_infos/reward_safety Mean                   0\n",
      "evaluation/env_infos/reward_safety Std                    0\n",
      "evaluation/env_infos/reward_safety Max                    0\n",
      "evaluation/env_infos/reward_safety Min                    0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         -0.0225556\n",
      "evaluation/env_infos/final/end_effector_loc Std           0.237725\n",
      "evaluation/env_infos/final/end_effector_loc Max           0.457962\n",
      "evaluation/env_infos/final/end_effector_loc Min          -0.545888\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       -0.00189668\n",
      "evaluation/env_infos/initial/end_effector_loc Std         0.0126497\n",
      "evaluation/env_infos/initial/end_effector_loc Max         0.0286576\n",
      "evaluation/env_infos/initial/end_effector_loc Min        -0.0392513\n",
      "evaluation/env_infos/end_effector_loc Mean               -0.0180289\n",
      "evaluation/env_infos/end_effector_loc Std                 0.162831\n",
      "evaluation/env_infos/end_effector_loc Max                 0.457962\n",
      "evaluation/env_infos/end_effector_loc Min                -0.545888\n",
      "time/data storing (s)                                     0.00320791\n",
      "time/evaluation sampling (s)                              1.13997\n",
      "time/exploration sampling (s)                             0.240155\n",
      "time/logging (s)                                          0.0213247\n",
      "time/saving (s)                                           0.0305463\n",
      "time/training (s)                                        60.0032\n",
      "time/epoch (s)                                           61.4384\n",
      "time/total (s)                                         6001.51\n",
      "Epoch                                                   111\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:54:17.548065 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 112 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                     2000\n",
      "trainer/QF1 Loss                                          0.000941903\n",
      "trainer/QF2 Loss                                          0.000821692\n",
      "trainer/Policy Loss                                       2.59379\n",
      "trainer/Q1 Predictions Mean                              -0.868014\n",
      "trainer/Q1 Predictions Std                                0.80265\n",
      "trainer/Q1 Predictions Max                                0.780451\n",
      "trainer/Q1 Predictions Min                               -3.24006\n",
      "trainer/Q2 Predictions Mean                              -0.86621\n",
      "trainer/Q2 Predictions Std                                0.812747\n",
      "trainer/Q2 Predictions Max                                0.810539\n",
      "trainer/Q2 Predictions Min                               -3.28086\n",
      "trainer/Q Targets Mean                                   -0.872887\n",
      "trainer/Q Targets Std                                     0.808448\n",
      "trainer/Q Targets Max                                     0.781852\n",
      "trainer/Q Targets Min                                    -3.33308\n",
      "trainer/Log Pis Mean                                      1.72502\n",
      "trainer/Log Pis Std                                       1.49113\n",
      "trainer/Log Pis Max                                       4.43531\n",
      "trainer/Log Pis Min                                      -3.7029\n",
      "trainer/Policy mu Mean                                    0.0255046\n",
      "trainer/Policy mu Std                                     0.317686\n",
      "trainer/Policy mu Max                                     2.269\n",
      "trainer/Policy mu Min                                    -1.6173\n",
      "trainer/Policy log std Mean                              -2.26367\n",
      "trainer/Policy log std Std                                0.5393\n",
      "trainer/Policy log std Max                               -0.162668\n",
      "trainer/Policy log std Min                               -3.2378\n",
      "trainer/Alpha                                             0.0212507\n",
      "trainer/Alpha Loss                                       -1.05894\n",
      "exploration/num steps total                           12300\n",
      "exploration/num paths total                             615\n",
      "exploration/path length Mean                             20\n",
      "exploration/path length Std                               0\n",
      "exploration/path length Max                              20\n",
      "exploration/path length Min                              20\n",
      "exploration/Rewards Mean                                 -0.0407299\n",
      "exploration/Rewards Std                                   0.0690936\n",
      "exploration/Rewards Max                                   0.100048\n",
      "exploration/Rewards Min                                  -0.205309\n",
      "exploration/Returns Mean                                 -0.814598\n",
      "exploration/Returns Std                                   1.0045\n",
      "exploration/Returns Max                                   0.560916\n",
      "exploration/Returns Min                                  -2.26442\n",
      "exploration/Actions Mean                                 -0.0065458\n",
      "exploration/Actions Std                                   0.126596\n",
      "exploration/Actions Max                                   0.585035\n",
      "exploration/Actions Min                                  -0.431903\n",
      "exploration/Num Paths                                     5\n",
      "exploration/Average Returns                              -0.814598\n",
      "exploration/env_infos/final/reward_dist Mean              0.272319\n",
      "exploration/env_infos/final/reward_dist Std               0.307818\n",
      "exploration/env_infos/final/reward_dist Max               0.686459\n",
      "exploration/env_infos/final/reward_dist Min               0.00110513\n",
      "exploration/env_infos/initial/reward_dist Mean            0.00304253\n",
      "exploration/env_infos/initial/reward_dist Std             0.00380879\n",
      "exploration/env_infos/initial/reward_dist Max             0.0101084\n",
      "exploration/env_infos/initial/reward_dist Min             0.00015304\n",
      "exploration/env_infos/reward_dist Mean                    0.263279\n",
      "exploration/env_infos/reward_dist Std                     0.328098\n",
      "exploration/env_infos/reward_dist Max                     0.951615\n",
      "exploration/env_infos/reward_dist Min                     0.00015304\n",
      "exploration/env_infos/final/reward_energy Mean           -0.128157\n",
      "exploration/env_infos/final/reward_energy Std             0.112312\n",
      "exploration/env_infos/final/reward_energy Max            -0.0400719\n",
      "exploration/env_infos/final/reward_energy Min            -0.346913\n",
      "exploration/env_infos/initial/reward_energy Mean         -0.189387\n",
      "exploration/env_infos/initial/reward_energy Std           0.126311\n",
      "exploration/env_infos/initial/reward_energy Max          -0.0194873\n",
      "exploration/env_infos/initial/reward_energy Min          -0.373848\n",
      "exploration/env_infos/reward_energy Mean                 -0.137304\n",
      "exploration/env_infos/reward_energy Std                   0.115267\n",
      "exploration/env_infos/reward_energy Max                  -0.00112282\n",
      "exploration/env_infos/reward_energy Min                  -0.58505\n",
      "exploration/env_infos/final/reward_safety Mean            0\n",
      "exploration/env_infos/final/reward_safety Std             0\n",
      "exploration/env_infos/final/reward_safety Max             0\n",
      "exploration/env_infos/final/reward_safety Min             0\n",
      "exploration/env_infos/initial/reward_safety Mean          0\n",
      "exploration/env_infos/initial/reward_safety Std           0\n",
      "exploration/env_infos/initial/reward_safety Max           0\n",
      "exploration/env_infos/initial/reward_safety Min           0\n",
      "exploration/env_infos/reward_safety Mean                  0\n",
      "exploration/env_infos/reward_safety Std                   0\n",
      "exploration/env_infos/reward_safety Max                   0\n",
      "exploration/env_infos/reward_safety Min                   0\n",
      "exploration/env_infos/final/end_effector_loc Mean         0.0495548\n",
      "exploration/env_infos/final/end_effector_loc Std          0.209346\n",
      "exploration/env_infos/final/end_effector_loc Max          0.291119\n",
      "exploration/env_infos/final/end_effector_loc Min         -0.331229\n",
      "exploration/env_infos/initial/end_effector_loc Mean       0.00189696\n",
      "exploration/env_infos/initial/end_effector_loc Std        0.00782171\n",
      "exploration/env_infos/initial/end_effector_loc Max        0.0132637\n",
      "exploration/env_infos/initial/end_effector_loc Min       -0.0131712\n",
      "exploration/env_infos/end_effector_loc Mean               0.0415809\n",
      "exploration/env_infos/end_effector_loc Std                0.148294\n",
      "exploration/env_infos/end_effector_loc Max                0.345038\n",
      "exploration/env_infos/end_effector_loc Min               -0.332299\n",
      "evaluation/num steps total                           113000\n",
      "evaluation/num paths total                             5650\n",
      "evaluation/path length Mean                              20\n",
      "evaluation/path length Std                                0\n",
      "evaluation/path length Max                               20\n",
      "evaluation/path length Min                               20\n",
      "evaluation/Rewards Mean                                  -0.0267286\n",
      "evaluation/Rewards Std                                    0.0728399\n",
      "evaluation/Rewards Max                                    0.151402\n",
      "evaluation/Rewards Min                                   -0.27468\n",
      "evaluation/Returns Mean                                  -0.534571\n",
      "evaluation/Returns Std                                    1.0608\n",
      "evaluation/Returns Max                                    2.25641\n",
      "evaluation/Returns Min                                   -2.7175\n",
      "evaluation/Actions Mean                                   0.0035407\n",
      "evaluation/Actions Std                                    0.0570537\n",
      "evaluation/Actions Max                                    0.441028\n",
      "evaluation/Actions Min                                   -0.546354\n",
      "evaluation/Num Paths                                     50\n",
      "evaluation/Average Returns                               -0.534571\n",
      "evaluation/env_infos/final/reward_dist Mean               0.235601\n",
      "evaluation/env_infos/final/reward_dist Std                0.31296\n",
      "evaluation/env_infos/final/reward_dist Max                0.989858\n",
      "evaluation/env_infos/final/reward_dist Min                3.50127e-18\n",
      "evaluation/env_infos/initial/reward_dist Mean             0.00534055\n",
      "evaluation/env_infos/initial/reward_dist Std              0.0085755\n",
      "evaluation/env_infos/initial/reward_dist Max              0.0269703\n",
      "evaluation/env_infos/initial/reward_dist Min              2.30695e-06\n",
      "evaluation/env_infos/reward_dist Mean                     0.2478\n",
      "evaluation/env_infos/reward_dist Std                      0.297455\n",
      "evaluation/env_infos/reward_dist Max                      0.997452\n",
      "evaluation/env_infos/reward_dist Min                      3.50127e-18\n",
      "evaluation/env_infos/final/reward_energy Mean            -0.0521381\n",
      "evaluation/env_infos/final/reward_energy Std              0.0652719\n",
      "evaluation/env_infos/final/reward_energy Max             -0.0020705\n",
      "evaluation/env_infos/final/reward_energy Min             -0.395588\n",
      "evaluation/env_infos/initial/reward_energy Mean          -0.180951\n",
      "evaluation/env_infos/initial/reward_energy Std            0.14357\n",
      "evaluation/env_infos/initial/reward_energy Max           -0.0074594\n",
      "evaluation/env_infos/initial/reward_energy Min           -0.560163\n",
      "evaluation/env_infos/reward_energy Mean                  -0.053272\n",
      "evaluation/env_infos/reward_energy Std                    0.0608064\n",
      "evaluation/env_infos/reward_energy Max                   -0.00101239\n",
      "evaluation/env_infos/reward_energy Min                   -0.560163\n",
      "evaluation/env_infos/final/reward_safety Mean             0\n",
      "evaluation/env_infos/final/reward_safety Std              0\n",
      "evaluation/env_infos/final/reward_safety Max              0\n",
      "evaluation/env_infos/final/reward_safety Min              0\n",
      "evaluation/env_infos/initial/reward_safety Mean           0\n",
      "evaluation/env_infos/initial/reward_safety Std            0\n",
      "evaluation/env_infos/initial/reward_safety Max            0\n",
      "evaluation/env_infos/initial/reward_safety Min            0\n",
      "evaluation/env_infos/reward_safety Mean                   0\n",
      "evaluation/env_infos/reward_safety Std                    0\n",
      "evaluation/env_infos/reward_safety Max                    0\n",
      "evaluation/env_infos/reward_safety Min                    0\n",
      "evaluation/env_infos/final/end_effector_loc Mean          0.0348164\n",
      "evaluation/env_infos/final/end_effector_loc Std           0.238945\n",
      "evaluation/env_infos/final/end_effector_loc Max           0.556735\n",
      "evaluation/env_infos/final/end_effector_loc Min          -0.626206\n",
      "evaluation/env_infos/initial/end_effector_loc Mean        0.000755843\n",
      "evaluation/env_infos/initial/end_effector_loc Std         0.00813162\n",
      "evaluation/env_infos/initial/end_effector_loc Max         0.0220514\n",
      "evaluation/env_infos/initial/end_effector_loc Min        -0.0273177\n",
      "evaluation/env_infos/end_effector_loc Mean                0.0174884\n",
      "evaluation/env_infos/end_effector_loc Std                 0.15272\n",
      "evaluation/env_infos/end_effector_loc Max                 0.556735\n",
      "evaluation/env_infos/end_effector_loc Min                -0.626206\n",
      "time/data storing (s)                                     0.00297954\n",
      "time/evaluation sampling (s)                              1.14387\n",
      "time/exploration sampling (s)                             0.134749\n",
      "time/logging (s)                                          0.0197451\n",
      "time/saving (s)                                           0.0298207\n",
      "time/training (s)                                        55.3593\n",
      "time/epoch (s)                                           56.6905\n",
      "time/total (s)                                         6059.7\n",
      "Epoch                                                   112\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:55:13.615736 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 113 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                     2000\n",
      "trainer/QF1 Loss                                          0.000634154\n",
      "trainer/QF2 Loss                                          0.000541573\n",
      "trainer/Policy Loss                                       2.90348\n",
      "trainer/Q1 Predictions Mean                              -0.984729\n",
      "trainer/Q1 Predictions Std                                0.842958\n",
      "trainer/Q1 Predictions Max                                0.617806\n",
      "trainer/Q1 Predictions Min                               -3.23576\n",
      "trainer/Q2 Predictions Mean                              -0.97913\n",
      "trainer/Q2 Predictions Std                                0.84267\n",
      "trainer/Q2 Predictions Max                                0.603182\n",
      "trainer/Q2 Predictions Min                               -3.25636\n",
      "trainer/Q Targets Mean                                   -0.979162\n",
      "trainer/Q Targets Std                                     0.842906\n",
      "trainer/Q Targets Max                                     0.612676\n",
      "trainer/Q Targets Min                                    -3.28536\n",
      "trainer/Log Pis Mean                                      1.93094\n",
      "trainer/Log Pis Std                                       1.31763\n",
      "trainer/Log Pis Max                                       6.37881\n",
      "trainer/Log Pis Min                                      -3.55265\n",
      "trainer/Policy mu Mean                                    0.0259064\n",
      "trainer/Policy mu Std                                     0.313485\n",
      "trainer/Policy mu Max                                     2.48207\n",
      "trainer/Policy mu Min                                    -0.793517\n",
      "trainer/Policy log std Mean                              -2.30421\n",
      "trainer/Policy log std Std                                0.520091\n",
      "trainer/Policy log std Max                               -0.101167\n",
      "trainer/Policy log std Min                               -3.31749\n",
      "trainer/Alpha                                             0.020878\n",
      "trainer/Alpha Loss                                       -0.267206\n",
      "exploration/num steps total                           12400\n",
      "exploration/num paths total                             620\n",
      "exploration/path length Mean                             20\n",
      "exploration/path length Std                               0\n",
      "exploration/path length Max                              20\n",
      "exploration/path length Min                              20\n",
      "exploration/Rewards Mean                                 -0.0919081\n",
      "exploration/Rewards Std                                   0.0612883\n",
      "exploration/Rewards Max                                   0.0213898\n",
      "exploration/Rewards Min                                  -0.287363\n",
      "exploration/Returns Mean                                 -1.83816\n",
      "exploration/Returns Std                                   0.683334\n",
      "exploration/Returns Max                                  -1.05443\n",
      "exploration/Returns Min                                  -2.85071\n",
      "exploration/Actions Mean                                  0.00248586\n",
      "exploration/Actions Std                                   0.0968369\n",
      "exploration/Actions Max                                   0.504429\n",
      "exploration/Actions Min                                  -0.241617\n",
      "exploration/Num Paths                                     5\n",
      "exploration/Average Returns                              -1.83816\n",
      "exploration/env_infos/final/reward_dist Mean              0.0275727\n",
      "exploration/env_infos/final/reward_dist Std               0.0273641\n",
      "exploration/env_infos/final/reward_dist Max               0.0713584\n",
      "exploration/env_infos/final/reward_dist Min               5.5668e-12\n",
      "exploration/env_infos/initial/reward_dist Mean            4.81448e-05\n",
      "exploration/env_infos/initial/reward_dist Std             3.73095e-05\n",
      "exploration/env_infos/initial/reward_dist Max             0.000115553\n",
      "exploration/env_infos/initial/reward_dist Min             3.13475e-06\n",
      "exploration/env_infos/reward_dist Mean                    0.130831\n",
      "exploration/env_infos/reward_dist Std                     0.215251\n",
      "exploration/env_infos/reward_dist Max                     0.859261\n",
      "exploration/env_infos/reward_dist Min                     5.5668e-12\n",
      "exploration/env_infos/final/reward_energy Mean           -0.140542\n",
      "exploration/env_infos/final/reward_energy Std             0.0434174\n",
      "exploration/env_infos/final/reward_energy Max            -0.0691268\n",
      "exploration/env_infos/final/reward_energy Min            -0.200372\n",
      "exploration/env_infos/initial/reward_energy Mean         -0.229108\n",
      "exploration/env_infos/initial/reward_energy Std           0.166334\n",
      "exploration/env_infos/initial/reward_energy Max          -0.0833496\n",
      "exploration/env_infos/initial/reward_energy Min          -0.535234\n",
      "exploration/env_infos/reward_energy Mean                 -0.112513\n",
      "exploration/env_infos/reward_energy Std                   0.0781535\n",
      "exploration/env_infos/reward_energy Max                  -0.00492373\n",
      "exploration/env_infos/reward_energy Min                  -0.535234\n",
      "exploration/env_infos/final/reward_safety Mean            0\n",
      "exploration/env_infos/final/reward_safety Std             0\n",
      "exploration/env_infos/final/reward_safety Max             0\n",
      "exploration/env_infos/final/reward_safety Min             0\n",
      "exploration/env_infos/initial/reward_safety Mean          0\n",
      "exploration/env_infos/initial/reward_safety Std           0\n",
      "exploration/env_infos/initial/reward_safety Max           0\n",
      "exploration/env_infos/initial/reward_safety Min           0\n",
      "exploration/env_infos/reward_safety Mean                  0\n",
      "exploration/env_infos/reward_safety Std                   0\n",
      "exploration/env_infos/reward_safety Max                   0\n",
      "exploration/env_infos/reward_safety Min                   0\n",
      "exploration/env_infos/final/end_effector_loc Mean         0.0593925\n",
      "exploration/env_infos/final/end_effector_loc Std          0.252616\n",
      "exploration/env_infos/final/end_effector_loc Max          0.431818\n",
      "exploration/env_infos/final/end_effector_loc Min         -0.326654\n",
      "exploration/env_infos/initial/end_effector_loc Mean       0.00446455\n",
      "exploration/env_infos/initial/end_effector_loc Std        0.00895907\n",
      "exploration/env_infos/initial/end_effector_loc Max        0.0252214\n",
      "exploration/env_infos/initial/end_effector_loc Min       -0.00626289\n",
      "exploration/env_infos/end_effector_loc Mean               0.0391995\n",
      "exploration/env_infos/end_effector_loc Std                0.163083\n",
      "exploration/env_infos/end_effector_loc Max                0.431818\n",
      "exploration/env_infos/end_effector_loc Min               -0.326654\n",
      "evaluation/num steps total                           114000\n",
      "evaluation/num paths total                             5700\n",
      "evaluation/path length Mean                              20\n",
      "evaluation/path length Std                                0\n",
      "evaluation/path length Max                               20\n",
      "evaluation/path length Min                               20\n",
      "evaluation/Rewards Mean                                  -0.0399629\n",
      "evaluation/Rewards Std                                    0.0721175\n",
      "evaluation/Rewards Max                                    0.159894\n",
      "evaluation/Rewards Min                                   -0.342154\n",
      "evaluation/Returns Mean                                  -0.799257\n",
      "evaluation/Returns Std                                    1.10689\n",
      "evaluation/Returns Max                                    2.2221\n",
      "evaluation/Returns Min                                   -3.18535\n",
      "evaluation/Actions Mean                                   0.00201938\n",
      "evaluation/Actions Std                                    0.076644\n",
      "evaluation/Actions Max                                    0.705306\n",
      "evaluation/Actions Min                                   -0.666459\n",
      "evaluation/Num Paths                                     50\n",
      "evaluation/Average Returns                               -0.799257\n",
      "evaluation/env_infos/final/reward_dist Mean               0.198835\n",
      "evaluation/env_infos/final/reward_dist Std                0.253616\n",
      "evaluation/env_infos/final/reward_dist Max                0.977469\n",
      "evaluation/env_infos/final/reward_dist Min                3.42304e-17\n",
      "evaluation/env_infos/initial/reward_dist Mean             0.00446012\n",
      "evaluation/env_infos/initial/reward_dist Std              0.00781832\n",
      "evaluation/env_infos/initial/reward_dist Max              0.0340837\n",
      "evaluation/env_infos/initial/reward_dist Min              2.66277e-06\n",
      "evaluation/env_infos/reward_dist Mean                     0.221691\n",
      "evaluation/env_infos/reward_dist Std                      0.290286\n",
      "evaluation/env_infos/reward_dist Max                      0.99056\n",
      "evaluation/env_infos/reward_dist Min                      3.42304e-17\n",
      "evaluation/env_infos/final/reward_energy Mean            -0.0518283\n",
      "evaluation/env_infos/final/reward_energy Std              0.0563237\n",
      "evaluation/env_infos/final/reward_energy Max             -0.00385333\n",
      "evaluation/env_infos/final/reward_energy Min             -0.32807\n",
      "evaluation/env_infos/initial/reward_energy Mean          -0.251206\n",
      "evaluation/env_infos/initial/reward_energy Std            0.216998\n",
      "evaluation/env_infos/initial/reward_energy Max           -0.0113987\n",
      "evaluation/env_infos/initial/reward_energy Min           -0.991655\n",
      "evaluation/env_infos/reward_energy Mean                  -0.0689688\n",
      "evaluation/env_infos/reward_energy Std                    0.0836664\n",
      "evaluation/env_infos/reward_energy Max                   -0.00120753\n",
      "evaluation/env_infos/reward_energy Min                   -0.991655\n",
      "evaluation/env_infos/final/reward_safety Mean             0\n",
      "evaluation/env_infos/final/reward_safety Std              0\n",
      "evaluation/env_infos/final/reward_safety Max              0\n",
      "evaluation/env_infos/final/reward_safety Min              0\n",
      "evaluation/env_infos/initial/reward_safety Mean           0\n",
      "evaluation/env_infos/initial/reward_safety Std            0\n",
      "evaluation/env_infos/initial/reward_safety Max            0\n",
      "evaluation/env_infos/initial/reward_safety Min            0\n",
      "evaluation/env_infos/reward_safety Mean                   0\n",
      "evaluation/env_infos/reward_safety Std                    0\n",
      "evaluation/env_infos/reward_safety Max                    0\n",
      "evaluation/env_infos/reward_safety Min                    0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         -0.00309311\n",
      "evaluation/env_infos/final/end_effector_loc Std           0.237432\n",
      "evaluation/env_infos/final/end_effector_loc Max           0.526235\n",
      "evaluation/env_infos/final/end_effector_loc Min          -0.521683\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       -0.000423974\n",
      "evaluation/env_infos/initial/end_effector_loc Std         0.0117286\n",
      "evaluation/env_infos/initial/end_effector_loc Max         0.0352653\n",
      "evaluation/env_infos/initial/end_effector_loc Min        -0.033323\n",
      "evaluation/env_infos/end_effector_loc Mean               -0.00654972\n",
      "evaluation/env_infos/end_effector_loc Std                 0.159597\n",
      "evaluation/env_infos/end_effector_loc Max                 0.526235\n",
      "evaluation/env_infos/end_effector_loc Min                -0.521683\n",
      "time/data storing (s)                                     0.00286888\n",
      "time/evaluation sampling (s)                              1.01906\n",
      "time/exploration sampling (s)                             0.127777\n",
      "time/logging (s)                                          0.0206799\n",
      "time/saving (s)                                           0.0306533\n",
      "time/training (s)                                        53.4522\n",
      "time/epoch (s)                                           54.6532\n",
      "time/total (s)                                         6115.76\n",
      "Epoch                                                   113\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:56:11.720806 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 114 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                     2000\n",
      "trainer/QF1 Loss                                          0.000632567\n",
      "trainer/QF2 Loss                                          0.000906563\n",
      "trainer/Policy Loss                                       3.08434\n",
      "trainer/Q1 Predictions Mean                              -0.908789\n",
      "trainer/Q1 Predictions Std                                0.792405\n",
      "trainer/Q1 Predictions Max                                0.710082\n",
      "trainer/Q1 Predictions Min                               -3.42325\n",
      "trainer/Q2 Predictions Mean                              -0.911101\n",
      "trainer/Q2 Predictions Std                                0.794404\n",
      "trainer/Q2 Predictions Max                                0.731121\n",
      "trainer/Q2 Predictions Min                               -3.41812\n",
      "trainer/Q Targets Mean                                   -0.897914\n",
      "trainer/Q Targets Std                                     0.792541\n",
      "trainer/Q Targets Max                                     0.734944\n",
      "trainer/Q Targets Min                                    -3.44307\n",
      "trainer/Log Pis Mean                                      2.1776\n",
      "trainer/Log Pis Std                                       1.21734\n",
      "trainer/Log Pis Max                                       4.5821\n",
      "trainer/Log Pis Min                                      -2.65586\n",
      "trainer/Policy mu Mean                                    0.0115835\n",
      "trainer/Policy mu Std                                     0.270492\n",
      "trainer/Policy mu Max                                     1.84137\n",
      "trainer/Policy mu Min                                    -1.31753\n",
      "trainer/Policy log std Mean                              -2.37768\n",
      "trainer/Policy log std Std                                0.542306\n",
      "trainer/Policy log std Max                               -0.560935\n",
      "trainer/Policy log std Min                               -3.28614\n",
      "trainer/Alpha                                             0.0212634\n",
      "trainer/Alpha Loss                                        0.684138\n",
      "exploration/num steps total                           12500\n",
      "exploration/num paths total                             625\n",
      "exploration/path length Mean                             20\n",
      "exploration/path length Std                               0\n",
      "exploration/path length Max                              20\n",
      "exploration/path length Min                              20\n",
      "exploration/Rewards Mean                                 -0.0718541\n",
      "exploration/Rewards Std                                   0.0682058\n",
      "exploration/Rewards Max                                   0.0663603\n",
      "exploration/Rewards Min                                  -0.249585\n",
      "exploration/Returns Mean                                 -1.43708\n",
      "exploration/Returns Std                                   0.873617\n",
      "exploration/Returns Max                                  -0.462762\n",
      "exploration/Returns Min                                  -2.64846\n",
      "exploration/Actions Mean                                  0.000440852\n",
      "exploration/Actions Std                                   0.113368\n",
      "exploration/Actions Max                                   0.371078\n",
      "exploration/Actions Min                                  -0.506794\n",
      "exploration/Num Paths                                     5\n",
      "exploration/Average Returns                              -1.43708\n",
      "exploration/env_infos/final/reward_dist Mean              0.0493646\n",
      "exploration/env_infos/final/reward_dist Std               0.0923007\n",
      "exploration/env_infos/final/reward_dist Max               0.233756\n",
      "exploration/env_infos/final/reward_dist Min               7.82197e-06\n",
      "exploration/env_infos/initial/reward_dist Mean            0.0126886\n",
      "exploration/env_infos/initial/reward_dist Std             0.0152503\n",
      "exploration/env_infos/initial/reward_dist Max             0.0416841\n",
      "exploration/env_infos/initial/reward_dist Min             2.89501e-05\n",
      "exploration/env_infos/reward_dist Mean                    0.11606\n",
      "exploration/env_infos/reward_dist Std                     0.173274\n",
      "exploration/env_infos/reward_dist Max                     0.873936\n",
      "exploration/env_infos/reward_dist Min                     7.82197e-06\n",
      "exploration/env_infos/final/reward_energy Mean           -0.101536\n",
      "exploration/env_infos/final/reward_energy Std             0.0712514\n",
      "exploration/env_infos/final/reward_energy Max            -0.0187363\n",
      "exploration/env_infos/final/reward_energy Min            -0.233275\n",
      "exploration/env_infos/initial/reward_energy Mean         -0.224461\n",
      "exploration/env_infos/initial/reward_energy Std           0.131347\n",
      "exploration/env_infos/initial/reward_energy Max          -0.0236112\n",
      "exploration/env_infos/initial/reward_energy Min          -0.416048\n",
      "exploration/env_infos/reward_energy Mean                 -0.132575\n",
      "exploration/env_infos/reward_energy Std                   0.0901588\n",
      "exploration/env_infos/reward_energy Max                  -0.0054293\n",
      "exploration/env_infos/reward_energy Min                  -0.546995\n",
      "exploration/env_infos/final/reward_safety Mean            0\n",
      "exploration/env_infos/final/reward_safety Std             0\n",
      "exploration/env_infos/final/reward_safety Max             0\n",
      "exploration/env_infos/final/reward_safety Min             0\n",
      "exploration/env_infos/initial/reward_safety Mean          0\n",
      "exploration/env_infos/initial/reward_safety Std           0\n",
      "exploration/env_infos/initial/reward_safety Max           0\n",
      "exploration/env_infos/initial/reward_safety Min           0\n",
      "exploration/env_infos/reward_safety Mean                  0\n",
      "exploration/env_infos/reward_safety Std                   0\n",
      "exploration/env_infos/reward_safety Max                   0\n",
      "exploration/env_infos/reward_safety Min                   0\n",
      "exploration/env_infos/final/end_effector_loc Mean        -0.0324098\n",
      "exploration/env_infos/final/end_effector_loc Std          0.207577\n",
      "exploration/env_infos/final/end_effector_loc Max          0.439825\n",
      "exploration/env_infos/final/end_effector_loc Min         -0.320268\n",
      "exploration/env_infos/initial/end_effector_loc Mean      -0.000507825\n",
      "exploration/env_infos/initial/end_effector_loc Std        0.00918072\n",
      "exploration/env_infos/initial/end_effector_loc Max        0.0133203\n",
      "exploration/env_infos/initial/end_effector_loc Min       -0.0203027\n",
      "exploration/env_infos/end_effector_loc Mean              -0.0172583\n",
      "exploration/env_infos/end_effector_loc Std                0.135147\n",
      "exploration/env_infos/end_effector_loc Max                0.439825\n",
      "exploration/env_infos/end_effector_loc Min               -0.320268\n",
      "evaluation/num steps total                           115000\n",
      "evaluation/num paths total                             5750\n",
      "evaluation/path length Mean                              20\n",
      "evaluation/path length Std                                0\n",
      "evaluation/path length Max                               20\n",
      "evaluation/path length Min                               20\n",
      "evaluation/Rewards Mean                                  -0.049093\n",
      "evaluation/Rewards Std                                    0.0772727\n",
      "evaluation/Rewards Max                                    0.137314\n",
      "evaluation/Rewards Min                                   -0.345131\n",
      "evaluation/Returns Mean                                  -0.981861\n",
      "evaluation/Returns Std                                    1.2158\n",
      "evaluation/Returns Max                                    2.11401\n",
      "evaluation/Returns Min                                   -3.67657\n",
      "evaluation/Actions Mean                                   0.00227941\n",
      "evaluation/Actions Std                                    0.0640907\n",
      "evaluation/Actions Max                                    0.594016\n",
      "evaluation/Actions Min                                   -0.598518\n",
      "evaluation/Num Paths                                     50\n",
      "evaluation/Average Returns                               -0.981861\n",
      "evaluation/env_infos/final/reward_dist Mean               0.186489\n",
      "evaluation/env_infos/final/reward_dist Std                0.287327\n",
      "evaluation/env_infos/final/reward_dist Max                0.994224\n",
      "evaluation/env_infos/final/reward_dist Min                3.85511e-19\n",
      "evaluation/env_infos/initial/reward_dist Mean             0.00417689\n",
      "evaluation/env_infos/initial/reward_dist Std              0.00985083\n",
      "evaluation/env_infos/initial/reward_dist Max              0.0543167\n",
      "evaluation/env_infos/initial/reward_dist Min              2.9799e-06\n",
      "evaluation/env_infos/reward_dist Mean                     0.19404\n",
      "evaluation/env_infos/reward_dist Std                      0.284096\n",
      "evaluation/env_infos/reward_dist Max                      0.994224\n",
      "evaluation/env_infos/reward_dist Min                      3.85511e-19\n",
      "evaluation/env_infos/final/reward_energy Mean            -0.0307128\n",
      "evaluation/env_infos/final/reward_energy Std              0.0254736\n",
      "evaluation/env_infos/final/reward_energy Max             -0.00158496\n",
      "evaluation/env_infos/final/reward_energy Min             -0.121107\n",
      "evaluation/env_infos/initial/reward_energy Mean          -0.2014\n",
      "evaluation/env_infos/initial/reward_energy Std            0.175404\n",
      "evaluation/env_infos/initial/reward_energy Max           -0.0203774\n",
      "evaluation/env_infos/initial/reward_energy Min           -0.713276\n",
      "evaluation/env_infos/reward_energy Mean                  -0.053186\n",
      "evaluation/env_infos/reward_energy Std                    0.0734634\n",
      "evaluation/env_infos/reward_energy Max                   -0.00158496\n",
      "evaluation/env_infos/reward_energy Min                   -0.713276\n",
      "evaluation/env_infos/final/reward_safety Mean             0\n",
      "evaluation/env_infos/final/reward_safety Std              0\n",
      "evaluation/env_infos/final/reward_safety Max              0\n",
      "evaluation/env_infos/final/reward_safety Min              0\n",
      "evaluation/env_infos/initial/reward_safety Mean           0\n",
      "evaluation/env_infos/initial/reward_safety Std            0\n",
      "evaluation/env_infos/initial/reward_safety Max            0\n",
      "evaluation/env_infos/initial/reward_safety Min            0\n",
      "evaluation/env_infos/reward_safety Mean                   0\n",
      "evaluation/env_infos/reward_safety Std                    0\n",
      "evaluation/env_infos/reward_safety Max                    0\n",
      "evaluation/env_infos/reward_safety Min                    0\n",
      "evaluation/env_infos/final/end_effector_loc Mean          0.015655\n",
      "evaluation/env_infos/final/end_effector_loc Std           0.251063\n",
      "evaluation/env_infos/final/end_effector_loc Max           0.568193\n",
      "evaluation/env_infos/final/end_effector_loc Min          -0.443725\n",
      "evaluation/env_infos/initial/end_effector_loc Mean        5.80865e-05\n",
      "evaluation/env_infos/initial/end_effector_loc Std         0.00944231\n",
      "evaluation/env_infos/initial/end_effector_loc Max         0.0297008\n",
      "evaluation/env_infos/initial/end_effector_loc Min        -0.0239785\n",
      "evaluation/env_infos/end_effector_loc Mean                0.00467696\n",
      "evaluation/env_infos/end_effector_loc Std                 0.163052\n",
      "evaluation/env_infos/end_effector_loc Max                 0.568193\n",
      "evaluation/env_infos/end_effector_loc Min                -0.454746\n",
      "time/data storing (s)                                     0.00304409\n",
      "time/evaluation sampling (s)                              0.914456\n",
      "time/exploration sampling (s)                             0.12762\n",
      "time/logging (s)                                          0.0243292\n",
      "time/saving (s)                                           0.0397479\n",
      "time/training (s)                                        55.6827\n",
      "time/epoch (s)                                           56.7919\n",
      "time/total (s)                                         6173.87\n",
      "Epoch                                                   114\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:57:09.515706 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 115 finished\n",
      "---------------------------------------------------  -----------------\n",
      "replay_buffer/size                                     2000\n",
      "trainer/QF1 Loss                                          0.000537775\n",
      "trainer/QF2 Loss                                          0.000740825\n",
      "trainer/Policy Loss                                       2.86356\n",
      "trainer/Q1 Predictions Mean                              -0.904992\n",
      "trainer/Q1 Predictions Std                                0.851987\n",
      "trainer/Q1 Predictions Max                                0.708101\n",
      "trainer/Q1 Predictions Min                               -3.28852\n",
      "trainer/Q2 Predictions Mean                              -0.91094\n",
      "trainer/Q2 Predictions Std                                0.851294\n",
      "trainer/Q2 Predictions Max                                0.677118\n",
      "trainer/Q2 Predictions Min                               -3.31757\n",
      "trainer/Q Targets Mean                                   -0.89744\n",
      "trainer/Q Targets Std                                     0.851147\n",
      "trainer/Q Targets Max                                     0.722598\n",
      "trainer/Q Targets Min                                    -3.33714\n",
      "trainer/Log Pis Mean                                      1.95997\n",
      "trainer/Log Pis Std                                       1.52092\n",
      "trainer/Log Pis Max                                       4.48698\n",
      "trainer/Log Pis Min                                      -7.42214\n",
      "trainer/Policy mu Mean                                    0.00407752\n",
      "trainer/Policy mu Std                                     0.244828\n",
      "trainer/Policy mu Max                                     2.01233\n",
      "trainer/Policy mu Min                                    -0.899885\n",
      "trainer/Policy log std Mean                              -2.34056\n",
      "trainer/Policy log std Std                                0.53225\n",
      "trainer/Policy log std Max                               -0.553975\n",
      "trainer/Policy log std Min                               -3.35286\n",
      "trainer/Alpha                                             0.0211964\n",
      "trainer/Alpha Loss                                       -0.154303\n",
      "exploration/num steps total                           12600\n",
      "exploration/num paths total                             630\n",
      "exploration/path length Mean                             20\n",
      "exploration/path length Std                               0\n",
      "exploration/path length Max                              20\n",
      "exploration/path length Min                              20\n",
      "exploration/Rewards Mean                                 -0.133896\n",
      "exploration/Rewards Std                                   0.281355\n",
      "exploration/Rewards Max                                   0.0870623\n",
      "exploration/Rewards Min                                  -1.61627\n",
      "exploration/Returns Mean                                 -2.67793\n",
      "exploration/Returns Std                                   3.03325\n",
      "exploration/Returns Max                                   0.242329\n",
      "exploration/Returns Min                                  -8.43585\n",
      "exploration/Actions Mean                                  0.0478813\n",
      "exploration/Actions Std                                   0.22664\n",
      "exploration/Actions Max                                   0.976813\n",
      "exploration/Actions Min                                  -0.605558\n",
      "exploration/Num Paths                                     5\n",
      "exploration/Average Returns                              -2.67793\n",
      "exploration/env_infos/final/reward_dist Mean              0.17989\n",
      "exploration/env_infos/final/reward_dist Std               0.343721\n",
      "exploration/env_infos/final/reward_dist Max               0.866869\n",
      "exploration/env_infos/final/reward_dist Min               1.07953e-116\n",
      "exploration/env_infos/initial/reward_dist Mean            0.0100543\n",
      "exploration/env_infos/initial/reward_dist Std             0.00797358\n",
      "exploration/env_infos/initial/reward_dist Max             0.020205\n",
      "exploration/env_infos/initial/reward_dist Min             0.000736594\n",
      "exploration/env_infos/reward_dist Mean                    0.120088\n",
      "exploration/env_infos/reward_dist Std                     0.257496\n",
      "exploration/env_infos/reward_dist Max                     0.999339\n",
      "exploration/env_infos/reward_dist Min                     1.07953e-116\n",
      "exploration/env_infos/final/reward_energy Mean           -0.408845\n",
      "exploration/env_infos/final/reward_energy Std             0.497589\n",
      "exploration/env_infos/final/reward_energy Max            -0.0556495\n",
      "exploration/env_infos/final/reward_energy Min            -1.37283\n",
      "exploration/env_infos/initial/reward_energy Mean         -0.230985\n",
      "exploration/env_infos/initial/reward_energy Std           0.230543\n",
      "exploration/env_infos/initial/reward_energy Max          -0.0148098\n",
      "exploration/env_infos/initial/reward_energy Min          -0.628933\n",
      "exploration/env_infos/reward_energy Mean                 -0.196365\n",
      "exploration/env_infos/reward_energy Std                   0.262216\n",
      "exploration/env_infos/reward_energy Max                  -0.0127184\n",
      "exploration/env_infos/reward_energy Min                  -1.37283\n",
      "exploration/env_infos/final/reward_safety Mean            0\n",
      "exploration/env_infos/final/reward_safety Std             0\n",
      "exploration/env_infos/final/reward_safety Max             0\n",
      "exploration/env_infos/final/reward_safety Min             0\n",
      "exploration/env_infos/initial/reward_safety Mean          0\n",
      "exploration/env_infos/initial/reward_safety Std           0\n",
      "exploration/env_infos/initial/reward_safety Max           0\n",
      "exploration/env_infos/initial/reward_safety Min           0\n",
      "exploration/env_infos/reward_safety Mean                  0\n",
      "exploration/env_infos/reward_safety Std                   0\n",
      "exploration/env_infos/reward_safety Max                   0\n",
      "exploration/env_infos/reward_safety Min                   0\n",
      "exploration/env_infos/final/end_effector_loc Mean         0.221426\n",
      "exploration/env_infos/final/end_effector_loc Std          0.430113\n",
      "exploration/env_infos/final/end_effector_loc Max          1\n",
      "exploration/env_infos/final/end_effector_loc Min         -0.217969\n",
      "exploration/env_infos/initial/end_effector_loc Mean      -0.00147477\n",
      "exploration/env_infos/initial/end_effector_loc Std        0.0114436\n",
      "exploration/env_infos/initial/end_effector_loc Max        0.00881376\n",
      "exploration/env_infos/initial/end_effector_loc Min       -0.0302779\n",
      "exploration/env_infos/end_effector_loc Mean               0.0554598\n",
      "exploration/env_infos/end_effector_loc Std                0.202121\n",
      "exploration/env_infos/end_effector_loc Max                1\n",
      "exploration/env_infos/end_effector_loc Min               -0.320278\n",
      "evaluation/num steps total                           116000\n",
      "evaluation/num paths total                             5800\n",
      "evaluation/path length Mean                              20\n",
      "evaluation/path length Std                                0\n",
      "evaluation/path length Max                               20\n",
      "evaluation/path length Min                               20\n",
      "evaluation/Rewards Mean                                  -0.0441811\n",
      "evaluation/Rewards Std                                    0.100028\n",
      "evaluation/Rewards Max                                    0.149042\n",
      "evaluation/Rewards Min                                   -1.26779\n",
      "evaluation/Returns Mean                                  -0.883623\n",
      "evaluation/Returns Std                                    1.37775\n",
      "evaluation/Returns Max                                    1.51962\n",
      "evaluation/Returns Min                                   -5.74995\n",
      "evaluation/Actions Mean                                   0.00855701\n",
      "evaluation/Actions Std                                    0.0805491\n",
      "evaluation/Actions Max                                    0.977708\n",
      "evaluation/Actions Min                                   -0.510416\n",
      "evaluation/Num Paths                                     50\n",
      "evaluation/Average Returns                               -0.883623\n",
      "evaluation/env_infos/final/reward_dist Mean               0.170216\n",
      "evaluation/env_infos/final/reward_dist Std                0.264435\n",
      "evaluation/env_infos/final/reward_dist Max                0.809197\n",
      "evaluation/env_infos/final/reward_dist Min                2.53302e-108\n",
      "evaluation/env_infos/initial/reward_dist Mean             0.00744222\n",
      "evaluation/env_infos/initial/reward_dist Std              0.012838\n",
      "evaluation/env_infos/initial/reward_dist Max              0.056793\n",
      "evaluation/env_infos/initial/reward_dist Min              1.88031e-06\n",
      "evaluation/env_infos/reward_dist Mean                     0.233046\n",
      "evaluation/env_infos/reward_dist Std                      0.298641\n",
      "evaluation/env_infos/reward_dist Max                      0.995054\n",
      "evaluation/env_infos/reward_dist Min                      2.53302e-108\n",
      "evaluation/env_infos/final/reward_energy Mean            -0.0708354\n",
      "evaluation/env_infos/final/reward_energy Std              0.199247\n",
      "evaluation/env_infos/final/reward_energy Max             -0.00174298\n",
      "evaluation/env_infos/final/reward_energy Min             -1.35962\n",
      "evaluation/env_infos/initial/reward_energy Mean          -0.190741\n",
      "evaluation/env_infos/initial/reward_energy Std            0.157186\n",
      "evaluation/env_infos/initial/reward_energy Max           -0.0117112\n",
      "evaluation/env_infos/initial/reward_energy Min           -0.685654\n",
      "evaluation/env_infos/reward_energy Mean                  -0.0555732\n",
      "evaluation/env_infos/reward_energy Std                    0.100172\n",
      "evaluation/env_infos/reward_energy Max                   -0.000565799\n",
      "evaluation/env_infos/reward_energy Min                   -1.35962\n",
      "evaluation/env_infos/final/reward_safety Mean             0\n",
      "evaluation/env_infos/final/reward_safety Std              0\n",
      "evaluation/env_infos/final/reward_safety Max              0\n",
      "evaluation/env_infos/final/reward_safety Min              0\n",
      "evaluation/env_infos/initial/reward_safety Mean           0\n",
      "evaluation/env_infos/initial/reward_safety Std            0\n",
      "evaluation/env_infos/initial/reward_safety Max            0\n",
      "evaluation/env_infos/initial/reward_safety Min            0\n",
      "evaluation/env_infos/reward_safety Mean                   0\n",
      "evaluation/env_infos/reward_safety Std                    0\n",
      "evaluation/env_infos/reward_safety Max                    0\n",
      "evaluation/env_infos/reward_safety Min                    0\n",
      "evaluation/env_infos/final/end_effector_loc Mean          0.058105\n",
      "evaluation/env_infos/final/end_effector_loc Std           0.282237\n",
      "evaluation/env_infos/final/end_effector_loc Max           1\n",
      "evaluation/env_infos/final/end_effector_loc Min          -0.524217\n",
      "evaluation/env_infos/initial/end_effector_loc Mean        0.00028646\n",
      "evaluation/env_infos/initial/end_effector_loc Std         0.00873384\n",
      "evaluation/env_infos/initial/end_effector_loc Max         0.0331746\n",
      "evaluation/env_infos/initial/end_effector_loc Min        -0.0255208\n",
      "evaluation/env_infos/end_effector_loc Mean                0.0207416\n",
      "evaluation/env_infos/end_effector_loc Std                 0.166249\n",
      "evaluation/env_infos/end_effector_loc Max                 1\n",
      "evaluation/env_infos/end_effector_loc Min                -0.524217\n",
      "time/data storing (s)                                     0.00385941\n",
      "time/evaluation sampling (s)                              1.62003\n",
      "time/exploration sampling (s)                             0.167885\n",
      "time/logging (s)                                          0.0210021\n",
      "time/saving (s)                                           0.0305073\n",
      "time/training (s)                                        53.5985\n",
      "time/epoch (s)                                           55.4418\n",
      "time/total (s)                                         6231.66\n",
      "Epoch                                                   115\n",
      "---------------------------------------------------  -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:58:10.613487 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 116 finished\n",
      "---------------------------------------------------  ----------------\n",
      "replay_buffer/size                                     2000\n",
      "trainer/QF1 Loss                                          0.000981136\n",
      "trainer/QF2 Loss                                          0.00116679\n",
      "trainer/Policy Loss                                       2.96175\n",
      "trainer/Q1 Predictions Mean                              -0.929287\n",
      "trainer/Q1 Predictions Std                                0.808525\n",
      "trainer/Q1 Predictions Max                                0.385722\n",
      "trainer/Q1 Predictions Min                               -3.14318\n",
      "trainer/Q2 Predictions Mean                              -0.927342\n",
      "trainer/Q2 Predictions Std                                0.80063\n",
      "trainer/Q2 Predictions Max                                0.381788\n",
      "trainer/Q2 Predictions Min                               -3.10363\n",
      "trainer/Q Targets Mean                                   -0.924638\n",
      "trainer/Q Targets Std                                     0.808689\n",
      "trainer/Q Targets Max                                     0.397138\n",
      "trainer/Q Targets Min                                    -3.09855\n",
      "trainer/Log Pis Mean                                      2.03218\n",
      "trainer/Log Pis Std                                       1.27705\n",
      "trainer/Log Pis Max                                       4.60818\n",
      "trainer/Log Pis Min                                      -1.85031\n",
      "trainer/Policy mu Mean                                   -0.00352682\n",
      "trainer/Policy mu Std                                     0.254886\n",
      "trainer/Policy mu Max                                     2.06036\n",
      "trainer/Policy mu Min                                    -2.0712\n",
      "trainer/Policy log std Mean                              -2.33058\n",
      "trainer/Policy log std Std                                0.573843\n",
      "trainer/Policy log std Max                               -0.222984\n",
      "trainer/Policy log std Min                               -3.36766\n",
      "trainer/Alpha                                             0.0207613\n",
      "trainer/Alpha Loss                                        0.124731\n",
      "exploration/num steps total                           12700\n",
      "exploration/num paths total                             635\n",
      "exploration/path length Mean                             20\n",
      "exploration/path length Std                               0\n",
      "exploration/path length Max                              20\n",
      "exploration/path length Min                              20\n",
      "exploration/Rewards Mean                                 -0.0867281\n",
      "exploration/Rewards Std                                   0.0682655\n",
      "exploration/Rewards Max                                   0.031513\n",
      "exploration/Rewards Min                                  -0.36895\n",
      "exploration/Returns Mean                                 -1.73456\n",
      "exploration/Returns Std                                   0.589982\n",
      "exploration/Returns Max                                  -1.17134\n",
      "exploration/Returns Min                                  -2.73471\n",
      "exploration/Actions Mean                                 -0.00180494\n",
      "exploration/Actions Std                                   0.121848\n",
      "exploration/Actions Max                                   0.440656\n",
      "exploration/Actions Min                                  -0.460676\n",
      "exploration/Num Paths                                     5\n",
      "exploration/Average Returns                              -1.73456\n",
      "exploration/env_infos/final/reward_dist Mean              0.0481871\n",
      "exploration/env_infos/final/reward_dist Std               0.094536\n",
      "exploration/env_infos/final/reward_dist Max               0.237237\n",
      "exploration/env_infos/final/reward_dist Min               1.22799e-13\n",
      "exploration/env_infos/initial/reward_dist Mean            0.00366737\n",
      "exploration/env_infos/initial/reward_dist Std             0.00390136\n",
      "exploration/env_infos/initial/reward_dist Max             0.0105915\n",
      "exploration/env_infos/initial/reward_dist Min             1.61404e-05\n",
      "exploration/env_infos/reward_dist Mean                    0.207844\n",
      "exploration/env_infos/reward_dist Std                     0.303357\n",
      "exploration/env_infos/reward_dist Max                     0.964488\n",
      "exploration/env_infos/reward_dist Min                     1.22799e-13\n",
      "exploration/env_infos/final/reward_energy Mean           -0.125257\n",
      "exploration/env_infos/final/reward_energy Std             0.0346209\n",
      "exploration/env_infos/final/reward_energy Max            -0.0763336\n",
      "exploration/env_infos/final/reward_energy Min            -0.178522\n",
      "exploration/env_infos/initial/reward_energy Mean         -0.247602\n",
      "exploration/env_infos/initial/reward_energy Std           0.123661\n",
      "exploration/env_infos/initial/reward_energy Max          -0.160871\n",
      "exploration/env_infos/initial/reward_energy Min          -0.491421\n",
      "exploration/env_infos/reward_energy Mean                 -0.142577\n",
      "exploration/env_infos/reward_energy Std                   0.0968111\n",
      "exploration/env_infos/reward_energy Max                  -0.0127288\n",
      "exploration/env_infos/reward_energy Min                  -0.598295\n",
      "exploration/env_infos/final/reward_safety Mean            0\n",
      "exploration/env_infos/final/reward_safety Std             0\n",
      "exploration/env_infos/final/reward_safety Max             0\n",
      "exploration/env_infos/final/reward_safety Min             0\n",
      "exploration/env_infos/initial/reward_safety Mean          0\n",
      "exploration/env_infos/initial/reward_safety Std           0\n",
      "exploration/env_infos/initial/reward_safety Max           0\n",
      "exploration/env_infos/initial/reward_safety Min           0\n",
      "exploration/env_infos/reward_safety Mean                  0\n",
      "exploration/env_infos/reward_safety Std                   0\n",
      "exploration/env_infos/reward_safety Max                   0\n",
      "exploration/env_infos/reward_safety Min                   0\n",
      "exploration/env_infos/final/end_effector_loc Mean        -0.112726\n",
      "exploration/env_infos/final/end_effector_loc Std          0.250229\n",
      "exploration/env_infos/final/end_effector_loc Max          0.325637\n",
      "exploration/env_infos/final/end_effector_loc Min         -0.480223\n",
      "exploration/env_infos/initial/end_effector_loc Mean       0.00222547\n",
      "exploration/env_infos/initial/end_effector_loc Std        0.00952869\n",
      "exploration/env_infos/initial/end_effector_loc Max        0.0220328\n",
      "exploration/env_infos/initial/end_effector_loc Min       -0.0111025\n",
      "exploration/env_infos/end_effector_loc Mean              -0.0598875\n",
      "exploration/env_infos/end_effector_loc Std                0.161412\n",
      "exploration/env_infos/end_effector_loc Max                0.325637\n",
      "exploration/env_infos/end_effector_loc Min               -0.480223\n",
      "evaluation/num steps total                           117000\n",
      "evaluation/num paths total                             5850\n",
      "evaluation/path length Mean                              20\n",
      "evaluation/path length Std                                0\n",
      "evaluation/path length Max                               20\n",
      "evaluation/path length Min                               20\n",
      "evaluation/Rewards Mean                                  -0.0284037\n",
      "evaluation/Rewards Std                                    0.0748379\n",
      "evaluation/Rewards Max                                    0.179186\n",
      "evaluation/Rewards Min                                   -0.258402\n",
      "evaluation/Returns Mean                                  -0.568074\n",
      "evaluation/Returns Std                                    1.10361\n",
      "evaluation/Returns Max                                    1.59635\n",
      "evaluation/Returns Min                                   -2.74628\n",
      "evaluation/Actions Mean                                   0.00268039\n",
      "evaluation/Actions Std                                    0.0735224\n",
      "evaluation/Actions Max                                    0.804168\n",
      "evaluation/Actions Min                                   -0.622059\n",
      "evaluation/Num Paths                                     50\n",
      "evaluation/Average Returns                               -0.568074\n",
      "evaluation/env_infos/final/reward_dist Mean               0.121939\n",
      "evaluation/env_infos/final/reward_dist Std                0.234138\n",
      "evaluation/env_infos/final/reward_dist Max                0.835666\n",
      "evaluation/env_infos/final/reward_dist Min                2.06713e-40\n",
      "evaluation/env_infos/initial/reward_dist Mean             0.00480142\n",
      "evaluation/env_infos/initial/reward_dist Std              0.0125274\n",
      "evaluation/env_infos/initial/reward_dist Max              0.0777737\n",
      "evaluation/env_infos/initial/reward_dist Min              1.49909e-06\n",
      "evaluation/env_infos/reward_dist Mean                     0.197041\n",
      "evaluation/env_infos/reward_dist Std                      0.285721\n",
      "evaluation/env_infos/reward_dist Max                      0.997199\n",
      "evaluation/env_infos/reward_dist Min                      2.06713e-40\n",
      "evaluation/env_infos/final/reward_energy Mean            -0.0385613\n",
      "evaluation/env_infos/final/reward_energy Std              0.0547298\n",
      "evaluation/env_infos/final/reward_energy Max             -0.00189546\n",
      "evaluation/env_infos/final/reward_energy Min             -0.289464\n",
      "evaluation/env_infos/initial/reward_energy Mean          -0.234969\n",
      "evaluation/env_infos/initial/reward_energy Std            0.233148\n",
      "evaluation/env_infos/initial/reward_energy Max           -0.00456271\n",
      "evaluation/env_infos/initial/reward_energy Min           -0.821036\n",
      "evaluation/env_infos/reward_energy Mean                  -0.0577508\n",
      "evaluation/env_infos/reward_energy Std                    0.0865465\n",
      "evaluation/env_infos/reward_energy Max                   -0.00118387\n",
      "evaluation/env_infos/reward_energy Min                   -0.821036\n",
      "evaluation/env_infos/final/reward_safety Mean             0\n",
      "evaluation/env_infos/final/reward_safety Std              0\n",
      "evaluation/env_infos/final/reward_safety Max              0\n",
      "evaluation/env_infos/final/reward_safety Min              0\n",
      "evaluation/env_infos/initial/reward_safety Mean           0\n",
      "evaluation/env_infos/initial/reward_safety Std            0\n",
      "evaluation/env_infos/initial/reward_safety Max            0\n",
      "evaluation/env_infos/initial/reward_safety Min            0\n",
      "evaluation/env_infos/reward_safety Mean                   0\n",
      "evaluation/env_infos/reward_safety Std                    0\n",
      "evaluation/env_infos/reward_safety Max                    0\n",
      "evaluation/env_infos/reward_safety Min                    0\n",
      "evaluation/env_infos/final/end_effector_loc Mean          0.0651259\n",
      "evaluation/env_infos/final/end_effector_loc Std           0.254127\n",
      "evaluation/env_infos/final/end_effector_loc Max           0.547625\n",
      "evaluation/env_infos/final/end_effector_loc Min          -0.66104\n",
      "evaluation/env_infos/initial/end_effector_loc Mean        0.00249393\n",
      "evaluation/env_infos/initial/end_effector_loc Std         0.0114342\n",
      "evaluation/env_infos/initial/end_effector_loc Max         0.0402084\n",
      "evaluation/env_infos/initial/end_effector_loc Min        -0.031103\n",
      "evaluation/env_infos/end_effector_loc Mean                0.0375929\n",
      "evaluation/env_infos/end_effector_loc Std                 0.162879\n",
      "evaluation/env_infos/end_effector_loc Max                 0.547625\n",
      "evaluation/env_infos/end_effector_loc Min                -0.66104\n",
      "time/data storing (s)                                     0.0034118\n",
      "time/evaluation sampling (s)                              1.16059\n",
      "time/exploration sampling (s)                             0.140974\n",
      "time/logging (s)                                          0.0208466\n",
      "time/saving (s)                                           0.029828\n",
      "time/training (s)                                        58.0182\n",
      "time/epoch (s)                                           59.3739\n",
      "time/total (s)                                         6292.74\n",
      "Epoch                                                   116\n",
      "---------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:59:07.233886 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 117 finished\r\n",
      "---------------------------------------------------  ----------------\r\n",
      "replay_buffer/size                                     2000\r\n",
      "trainer/QF1 Loss                                          0.00061241\r\n",
      "trainer/QF2 Loss                                          0.000641443\r\n",
      "trainer/Policy Loss                                       2.92731\r\n",
      "trainer/Q1 Predictions Mean                              -0.913149\r\n",
      "trainer/Q1 Predictions Std                                0.796305\r\n",
      "trainer/Q1 Predictions Max                                0.675178\r\n",
      "trainer/Q1 Predictions Min                               -3.07649\r\n",
      "trainer/Q2 Predictions Mean                              -0.904085\r\n",
      "trainer/Q2 Predictions Std                                0.799416\r\n",
      "trainer/Q2 Predictions Max                                0.679758\r\n",
      "trainer/Q2 Predictions Min                               -3.08679\r\n",
      "trainer/Q Targets Mean                                   -0.910312\r\n",
      "trainer/Q Targets Std                                     0.795027\r\n",
      "trainer/Q Targets Max                                     0.699894\r\n",
      "trainer/Q Targets Min                                    -3.11027\r\n",
      "trainer/Log Pis Mean                                      2.01799\r\n",
      "trainer/Log Pis Std                                       1.38418\r\n",
      "trainer/Log Pis Max                                       4.49618\r\n",
      "trainer/Log Pis Min                                      -5.88323\r\n",
      "trainer/Policy mu Mean                                   -0.0144399\r\n",
      "trainer/Policy mu Std                                     0.198848\r\n",
      "trainer/Policy mu Max                                     1.34726\r\n",
      "trainer/Policy mu Min                                    -1.36985\r\n",
      "trainer/Policy log std Mean                              -2.40145\r\n",
      "trainer/Policy log std Std                                0.528467\r\n",
      "trainer/Policy log std Max                               -0.512592\r\n",
      "trainer/Policy log std Min                               -3.29771\r\n",
      "trainer/Alpha                                             0.0215424\r\n",
      "trainer/Alpha Loss                                        0.0690633\r\n",
      "exploration/num steps total                           12800\r\n",
      "exploration/num paths total                             640\r\n",
      "exploration/path length Mean                             20\r\n",
      "exploration/path length Std                               0\r\n",
      "exploration/path length Max                              20\r\n",
      "exploration/path length Min                              20\r\n",
      "exploration/Rewards Mean                                 -0.116282\r\n",
      "exploration/Rewards Std                                   0.0602432\r\n",
      "exploration/Rewards Max                                   0.00990557\r\n",
      "exploration/Rewards Min                                  -0.289434\r\n",
      "exploration/Returns Mean                                 -2.32565\r\n",
      "exploration/Returns Std                                   0.873942\r\n",
      "exploration/Returns Max                                  -0.920343\r\n",
      "exploration/Returns Min                                  -3.56847\r\n",
      "exploration/Actions Mean                                  0.00340758\r\n",
      "exploration/Actions Std                                   0.137206\r\n",
      "exploration/Actions Max                                   0.517351\r\n",
      "exploration/Actions Min                                  -0.675911\r\n",
      "exploration/Num Paths                                     5\r\n",
      "exploration/Average Returns                              -2.32565\r\n",
      "exploration/env_infos/final/reward_dist Mean              0.194212\r\n",
      "exploration/env_infos/final/reward_dist Std               0.332667\r\n",
      "exploration/env_infos/final/reward_dist Max               0.855105\r\n",
      "exploration/env_infos/final/reward_dist Min               4.88648e-20\r\n",
      "exploration/env_infos/initial/reward_dist Mean            0.000397086\r\n",
      "exploration/env_infos/initial/reward_dist Std             0.00075733\r\n",
      "exploration/env_infos/initial/reward_dist Max             0.00191132\r\n",
      "exploration/env_infos/initial/reward_dist Min             5.17514e-06\r\n",
      "exploration/env_infos/reward_dist Mean                    0.192557\r\n",
      "exploration/env_infos/reward_dist Std                     0.295711\r\n",
      "exploration/env_infos/reward_dist Max                     0.974806\r\n",
      "exploration/env_infos/reward_dist Min                     4.88648e-20\r\n",
      "exploration/env_infos/final/reward_energy Mean           -0.181753\r\n",
      "exploration/env_infos/final/reward_energy Std             0.161306\r\n",
      "exploration/env_infos/final/reward_energy Max            -0.0423721\r\n",
      "exploration/env_infos/final/reward_energy Min            -0.454548\r\n",
      "exploration/env_infos/initial/reward_energy Mean         -0.192225\r\n",
      "exploration/env_infos/initial/reward_energy Std           0.0598322\r\n",
      "exploration/env_infos/initial/reward_energy Max          -0.103655\r\n",
      "exploration/env_infos/initial/reward_energy Min          -0.278196\r\n",
      "exploration/env_infos/reward_energy Mean                 -0.148221\r\n",
      "exploration/env_infos/reward_energy Std                   0.125319\r\n",
      "exploration/env_infos/reward_energy Max                  -0.0123601\r\n",
      "exploration/env_infos/reward_energy Min                  -0.681118\r\n",
      "exploration/env_infos/final/reward_safety Mean            0\r\n",
      "exploration/env_infos/final/reward_safety Std             0\r\n",
      "exploration/env_infos/final/reward_safety Max             0\r\n",
      "exploration/env_infos/final/reward_safety Min             0\r\n",
      "exploration/env_infos/initial/reward_safety Mean          0\r\n",
      "exploration/env_infos/initial/reward_safety Std           0\r\n",
      "exploration/env_infos/initial/reward_safety Max           0\r\n",
      "exploration/env_infos/initial/reward_safety Min           0\r\n",
      "exploration/env_infos/reward_safety Mean                  0\r\n",
      "exploration/env_infos/reward_safety Std                   0\r\n",
      "exploration/env_infos/reward_safety Max                   0\r\n",
      "exploration/env_infos/reward_safety Min                   0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean         0.0997276\r\n",
      "exploration/env_infos/final/end_effector_loc Std          0.305054\r\n",
      "exploration/env_infos/final/end_effector_loc Max          0.704313\r\n",
      "exploration/env_infos/final/end_effector_loc Min         -0.499112\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean       0.00303989\r\n",
      "exploration/env_infos/initial/end_effector_loc Std        0.00643598\r\n",
      "exploration/env_infos/initial/end_effector_loc Max        0.0126536\r\n",
      "exploration/env_infos/initial/end_effector_loc Min       -0.00593852\r\n",
      "exploration/env_infos/end_effector_loc Mean               0.0564014\r\n",
      "exploration/env_infos/end_effector_loc Std                0.196149\r\n",
      "exploration/env_infos/end_effector_loc Max                0.704313\r\n",
      "exploration/env_infos/end_effector_loc Min               -0.499112\r\n",
      "evaluation/num steps total                           118000\r\n",
      "evaluation/num paths total                             5900\r\n",
      "evaluation/path length Mean                              20\r\n",
      "evaluation/path length Std                                0\r\n",
      "evaluation/path length Max                               20\r\n",
      "evaluation/path length Min                               20\r\n",
      "evaluation/Rewards Mean                                  -0.0384652\r\n",
      "evaluation/Rewards Std                                    0.0659167\r\n",
      "evaluation/Rewards Max                                    0.15031\r\n",
      "evaluation/Rewards Min                                   -0.256034\r\n",
      "evaluation/Returns Mean                                  -0.769304\r\n",
      "evaluation/Returns Std                                    0.986934\r\n",
      "evaluation/Returns Max                                    1.3238\r\n",
      "evaluation/Returns Min                                   -2.9199\r\n",
      "evaluation/Actions Mean                                   0.000362255\r\n",
      "evaluation/Actions Std                                    0.0531485\r\n",
      "evaluation/Actions Max                                    0.461716\r\n",
      "evaluation/Actions Min                                   -0.788489\r\n",
      "evaluation/Num Paths                                     50\r\n",
      "evaluation/Average Returns                               -0.769304\r\n",
      "evaluation/env_infos/final/reward_dist Mean               0.147549\r\n",
      "evaluation/env_infos/final/reward_dist Std                0.240807\r\n",
      "evaluation/env_infos/final/reward_dist Max                0.95822\r\n",
      "evaluation/env_infos/final/reward_dist Min                5.75419e-07\r\n",
      "evaluation/env_infos/initial/reward_dist Mean             0.00624705\r\n",
      "evaluation/env_infos/initial/reward_dist Std              0.0107567\r\n",
      "evaluation/env_infos/initial/reward_dist Max              0.0531034\r\n",
      "evaluation/env_infos/initial/reward_dist Min              1.62027e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                     0.202728\r\n",
      "evaluation/env_infos/reward_dist Std                      0.281892\r\n",
      "evaluation/env_infos/reward_dist Max                      0.998577\r\n",
      "evaluation/env_infos/reward_dist Min                      5.75419e-07\r\n",
      "evaluation/env_infos/final/reward_energy Mean            -0.0259638\r\n",
      "evaluation/env_infos/final/reward_energy Std              0.0225703\r\n",
      "evaluation/env_infos/final/reward_energy Max             -0.000957516\r\n",
      "evaluation/env_infos/final/reward_energy Min             -0.116467\r\n",
      "evaluation/env_infos/initial/reward_energy Mean          -0.171478\r\n",
      "evaluation/env_infos/initial/reward_energy Std            0.16018\r\n",
      "evaluation/env_infos/initial/reward_energy Max           -0.0121049\r\n",
      "evaluation/env_infos/initial/reward_energy Min           -0.869185\r\n",
      "evaluation/env_infos/reward_energy Mean                  -0.0463866\r\n",
      "evaluation/env_infos/reward_energy Std                    0.0591446\r\n",
      "evaluation/env_infos/reward_energy Max                   -0.000190102\r\n",
      "evaluation/env_infos/reward_energy Min                   -0.869185\r\n",
      "evaluation/env_infos/final/reward_safety Mean             0\r\n",
      "evaluation/env_infos/final/reward_safety Std              0\r\n",
      "evaluation/env_infos/final/reward_safety Max              0\r\n",
      "evaluation/env_infos/final/reward_safety Min              0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean           0\r\n",
      "evaluation/env_infos/initial/reward_safety Std            0\r\n",
      "evaluation/env_infos/initial/reward_safety Max            0\r\n",
      "evaluation/env_infos/initial/reward_safety Min            0\r\n",
      "evaluation/env_infos/reward_safety Mean                   0\r\n",
      "evaluation/env_infos/reward_safety Std                    0\r\n",
      "evaluation/env_infos/reward_safety Max                    0\r\n",
      "evaluation/env_infos/reward_safety Min                    0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean          0.0215419\r\n",
      "evaluation/env_infos/final/end_effector_loc Std           0.227355\r\n",
      "evaluation/env_infos/final/end_effector_loc Max           0.408551\r\n",
      "evaluation/env_infos/final/end_effector_loc Min          -0.437799\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean        0.00110522\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std         0.00822232\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max         0.0230858\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min        -0.0394244\r\n",
      "evaluation/env_infos/end_effector_loc Mean                0.0145518\r\n",
      "evaluation/env_infos/end_effector_loc Std                 0.145607\r\n",
      "evaluation/env_infos/end_effector_loc Max                 0.424833\r\n",
      "evaluation/env_infos/end_effector_loc Min                -0.437799\r\n",
      "time/data storing (s)                                     0.00293989\r\n",
      "time/evaluation sampling (s)                              1.03477\r\n",
      "time/exploration sampling (s)                             0.126785\r\n",
      "time/logging (s)                                          0.0194953\r\n",
      "time/saving (s)                                           0.0321608\r\n",
      "time/training (s)                                        53.8523\r\n",
      "time/epoch (s)                                           55.0684\r\n",
      "time/total (s)                                         6349.36\r\n",
      "Epoch                                                   117\r\n",
      "---------------------------------------------------  ----------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 21:00:06.053673 PDT | [gher-pointreacherobs-SAC-300e-20s-disc0.97-horizon20_2021_05_25_19_13_32_0000--s-10] Epoch 118 finished\r\n",
      "---------------------------------------------------  ----------------\r\n",
      "replay_buffer/size                                     2000\r\n",
      "trainer/QF1 Loss                                          0.000762996\r\n",
      "trainer/QF2 Loss                                          0.000717099\r\n",
      "trainer/Policy Loss                                       2.92725\r\n",
      "trainer/Q1 Predictions Mean                              -0.946485\r\n",
      "trainer/Q1 Predictions Std                                0.875184\r\n",
      "trainer/Q1 Predictions Max                                0.68144\r\n",
      "trainer/Q1 Predictions Min                               -3.22415\r\n",
      "trainer/Q2 Predictions Mean                              -0.93967\r\n",
      "trainer/Q2 Predictions Std                                0.877685\r\n",
      "trainer/Q2 Predictions Max                                0.70794\r\n",
      "trainer/Q2 Predictions Min                               -3.21771\r\n",
      "trainer/Q Targets Mean                                   -0.947598\r\n",
      "trainer/Q Targets Std                                     0.87829\r\n",
      "trainer/Q Targets Max                                     0.708257\r\n",
      "trainer/Q Targets Min                                    -3.25063\r\n",
      "trainer/Log Pis Mean                                      1.98664\r\n",
      "trainer/Log Pis Std                                       1.35898\r\n",
      "trainer/Log Pis Max                                       4.32414\r\n",
      "trainer/Log Pis Min                                      -3.01\r\n",
      "trainer/Policy mu Mean                                    0.00691548\r\n",
      "trainer/Policy mu Std                                     0.263694\r\n",
      "trainer/Policy mu Max                                     1.70058\r\n",
      "trainer/Policy mu Min                                    -1.50055\r\n",
      "trainer/Policy log std Mean                              -2.32704\r\n",
      "trainer/Policy log std Std                                0.573678\r\n",
      "trainer/Policy log std Max                               -0.39182\r\n",
      "trainer/Policy log std Min                               -3.33886\r\n",
      "trainer/Alpha                                             0.021769\r\n",
      "trainer/Alpha Loss                                       -0.0511155\r\n",
      "exploration/num steps total                           12900\r\n",
      "exploration/num paths total                             645\r\n",
      "exploration/path length Mean                             20\r\n",
      "exploration/path length Std                               0\r\n",
      "exploration/path length Max                              20\r\n",
      "exploration/path length Min                              20\r\n",
      "exploration/Rewards Mean                                 -0.0993681\r\n",
      "exploration/Rewards Std                                   0.076666\r\n",
      "exploration/Rewards Max                                   0.0654167\r\n",
      "exploration/Rewards Min                                  -0.276415\r\n",
      "exploration/Returns Mean                                 -1.98736\r\n",
      "exploration/Returns Std                                   1.22023\r\n",
      "exploration/Returns Max                                   0.0577869\r\n",
      "exploration/Returns Min                                  -3.38139\r\n",
      "exploration/Actions Mean                                  0.00161334\r\n",
      "exploration/Actions Std                                   0.0928045\r\n",
      "exploration/Actions Max                                   0.310593\r\n",
      "exploration/Actions Min                                  -0.27334\r\n",
      "exploration/Num Paths                                     5\r\n",
      "exploration/Average Returns                              -1.98736\r\n",
      "exploration/env_infos/final/reward_dist Mean              0.0697754\r\n",
      "exploration/env_infos/final/reward_dist Std               0.13953\r\n",
      "exploration/env_infos/final/reward_dist Max               0.348834\r\n",
      "exploration/env_infos/final/reward_dist Min               2.18222e-37\r\n",
      "exploration/env_infos/initial/reward_dist Mean            0.00286732\r\n",
      "exploration/env_infos/initial/reward_dist Std             0.0029536\r\n",
      "exploration/env_infos/initial/reward_dist Max             0.00706244\r\n",
      "exploration/env_infos/initial/reward_dist Min             8.01322e-05\r\n",
      "exploration/env_infos/reward_dist Mean                    0.115111\r\n",
      "exploration/env_infos/reward_dist Std                     0.189402\r\n",
      "exploration/env_infos/reward_dist Max                     0.615485\r\n",
      "exploration/env_infos/reward_dist Min                     2.18222e-37\r\n",
      "exploration/env_infos/final/reward_energy Mean           -0.0797157\r\n",
      "exploration/env_infos/final/reward_energy Std             0.0545378\r\n",
      "exploration/env_infos/final/reward_energy Max            -0.035196\r\n",
      "exploration/env_infos/final/reward_energy Min            -0.184381\r\n",
      "exploration/env_infos/initial/reward_energy Mean         -0.194004\r\n",
      "exploration/env_infos/initial/reward_energy Std           0.0995456\r\n",
      "exploration/env_infos/initial/reward_energy Max          -0.0302095\r\n",
      "exploration/env_infos/initial/reward_energy Min          -0.318906\r\n",
      "exploration/env_infos/reward_energy Mean                 -0.113887\r\n",
      "exploration/env_infos/reward_energy Std                   0.0652706\r\n",
      "exploration/env_infos/reward_energy Max                  -0.000385532\r\n",
      "exploration/env_infos/reward_energy Min                  -0.318906\r\n",
      "exploration/env_infos/final/reward_safety Mean            0\r\n",
      "exploration/env_infos/final/reward_safety Std             0\r\n",
      "exploration/env_infos/final/reward_safety Max             0\r\n",
      "exploration/env_infos/final/reward_safety Min             0\r\n",
      "exploration/env_infos/initial/reward_safety Mean          0\r\n",
      "exploration/env_infos/initial/reward_safety Std           0\r\n",
      "exploration/env_infos/initial/reward_safety Max           0\r\n",
      "exploration/env_infos/initial/reward_safety Min           0\r\n",
      "exploration/env_infos/reward_safety Mean                  0\r\n",
      "exploration/env_infos/reward_safety Std                   0\r\n",
      "exploration/env_infos/reward_safety Max                   0\r\n",
      "exploration/env_infos/reward_safety Min                   0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean         0.00428744\r\n",
      "exploration/env_infos/final/end_effector_loc Std          0.369445\r\n",
      "exploration/env_infos/final/end_effector_loc Max          0.614172\r\n",
      "exploration/env_infos/final/end_effector_loc Min         -0.768425\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean       0.00248838\r\n",
      "exploration/env_infos/initial/end_effector_loc Std        0.00729669\r\n",
      "exploration/env_infos/initial/end_effector_loc Max        0.0155297\r\n",
      "exploration/env_infos/initial/end_effector_loc Min       -0.0097217\r\n",
      "exploration/env_infos/end_effector_loc Mean               0.00915179\r\n",
      "exploration/env_infos/end_effector_loc Std                0.200918\r\n",
      "exploration/env_infos/end_effector_loc Max                0.614172\r\n",
      "exploration/env_infos/end_effector_loc Min               -0.768425\r\n",
      "evaluation/num steps total                           119000\r\n",
      "evaluation/num paths total                             5950\r\n",
      "evaluation/path length Mean                              20\r\n",
      "evaluation/path length Std                                0\r\n",
      "evaluation/path length Max                               20\r\n",
      "evaluation/path length Min                               20\r\n",
      "evaluation/Rewards Mean                                  -0.0393236\r\n",
      "evaluation/Rewards Std                                    0.0653548\r\n",
      "evaluation/Rewards Max                                    0.147852\r\n",
      "evaluation/Rewards Min                                   -0.293331\r\n",
      "evaluation/Returns Mean                                  -0.786471\r\n",
      "evaluation/Returns Std                                    0.975009\r\n",
      "evaluation/Returns Max                                    1.70855\r\n",
      "evaluation/Returns Min                                   -2.70018\r\n",
      "evaluation/Actions Mean                                   0.00144539\r\n",
      "evaluation/Actions Std                                    0.0572295\r\n",
      "evaluation/Actions Max                                    0.541064\r\n",
      "evaluation/Actions Min                                   -0.484894\r\n",
      "evaluation/Num Paths                                     50\r\n",
      "evaluation/Average Returns                               -0.786471\r\n",
      "evaluation/env_infos/final/reward_dist Mean               0.0973961\r\n",
      "evaluation/env_infos/final/reward_dist Std                0.162187\r\n",
      "evaluation/env_infos/final/reward_dist Max                0.757884\r\n",
      "evaluation/env_infos/final/reward_dist Min                4.66575e-19\r\n",
      "evaluation/env_infos/initial/reward_dist Mean             0.00787884\r\n",
      "evaluation/env_infos/initial/reward_dist Std              0.0101659\r\n",
      "evaluation/env_infos/initial/reward_dist Max              0.0373568\r\n",
      "evaluation/env_infos/initial/reward_dist Min              1.26719e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                     0.182109\r\n",
      "evaluation/env_infos/reward_dist Std                      0.246422\r\n",
      "evaluation/env_infos/reward_dist Max                      0.991924\r\n",
      "evaluation/env_infos/reward_dist Min                      4.66575e-19\r\n",
      "evaluation/env_infos/final/reward_energy Mean            -0.0345357\r\n",
      "evaluation/env_infos/final/reward_energy Std              0.0359548\r\n",
      "evaluation/env_infos/final/reward_energy Max             -0.000635413\r\n",
      "evaluation/env_infos/final/reward_energy Min             -0.143242\r\n",
      "evaluation/env_infos/initial/reward_energy Mean          -0.190121\r\n",
      "evaluation/env_infos/initial/reward_energy Std            0.149044\r\n",
      "evaluation/env_infos/initial/reward_energy Max           -0.00240674\r\n",
      "evaluation/env_infos/initial/reward_energy Min           -0.554211\r\n",
      "evaluation/env_infos/reward_energy Mean                  -0.0510155\r\n",
      "evaluation/env_infos/reward_energy Std                    0.0628652\r\n",
      "evaluation/env_infos/reward_energy Max                   -0.000635413\r\n",
      "evaluation/env_infos/reward_energy Min                   -0.554211\r\n",
      "evaluation/env_infos/final/reward_safety Mean             0\r\n",
      "evaluation/env_infos/final/reward_safety Std              0\r\n",
      "evaluation/env_infos/final/reward_safety Max              0\r\n",
      "evaluation/env_infos/final/reward_safety Min              0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean           0\r\n",
      "evaluation/env_infos/initial/reward_safety Std            0\r\n",
      "evaluation/env_infos/initial/reward_safety Max            0\r\n",
      "evaluation/env_infos/initial/reward_safety Min            0\r\n",
      "evaluation/env_infos/reward_safety Mean                   0\r\n",
      "evaluation/env_infos/reward_safety Std                    0\r\n",
      "evaluation/env_infos/reward_safety Max                    0\r\n",
      "evaluation/env_infos/reward_safety Min                    0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean          0.00827361\r\n",
      "evaluation/env_infos/final/end_effector_loc Std           0.248221\r\n",
      "evaluation/env_infos/final/end_effector_loc Max           0.486643\r\n",
      "evaluation/env_infos/final/end_effector_loc Min          -0.555247\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean        0.000262598\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std         0.00853704\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max         0.0270532\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min        -0.0242447\r\n",
      "evaluation/env_infos/end_effector_loc Mean                0.00279596\r\n",
      "evaluation/env_infos/end_effector_loc Std                 0.160737\r\n",
      "evaluation/env_infos/end_effector_loc Max                 0.486643\r\n",
      "evaluation/env_infos/end_effector_loc Min                -0.555247\r\n",
      "time/data storing (s)                                     0.00313071\r\n",
      "time/evaluation sampling (s)                              1.03783\r\n",
      "time/exploration sampling (s)                             0.149035\r\n",
      "time/logging (s)                                          0.0207978\r\n",
      "time/saving (s)                                           0.0321887\r\n",
      "time/training (s)                                        55.9749\r\n",
      "time/epoch (s)                                           57.2178\r\n",
      "time/total (s)                                         6408.18\r\n",
      "Epoch                                                   118\r\n",
      "---------------------------------------------------  ----------------\r\n"
     ]
    }
   ],
   "source": [
    "!python launch_gher.py --epochs 300 --env pointreacherobs\n",
    "!python launch_gher.py --epochs 300 --relabel --n_sampled_latents 1 --env pointreacherobs\n",
    "!python launch_gher.py --epochs 300 --relabel --n_sampled_latents 100 --use_advantages --env pointreacherobs\n",
    "!python launch_gher.py --epochs 300 --relabel --n_sampled_latents 100 --use_advantages --env pointreacherobs --cache --irl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496b602e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_no_r = pd.read_csv('/Users/shanliyu/School/CSE257/own/generalized-hindsight/data/gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20/gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10/progress.csv')\n",
    "# df_rr   = pd.read_csv('/Users/shanliyu/School/CSE257/own/generalized-hindsight/data/gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20/gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_11_48_41_0000--s-10/progress.csv')\n",
    "# df_air  = pd.read_csv('/Users/shanliyu/School/CSE257/own/generalized-hindsight/data/gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20/gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_13_12_33_0000--s-10/progress.csv')\n",
    "# df_ar   = pd.read_csv('/Users/shanliyu/School/CSE257/own/generalized-hindsight/data/gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20/gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_14_32_26_0000--s-10/progress.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e09e740",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_r.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e0cb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = 'evaluation/Average Returns'\n",
    "# plt.plot(df_no_r['evaluation/num steps total'], df_no_r[name], label=\"No Relabeling\")\n",
    "# plt.plot(df_no_r['evaluation/num steps total'], df_rr[name], label=\"Random Relabeling\")\n",
    "# plt.plot(df_no_r['evaluation/num steps total'], df_air[name], label=\"AIR\")\n",
    "# plt.plot(df_no_r['evaluation/num steps total'], df_ar[name], label=\"AR\")\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17df89c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
