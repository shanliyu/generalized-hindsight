{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e892d7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doodad not detected\n",
      "objc[14809]: Class GLFWApplicationDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a235b3778) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a23634740). One of the two will be used. Which one is undefined.\n",
      "objc[14809]: Class GLFWWindowDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a235b3700) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a23634768). One of the two will be used. Which one is undefined.\n",
      "objc[14809]: Class GLFWContentView is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a235b37a0) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a236347b8). One of the two will be used. Which one is undefined.\n",
      "objc[14809]: Class GLFWWindow is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a235b3818) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a23634830). One of the two will be used. Which one is undefined.\n",
      "2021-05-25 09:48:29.478747 PDT | Variant:\n",
      "2021-05-25 09:48:29.479248 PDT | {\n",
      "  \"seed\": 10,\n",
      "  \"algorithm\": \"SAC\",\n",
      "  \"env_name\": \"halfcheetahhard\",\n",
      "  \"algo_kwargs\": {\n",
      "    \"batch_size\": 256,\n",
      "    \"num_epochs\": 10,\n",
      "    \"num_eval_steps_per_epoch\": 25000,\n",
      "    \"num_expl_steps_per_train_loop\": 1000,\n",
      "    \"num_trains_per_train_loop\": 100,\n",
      "    \"min_num_steps_before_training\": 1000,\n",
      "    \"max_path_length\": 1000,\n",
      "    \"num_train_loops_per_epoch\": 1\n",
      "  },\n",
      "  \"trainer_kwargs\": {\n",
      "    \"discount\": 0.99,\n",
      "    \"soft_target_tau\": 0.005,\n",
      "    \"target_update_period\": 1,\n",
      "    \"policy_lr\": 0.003,\n",
      "    \"qf_lr\": 0.003,\n",
      "    \"reward_scale\": 1,\n",
      "    \"use_automatic_entropy_tuning\": true\n",
      "  },\n",
      "  \"replay_buffer_kwargs\": {\n",
      "    \"max_replay_buffer_size\": 1000000,\n",
      "    \"latent_dim\": 4,\n",
      "    \"approx_irl\": false,\n",
      "    \"plot\": false\n",
      "  },\n",
      "  \"relabeler_kwargs\": {\n",
      "    \"relabel\": false,\n",
      "    \"use_adv\": false,\n",
      "    \"n_sampled_latents\": 5,\n",
      "    \"n_to_take\": 1,\n",
      "    \"cache\": false\n",
      "  },\n",
      "  \"qf_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      256,\n",
      "      256\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"policy_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      256,\n",
      "      256\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"path_collector_kwargs\": {\n",
      "    \"save_videos\": false\n",
      "  },\n",
      "  \"use_advantages\": false,\n",
      "  \"proper_advantages\": true,\n",
      "  \"plot\": false,\n",
      "  \"test\": false,\n",
      "  \"gpu\": 0,\n",
      "  \"mode\": \"here_no_doodad\",\n",
      "  \"local_docker\": false,\n",
      "  \"insert_time\": false,\n",
      "  \"latent_shape_multiplier\": 1\n",
      "}\n",
      "halfcheetah\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "2021-05-25 09:48:48.777790 PDT | [gher-halfcheetahhard-SAC-10e-1000s-disc0.99_2021_05_25_09_48_29_0000--s-10] Epoch 0 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  2000\n",
      "trainer/QF1 Loss                                      15.308\n",
      "trainer/QF2 Loss                                      15.2173\n",
      "trainer/Policy Loss                                   -4.03244\n",
      "trainer/Q1 Predictions Mean                           -0.00837021\n",
      "trainer/Q1 Predictions Std                             0.00539224\n",
      "trainer/Q1 Predictions Max                             0.00351438\n",
      "trainer/Q1 Predictions Min                            -0.0247585\n",
      "trainer/Q2 Predictions Mean                            0.00373413\n",
      "trainer/Q2 Predictions Std                             0.00425307\n",
      "trainer/Q2 Predictions Max                             0.0150093\n",
      "trainer/Q2 Predictions Min                            -0.00635403\n",
      "trainer/Q Targets Mean                                 3.6897\n",
      "trainer/Q Targets Std                                  1.27715\n",
      "trainer/Q Targets Max                                  7.91628\n",
      "trainer/Q Targets Min                                  0.538645\n",
      "trainer/Log Pis Mean                                  -4.04089\n",
      "trainer/Log Pis Std                                    0.507264\n",
      "trainer/Log Pis Max                                   -2.36068\n",
      "trainer/Log Pis Min                                   -5.48774\n",
      "trainer/Policy mu Mean                                -0.000890531\n",
      "trainer/Policy mu Std                                  0.00234536\n",
      "trainer/Policy mu Max                                  0.0068469\n",
      "trainer/Policy mu Min                                 -0.00728334\n",
      "trainer/Policy log std Mean                           -0.001294\n",
      "trainer/Policy log std Std                             0.00167964\n",
      "trainer/Policy log std Max                             0.00386344\n",
      "trainer/Policy log std Min                            -0.00808519\n",
      "trainer/Alpha                                          0.997005\n",
      "trainer/Alpha Loss                                    -0\n",
      "exploration/num steps total                         2000\n",
      "exploration/num paths total                            2\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.242739\n",
      "exploration/Rewards Std                                0.602001\n",
      "exploration/Rewards Max                                1.86132\n",
      "exploration/Rewards Min                               -1.97314\n",
      "exploration/Returns Mean                            -242.739\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -242.739\n",
      "exploration/Returns Min                             -242.739\n",
      "exploration/Actions Mean                               0.00115652\n",
      "exploration/Actions Std                                0.627109\n",
      "exploration/Actions Max                                0.999101\n",
      "exploration/Actions Min                               -0.999804\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -242.739\n",
      "exploration/env_infos/final/reward_run Mean            0.852688\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max             0.852688\n",
      "exploration/env_infos/final/reward_run Min             0.852688\n",
      "exploration/env_infos/initial/reward_run Mean         -0.428326\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max          -0.428326\n",
      "exploration/env_infos/initial/reward_run Min          -0.428326\n",
      "exploration/env_infos/reward_run Mean                 -0.144494\n",
      "exploration/env_infos/reward_run Std                   0.741901\n",
      "exploration/env_infos/reward_run Max                   2.56896\n",
      "exploration/env_infos/reward_run Min                  -2.15828\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.112538\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.112538\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.112538\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.144606\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.144606\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.144606\n",
      "exploration/env_infos/reward_ctrl Mean                -0.235961\n",
      "exploration/env_infos/reward_ctrl Std                  0.0750452\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0181969\n",
      "exploration/env_infos/reward_ctrl Min                 -0.476064\n",
      "exploration/env_infos/final/height Mean               -0.113253\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.113253\n",
      "exploration/env_infos/final/height Min                -0.113253\n",
      "exploration/env_infos/initial/height Mean             -0.0106174\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max              -0.0106174\n",
      "exploration/env_infos/initial/height Min              -0.0106174\n",
      "exploration/env_infos/height Mean                     -0.0677779\n",
      "exploration/env_infos/height Std                       0.090601\n",
      "exploration/env_infos/height Max                       0.28877\n",
      "exploration/env_infos/height Min                      -0.359537\n",
      "exploration/env_infos/final/reward_angular Mean       -0.673099\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max        -0.673099\n",
      "exploration/env_infos/final/reward_angular Min        -0.673099\n",
      "exploration/env_infos/initial/reward_angular Mean      0.243925\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       0.243925\n",
      "exploration/env_infos/initial/reward_angular Min       0.243925\n",
      "exploration/env_infos/reward_angular Mean             -0.0160058\n",
      "exploration/env_infos/reward_angular Std               1.68347\n",
      "exploration/env_infos/reward_angular Max               8.25407\n",
      "exploration/env_infos/reward_angular Min              -4.79874\n",
      "evaluation/num steps total                         25000\n",
      "evaluation/num paths total                            25\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.0616557\n",
      "evaluation/Rewards Std                                 0.0464541\n",
      "evaluation/Rewards Max                                 1.03703\n",
      "evaluation/Rewards Min                                -1.79084\n",
      "evaluation/Returns Mean                              -61.6557\n",
      "evaluation/Returns Std                                37.9567\n",
      "evaluation/Returns Max                                -1.74302\n",
      "evaluation/Returns Min                              -127.828\n",
      "evaluation/Actions Mean                               -0.000322867\n",
      "evaluation/Actions Std                                 0.00121833\n",
      "evaluation/Actions Max                                 0.00337062\n",
      "evaluation/Actions Min                                -0.00309706\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           -61.6557\n",
      "evaluation/env_infos/final/reward_run Mean            -1.38778e-18\n",
      "evaluation/env_infos/final/reward_run Std              1.31099e-16\n",
      "evaluation/env_infos/final/reward_run Max              3.46945e-16\n",
      "evaluation/env_infos/final/reward_run Min             -4.85723e-16\n",
      "evaluation/env_infos/initial/reward_run Mean          -0.00648149\n",
      "evaluation/env_infos/initial/reward_run Std            0.128515\n",
      "evaluation/env_infos/initial/reward_run Max            0.224342\n",
      "evaluation/env_infos/initial/reward_run Min           -0.209714\n",
      "evaluation/env_infos/reward_run Mean                  -0.000342552\n",
      "evaluation/env_infos/reward_run Std                    0.0159969\n",
      "evaluation/env_infos/reward_run Max                    0.331155\n",
      "evaluation/env_infos/reward_run Min                   -0.596655\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -9.52177e-07\n",
      "evaluation/env_infos/final/reward_ctrl Std             9.0322e-08\n",
      "evaluation/env_infos/final/reward_ctrl Max            -8.07404e-07\n",
      "evaluation/env_infos/final/reward_ctrl Min            -1.10654e-06\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -1.00818e-06\n",
      "evaluation/env_infos/initial/reward_ctrl Std           1.10186e-07\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -8.31595e-07\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -1.27073e-06\n",
      "evaluation/env_infos/reward_ctrl Mean                 -9.53144e-07\n",
      "evaluation/env_infos/reward_ctrl Std                   9.47071e-08\n",
      "evaluation/env_infos/reward_ctrl Max                  -5.85402e-07\n",
      "evaluation/env_infos/reward_ctrl Min                  -2.62191e-06\n",
      "evaluation/env_infos/final/height Mean                -0.132891\n",
      "evaluation/env_infos/final/height Std                  3.5865e-05\n",
      "evaluation/env_infos/final/height Max                 -0.132829\n",
      "evaluation/env_infos/final/height Min                 -0.132952\n",
      "evaluation/env_infos/initial/height Mean              -0.0148674\n",
      "evaluation/env_infos/initial/height Std                0.0511682\n",
      "evaluation/env_infos/initial/height Max                0.0698418\n",
      "evaluation/env_infos/initial/height Min               -0.0935499\n",
      "evaluation/env_infos/height Mean                      -0.132446\n",
      "evaluation/env_infos/height Std                        0.00548373\n",
      "evaluation/env_infos/height Max                        0.0698418\n",
      "evaluation/env_infos/height Min                       -0.151457\n",
      "evaluation/env_infos/final/reward_angular Mean        -2.47126e-16\n",
      "evaluation/env_infos/final/reward_angular Std          1.28174e-15\n",
      "evaluation/env_infos/final/reward_angular Max          2.03653e-15\n",
      "evaluation/env_infos/final/reward_angular Min         -2.78989e-15\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.0871386\n",
      "evaluation/env_infos/initial/reward_angular Std        0.35347\n",
      "evaluation/env_infos/initial/reward_angular Max        1.00049\n",
      "evaluation/env_infos/initial/reward_angular Min       -0.69915\n",
      "evaluation/env_infos/reward_angular Mean               0.000916425\n",
      "evaluation/env_infos/reward_angular Std                0.0405368\n",
      "evaluation/env_infos/reward_angular Max                2.00984\n",
      "evaluation/env_infos/reward_angular Min               -1.11431\n",
      "time/data storing (s)                                  0.0102982\n",
      "time/evaluation sampling (s)                          14.2598\n",
      "time/exploration sampling (s)                          0.747804\n",
      "time/logging (s)                                       0.160112\n",
      "time/saving (s)                                        0.119761\n",
      "time/training (s)                                      3.56843\n",
      "time/epoch (s)                                        18.8662\n",
      "time/total (s)                                        21.4794\n",
      "Epoch                                                  0\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 09:49:21.666399 PDT | [gher-halfcheetahhard-SAC-10e-1000s-disc0.99_2021_05_25_09_48_29_0000--s-10] Epoch 1 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  3000\n",
      "trainer/QF1 Loss                                       0.716878\n",
      "trainer/QF2 Loss                                       0.715369\n",
      "trainer/Policy Loss                                   -7.39095\n",
      "trainer/Q1 Predictions Mean                            3.36931\n",
      "trainer/Q1 Predictions Std                             0.821825\n",
      "trainer/Q1 Predictions Max                             6.99106\n",
      "trainer/Q1 Predictions Min                             1.67353\n",
      "trainer/Q2 Predictions Mean                            3.35358\n",
      "trainer/Q2 Predictions Std                             0.824303\n",
      "trainer/Q2 Predictions Max                             7.01771\n",
      "trainer/Q2 Predictions Min                             1.5576\n",
      "trainer/Q Targets Mean                                 3.31067\n",
      "trainer/Q Targets Std                                  0.885876\n",
      "trainer/Q Targets Max                                  6.95489\n",
      "trainer/Q Targets Min                                  0.131229\n",
      "trainer/Log Pis Mean                                  -4.04014\n",
      "trainer/Log Pis Std                                    0.505871\n",
      "trainer/Log Pis Max                                   -2.61216\n",
      "trainer/Log Pis Min                                   -7.05572\n",
      "trainer/Policy mu Mean                                -0.0549241\n",
      "trainer/Policy mu Std                                  0.124947\n",
      "trainer/Policy mu Max                                  0.116096\n",
      "trainer/Policy mu Min                                 -0.409603\n",
      "trainer/Policy log std Mean                           -0.123359\n",
      "trainer/Policy log std Std                             0.0235149\n",
      "trainer/Policy log std Max                            -0.0794437\n",
      "trainer/Policy log std Min                            -0.21424\n",
      "trainer/Alpha                                          0.738873\n",
      "trainer/Alpha Loss                                    -3.00836\n",
      "exploration/num steps total                         3000\n",
      "exploration/num paths total                            3\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.179404\n",
      "exploration/Rewards Std                                0.400273\n",
      "exploration/Rewards Max                                0.988786\n",
      "exploration/Rewards Min                               -1.65963\n",
      "exploration/Returns Mean                            -179.404\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -179.404\n",
      "exploration/Returns Min                             -179.404\n",
      "exploration/Actions Mean                              -0.042028\n",
      "exploration/Actions Std                                0.596068\n",
      "exploration/Actions Max                                0.995805\n",
      "exploration/Actions Min                               -0.998916\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -179.404\n",
      "exploration/env_infos/final/reward_run Mean           -0.0988751\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max            -0.0988751\n",
      "exploration/env_infos/final/reward_run Min            -0.0988751\n",
      "exploration/env_infos/initial/reward_run Mean         -0.237643\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max          -0.237643\n",
      "exploration/env_infos/initial/reward_run Min          -0.237643\n",
      "exploration/env_infos/reward_run Mean                  0.00431036\n",
      "exploration/env_infos/reward_run Std                   0.593761\n",
      "exploration/env_infos/reward_run Max                   1.9975\n",
      "exploration/env_infos/reward_run Min                  -2.25639\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.20893\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.20893\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.20893\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.221924\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.221924\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.221924\n",
      "exploration/env_infos/reward_ctrl Mean                -0.214238\n",
      "exploration/env_infos/reward_ctrl Std                  0.0708193\n",
      "exploration/env_infos/reward_ctrl Max                 -0.025795\n",
      "exploration/env_infos/reward_ctrl Min                 -0.488112\n",
      "exploration/env_infos/final/height Mean               -0.539993\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.539993\n",
      "exploration/env_infos/final/height Min                -0.539993\n",
      "exploration/env_infos/initial/height Mean              0.059951\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max               0.059951\n",
      "exploration/env_infos/initial/height Min               0.059951\n",
      "exploration/env_infos/height Mean                     -0.376495\n",
      "exploration/env_infos/height Std                       0.249335\n",
      "exploration/env_infos/height Max                       0.20668\n",
      "exploration/env_infos/height Min                      -0.582894\n",
      "exploration/env_infos/final/reward_angular Mean        1.41597\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max         1.41597\n",
      "exploration/env_infos/final/reward_angular Min         1.41597\n",
      "exploration/env_infos/initial/reward_angular Mean      0.728871\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       0.728871\n",
      "exploration/env_infos/initial/reward_angular Min       0.728871\n",
      "exploration/env_infos/reward_angular Mean              0.0618126\n",
      "exploration/env_infos/reward_angular Std               1.3217\n",
      "exploration/env_infos/reward_angular Max               5.73625\n",
      "exploration/env_infos/reward_angular Min              -5.32131\n",
      "evaluation/num steps total                         50000\n",
      "evaluation/num paths total                            50\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.0577114\n",
      "evaluation/Rewards Std                                 0.0428513\n",
      "evaluation/Rewards Max                                 1.62833\n",
      "evaluation/Rewards Min                                -1.175\n",
      "evaluation/Returns Mean                              -57.7114\n",
      "evaluation/Returns Std                                34.2455\n",
      "evaluation/Returns Max                                -2.66717\n",
      "evaluation/Returns Min                              -115.229\n",
      "evaluation/Actions Mean                               -0.0428304\n",
      "evaluation/Actions Std                                 0.0954994\n",
      "evaluation/Actions Max                                 0.0636395\n",
      "evaluation/Actions Min                                -0.264937\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           -57.7114\n",
      "evaluation/env_infos/final/reward_run Mean            -3.62743e-10\n",
      "evaluation/env_infos/final/reward_run Std              1.25056e-09\n",
      "evaluation/env_infos/final/reward_run Max              1.38778e-16\n",
      "evaluation/env_infos/final/reward_run Min             -5.33045e-09\n",
      "evaluation/env_infos/initial/reward_run Mean           0.0227596\n",
      "evaluation/env_infos/initial/reward_run Std            0.133401\n",
      "evaluation/env_infos/initial/reward_run Max            0.316651\n",
      "evaluation/env_infos/initial/reward_run Min           -0.322676\n",
      "evaluation/env_infos/reward_run Mean                  -0.000231162\n",
      "evaluation/env_infos/reward_run Std                    0.0154693\n",
      "evaluation/env_infos/reward_run Max                    0.324633\n",
      "evaluation/env_infos/reward_run Min                   -0.392668\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.00657406\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.00123094\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.00392907\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.00802268\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.00624336\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.00123292\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.00364606\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.00765538\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.00657275\n",
      "evaluation/env_infos/reward_ctrl Std                   0.00123213\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0028219\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.00866068\n",
      "evaluation/env_infos/final/height Mean                -0.119925\n",
      "evaluation/env_infos/final/height Std                  0.0011577\n",
      "evaluation/env_infos/final/height Max                 -0.118665\n",
      "evaluation/env_infos/final/height Min                 -0.122642\n",
      "evaluation/env_infos/initial/height Mean              -0.00357963\n",
      "evaluation/env_infos/initial/height Std                0.0557488\n",
      "evaluation/env_infos/initial/height Max                0.0874939\n",
      "evaluation/env_infos/initial/height Min               -0.0852113\n",
      "evaluation/env_infos/height Mean                      -0.119482\n",
      "evaluation/env_infos/height Std                        0.00563925\n",
      "evaluation/env_infos/height Max                        0.0874939\n",
      "evaluation/env_infos/height Min                       -0.130895\n",
      "evaluation/env_infos/final/reward_angular Mean        -1.92928e-09\n",
      "evaluation/env_infos/final/reward_angular Std          7.80218e-09\n",
      "evaluation/env_infos/final/reward_angular Max          3.59114e-15\n",
      "evaluation/env_infos/final/reward_angular Min         -3.9145e-08\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.34925\n",
      "evaluation/env_infos/initial/reward_angular Std        0.380702\n",
      "evaluation/env_infos/initial/reward_angular Max        1.52389\n",
      "evaluation/env_infos/initial/reward_angular Min       -0.565741\n",
      "evaluation/env_infos/reward_angular Mean               0.00144815\n",
      "evaluation/env_infos/reward_angular Std                0.0434023\n",
      "evaluation/env_infos/reward_angular Max                2.06922\n",
      "evaluation/env_infos/reward_angular Min               -0.748635\n",
      "time/data storing (s)                                  0.0164586\n",
      "time/evaluation sampling (s)                          26.9206\n",
      "time/exploration sampling (s)                          1.16573\n",
      "time/logging (s)                                       0.251287\n",
      "time/saving (s)                                        0.0346648\n",
      "time/training (s)                                      4.37341\n",
      "time/epoch (s)                                        32.7621\n",
      "time/total (s)                                        54.4545\n",
      "Epoch                                                  1\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 09:49:51.109459 PDT | [gher-halfcheetahhard-SAC-10e-1000s-disc0.99_2021_05_25_09_48_29_0000--s-10] Epoch 2 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  4000\n",
      "trainer/QF1 Loss                                       0.628485\n",
      "trainer/QF2 Loss                                       0.646088\n",
      "trainer/Policy Loss                                   -7.02914\n",
      "trainer/Q1 Predictions Mean                            2.99543\n",
      "trainer/Q1 Predictions Std                             0.585404\n",
      "trainer/Q1 Predictions Max                             5.79048\n",
      "trainer/Q1 Predictions Min                             1.65658\n",
      "trainer/Q2 Predictions Mean                            2.98037\n",
      "trainer/Q2 Predictions Std                             0.590456\n",
      "trainer/Q2 Predictions Max                             5.72335\n",
      "trainer/Q2 Predictions Min                             1.55598\n",
      "trainer/Q Targets Mean                                 3.19934\n",
      "trainer/Q Targets Std                                  1.02422\n",
      "trainer/Q Targets Max                                  7.15342\n",
      "trainer/Q Targets Min                                 -0.106277\n",
      "trainer/Log Pis Mean                                  -4.02866\n",
      "trainer/Log Pis Std                                    0.436922\n",
      "trainer/Log Pis Max                                   -2.74928\n",
      "trainer/Log Pis Min                                   -5.57368\n",
      "trainer/Policy mu Mean                                -0.0371961\n",
      "trainer/Policy mu Std                                  0.104126\n",
      "trainer/Policy mu Max                                  0.174134\n",
      "trainer/Policy mu Min                                 -0.417336\n",
      "trainer/Policy log std Mean                           -0.108691\n",
      "trainer/Policy log std Std                             0.0189877\n",
      "trainer/Policy log std Max                            -0.0628312\n",
      "trainer/Policy log std Min                            -0.19865\n",
      "trainer/Alpha                                          0.547266\n",
      "trainer/Alpha Loss                                    -6.01543\n",
      "exploration/num steps total                         4000\n",
      "exploration/num paths total                            4\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.105386\n",
      "exploration/Rewards Std                                0.651082\n",
      "exploration/Rewards Max                                1.86988\n",
      "exploration/Rewards Min                               -2.73208\n",
      "exploration/Returns Mean                            -105.386\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -105.386\n",
      "exploration/Returns Min                             -105.386\n",
      "exploration/Actions Mean                              -0.0353178\n",
      "exploration/Actions Std                                0.598077\n",
      "exploration/Actions Max                                0.997966\n",
      "exploration/Actions Min                               -0.997035\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -105.386\n",
      "exploration/env_infos/final/reward_run Mean           -0.565021\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max            -0.565021\n",
      "exploration/env_infos/final/reward_run Min            -0.565021\n",
      "exploration/env_infos/initial/reward_run Mean          0.109238\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max           0.109238\n",
      "exploration/env_infos/initial/reward_run Min           0.109238\n",
      "exploration/env_infos/reward_run Mean                 -0.0185825\n",
      "exploration/env_infos/reward_run Std                   0.629628\n",
      "exploration/env_infos/reward_run Max                   1.92974\n",
      "exploration/env_infos/reward_run Min                  -2.52668\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.163203\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.163203\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.163203\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.186445\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.186445\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.186445\n",
      "exploration/env_infos/reward_ctrl Mean                -0.215366\n",
      "exploration/env_infos/reward_ctrl Std                  0.0746918\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0240411\n",
      "exploration/env_infos/reward_ctrl Min                 -0.428745\n",
      "exploration/env_infos/final/height Mean               -0.0314477\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.0314477\n",
      "exploration/env_infos/final/height Min                -0.0314477\n",
      "exploration/env_infos/initial/height Mean             -0.00889424\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max              -0.00889424\n",
      "exploration/env_infos/initial/height Min              -0.00889424\n",
      "exploration/env_infos/height Mean                     -0.0781313\n",
      "exploration/env_infos/height Std                       0.0761144\n",
      "exploration/env_infos/height Max                       0.15484\n",
      "exploration/env_infos/height Min                      -0.308\n",
      "exploration/env_infos/final/reward_angular Mean        0.545194\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max         0.545194\n",
      "exploration/env_infos/final/reward_angular Min         0.545194\n",
      "exploration/env_infos/initial/reward_angular Mean      0.760772\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       0.760772\n",
      "exploration/env_infos/initial/reward_angular Min       0.760772\n",
      "exploration/env_infos/reward_angular Mean             -0.0108224\n",
      "exploration/env_infos/reward_angular Std               1.65618\n",
      "exploration/env_infos/reward_angular Max               5.39993\n",
      "exploration/env_infos/reward_angular Min              -5.43889\n",
      "evaluation/num steps total                         75000\n",
      "evaluation/num paths total                            75\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.060113\n",
      "evaluation/Rewards Std                                 0.0470167\n",
      "evaluation/Rewards Max                                 1.47429\n",
      "evaluation/Rewards Min                                -1.62088\n",
      "evaluation/Returns Mean                              -60.113\n",
      "evaluation/Returns Std                                35.8859\n",
      "evaluation/Returns Max                                -1.8989\n",
      "evaluation/Returns Min                              -119.99\n",
      "evaluation/Actions Mean                               -0.0338328\n",
      "evaluation/Actions Std                                 0.0894766\n",
      "evaluation/Actions Max                                 0.120604\n",
      "evaluation/Actions Min                                -0.232423\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           -60.113\n",
      "evaluation/env_infos/final/reward_run Mean            -3.25914e-10\n",
      "evaluation/env_infos/final/reward_run Std              1.05633e-09\n",
      "evaluation/env_infos/final/reward_run Max              2.77556e-16\n",
      "evaluation/env_infos/final/reward_run Min             -4.1994e-09\n",
      "evaluation/env_infos/initial/reward_run Mean           0.0430652\n",
      "evaluation/env_infos/initial/reward_run Std            0.120804\n",
      "evaluation/env_infos/initial/reward_run Max            0.363721\n",
      "evaluation/env_infos/initial/reward_run Min           -0.199467\n",
      "evaluation/env_infos/reward_run Mean                  -0.000150933\n",
      "evaluation/env_infos/reward_run Std                    0.0147823\n",
      "evaluation/env_infos/reward_run Max                    0.363721\n",
      "evaluation/env_infos/reward_run Min                   -0.340951\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.00548897\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.00180283\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0025408\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.00789672\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.00592181\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0018866\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0026943\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.00848925\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.00549043\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0018027\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00250418\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.0100448\n",
      "evaluation/env_infos/final/height Mean                -0.125501\n",
      "evaluation/env_infos/final/height Std                  0.000633597\n",
      "evaluation/env_infos/final/height Max                 -0.124702\n",
      "evaluation/env_infos/final/height Min                 -0.126672\n",
      "evaluation/env_infos/initial/height Mean              -0.00855331\n",
      "evaluation/env_infos/initial/height Std                0.0597939\n",
      "evaluation/env_infos/initial/height Max                0.0799865\n",
      "evaluation/env_infos/initial/height Min               -0.0902987\n",
      "evaluation/env_infos/height Mean                      -0.125038\n",
      "evaluation/env_infos/height Std                        0.00578381\n",
      "evaluation/env_infos/height Max                        0.0799865\n",
      "evaluation/env_infos/height Min                       -0.140439\n",
      "evaluation/env_infos/final/reward_angular Mean        -3.86809e-09\n",
      "evaluation/env_infos/final/reward_angular Std          1.31581e-08\n",
      "evaluation/env_infos/final/reward_angular Max          3.70544e-15\n",
      "evaluation/env_infos/final/reward_angular Min         -5.57129e-08\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.35878\n",
      "evaluation/env_infos/initial/reward_angular Std        0.401155\n",
      "evaluation/env_infos/initial/reward_angular Max        1.58207\n",
      "evaluation/env_infos/initial/reward_angular Min       -0.307276\n",
      "evaluation/env_infos/reward_angular Mean               0.00191158\n",
      "evaluation/env_infos/reward_angular Std                0.0512515\n",
      "evaluation/env_infos/reward_angular Max                2.0699\n",
      "evaluation/env_infos/reward_angular Min               -0.588602\n",
      "time/data storing (s)                                  0.0154471\n",
      "time/evaluation sampling (s)                          24.1673\n",
      "time/exploration sampling (s)                          1.08191\n",
      "time/logging (s)                                       0.242812\n",
      "time/saving (s)                                        0.027651\n",
      "time/training (s)                                      3.75722\n",
      "time/epoch (s)                                        29.2923\n",
      "time/total (s)                                        83.8878\n",
      "Epoch                                                  2\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 09:50:18.610592 PDT | [gher-halfcheetahhard-SAC-10e-1000s-disc0.99_2021_05_25_09_48_29_0000--s-10] Epoch 3 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                   5000\n",
      "trainer/QF1 Loss                                        0.671078\n",
      "trainer/QF2 Loss                                        0.683196\n",
      "trainer/Policy Loss                                    -6.95372\n",
      "trainer/Q1 Predictions Mean                             2.96541\n",
      "trainer/Q1 Predictions Std                              0.834434\n",
      "trainer/Q1 Predictions Max                              5.76822\n",
      "trainer/Q1 Predictions Min                              0.989266\n",
      "trainer/Q2 Predictions Mean                             2.9246\n",
      "trainer/Q2 Predictions Std                              0.777573\n",
      "trainer/Q2 Predictions Max                              5.85219\n",
      "trainer/Q2 Predictions Min                              1.18994\n",
      "trainer/Q Targets Mean                                  3.10784\n",
      "trainer/Q Targets Std                                   0.925289\n",
      "trainer/Q Targets Max                                   5.99127\n",
      "trainer/Q Targets Min                                  -0.813155\n",
      "trainer/Log Pis Mean                                   -3.95394\n",
      "trainer/Log Pis Std                                     0.688271\n",
      "trainer/Log Pis Max                                    -1.76871\n",
      "trainer/Log Pis Min                                    -5.86559\n",
      "trainer/Policy mu Mean                                  0.0303971\n",
      "trainer/Policy mu Std                                   0.226234\n",
      "trainer/Policy mu Max                                   0.763774\n",
      "trainer/Policy mu Min                                  -0.951897\n",
      "trainer/Policy log std Mean                            -0.0959697\n",
      "trainer/Policy log std Std                              0.0333657\n",
      "trainer/Policy log std Max                              0.010913\n",
      "trainer/Policy log std Min                             -0.208743\n",
      "trainer/Alpha                                           0.405783\n",
      "trainer/Alpha Loss                                     -8.94822\n",
      "exploration/num steps total                          5000\n",
      "exploration/num paths total                             5\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.0742532\n",
      "exploration/Rewards Std                                 0.520053\n",
      "exploration/Rewards Max                                 1.72666\n",
      "exploration/Rewards Min                                -1.69894\n",
      "exploration/Returns Mean                              -74.2532\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               -74.2532\n",
      "exploration/Returns Min                               -74.2532\n",
      "exploration/Actions Mean                                0.0404318\n",
      "exploration/Actions Std                                 0.602795\n",
      "exploration/Actions Max                                 0.99962\n",
      "exploration/Actions Min                                -0.997809\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           -74.2532\n",
      "exploration/env_infos/final/reward_run Mean            -0.401397\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.401397\n",
      "exploration/env_infos/final/reward_run Min             -0.401397\n",
      "exploration/env_infos/initial/reward_run Mean           0.118905\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.118905\n",
      "exploration/env_infos/initial/reward_run Min            0.118905\n",
      "exploration/env_infos/reward_run Mean                  -0.0342577\n",
      "exploration/env_infos/reward_run Std                    0.672666\n",
      "exploration/env_infos/reward_run Max                    2.0671\n",
      "exploration/env_infos/reward_run Min                   -2.15234\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.185427\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.185427\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.185427\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.197149\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.197149\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.197149\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.218998\n",
      "exploration/env_infos/reward_ctrl Std                   0.0703604\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0244489\n",
      "exploration/env_infos/reward_ctrl Min                  -0.471586\n",
      "exploration/env_infos/final/height Mean                -0.165566\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.165566\n",
      "exploration/env_infos/final/height Min                 -0.165566\n",
      "exploration/env_infos/initial/height Mean              -0.0265399\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0265399\n",
      "exploration/env_infos/initial/height Min               -0.0265399\n",
      "exploration/env_infos/height Mean                      -0.0727836\n",
      "exploration/env_infos/height Std                        0.0910712\n",
      "exploration/env_infos/height Max                        0.251457\n",
      "exploration/env_infos/height Min                       -0.338026\n",
      "exploration/env_infos/final/reward_angular Mean         0.29055\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.29055\n",
      "exploration/env_infos/final/reward_angular Min          0.29055\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.347146\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.347146\n",
      "exploration/env_infos/initial/reward_angular Min       -0.347146\n",
      "exploration/env_infos/reward_angular Mean              -0.0109864\n",
      "exploration/env_infos/reward_angular Std                1.72942\n",
      "exploration/env_infos/reward_angular Max                5.16218\n",
      "exploration/env_infos/reward_angular Min               -5.49761\n",
      "evaluation/num steps total                         100000\n",
      "evaluation/num paths total                            100\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.0658545\n",
      "evaluation/Rewards Std                                  0.0467575\n",
      "evaluation/Rewards Max                                  1.234\n",
      "evaluation/Rewards Min                                 -1.39027\n",
      "evaluation/Returns Mean                               -65.8545\n",
      "evaluation/Returns Std                                 33.4986\n",
      "evaluation/Returns Max                                 -5.06069\n",
      "evaluation/Returns Min                               -123.527\n",
      "evaluation/Actions Mean                                 0.0393426\n",
      "evaluation/Actions Std                                  0.196194\n",
      "evaluation/Actions Max                                  0.455392\n",
      "evaluation/Actions Min                                 -0.570075\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            -65.8545\n",
      "evaluation/env_infos/final/reward_run Mean              5.162e-09\n",
      "evaluation/env_infos/final/reward_run Std               1.76333e-08\n",
      "evaluation/env_infos/final/reward_run Max               7.80947e-08\n",
      "evaluation/env_infos/final/reward_run Min              -1.75163e-08\n",
      "evaluation/env_infos/initial/reward_run Mean            0.234971\n",
      "evaluation/env_infos/initial/reward_run Std             0.139012\n",
      "evaluation/env_infos/initial/reward_run Max             0.476572\n",
      "evaluation/env_infos/initial/reward_run Min            -0.0167746\n",
      "evaluation/env_infos/reward_run Mean                   -0.00038533\n",
      "evaluation/env_infos/reward_run Std                     0.0232988\n",
      "evaluation/env_infos/reward_run Max                     0.6351\n",
      "evaluation/env_infos/reward_run Min                    -0.527434\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0240146\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00938351\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00917052\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0379687\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0333956\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0123787\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0130577\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0534111\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0240239\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00940907\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00505427\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.0596606\n",
      "evaluation/env_infos/final/height Mean                 -0.126515\n",
      "evaluation/env_infos/final/height Std                   0.00882562\n",
      "evaluation/env_infos/final/height Max                  -0.11605\n",
      "evaluation/env_infos/final/height Min                  -0.140163\n",
      "evaluation/env_infos/initial/height Mean               -0.0151513\n",
      "evaluation/env_infos/initial/height Std                 0.0533498\n",
      "evaluation/env_infos/initial/height Max                 0.0657445\n",
      "evaluation/env_infos/initial/height Min                -0.0993215\n",
      "evaluation/env_infos/height Mean                       -0.126078\n",
      "evaluation/env_infos/height Std                         0.0102647\n",
      "evaluation/env_infos/height Max                         0.0657445\n",
      "evaluation/env_infos/height Min                        -0.164496\n",
      "evaluation/env_infos/final/reward_angular Mean         -1.21999e-08\n",
      "evaluation/env_infos/final/reward_angular Std           5.84149e-08\n",
      "evaluation/env_infos/final/reward_angular Max           1.15282e-07\n",
      "evaluation/env_infos/final/reward_angular Min          -1.78484e-07\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.0518434\n",
      "evaluation/env_infos/initial/reward_angular Std         0.76838\n",
      "evaluation/env_infos/initial/reward_angular Max         1.44269\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.0863\n",
      "evaluation/env_infos/reward_angular Mean                0.000715139\n",
      "evaluation/env_infos/reward_angular Std                 0.0572684\n",
      "evaluation/env_infos/reward_angular Max                 2.91354\n",
      "evaluation/env_infos/reward_angular Min                -1.34701\n",
      "time/data storing (s)                                   0.0150814\n",
      "time/evaluation sampling (s)                           22.6648\n",
      "time/exploration sampling (s)                           1.04588\n",
      "time/logging (s)                                        0.229053\n",
      "time/saving (s)                                         0.0273323\n",
      "time/training (s)                                       3.37297\n",
      "time/epoch (s)                                         27.3551\n",
      "time/total (s)                                        111.374\n",
      "Epoch                                                   3\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 09:50:45.668453 PDT | [gher-halfcheetahhard-SAC-10e-1000s-disc0.99_2021_05_25_09_48_29_0000--s-10] Epoch 4 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                   6000\n",
      "trainer/QF1 Loss                                        0.385456\n",
      "trainer/QF2 Loss                                        0.393143\n",
      "trainer/Policy Loss                                    -6.78767\n",
      "trainer/Q1 Predictions Mean                             2.88639\n",
      "trainer/Q1 Predictions Std                              0.70475\n",
      "trainer/Q1 Predictions Max                              5.6191\n",
      "trainer/Q1 Predictions Min                              1.42525\n",
      "trainer/Q2 Predictions Mean                             2.88131\n",
      "trainer/Q2 Predictions Std                              0.690321\n",
      "trainer/Q2 Predictions Max                              5.76134\n",
      "trainer/Q2 Predictions Min                              1.45344\n",
      "trainer/Q Targets Mean                                  2.9628\n",
      "trainer/Q Targets Std                                   0.942532\n",
      "trainer/Q Targets Max                                   6.05903\n",
      "trainer/Q Targets Min                                  -0.123184\n",
      "trainer/Log Pis Mean                                   -3.78932\n",
      "trainer/Log Pis Std                                     0.874073\n",
      "trainer/Log Pis Max                                    -0.352144\n",
      "trainer/Log Pis Min                                    -7.11505\n",
      "trainer/Policy mu Mean                                  0.0126846\n",
      "trainer/Policy mu Std                                   0.247001\n",
      "trainer/Policy mu Max                                   0.52275\n",
      "trainer/Policy mu Min                                  -1.14558\n",
      "trainer/Policy log std Mean                            -0.102518\n",
      "trainer/Policy log std Std                              0.04226\n",
      "trainer/Policy log std Max                             -0.0114533\n",
      "trainer/Policy log std Min                             -0.362191\n",
      "trainer/Alpha                                           0.302273\n",
      "trainer/Alpha Loss                                    -11.6834\n",
      "exploration/num steps total                          6000\n",
      "exploration/num paths total                             6\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.212309\n",
      "exploration/Rewards Std                                 0.765662\n",
      "exploration/Rewards Max                                 2.07652\n",
      "exploration/Rewards Min                                -2.41979\n",
      "exploration/Returns Mean                             -212.309\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -212.309\n",
      "exploration/Returns Min                              -212.309\n",
      "exploration/Actions Mean                               -0.00817141\n",
      "exploration/Actions Std                                 0.600258\n",
      "exploration/Actions Max                                 0.996551\n",
      "exploration/Actions Min                                -0.995967\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -212.309\n",
      "exploration/env_infos/final/reward_run Mean             0.626427\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.626427\n",
      "exploration/env_infos/final/reward_run Min              0.626427\n",
      "exploration/env_infos/initial/reward_run Mean           0.622065\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.622065\n",
      "exploration/env_infos/initial/reward_run Min            0.622065\n",
      "exploration/env_infos/reward_run Mean                  -0.134635\n",
      "exploration/env_infos/reward_run Std                    0.707544\n",
      "exploration/env_infos/reward_run Max                    1.64965\n",
      "exploration/env_infos/reward_run Min                   -2.49962\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.277064\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.277064\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.277064\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.214479\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.214479\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.214479\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.216226\n",
      "exploration/env_infos/reward_ctrl Std                   0.0733853\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0278889\n",
      "exploration/env_infos/reward_ctrl Min                  -0.511025\n",
      "exploration/env_infos/final/height Mean                -0.267516\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.267516\n",
      "exploration/env_infos/final/height Min                 -0.267516\n",
      "exploration/env_infos/initial/height Mean              -0.0617833\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0617833\n",
      "exploration/env_infos/initial/height Min               -0.0617833\n",
      "exploration/env_infos/height Mean                      -0.0683273\n",
      "exploration/env_infos/height Std                        0.0834166\n",
      "exploration/env_infos/height Max                        0.246211\n",
      "exploration/env_infos/height Min                       -0.353116\n",
      "exploration/env_infos/final/reward_angular Mean         2.10756\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          2.10756\n",
      "exploration/env_infos/final/reward_angular Min          2.10756\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.20711\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.20711\n",
      "exploration/env_infos/initial/reward_angular Min       -1.20711\n",
      "exploration/env_infos/reward_angular Mean              -0.0234251\n",
      "exploration/env_infos/reward_angular Std                1.71937\n",
      "exploration/env_infos/reward_angular Max                5.71013\n",
      "exploration/env_infos/reward_angular Min               -4.73031\n",
      "evaluation/num steps total                         125000\n",
      "evaluation/num paths total                            125\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.0716465\n",
      "evaluation/Rewards Std                                  0.0518966\n",
      "evaluation/Rewards Max                                  2.30964\n",
      "evaluation/Rewards Min                                 -1.66962\n",
      "evaluation/Returns Mean                               -71.6465\n",
      "evaluation/Returns Std                                 34.1158\n",
      "evaluation/Returns Max                                 -4.96637\n",
      "evaluation/Returns Min                               -133.756\n",
      "evaluation/Actions Mean                                 0.0475803\n",
      "evaluation/Actions Std                                  0.228039\n",
      "evaluation/Actions Max                                  0.411667\n",
      "evaluation/Actions Min                                 -0.742044\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            -71.6465\n",
      "evaluation/env_infos/final/reward_run Mean              3.43173e-09\n",
      "evaluation/env_infos/final/reward_run Std               3.03084e-08\n",
      "evaluation/env_infos/final/reward_run Max               8.33724e-08\n",
      "evaluation/env_infos/final/reward_run Min              -6.30587e-08\n",
      "evaluation/env_infos/initial/reward_run Mean            0.198217\n",
      "evaluation/env_infos/initial/reward_run Std             0.223948\n",
      "evaluation/env_infos/initial/reward_run Max             0.531368\n",
      "evaluation/env_infos/initial/reward_run Min            -0.329508\n",
      "evaluation/env_infos/reward_run Mean                   -0.000224088\n",
      "evaluation/env_infos/reward_run Std                     0.0206426\n",
      "evaluation/env_infos/reward_run Max                     0.572663\n",
      "evaluation/env_infos/reward_run Min                    -0.409088\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0325614\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0218491\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00370944\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0833273\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0467012\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0307645\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00440451\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.117166\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0325593\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0218711\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00164071\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.117166\n",
      "evaluation/env_infos/final/height Mean                 -0.129629\n",
      "evaluation/env_infos/final/height Std                   0.0100412\n",
      "evaluation/env_infos/final/height Max                  -0.114863\n",
      "evaluation/env_infos/final/height Min                  -0.140776\n",
      "evaluation/env_infos/initial/height Mean               -0.0156769\n",
      "evaluation/env_infos/initial/height Std                 0.0578368\n",
      "evaluation/env_infos/initial/height Max                 0.0668278\n",
      "evaluation/env_infos/initial/height Min                -0.0966411\n",
      "evaluation/env_infos/height Mean                       -0.129227\n",
      "evaluation/env_infos/height Std                         0.011357\n",
      "evaluation/env_infos/height Max                         0.0668278\n",
      "evaluation/env_infos/height Min                        -0.152595\n",
      "evaluation/env_infos/final/reward_angular Mean          2.20033e-08\n",
      "evaluation/env_infos/final/reward_angular Std           8.75571e-08\n",
      "evaluation/env_infos/final/reward_angular Max           3.19621e-07\n",
      "evaluation/env_infos/final/reward_angular Min          -1.28905e-07\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.015457\n",
      "evaluation/env_infos/initial/reward_angular Std         0.962761\n",
      "evaluation/env_infos/initial/reward_angular Max         2.67569\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.36756\n",
      "evaluation/env_infos/reward_angular Mean                0.00084353\n",
      "evaluation/env_infos/reward_angular Std                 0.0589676\n",
      "evaluation/env_infos/reward_angular Max                 2.67569\n",
      "evaluation/env_infos/reward_angular Min                -1.36756\n",
      "time/data storing (s)                                   0.0161052\n",
      "time/evaluation sampling (s)                           22.0808\n",
      "time/exploration sampling (s)                           1.08937\n",
      "time/logging (s)                                        0.239257\n",
      "time/saving (s)                                         0.0279876\n",
      "time/training (s)                                       3.48359\n",
      "time/epoch (s)                                         26.9371\n",
      "time/total (s)                                        138.442\n",
      "Epoch                                                   4\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 09:51:13.153040 PDT | [gher-halfcheetahhard-SAC-10e-1000s-disc0.99_2021_05_25_09_48_29_0000--s-10] Epoch 5 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                   7000\n",
      "trainer/QF1 Loss                                        0.425588\n",
      "trainer/QF2 Loss                                        0.366466\n",
      "trainer/Policy Loss                                    -6.97916\n",
      "trainer/Q1 Predictions Mean                             3.34195\n",
      "trainer/Q1 Predictions Std                              0.952477\n",
      "trainer/Q1 Predictions Max                              8.0748\n",
      "trainer/Q1 Predictions Min                              1.17839\n",
      "trainer/Q2 Predictions Mean                             3.29428\n",
      "trainer/Q2 Predictions Std                              0.948743\n",
      "trainer/Q2 Predictions Max                              8.12023\n",
      "trainer/Q2 Predictions Min                              1.25744\n",
      "trainer/Q Targets Mean                                  3.23236\n",
      "trainer/Q Targets Std                                   1.14129\n",
      "trainer/Q Targets Max                                   7.78724\n",
      "trainer/Q Targets Min                                   0.119522\n",
      "trainer/Log Pis Mean                                   -3.48829\n",
      "trainer/Log Pis Std                                     1.28913\n",
      "trainer/Log Pis Max                                     0.243087\n",
      "trainer/Log Pis Min                                   -10.9918\n",
      "trainer/Policy mu Mean                                  0.111512\n",
      "trainer/Policy mu Std                                   0.387748\n",
      "trainer/Policy mu Max                                   1.20538\n",
      "trainer/Policy mu Min                                  -1.2481\n",
      "trainer/Policy log std Mean                            -0.146073\n",
      "trainer/Policy log std Std                              0.0417686\n",
      "trainer/Policy log std Max                             -0.00548135\n",
      "trainer/Policy log std Min                             -0.364147\n",
      "trainer/Alpha                                           0.226114\n",
      "trainer/Alpha Loss                                    -14.0791\n",
      "exploration/num steps total                          7000\n",
      "exploration/num paths total                             7\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.136915\n",
      "exploration/Rewards Std                                 0.956433\n",
      "exploration/Rewards Max                                 3.25356\n",
      "exploration/Rewards Min                                -3.41189\n",
      "exploration/Returns Mean                             -136.915\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -136.915\n",
      "exploration/Returns Min                              -136.915\n",
      "exploration/Actions Mean                                0.147399\n",
      "exploration/Actions Std                                 0.58611\n",
      "exploration/Actions Max                                 0.997041\n",
      "exploration/Actions Min                                -0.998226\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -136.915\n",
      "exploration/env_infos/final/reward_run Mean             1.16227\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              1.16227\n",
      "exploration/env_infos/final/reward_run Min              1.16227\n",
      "exploration/env_infos/initial/reward_run Mean           0.293609\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.293609\n",
      "exploration/env_infos/initial/reward_run Min            0.293609\n",
      "exploration/env_infos/reward_run Mean                   0.00577176\n",
      "exploration/env_infos/reward_run Std                    0.731139\n",
      "exploration/env_infos/reward_run Max                    2.16127\n",
      "exploration/env_infos/reward_run Min                   -2.24386\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.33817\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.33817\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.33817\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.147861\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.147861\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.147861\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.219151\n",
      "exploration/env_infos/reward_ctrl Std                   0.0735075\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0518305\n",
      "exploration/env_infos/reward_ctrl Min                  -0.476198\n",
      "exploration/env_infos/final/height Mean                -0.165953\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.165953\n",
      "exploration/env_infos/final/height Min                 -0.165953\n",
      "exploration/env_infos/initial/height Mean              -0.0197044\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0197044\n",
      "exploration/env_infos/initial/height Min               -0.0197044\n",
      "exploration/env_infos/height Mean                      -0.0826236\n",
      "exploration/env_infos/height Std                        0.0809437\n",
      "exploration/env_infos/height Max                        0.17918\n",
      "exploration/env_infos/height Min                       -0.346517\n",
      "exploration/env_infos/final/reward_angular Mean        -1.47622\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -1.47622\n",
      "exploration/env_infos/final/reward_angular Min         -1.47622\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.296345\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.296345\n",
      "exploration/env_infos/initial/reward_angular Min       -0.296345\n",
      "exploration/env_infos/reward_angular Mean              -0.0164014\n",
      "exploration/env_infos/reward_angular Std                1.52555\n",
      "exploration/env_infos/reward_angular Max                5.16363\n",
      "exploration/env_infos/reward_angular Min               -5.02249\n",
      "evaluation/num steps total                         150000\n",
      "evaluation/num paths total                            150\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.0785634\n",
      "evaluation/Rewards Std                                  0.0521222\n",
      "evaluation/Rewards Max                                  1.6638\n",
      "evaluation/Rewards Min                                 -1.20457\n",
      "evaluation/Returns Mean                               -78.5634\n",
      "evaluation/Returns Std                                 35.2701\n",
      "evaluation/Returns Max                                -13.6429\n",
      "evaluation/Returns Min                               -141.101\n",
      "evaluation/Actions Mean                                 0.0963684\n",
      "evaluation/Actions Std                                  0.273855\n",
      "evaluation/Actions Max                                  0.701958\n",
      "evaluation/Actions Min                                 -0.765591\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            -78.5634\n",
      "evaluation/env_infos/final/reward_run Mean              4.35395e-09\n",
      "evaluation/env_infos/final/reward_run Std               6.22217e-08\n",
      "evaluation/env_infos/final/reward_run Max               1.12188e-07\n",
      "evaluation/env_infos/final/reward_run Min              -1.74762e-07\n",
      "evaluation/env_infos/initial/reward_run Mean            0.311209\n",
      "evaluation/env_infos/initial/reward_run Std             0.227177\n",
      "evaluation/env_infos/initial/reward_run Max             0.618921\n",
      "evaluation/env_infos/initial/reward_run Min            -0.195249\n",
      "evaluation/env_infos/reward_run Mean                   -0.00057548\n",
      "evaluation/env_infos/reward_run Std                     0.02582\n",
      "evaluation/env_infos/reward_run Max                     0.659968\n",
      "evaluation/env_infos/reward_run Min                    -0.455153\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.050588\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0219983\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00499841\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0941293\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0757243\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0295331\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.014396\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.129021\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0505702\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0220111\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00233534\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.129021\n",
      "evaluation/env_infos/final/height Mean                 -0.131378\n",
      "evaluation/env_infos/final/height Std                   0.0146127\n",
      "evaluation/env_infos/final/height Max                  -0.108712\n",
      "evaluation/env_infos/final/height Min                  -0.151645\n",
      "evaluation/env_infos/initial/height Mean                0.0171741\n",
      "evaluation/env_infos/initial/height Std                 0.0461742\n",
      "evaluation/env_infos/initial/height Max                 0.0855516\n",
      "evaluation/env_infos/initial/height Min                -0.0746675\n",
      "evaluation/env_infos/height Mean                       -0.130903\n",
      "evaluation/env_infos/height Std                         0.0159609\n",
      "evaluation/env_infos/height Max                         0.0855516\n",
      "evaluation/env_infos/height Min                        -0.164193\n",
      "evaluation/env_infos/final/reward_angular Mean          2.76793e-08\n",
      "evaluation/env_infos/final/reward_angular Std           6.10374e-08\n",
      "evaluation/env_infos/final/reward_angular Max           1.78293e-07\n",
      "evaluation/env_infos/final/reward_angular Min          -9.69786e-08\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.392281\n",
      "evaluation/env_infos/initial/reward_angular Std         1.00346\n",
      "evaluation/env_infos/initial/reward_angular Max         1.93882\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.51935\n",
      "evaluation/env_infos/reward_angular Mean                0.000118965\n",
      "evaluation/env_infos/reward_angular Std                 0.0556159\n",
      "evaluation/env_infos/reward_angular Max                 2.02836\n",
      "evaluation/env_infos/reward_angular Min                -1.57961\n",
      "time/data storing (s)                                   0.013595\n",
      "time/evaluation sampling (s)                           22.7306\n",
      "time/exploration sampling (s)                           1.00052\n",
      "time/logging (s)                                        0.241698\n",
      "time/saving (s)                                         0.0270058\n",
      "time/training (s)                                       3.32709\n",
      "time/epoch (s)                                         27.3405\n",
      "time/total (s)                                        165.928\n",
      "Epoch                                                   5\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 09:51:41.383830 PDT | [gher-halfcheetahhard-SAC-10e-1000s-disc0.99_2021_05_25_09_48_29_0000--s-10] Epoch 6 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                   8000\n",
      "trainer/QF1 Loss                                        0.384308\n",
      "trainer/QF2 Loss                                        0.44054\n",
      "trainer/Policy Loss                                    -6.28119\n",
      "trainer/Q1 Predictions Mean                             3.08096\n",
      "trainer/Q1 Predictions Std                              0.895825\n",
      "trainer/Q1 Predictions Max                              6.51077\n",
      "trainer/Q1 Predictions Min                              0.826849\n",
      "trainer/Q2 Predictions Mean                             3.16361\n",
      "trainer/Q2 Predictions Std                              0.894146\n",
      "trainer/Q2 Predictions Max                              6.5591\n",
      "trainer/Q2 Predictions Min                              0.894752\n",
      "trainer/Q Targets Mean                                  3.12619\n",
      "trainer/Q Targets Std                                   1.08507\n",
      "trainer/Q Targets Max                                   7.40082\n",
      "trainer/Q Targets Min                                   0.060482\n",
      "trainer/Log Pis Mean                                   -2.92459\n",
      "trainer/Log Pis Std                                     1.40241\n",
      "trainer/Log Pis Max                                     1.94657\n",
      "trainer/Log Pis Min                                    -6.03568\n",
      "trainer/Policy mu Mean                                  0.208444\n",
      "trainer/Policy mu Std                                   0.452851\n",
      "trainer/Policy mu Max                                   1.5342\n",
      "trainer/Policy mu Min                                  -1.39052\n",
      "trainer/Policy log std Mean                            -0.168483\n",
      "trainer/Policy log std Std                              0.0606908\n",
      "trainer/Policy log std Max                             -0.0724508\n",
      "trainer/Policy log std Min                             -0.41085\n",
      "trainer/Alpha                                           0.170119\n",
      "trainer/Alpha Loss                                    -15.7828\n",
      "exploration/num steps total                          8000\n",
      "exploration/num paths total                             8\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.140781\n",
      "exploration/Rewards Std                                 0.800922\n",
      "exploration/Rewards Max                                 2.49723\n",
      "exploration/Rewards Min                                -2.78159\n",
      "exploration/Returns Mean                             -140.781\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -140.781\n",
      "exploration/Returns Min                              -140.781\n",
      "exploration/Actions Mean                                0.234729\n",
      "exploration/Actions Std                                 0.579265\n",
      "exploration/Actions Max                                 0.998671\n",
      "exploration/Actions Min                                -0.989921\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -140.781\n",
      "exploration/env_infos/final/reward_run Mean             0.157749\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.157749\n",
      "exploration/env_infos/final/reward_run Min              0.157749\n",
      "exploration/env_infos/initial/reward_run Mean           0.645462\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.645462\n",
      "exploration/env_infos/initial/reward_run Min            0.645462\n",
      "exploration/env_infos/reward_run Mean                   0.152278\n",
      "exploration/env_infos/reward_run Std                    0.485185\n",
      "exploration/env_infos/reward_run Max                    1.65423\n",
      "exploration/env_infos/reward_run Min                   -1.34171\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.306337\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.306337\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.306337\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.156038\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.156038\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.156038\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.234387\n",
      "exploration/env_infos/reward_ctrl Std                   0.0749873\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0106042\n",
      "exploration/env_infos/reward_ctrl Min                  -0.448316\n",
      "exploration/env_infos/final/height Mean                -0.149828\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.149828\n",
      "exploration/env_infos/final/height Min                 -0.149828\n",
      "exploration/env_infos/initial/height Mean              -0.105467\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.105467\n",
      "exploration/env_infos/initial/height Min               -0.105467\n",
      "exploration/env_infos/height Mean                      -0.121769\n",
      "exploration/env_infos/height Std                        0.103772\n",
      "exploration/env_infos/height Max                        0.218792\n",
      "exploration/env_infos/height Min                       -0.416627\n",
      "exploration/env_infos/final/reward_angular Mean        -4.61706\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -4.61706\n",
      "exploration/env_infos/final/reward_angular Min         -4.61706\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.360852\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.360852\n",
      "exploration/env_infos/initial/reward_angular Min       -0.360852\n",
      "exploration/env_infos/reward_angular Mean              -0.00777145\n",
      "exploration/env_infos/reward_angular Std                1.41691\n",
      "exploration/env_infos/reward_angular Max                4.59079\n",
      "exploration/env_infos/reward_angular Min               -4.73751\n",
      "evaluation/num steps total                         175000\n",
      "evaluation/num paths total                            175\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.0998709\n",
      "evaluation/Rewards Std                                  0.0686956\n",
      "evaluation/Rewards Max                                  2.85088\n",
      "evaluation/Rewards Min                                 -1.15315\n",
      "evaluation/Returns Mean                               -99.8709\n",
      "evaluation/Returns Std                                 43.5106\n",
      "evaluation/Returns Max                                 -2.14124\n",
      "evaluation/Returns Min                               -170.002\n",
      "evaluation/Actions Mean                                 0.15689\n",
      "evaluation/Actions Std                                  0.352173\n",
      "evaluation/Actions Max                                  0.77863\n",
      "evaluation/Actions Min                                 -0.81797\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            -99.8709\n",
      "evaluation/env_infos/final/reward_run Mean              1.93242e-09\n",
      "evaluation/env_infos/final/reward_run Std               4.07842e-08\n",
      "evaluation/env_infos/final/reward_run Max               1.44553e-07\n",
      "evaluation/env_infos/final/reward_run Min              -5.82962e-08\n",
      "evaluation/env_infos/initial/reward_run Mean            0.294372\n",
      "evaluation/env_infos/initial/reward_run Std             0.334344\n",
      "evaluation/env_infos/initial/reward_run Max             0.895529\n",
      "evaluation/env_infos/initial/reward_run Min            -0.272448\n",
      "evaluation/env_infos/reward_run Mean                    0.000357042\n",
      "evaluation/env_infos/reward_run Std                     0.0312879\n",
      "evaluation/env_infos/reward_run Max                     0.957355\n",
      "evaluation/env_infos/reward_run Min                    -0.376556\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0892194\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0413485\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0181612\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.166534\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.113863\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0405462\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0377743\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.180468\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0891843\n",
      "evaluation/env_infos/reward_ctrl Std                    0.041216\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.014599\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.180468\n",
      "evaluation/env_infos/final/height Mean                 -0.151478\n",
      "evaluation/env_infos/final/height Std                   0.0260577\n",
      "evaluation/env_infos/final/height Max                  -0.121093\n",
      "evaluation/env_infos/final/height Min                  -0.246016\n",
      "evaluation/env_infos/initial/height Mean               -0.0244623\n",
      "evaluation/env_infos/initial/height Std                 0.0519585\n",
      "evaluation/env_infos/initial/height Max                 0.0886208\n",
      "evaluation/env_infos/initial/height Min                -0.0960234\n",
      "evaluation/env_infos/height Mean                       -0.15126\n",
      "evaluation/env_infos/height Std                         0.027149\n",
      "evaluation/env_infos/height Max                         0.0886208\n",
      "evaluation/env_infos/height Min                        -0.344186\n",
      "evaluation/env_infos/final/reward_angular Mean          2.8933e-08\n",
      "evaluation/env_infos/final/reward_angular Std           1.33006e-07\n",
      "evaluation/env_infos/final/reward_angular Max           4.7775e-07\n",
      "evaluation/env_infos/final/reward_angular Min          -2.7491e-07\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.339141\n",
      "evaluation/env_infos/initial/reward_angular Std         1.46041\n",
      "evaluation/env_infos/initial/reward_angular Max         3.2826\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.02517\n",
      "evaluation/env_infos/reward_angular Mean                0.000426462\n",
      "evaluation/env_infos/reward_angular Std                 0.0783024\n",
      "evaluation/env_infos/reward_angular Max                 3.2826\n",
      "evaluation/env_infos/reward_angular Min                -2.02517\n",
      "time/data storing (s)                                   0.0156753\n",
      "time/evaluation sampling (s)                           22.9998\n",
      "time/exploration sampling (s)                           1.18635\n",
      "time/logging (s)                                        0.231132\n",
      "time/saving (s)                                         0.0268259\n",
      "time/training (s)                                       3.61279\n",
      "time/epoch (s)                                         28.0726\n",
      "time/total (s)                                        194.147\n",
      "Epoch                                                   6\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 09:52:09.664865 PDT | [gher-halfcheetahhard-SAC-10e-1000s-disc0.99_2021_05_25_09_48_29_0000--s-10] Epoch 7 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                   9000\n",
      "trainer/QF1 Loss                                        0.407611\n",
      "trainer/QF2 Loss                                        0.418717\n",
      "trainer/Policy Loss                                    -5.83929\n",
      "trainer/Q1 Predictions Mean                             3.23096\n",
      "trainer/Q1 Predictions Std                              1.13229\n",
      "trainer/Q1 Predictions Max                              8.2838\n",
      "trainer/Q1 Predictions Min                              0.522407\n",
      "trainer/Q2 Predictions Mean                             3.30985\n",
      "trainer/Q2 Predictions Std                              1.10244\n",
      "trainer/Q2 Predictions Max                              8.50654\n",
      "trainer/Q2 Predictions Min                              0.563415\n",
      "trainer/Q Targets Mean                                  3.14263\n",
      "trainer/Q Targets Std                                   1.28166\n",
      "trainer/Q Targets Max                                   8.7538\n",
      "trainer/Q Targets Min                                  -0.468826\n",
      "trainer/Log Pis Mean                                   -2.33388\n",
      "trainer/Log Pis Std                                     2.011\n",
      "trainer/Log Pis Max                                     3.7216\n",
      "trainer/Log Pis Min                                    -7.42093\n",
      "trainer/Policy mu Mean                                  0.321693\n",
      "trainer/Policy mu Std                                   0.56787\n",
      "trainer/Policy mu Max                                   2.01068\n",
      "trainer/Policy mu Min                                  -1.06828\n",
      "trainer/Policy log std Mean                            -0.209325\n",
      "trainer/Policy log std Std                              0.100151\n",
      "trainer/Policy log std Max                             -0.0489673\n",
      "trainer/Policy log std Min                             -0.677832\n",
      "trainer/Alpha                                           0.12877\n",
      "trainer/Alpha Loss                                    -17.0596\n",
      "exploration/num steps total                          9000\n",
      "exploration/num paths total                             9\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.409177\n",
      "exploration/Rewards Std                                 0.310862\n",
      "exploration/Rewards Max                                 0.694999\n",
      "exploration/Rewards Min                                -1.35182\n",
      "exploration/Returns Mean                             -409.177\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -409.177\n",
      "exploration/Returns Min                              -409.177\n",
      "exploration/Actions Mean                                0.0142657\n",
      "exploration/Actions Std                                 0.608727\n",
      "exploration/Actions Max                                 0.998878\n",
      "exploration/Actions Min                                -0.997658\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -409.177\n",
      "exploration/env_infos/final/reward_run Mean             0.327817\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.327817\n",
      "exploration/env_infos/final/reward_run Min              0.327817\n",
      "exploration/env_infos/initial/reward_run Mean           0.329271\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.329271\n",
      "exploration/env_infos/initial/reward_run Min            0.329271\n",
      "exploration/env_infos/reward_run Mean                   0.00438123\n",
      "exploration/env_infos/reward_run Std                    0.489793\n",
      "exploration/env_infos/reward_run Max                    1.56566\n",
      "exploration/env_infos/reward_run Min                   -1.59582\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.385016\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.385016\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.385016\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.357853\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.357853\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.357853\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.222452\n",
      "exploration/env_infos/reward_ctrl Std                   0.0739904\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0454727\n",
      "exploration/env_infos/reward_ctrl Min                  -0.488717\n",
      "exploration/env_infos/final/height Mean                -0.571093\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.571093\n",
      "exploration/env_infos/final/height Min                 -0.571093\n",
      "exploration/env_infos/initial/height Mean               0.0722804\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0722804\n",
      "exploration/env_infos/initial/height Min                0.0722804\n",
      "exploration/env_infos/height Mean                      -0.49924\n",
      "exploration/env_infos/height Std                        0.154184\n",
      "exploration/env_infos/height Max                        0.116541\n",
      "exploration/env_infos/height Min                       -0.592427\n",
      "exploration/env_infos/final/reward_angular Mean         1.52798\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          1.52798\n",
      "exploration/env_infos/final/reward_angular Min          1.52798\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.96915\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.96915\n",
      "exploration/env_infos/initial/reward_angular Min       -1.96915\n",
      "exploration/env_infos/reward_angular Mean               0.0630643\n",
      "exploration/env_infos/reward_angular Std                0.994211\n",
      "exploration/env_infos/reward_angular Max                3.84599\n",
      "exploration/env_infos/reward_angular Min               -3.73946\n",
      "evaluation/num steps total                         200000\n",
      "evaluation/num paths total                            200\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.122875\n",
      "evaluation/Rewards Std                                  0.0945728\n",
      "evaluation/Rewards Max                                  2.74563\n",
      "evaluation/Rewards Min                                 -0.85955\n",
      "evaluation/Returns Mean                              -122.875\n",
      "evaluation/Returns Std                                 55.8418\n",
      "evaluation/Returns Max                                -31.0055\n",
      "evaluation/Returns Min                               -233.879\n",
      "evaluation/Actions Mean                                 0.209302\n",
      "evaluation/Actions Std                                  0.437009\n",
      "evaluation/Actions Max                                  0.92478\n",
      "evaluation/Actions Min                                 -0.779145\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -122.875\n",
      "evaluation/env_infos/final/reward_run Mean             -9.14622e-07\n",
      "evaluation/env_infos/final/reward_run Std               4.45991e-06\n",
      "evaluation/env_infos/final/reward_run Max               1.70157e-07\n",
      "evaluation/env_infos/final/reward_run Min              -2.27614e-05\n",
      "evaluation/env_infos/initial/reward_run Mean            0.148545\n",
      "evaluation/env_infos/initial/reward_run Std             0.34276\n",
      "evaluation/env_infos/initial/reward_run Max             0.734132\n",
      "evaluation/env_infos/initial/reward_run Min            -0.385645\n",
      "evaluation/env_infos/reward_run Mean                    0.00127644\n",
      "evaluation/env_infos/reward_run Std                     0.0539425\n",
      "evaluation/env_infos/reward_run Max                     0.943595\n",
      "evaluation/env_infos/reward_run Min                    -0.808941\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.140736\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0772377\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0107062\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.281053\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.155164\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0581823\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0473397\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.25841\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.140871\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0771353\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0104654\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.296905\n",
      "evaluation/env_infos/final/height Mean                 -0.171127\n",
      "evaluation/env_infos/final/height Std                   0.0520856\n",
      "evaluation/env_infos/final/height Max                  -0.0848976\n",
      "evaluation/env_infos/final/height Min                  -0.383097\n",
      "evaluation/env_infos/initial/height Mean               -0.00615813\n",
      "evaluation/env_infos/initial/height Std                 0.0458331\n",
      "evaluation/env_infos/initial/height Max                 0.0784195\n",
      "evaluation/env_infos/initial/height Min                -0.0765005\n",
      "evaluation/env_infos/height Mean                       -0.170808\n",
      "evaluation/env_infos/height Std                         0.0536565\n",
      "evaluation/env_infos/height Max                         0.0792107\n",
      "evaluation/env_infos/height Min                        -0.435124\n",
      "evaluation/env_infos/final/reward_angular Mean         -1.7424e-06\n",
      "evaluation/env_infos/final/reward_angular Std           8.79181e-06\n",
      "evaluation/env_infos/final/reward_angular Max           6.61359e-07\n",
      "evaluation/env_infos/final/reward_angular Min          -4.48071e-05\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.698735\n",
      "evaluation/env_infos/initial/reward_angular Std         1.36561\n",
      "evaluation/env_infos/initial/reward_angular Max         2.59532\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.9459\n",
      "evaluation/env_infos/reward_angular Mean                0.00139992\n",
      "evaluation/env_infos/reward_angular Std                 0.147251\n",
      "evaluation/env_infos/reward_angular Max                 2.92476\n",
      "evaluation/env_infos/reward_angular Min                -2.9459\n",
      "time/data storing (s)                                   0.0157095\n",
      "time/evaluation sampling (s)                           22.4353\n",
      "time/exploration sampling (s)                           1.09504\n",
      "time/logging (s)                                        0.239408\n",
      "time/saving (s)                                         0.0333826\n",
      "time/training (s)                                       4.31688\n",
      "time/epoch (s)                                         28.1357\n",
      "time/total (s)                                        222.436\n",
      "Epoch                                                   7\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 09:52:37.902823 PDT | [gher-halfcheetahhard-SAC-10e-1000s-disc0.99_2021_05_25_09_48_29_0000--s-10] Epoch 8 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  10000\n",
      "trainer/QF1 Loss                                        0.335557\n",
      "trainer/QF2 Loss                                        0.378134\n",
      "trainer/Policy Loss                                    -6.06696\n",
      "trainer/Q1 Predictions Mean                             3.00234\n",
      "trainer/Q1 Predictions Std                              1.02699\n",
      "trainer/Q1 Predictions Max                              6.98008\n",
      "trainer/Q1 Predictions Min                              0.467989\n",
      "trainer/Q2 Predictions Mean                             2.84171\n",
      "trainer/Q2 Predictions Std                              0.993679\n",
      "trainer/Q2 Predictions Max                              6.43974\n",
      "trainer/Q2 Predictions Min                              0.452873\n",
      "trainer/Q Targets Mean                                  3.02682\n",
      "trainer/Q Targets Std                                   1.17703\n",
      "trainer/Q Targets Max                                   7.85685\n",
      "trainer/Q Targets Min                                  -0.423063\n",
      "trainer/Log Pis Mean                                   -3.03589\n",
      "trainer/Log Pis Std                                     1.6082\n",
      "trainer/Log Pis Max                                     2.14173\n",
      "trainer/Log Pis Min                                    -6.63515\n",
      "trainer/Policy mu Mean                                  0.0824587\n",
      "trainer/Policy mu Std                                   0.53343\n",
      "trainer/Policy mu Max                                   1.59956\n",
      "trainer/Policy mu Min                                  -1.85533\n",
      "trainer/Policy log std Mean                            -0.183573\n",
      "trainer/Policy log std Std                              0.0738382\n",
      "trainer/Policy log std Max                              0.044068\n",
      "trainer/Policy log std Min                             -0.510759\n",
      "trainer/Alpha                                           0.09784\n",
      "trainer/Alpha Loss                                    -20.9785\n",
      "exploration/num steps total                         10000\n",
      "exploration/num paths total                            10\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.250325\n",
      "exploration/Rewards Std                                 0.538092\n",
      "exploration/Rewards Max                                 1.92164\n",
      "exploration/Rewards Min                                -2.21202\n",
      "exploration/Returns Mean                             -250.325\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -250.325\n",
      "exploration/Returns Min                              -250.325\n",
      "exploration/Actions Mean                               -0.156206\n",
      "exploration/Actions Std                                 0.589309\n",
      "exploration/Actions Max                                 0.997395\n",
      "exploration/Actions Min                                -0.995907\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -250.325\n",
      "exploration/env_infos/final/reward_run Mean            -0.553659\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.553659\n",
      "exploration/env_infos/final/reward_run Min             -0.553659\n",
      "exploration/env_infos/initial/reward_run Mean           0.885651\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.885651\n",
      "exploration/env_infos/initial/reward_run Min            0.885651\n",
      "exploration/env_infos/reward_run Mean                   0.0177774\n",
      "exploration/env_infos/reward_run Std                    0.55675\n",
      "exploration/env_infos/reward_run Max                    1.83863\n",
      "exploration/env_infos/reward_run Min                   -1.57393\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.150166\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.150166\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.150166\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.364045\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.364045\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.364045\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.223011\n",
      "exploration/env_infos/reward_ctrl Std                   0.0744909\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0385627\n",
      "exploration/env_infos/reward_ctrl Min                  -0.498261\n",
      "exploration/env_infos/final/height Mean                -0.558192\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.558192\n",
      "exploration/env_infos/final/height Min                 -0.558192\n",
      "exploration/env_infos/initial/height Mean               0.0133971\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0133971\n",
      "exploration/env_infos/initial/height Min                0.0133971\n",
      "exploration/env_infos/height Mean                      -0.448793\n",
      "exploration/env_infos/height Std                        0.217394\n",
      "exploration/env_infos/height Max                        0.268403\n",
      "exploration/env_infos/height Min                       -0.582994\n",
      "exploration/env_infos/final/reward_angular Mean         0.88556\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.88556\n",
      "exploration/env_infos/final/reward_angular Min          0.88556\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.562495\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.562495\n",
      "exploration/env_infos/initial/reward_angular Min       -0.562495\n",
      "exploration/env_infos/reward_angular Mean               0.0610207\n",
      "exploration/env_infos/reward_angular Std                1.24464\n",
      "exploration/env_infos/reward_angular Max                5.20797\n",
      "exploration/env_infos/reward_angular Min               -4.24906\n",
      "evaluation/num steps total                         225000\n",
      "evaluation/num paths total                            225\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.130318\n",
      "evaluation/Rewards Std                                  0.0765158\n",
      "evaluation/Rewards Max                                  1.97604\n",
      "evaluation/Rewards Min                                 -1.0214\n",
      "evaluation/Returns Mean                              -130.318\n",
      "evaluation/Returns Std                                 51.7849\n",
      "evaluation/Returns Max                                -16.253\n",
      "evaluation/Returns Min                               -205.182\n",
      "evaluation/Actions Mean                                 0.124633\n",
      "evaluation/Actions Std                                  0.472903\n",
      "evaluation/Actions Max                                  0.919875\n",
      "evaluation/Actions Min                                 -0.872154\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -130.318\n",
      "evaluation/env_infos/final/reward_run Mean              3.65881e-09\n",
      "evaluation/env_infos/final/reward_run Std               4.10973e-08\n",
      "evaluation/env_infos/final/reward_run Max               1.10372e-07\n",
      "evaluation/env_infos/final/reward_run Min              -8.44168e-08\n",
      "evaluation/env_infos/initial/reward_run Mean            0.0281034\n",
      "evaluation/env_infos/initial/reward_run Std             0.401316\n",
      "evaluation/env_infos/initial/reward_run Max             0.833645\n",
      "evaluation/env_infos/initial/reward_run Min            -0.610449\n",
      "evaluation/env_infos/reward_run Mean                    0.000594007\n",
      "evaluation/env_infos/reward_run Std                     0.0340662\n",
      "evaluation/env_infos/reward_run Max                     1.18496\n",
      "evaluation/env_infos/reward_run Min                    -0.673622\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.143568\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0540181\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0639057\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.260611\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.145303\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0471588\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0553918\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.222468\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.143502\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0541066\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0269642\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.266783\n",
      "evaluation/env_infos/final/height Mean                 -0.174924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation/env_infos/final/height Std                   0.0410278\n",
      "evaluation/env_infos/final/height Max                  -0.114312\n",
      "evaluation/env_infos/final/height Min                  -0.295429\n",
      "evaluation/env_infos/initial/height Mean               -0.0180978\n",
      "evaluation/env_infos/initial/height Std                 0.0511822\n",
      "evaluation/env_infos/initial/height Max                 0.0740085\n",
      "evaluation/env_infos/initial/height Min                -0.0928938\n",
      "evaluation/env_infos/height Mean                       -0.174622\n",
      "evaluation/env_infos/height Std                         0.0418956\n",
      "evaluation/env_infos/height Max                         0.0740085\n",
      "evaluation/env_infos/height Min                        -0.42318\n",
      "evaluation/env_infos/final/reward_angular Mean         -6.21871e-09\n",
      "evaluation/env_infos/final/reward_angular Std           7.47019e-08\n",
      "evaluation/env_infos/final/reward_angular Max           2.01767e-07\n",
      "evaluation/env_infos/final/reward_angular Min          -2.35602e-07\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.27251\n",
      "evaluation/env_infos/initial/reward_angular Std         1.21458\n",
      "evaluation/env_infos/initial/reward_angular Max         2.15187\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.37682\n",
      "evaluation/env_infos/reward_angular Mean                0.00114874\n",
      "evaluation/env_infos/reward_angular Std                 0.0910334\n",
      "evaluation/env_infos/reward_angular Max                 2.39663\n",
      "evaluation/env_infos/reward_angular Min                -2.37682\n",
      "time/data storing (s)                                   0.0148615\n",
      "time/evaluation sampling (s)                           22.7025\n",
      "time/exploration sampling (s)                           1.05699\n",
      "time/logging (s)                                        0.231833\n",
      "time/saving (s)                                         0.0259941\n",
      "time/training (s)                                       4.03878\n",
      "time/epoch (s)                                         28.0709\n",
      "time/total (s)                                        250.665\n",
      "Epoch                                                   8\n",
      "-------------------------------------------------  ----------------\n",
      "2021-05-25 09:53:05.788974 PDT | [gher-halfcheetahhard-SAC-10e-1000s-disc0.99_2021_05_25_09_48_29_0000--s-10] Epoch 9 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  11000\n",
      "trainer/QF1 Loss                                        0.418886\n",
      "trainer/QF2 Loss                                        0.489847\n",
      "trainer/Policy Loss                                    -5.20177\n",
      "trainer/Q1 Predictions Mean                             2.8631\n",
      "trainer/Q1 Predictions Std                              1.31603\n",
      "trainer/Q1 Predictions Max                             12.1345\n",
      "trainer/Q1 Predictions Min                              0.482715\n",
      "trainer/Q2 Predictions Mean                             2.89767\n",
      "trainer/Q2 Predictions Std                              1.26039\n",
      "trainer/Q2 Predictions Max                             11.64\n",
      "trainer/Q2 Predictions Min                              0.71042\n",
      "trainer/Q Targets Mean                                  2.92802\n",
      "trainer/Q Targets Std                                   1.48082\n",
      "trainer/Q Targets Max                                  12.6618\n",
      "trainer/Q Targets Min                                  -1.14523\n",
      "trainer/Log Pis Mean                                   -2.1144\n",
      "trainer/Log Pis Std                                     2.07853\n",
      "trainer/Log Pis Max                                     3.78041\n",
      "trainer/Log Pis Min                                    -7.17368\n",
      "trainer/Policy mu Mean                                  0.159658\n",
      "trainer/Policy mu Std                                   0.633432\n",
      "trainer/Policy mu Max                                   2.66242\n",
      "trainer/Policy mu Min                                  -1.93672\n",
      "trainer/Policy log std Mean                            -0.235249\n",
      "trainer/Policy log std Std                              0.098399\n",
      "trainer/Policy log std Max                             -0.0402486\n",
      "trainer/Policy log std Min                             -0.572462\n",
      "trainer/Alpha                                           0.0745987\n",
      "trainer/Alpha Loss                                    -21.0403\n",
      "exploration/num steps total                         11000\n",
      "exploration/num paths total                            11\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.219752\n",
      "exploration/Rewards Std                                 0.436294\n",
      "exploration/Rewards Max                                 1.65647\n",
      "exploration/Rewards Min                                -1.95596\n",
      "exploration/Returns Mean                             -219.752\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -219.752\n",
      "exploration/Returns Min                              -219.752\n",
      "exploration/Actions Mean                               -0.257596\n",
      "exploration/Actions Std                                 0.577627\n",
      "exploration/Actions Max                                 0.995799\n",
      "exploration/Actions Min                                -0.997455\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -219.752\n",
      "exploration/env_infos/final/reward_run Mean            -0.196676\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.196676\n",
      "exploration/env_infos/final/reward_run Min             -0.196676\n",
      "exploration/env_infos/initial/reward_run Mean          -0.00521741\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.00521741\n",
      "exploration/env_infos/initial/reward_run Min           -0.00521741\n",
      "exploration/env_infos/reward_run Mean                   0.0277202\n",
      "exploration/env_infos/reward_run Std                    0.567311\n",
      "exploration/env_infos/reward_run Max                    2.32785\n",
      "exploration/env_infos/reward_run Min                   -1.69049\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.227073\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.227073\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.227073\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.188533\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.188533\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.188533\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.240005\n",
      "exploration/env_infos/reward_ctrl Std                   0.0724019\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0339268\n",
      "exploration/env_infos/reward_ctrl Min                  -0.487459\n",
      "exploration/env_infos/final/height Mean                -0.549259\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.549259\n",
      "exploration/env_infos/final/height Min                 -0.549259\n",
      "exploration/env_infos/initial/height Mean              -0.0926412\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0926412\n",
      "exploration/env_infos/initial/height Min               -0.0926412\n",
      "exploration/env_infos/height Mean                      -0.423416\n",
      "exploration/env_infos/height Std                        0.236293\n",
      "exploration/env_infos/height Max                        0.327014\n",
      "exploration/env_infos/height Min                       -0.582928\n",
      "exploration/env_infos/final/reward_angular Mean        -1.62211\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -1.62211\n",
      "exploration/env_infos/final/reward_angular Min         -1.62211\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.816247\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.816247\n",
      "exploration/env_infos/initial/reward_angular Min       -0.816247\n",
      "exploration/env_infos/reward_angular Mean               0.0637283\n",
      "exploration/env_infos/reward_angular Std                1.22026\n",
      "exploration/env_infos/reward_angular Max                6.19961\n",
      "exploration/env_infos/reward_angular Min               -5.12252\n",
      "evaluation/num steps total                         250000\n",
      "evaluation/num paths total                            250\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.162014\n",
      "evaluation/Rewards Std                                  0.12149\n",
      "evaluation/Rewards Max                                  2.66842\n",
      "evaluation/Rewards Min                                 -0.79439\n",
      "evaluation/Returns Mean                              -162.014\n",
      "evaluation/Returns Std                                 91.6884\n",
      "evaluation/Returns Max                                -31.9516\n",
      "evaluation/Returns Min                               -412.79\n",
      "evaluation/Actions Mean                                 0.0998169\n",
      "evaluation/Actions Std                                  0.52294\n",
      "evaluation/Actions Max                                  0.981638\n",
      "evaluation/Actions Min                                 -0.893939\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -162.014\n",
      "evaluation/env_infos/final/reward_run Mean             -9.88978e-09\n",
      "evaluation/env_infos/final/reward_run Std               2.66393e-08\n",
      "evaluation/env_infos/final/reward_run Max               1.80812e-08\n",
      "evaluation/env_infos/final/reward_run Min              -1.16754e-07\n",
      "evaluation/env_infos/initial/reward_run Mean            0.0330491\n",
      "evaluation/env_infos/initial/reward_run Std             0.47449\n",
      "evaluation/env_infos/initial/reward_run Max             0.789888\n",
      "evaluation/env_infos/initial/reward_run Min            -0.757174\n",
      "evaluation/env_infos/reward_run Mean                    0.00221882\n",
      "evaluation/env_infos/reward_run Std                     0.0480824\n",
      "evaluation/env_infos/reward_run Max                     0.911886\n",
      "evaluation/env_infos/reward_run Min                    -0.974309\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.170048\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0686745\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0561197\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.311249\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.168228\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0558438\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0568607\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.270398\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.170058\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0688879\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00658634\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.328225\n",
      "evaluation/env_infos/final/height Mean                 -0.21846\n",
      "evaluation/env_infos/final/height Std                   0.117136\n",
      "evaluation/env_infos/final/height Max                  -0.125714\n",
      "evaluation/env_infos/final/height Min                  -0.57728\n",
      "evaluation/env_infos/initial/height Mean               -0.0206918\n",
      "evaluation/env_infos/initial/height Std                 0.0558884\n",
      "evaluation/env_infos/initial/height Max                 0.0775088\n",
      "evaluation/env_infos/initial/height Min                -0.0971916\n",
      "evaluation/env_infos/height Mean                       -0.216026\n",
      "evaluation/env_infos/height Std                         0.115658\n",
      "evaluation/env_infos/height Max                         0.203694\n",
      "evaluation/env_infos/height Min                        -0.577281\n",
      "evaluation/env_infos/final/reward_angular Mean         -1.79322e-08\n",
      "evaluation/env_infos/final/reward_angular Std           8.13701e-08\n",
      "evaluation/env_infos/final/reward_angular Max           8.86631e-08\n",
      "evaluation/env_infos/final/reward_angular Min          -3.37619e-07\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.429139\n",
      "evaluation/env_infos/initial/reward_angular Std         1.3853\n",
      "evaluation/env_infos/initial/reward_angular Max         2.04396\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.9611\n",
      "evaluation/env_infos/reward_angular Mean                0.00581947\n",
      "evaluation/env_infos/reward_angular Std                 0.145912\n",
      "evaluation/env_infos/reward_angular Max                 4.20658\n",
      "evaluation/env_infos/reward_angular Min                -2.9611\n",
      "time/data storing (s)                                   0.0148977\n",
      "time/evaluation sampling (s)                           22.5408\n",
      "time/exploration sampling (s)                           1.05203\n",
      "time/logging (s)                                        0.23694\n",
      "time/saving (s)                                         0.0527276\n",
      "time/training (s)                                       3.82316\n",
      "time/epoch (s)                                         27.7205\n",
      "time/total (s)                                        278.556\n",
      "Epoch                                                   9\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doodad not detected\n",
      "objc[14897]: Class GLFWApplicationDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a1bbc6778) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a1bc47740). One of the two will be used. Which one is undefined.\n",
      "objc[14897]: Class GLFWWindowDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a1bbc6700) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a1bc47768). One of the two will be used. Which one is undefined.\n",
      "objc[14897]: Class GLFWContentView is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a1bbc67a0) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a1bc477b8). One of the two will be used. Which one is undefined.\n",
      "objc[14897]: Class GLFWWindow is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a1bbc6818) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a1bc47830). One of the two will be used. Which one is undefined.\n",
      "2021-05-25 09:53:11.904900 PDT | Variant:\n",
      "2021-05-25 09:53:11.905578 PDT | {\n",
      "  \"seed\": 10,\n",
      "  \"algorithm\": \"SAC\",\n",
      "  \"env_name\": \"halfcheetahhard\",\n",
      "  \"algo_kwargs\": {\n",
      "    \"batch_size\": 256,\n",
      "    \"num_epochs\": 10,\n",
      "    \"num_eval_steps_per_epoch\": 25000,\n",
      "    \"num_expl_steps_per_train_loop\": 1000,\n",
      "    \"num_trains_per_train_loop\": 100,\n",
      "    \"min_num_steps_before_training\": 1000,\n",
      "    \"max_path_length\": 1000,\n",
      "    \"num_train_loops_per_epoch\": 1\n",
      "  },\n",
      "  \"trainer_kwargs\": {\n",
      "    \"discount\": 0.99,\n",
      "    \"soft_target_tau\": 0.005,\n",
      "    \"target_update_period\": 1,\n",
      "    \"policy_lr\": 0.003,\n",
      "    \"qf_lr\": 0.003,\n",
      "    \"reward_scale\": 1,\n",
      "    \"use_automatic_entropy_tuning\": true\n",
      "  },\n",
      "  \"replay_buffer_kwargs\": {\n",
      "    \"max_replay_buffer_size\": 1000000,\n",
      "    \"latent_dim\": 4,\n",
      "    \"approx_irl\": false,\n",
      "    \"plot\": false\n",
      "  },\n",
      "  \"relabeler_kwargs\": {\n",
      "    \"relabel\": true,\n",
      "    \"use_adv\": false,\n",
      "    \"n_sampled_latents\": 1,\n",
      "    \"n_to_take\": 1,\n",
      "    \"cache\": false\n",
      "  },\n",
      "  \"qf_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      256,\n",
      "      256\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"policy_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      256,\n",
      "      256\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"path_collector_kwargs\": {\n",
      "    \"save_videos\": false\n",
      "  },\n",
      "  \"use_advantages\": false,\n",
      "  \"proper_advantages\": true,\n",
      "  \"plot\": false,\n",
      "  \"test\": false,\n",
      "  \"gpu\": 0,\n",
      "  \"mode\": \"here_no_doodad\",\n",
      "  \"local_docker\": false,\n",
      "  \"insert_time\": false,\n",
      "  \"latent_shape_multiplier\": 1\n",
      "}\n",
      "halfcheetah\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "2021-05-25 09:53:40.593437 PDT | [gher-halfcheetahhard-SAC-10e-1000s-disc0.99_2021_05_25_09_53_11_0000--s-10] Epoch 0 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  4000\n",
      "trainer/QF1 Loss                                      15.3269\n",
      "trainer/QF2 Loss                                      15.238\n",
      "trainer/Policy Loss                                   -4.03366\n",
      "trainer/Q1 Predictions Mean                           -0.00783563\n",
      "trainer/Q1 Predictions Std                             0.0058065\n",
      "trainer/Q1 Predictions Max                             0.00421082\n",
      "trainer/Q1 Predictions Min                            -0.0316749\n",
      "trainer/Q2 Predictions Mean                            0.00402962\n",
      "trainer/Q2 Predictions Std                             0.00446795\n",
      "trainer/Q2 Predictions Max                             0.0150868\n",
      "trainer/Q2 Predictions Min                            -0.00713501\n",
      "trainer/Q Targets Mean                                 3.71426\n",
      "trainer/Q Targets Std                                  1.21357\n",
      "trainer/Q Targets Max                                  7.32799\n",
      "trainer/Q Targets Min                                  0.547262\n",
      "trainer/Log Pis Mean                                  -4.04152\n",
      "trainer/Log Pis Std                                    0.506937\n",
      "trainer/Log Pis Max                                   -2.35869\n",
      "trainer/Log Pis Min                                   -5.48474\n",
      "trainer/Policy mu Mean                                -0.000865881\n",
      "trainer/Policy mu Std                                  0.00238213\n",
      "trainer/Policy mu Max                                  0.00771935\n",
      "trainer/Policy mu Min                                 -0.00766312\n",
      "trainer/Policy log std Mean                           -0.00131967\n",
      "trainer/Policy log std Std                             0.00164469\n",
      "trainer/Policy log std Max                             0.0035715\n",
      "trainer/Policy log std Min                            -0.00800097\n",
      "trainer/Alpha                                          0.997005\n",
      "trainer/Alpha Loss                                    -0\n",
      "exploration/num steps total                         2000\n",
      "exploration/num paths total                            2\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.0272778\n",
      "exploration/Rewards Std                                1.57289\n",
      "exploration/Rewards Max                                5.23244\n",
      "exploration/Rewards Min                               -4.92095\n",
      "exploration/Returns Mean                             -27.2778\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              -27.2778\n",
      "exploration/Returns Min                              -27.2778\n",
      "exploration/Actions Mean                               0.00116263\n",
      "exploration/Actions Std                                0.627096\n",
      "exploration/Actions Max                                0.999106\n",
      "exploration/Actions Min                               -0.999803\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          -27.2778\n",
      "exploration/env_infos/final/reward_run Mean            1.25983\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max             1.25983\n",
      "exploration/env_infos/final/reward_run Min             1.25983\n",
      "exploration/env_infos/initial/reward_run Mean         -0.12297\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max          -0.12297\n",
      "exploration/env_infos/initial/reward_run Min          -0.12297\n",
      "exploration/env_infos/reward_run Mean                  0.054153\n",
      "exploration/env_infos/reward_run Std                   0.642465\n",
      "exploration/env_infos/reward_run Max                   3.08463\n",
      "exploration/env_infos/reward_run Min                  -1.66697\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.11265\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.11265\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.11265\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.144553\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.144553\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.144553\n",
      "exploration/env_infos/reward_ctrl Mean                -0.23595\n",
      "exploration/env_infos/reward_ctrl Std                  0.0750464\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0181959\n",
      "exploration/env_infos/reward_ctrl Min                 -0.475779\n",
      "exploration/env_infos/final/height Mean               -0.53631\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.53631\n",
      "exploration/env_infos/final/height Min                -0.53631\n",
      "exploration/env_infos/initial/height Mean             -0.0590837\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max              -0.0590837\n",
      "exploration/env_infos/initial/height Min              -0.0590837\n",
      "exploration/env_infos/height Mean                     -0.0654237\n",
      "exploration/env_infos/height Std                       0.100538\n",
      "exploration/env_infos/height Max                       0.216356\n",
      "exploration/env_infos/height Min                      -0.549681\n",
      "exploration/env_infos/final/reward_angular Mean        0.930011\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max         0.930011\n",
      "exploration/env_infos/final/reward_angular Min         0.930011\n",
      "exploration/env_infos/initial/reward_angular Mean      0.00791141\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       0.00791141\n",
      "exploration/env_infos/initial/reward_angular Min       0.00791141\n",
      "exploration/env_infos/reward_angular Mean              0.0484033\n",
      "exploration/env_infos/reward_angular Std               1.64769\n",
      "exploration/env_infos/reward_angular Max               5.71955\n",
      "exploration/env_infos/reward_angular Min              -5.0347\n",
      "evaluation/num steps total                         25000\n",
      "evaluation/num paths total                            25\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.0614858\n",
      "evaluation/Rewards Std                                 0.0499058\n",
      "evaluation/Rewards Max                                 1.26813\n",
      "evaluation/Rewards Min                                -1.51007\n",
      "evaluation/Returns Mean                              -61.4858\n",
      "evaluation/Returns Std                                38.0789\n",
      "evaluation/Returns Max                                -0.892261\n",
      "evaluation/Returns Min                              -127.423\n",
      "evaluation/Actions Mean                               -0.000323142\n",
      "evaluation/Actions Std                                 0.00121869\n",
      "evaluation/Actions Max                                 0.00422043\n",
      "evaluation/Actions Min                                -0.00371634\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           -61.4858\n",
      "evaluation/env_infos/final/reward_run Mean             1.11022e-17\n",
      "evaluation/env_infos/final/reward_run Std              2.63078e-16\n",
      "evaluation/env_infos/final/reward_run Max              8.32667e-16\n",
      "evaluation/env_infos/final/reward_run Min             -6.93889e-16\n",
      "evaluation/env_infos/initial/reward_run Mean           0.00372491\n",
      "evaluation/env_infos/initial/reward_run Std            0.131693\n",
      "evaluation/env_infos/initial/reward_run Max            0.29157\n",
      "evaluation/env_infos/initial/reward_run Min           -0.289685\n",
      "evaluation/env_infos/reward_run Mean                   7.65262e-05\n",
      "evaluation/env_infos/reward_run Std                    0.0157641\n",
      "evaluation/env_infos/reward_run Max                    0.502553\n",
      "evaluation/env_infos/reward_run Min                   -0.377546\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -9.52177e-07\n",
      "evaluation/env_infos/final/reward_ctrl Std             9.0322e-08\n",
      "evaluation/env_infos/final/reward_ctrl Max            -8.07404e-07\n",
      "evaluation/env_infos/final/reward_ctrl Min            -1.10654e-06\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -1.00922e-06\n",
      "evaluation/env_infos/initial/reward_ctrl Std           9.1095e-08\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -8.72219e-07\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -1.16497e-06\n",
      "evaluation/env_infos/reward_ctrl Mean                 -9.53778e-07\n",
      "evaluation/env_infos/reward_ctrl Std                   1.02105e-07\n",
      "evaluation/env_infos/reward_ctrl Max                  -5.583e-07\n",
      "evaluation/env_infos/reward_ctrl Min                  -4.30826e-06\n",
      "evaluation/env_infos/final/height Mean                -0.132891\n",
      "evaluation/env_infos/final/height Std                  3.5865e-05\n",
      "evaluation/env_infos/final/height Max                 -0.132829\n",
      "evaluation/env_infos/final/height Min                 -0.132952\n",
      "evaluation/env_infos/initial/height Mean              -0.00354426\n",
      "evaluation/env_infos/initial/height Std                0.0473749\n",
      "evaluation/env_infos/initial/height Max                0.0927329\n",
      "evaluation/env_infos/initial/height Min               -0.0890003\n",
      "evaluation/env_infos/height Mean                      -0.132402\n",
      "evaluation/env_infos/height Std                        0.00598139\n",
      "evaluation/env_infos/height Max                        0.0927329\n",
      "evaluation/env_infos/height Min                       -0.149142\n",
      "evaluation/env_infos/final/reward_angular Mean        -4.21006e-16\n",
      "evaluation/env_infos/final/reward_angular Std          1.81736e-15\n",
      "evaluation/env_infos/final/reward_angular Max          2.8216e-15\n",
      "evaluation/env_infos/final/reward_angular Min         -4.18099e-15\n",
      "evaluation/env_infos/initial/reward_angular Mean      -0.00717915\n",
      "evaluation/env_infos/initial/reward_angular Std        0.188434\n",
      "evaluation/env_infos/initial/reward_angular Max        0.426623\n",
      "evaluation/env_infos/initial/reward_angular Min       -0.351699\n",
      "evaluation/env_infos/reward_angular Mean               0.00151609\n",
      "evaluation/env_infos/reward_angular Std                0.0500711\n",
      "evaluation/env_infos/reward_angular Max                2.09357\n",
      "evaluation/env_infos/reward_angular Min               -1.20257\n",
      "time/data storing (s)                                  0.0289912\n",
      "time/evaluation sampling (s)                          22.9433\n",
      "time/exploration sampling (s)                          1.04285\n",
      "time/logging (s)                                       0.238943\n",
      "time/saving (s)                                        0.0691732\n",
      "time/training (s)                                      3.37603\n",
      "time/epoch (s)                                        27.6993\n",
      "time/total (s)                                        32.1776\n",
      "Epoch                                                  0\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 09:54:08.403952 PDT | [gher-halfcheetahhard-SAC-10e-1000s-disc0.99_2021_05_25_09_53_11_0000--s-10] Epoch 1 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  6000\n",
      "trainer/QF1 Loss                                       1.16347\n",
      "trainer/QF2 Loss                                       1.15692\n",
      "trainer/Policy Loss                                   -7.23342\n",
      "trainer/Q1 Predictions Mean                            3.23425\n",
      "trainer/Q1 Predictions Std                             0.806728\n",
      "trainer/Q1 Predictions Max                             5.29649\n",
      "trainer/Q1 Predictions Min                             1.42611\n",
      "trainer/Q2 Predictions Mean                            3.23838\n",
      "trainer/Q2 Predictions Std                             0.803661\n",
      "trainer/Q2 Predictions Max                             5.32151\n",
      "trainer/Q2 Predictions Min                             1.51808\n",
      "trainer/Q Targets Mean                                 3.29322\n",
      "trainer/Q Targets Std                                  1.2165\n",
      "trainer/Q Targets Max                                  7.68043\n",
      "trainer/Q Targets Min                                  0.322782\n",
      "trainer/Log Pis Mean                                  -3.95007\n",
      "trainer/Log Pis Std                                    0.652274\n",
      "trainer/Log Pis Max                                   -2.00268\n",
      "trainer/Log Pis Min                                   -6.93935\n",
      "trainer/Policy mu Mean                                -0.153469\n",
      "trainer/Policy mu Std                                  0.151768\n",
      "trainer/Policy mu Max                                  0.0141714\n",
      "trainer/Policy mu Min                                 -0.662057\n",
      "trainer/Policy log std Mean                           -0.140578\n",
      "trainer/Policy log std Std                             0.0306625\n",
      "trainer/Policy log std Max                            -0.0819739\n",
      "trainer/Policy log std Min                            -0.269909\n",
      "trainer/Alpha                                          0.739143\n",
      "trainer/Alpha Loss                                    -2.97784\n",
      "exploration/num steps total                         3000\n",
      "exploration/num paths total                            3\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.114296\n",
      "exploration/Rewards Std                                0.776365\n",
      "exploration/Rewards Max                                2.77903\n",
      "exploration/Rewards Min                               -2.53234\n",
      "exploration/Returns Mean                            -114.296\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -114.296\n",
      "exploration/Returns Min                             -114.296\n",
      "exploration/Actions Mean                              -0.0961741\n",
      "exploration/Actions Std                                0.591422\n",
      "exploration/Actions Max                                0.994114\n",
      "exploration/Actions Min                               -0.998861\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -114.296\n",
      "exploration/env_infos/final/reward_run Mean           -0.927734\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max            -0.927734\n",
      "exploration/env_infos/final/reward_run Min            -0.927734\n",
      "exploration/env_infos/initial/reward_run Mean         -0.457034\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max          -0.457034\n",
      "exploration/env_infos/initial/reward_run Min          -0.457034\n",
      "exploration/env_infos/reward_run Mean                 -0.105857\n",
      "exploration/env_infos/reward_run Std                   0.688417\n",
      "exploration/env_infos/reward_run Max                   1.85912\n",
      "exploration/env_infos/reward_run Min                  -2.21723\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.204407\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.204407\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.204407\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.230009\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.230009\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.230009\n",
      "exploration/env_infos/reward_ctrl Mean                -0.215418\n",
      "exploration/env_infos/reward_ctrl Std                  0.0714035\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0276139\n",
      "exploration/env_infos/reward_ctrl Min                 -0.490209\n",
      "exploration/env_infos/final/height Mean               -0.12623\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.12623\n",
      "exploration/env_infos/final/height Min                -0.12623\n",
      "exploration/env_infos/initial/height Mean              0.0778134\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max               0.0778134\n",
      "exploration/env_infos/initial/height Min               0.0778134\n",
      "exploration/env_infos/height Mean                     -0.0820361\n",
      "exploration/env_infos/height Std                       0.0686552\n",
      "exploration/env_infos/height Max                       0.172241\n",
      "exploration/env_infos/height Min                      -0.293578\n",
      "exploration/env_infos/final/reward_angular Mean       -0.54264\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max        -0.54264\n",
      "exploration/env_infos/final/reward_angular Min        -0.54264\n",
      "exploration/env_infos/initial/reward_angular Mean      1.16658\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       1.16658\n",
      "exploration/env_infos/initial/reward_angular Min       1.16658\n",
      "exploration/env_infos/reward_angular Mean             -0.00660536\n",
      "exploration/env_infos/reward_angular Std               1.60173\n",
      "exploration/env_infos/reward_angular Max               4.99544\n",
      "exploration/env_infos/reward_angular Min              -5.59104\n",
      "evaluation/num steps total                         50000\n",
      "evaluation/num paths total                            50\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.0624549\n",
      "evaluation/Rewards Std                                 0.0484406\n",
      "evaluation/Rewards Max                                 1.18757\n",
      "evaluation/Rewards Min                                -1.446\n",
      "evaluation/Returns Mean                              -62.4549\n",
      "evaluation/Returns Std                                35.3486\n",
      "evaluation/Returns Max                                -4.33156\n",
      "evaluation/Returns Min                              -119.335\n",
      "evaluation/Actions Mean                               -0.109367\n",
      "evaluation/Actions Std                                 0.106566\n",
      "evaluation/Actions Max                                -0.00297282\n",
      "evaluation/Actions Min                                -0.366371\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           -62.4549\n",
      "evaluation/env_infos/final/reward_run Mean            -5.85971e-10\n",
      "evaluation/env_infos/final/reward_run Std              5.28556e-09\n",
      "evaluation/env_infos/final/reward_run Max              8.04439e-09\n",
      "evaluation/env_infos/final/reward_run Min             -2.52177e-08\n",
      "evaluation/env_infos/initial/reward_run Mean          -0.0312346\n",
      "evaluation/env_infos/initial/reward_run Std            0.116242\n",
      "evaluation/env_infos/initial/reward_run Max            0.238434\n",
      "evaluation/env_infos/initial/reward_run Min           -0.292928\n",
      "evaluation/env_infos/reward_run Mean                   9.5306e-06\n",
      "evaluation/env_infos/reward_run Std                    0.0164155\n",
      "evaluation/env_infos/reward_run Max                    0.289341\n",
      "evaluation/env_infos/reward_run Min                   -0.39162\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0139847\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.00275023\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.00846987\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.0174207\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0147147\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.00275806\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.00906524\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0183114\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0139904\n",
      "evaluation/env_infos/reward_ctrl Std                   0.00275101\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0081657\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.0191002\n",
      "evaluation/env_infos/final/height Mean                -0.123601\n",
      "evaluation/env_infos/final/height Std                  0.000370321\n",
      "evaluation/env_infos/final/height Max                 -0.123217\n",
      "evaluation/env_infos/final/height Min                 -0.124481\n",
      "evaluation/env_infos/initial/height Mean              -0.0173698\n",
      "evaluation/env_infos/initial/height Std                0.0509982\n",
      "evaluation/env_infos/initial/height Max                0.0774869\n",
      "evaluation/env_infos/initial/height Min               -0.0915047\n",
      "evaluation/env_infos/height Mean                      -0.123241\n",
      "evaluation/env_infos/height Std                        0.00501719\n",
      "evaluation/env_infos/height Max                        0.0774869\n",
      "evaluation/env_infos/height Min                       -0.139368\n",
      "evaluation/env_infos/final/reward_angular Mean        -2.99798e-09\n",
      "evaluation/env_infos/final/reward_angular Std          1.43409e-08\n",
      "evaluation/env_infos/final/reward_angular Max          3.30282e-15\n",
      "evaluation/env_infos/final/reward_angular Min         -7.3238e-08\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.6265\n",
      "evaluation/env_infos/initial/reward_angular Std        0.353308\n",
      "evaluation/env_infos/initial/reward_angular Max        1.57092\n",
      "evaluation/env_infos/initial/reward_angular Min        0.117176\n",
      "evaluation/env_infos/reward_angular Mean               0.00247391\n",
      "evaluation/env_infos/reward_angular Std                0.0567482\n",
      "evaluation/env_infos/reward_angular Max                1.71319\n",
      "evaluation/env_infos/reward_angular Min               -0.656419\n",
      "time/data storing (s)                                  0.0285556\n",
      "time/evaluation sampling (s)                          22.7944\n",
      "time/exploration sampling (s)                          1.07017\n",
      "time/logging (s)                                       0.232886\n",
      "time/saving (s)                                        0.0314507\n",
      "time/training (s)                                      3.40978\n",
      "time/epoch (s)                                        27.5672\n",
      "time/total (s)                                        59.9815\n",
      "Epoch                                                  1\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 09:54:35.540695 PDT | [gher-halfcheetahhard-SAC-10e-1000s-disc0.99_2021_05_25_09_53_11_0000--s-10] Epoch 2 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  8000\n",
      "trainer/QF1 Loss                                       0.861604\n",
      "trainer/QF2 Loss                                       0.88927\n",
      "trainer/Policy Loss                                   -6.98633\n",
      "trainer/Q1 Predictions Mean                            3.04814\n",
      "trainer/Q1 Predictions Std                             0.729165\n",
      "trainer/Q1 Predictions Max                             5.90395\n",
      "trainer/Q1 Predictions Min                             1.59853\n",
      "trainer/Q2 Predictions Mean                            3.05442\n",
      "trainer/Q2 Predictions Std                             0.656126\n",
      "trainer/Q2 Predictions Max                             5.5941\n",
      "trainer/Q2 Predictions Min                             1.71106\n",
      "trainer/Q Targets Mean                                 3.17953\n",
      "trainer/Q Targets Std                                  1.03354\n",
      "trainer/Q Targets Max                                  6.16944\n",
      "trainer/Q Targets Min                                  0.161548\n",
      "trainer/Log Pis Mean                                  -3.96173\n",
      "trainer/Log Pis Std                                    0.573706\n",
      "trainer/Log Pis Max                                   -2.43289\n",
      "trainer/Log Pis Min                                   -5.64855\n",
      "trainer/Policy mu Mean                                -0.0958589\n",
      "trainer/Policy mu Std                                  0.141019\n",
      "trainer/Policy mu Max                                  0.137192\n",
      "trainer/Policy mu Min                                 -0.56499\n",
      "trainer/Policy log std Mean                           -0.11578\n",
      "trainer/Policy log std Std                             0.0240278\n",
      "trainer/Policy log std Max                            -0.0627972\n",
      "trainer/Policy log std Min                            -0.224525\n",
      "trainer/Alpha                                          0.548046\n",
      "trainer/Alpha Loss                                    -5.96111\n",
      "exploration/num steps total                         4000\n",
      "exploration/num paths total                            4\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.0745154\n",
      "exploration/Rewards Std                                0.739179\n",
      "exploration/Rewards Max                                2.14627\n",
      "exploration/Rewards Min                               -2.53215\n",
      "exploration/Returns Mean                             -74.5154\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              -74.5154\n",
      "exploration/Returns Min                              -74.5154\n",
      "exploration/Actions Mean                              -0.0729242\n",
      "exploration/Actions Std                                0.597195\n",
      "exploration/Actions Max                                0.997958\n",
      "exploration/Actions Min                               -0.997576\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          -74.5154\n",
      "exploration/env_infos/final/reward_run Mean            0.341971\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max             0.341971\n",
      "exploration/env_infos/final/reward_run Min             0.341971\n",
      "exploration/env_infos/initial/reward_run Mean         -0.232355\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max          -0.232355\n",
      "exploration/env_infos/initial/reward_run Min          -0.232355\n",
      "exploration/env_infos/reward_run Mean                 -0.104553\n",
      "exploration/env_infos/reward_run Std                   0.682127\n",
      "exploration/env_infos/reward_run Max                   2.0714\n",
      "exploration/env_infos/reward_run Min                  -2.25643\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.160351\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.160351\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.160351\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.206376\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.206376\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.206376\n",
      "exploration/env_infos/reward_ctrl Mean                -0.217176\n",
      "exploration/env_infos/reward_ctrl Std                  0.0746391\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0294259\n",
      "exploration/env_infos/reward_ctrl Min                 -0.448791\n",
      "exploration/env_infos/final/height Mean               -0.00437659\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.00437659\n",
      "exploration/env_infos/final/height Min                -0.00437659\n",
      "exploration/env_infos/initial/height Mean             -0.0426057\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max              -0.0426057\n",
      "exploration/env_infos/initial/height Min              -0.0426057\n",
      "exploration/env_infos/height Mean                     -0.0720375\n",
      "exploration/env_infos/height Std                       0.0747466\n",
      "exploration/env_infos/height Max                       0.255081\n",
      "exploration/env_infos/height Min                      -0.269326\n",
      "exploration/env_infos/final/reward_angular Mean       -0.231004\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max        -0.231004\n",
      "exploration/env_infos/final/reward_angular Min        -0.231004\n",
      "exploration/env_infos/initial/reward_angular Mean      1.6158\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       1.6158\n",
      "exploration/env_infos/initial/reward_angular Min       1.6158\n",
      "exploration/env_infos/reward_angular Mean             -0.0136256\n",
      "exploration/env_infos/reward_angular Std               1.61417\n",
      "exploration/env_infos/reward_angular Max               5.03863\n",
      "exploration/env_infos/reward_angular Min              -5.26493\n",
      "evaluation/num steps total                         75000\n",
      "evaluation/num paths total                            75\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.0607008\n",
      "evaluation/Rewards Std                                 0.0446304\n",
      "evaluation/Rewards Max                                 1.04812\n",
      "evaluation/Rewards Min                                -1.51211\n",
      "evaluation/Returns Mean                              -60.7008\n",
      "evaluation/Returns Std                                33.4753\n",
      "evaluation/Returns Max                                -4.37191\n",
      "evaluation/Returns Min                              -113.727\n",
      "evaluation/Actions Mean                               -0.0931746\n",
      "evaluation/Actions Std                                 0.131008\n",
      "evaluation/Actions Max                                 0.0718513\n",
      "evaluation/Actions Min                                -0.409571\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           -60.7008\n",
      "evaluation/env_infos/final/reward_run Mean             3.69524e-10\n",
      "evaluation/env_infos/final/reward_run Std              5.70111e-09\n",
      "evaluation/env_infos/final/reward_run Max              2.55106e-08\n",
      "evaluation/env_infos/final/reward_run Min             -1.15782e-08\n",
      "evaluation/env_infos/initial/reward_run Mean          -0.014498\n",
      "evaluation/env_infos/initial/reward_run Std            0.116589\n",
      "evaluation/env_infos/initial/reward_run Max            0.280736\n",
      "evaluation/env_infos/initial/reward_run Min           -0.203485\n",
      "evaluation/env_infos/reward_run Mean                  -0.00022858\n",
      "evaluation/env_infos/reward_run Std                    0.0162064\n",
      "evaluation/env_infos/reward_run Max                    0.371422\n",
      "evaluation/env_infos/reward_run Min                   -0.398471\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0155091\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.00533514\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.00638621\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.0226088\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0164106\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.00542148\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.00666233\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.024384\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0155067\n",
      "evaluation/env_infos/reward_ctrl Std                   0.00533372\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0060797\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.0244445\n",
      "evaluation/env_infos/final/height Mean                -0.117982\n",
      "evaluation/env_infos/final/height Std                  0.0017881\n",
      "evaluation/env_infos/final/height Max                 -0.11579\n",
      "evaluation/env_infos/final/height Min                 -0.121401\n",
      "evaluation/env_infos/initial/height Mean              -0.00129931\n",
      "evaluation/env_infos/initial/height Std                0.0522147\n",
      "evaluation/env_infos/initial/height Max                0.0930226\n",
      "evaluation/env_infos/initial/height Min               -0.0908245\n",
      "evaluation/env_infos/height Mean                      -0.117578\n",
      "evaluation/env_infos/height Std                        0.00567728\n",
      "evaluation/env_infos/height Max                        0.0930226\n",
      "evaluation/env_infos/height Min                       -0.131078\n",
      "evaluation/env_infos/final/reward_angular Mean         7.16573e-10\n",
      "evaluation/env_infos/final/reward_angular Std          1.0781e-08\n",
      "evaluation/env_infos/final/reward_angular Max          5.0348e-08\n",
      "evaluation/env_infos/final/reward_angular Min         -1.40239e-08\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.522728\n",
      "evaluation/env_infos/initial/reward_angular Std        0.372674\n",
      "evaluation/env_infos/initial/reward_angular Max        1.80095\n",
      "evaluation/env_infos/initial/reward_angular Min       -0.0158663\n",
      "evaluation/env_infos/reward_angular Mean               0.00192798\n",
      "evaluation/env_infos/reward_angular Std                0.0483175\n",
      "evaluation/env_infos/reward_angular Max                1.80095\n",
      "evaluation/env_infos/reward_angular Min               -0.572452\n",
      "time/data storing (s)                                  0.0284062\n",
      "time/evaluation sampling (s)                          22.3817\n",
      "time/exploration sampling (s)                          1.04732\n",
      "time/logging (s)                                       0.231601\n",
      "time/saving (s)                                        0.0262327\n",
      "time/training (s)                                      3.30298\n",
      "time/epoch (s)                                        27.0182\n",
      "time/total (s)                                        87.116\n",
      "Epoch                                                  2\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 09:55:03.208782 PDT | [gher-halfcheetahhard-SAC-10e-1000s-disc0.99_2021_05_25_09_53_11_0000--s-10] Epoch 3 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  10000\n",
      "trainer/QF1 Loss                                        0.615121\n",
      "trainer/QF2 Loss                                        0.623474\n",
      "trainer/Policy Loss                                    -6.87211\n",
      "trainer/Q1 Predictions Mean                             3.00974\n",
      "trainer/Q1 Predictions Std                              0.834614\n",
      "trainer/Q1 Predictions Max                              6.88281\n",
      "trainer/Q1 Predictions Min                              1.18805\n",
      "trainer/Q2 Predictions Mean                             2.96175\n",
      "trainer/Q2 Predictions Std                              0.834242\n",
      "trainer/Q2 Predictions Max                              6.96127\n",
      "trainer/Q2 Predictions Min                              1.35025\n",
      "trainer/Q Targets Mean                                  3.00746\n",
      "trainer/Q Targets Std                                   1.11642\n",
      "trainer/Q Targets Max                                   7.02875\n",
      "trainer/Q Targets Min                                  -0.461315\n",
      "trainer/Log Pis Mean                                   -3.64744\n",
      "trainer/Log Pis Std                                     0.972519\n",
      "trainer/Log Pis Max                                     0.832376\n",
      "trainer/Log Pis Min                                    -6.10651\n",
      "trainer/Policy mu Mean                                 -0.044373\n",
      "trainer/Policy mu Std                                   0.325867\n",
      "trainer/Policy mu Max                                   1.1117\n",
      "trainer/Policy mu Min                                  -1.35822\n",
      "trainer/Policy log std Mean                            -0.115537\n",
      "trainer/Policy log std Std                              0.0480933\n",
      "trainer/Policy log std Max                             -0.014284\n",
      "trainer/Policy log std Min                             -0.308851\n",
      "trainer/Alpha                                           0.407646\n",
      "trainer/Alpha Loss                                     -8.62896\n",
      "exploration/num steps total                          5000\n",
      "exploration/num paths total                             5\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.320196\n",
      "exploration/Rewards Std                                 0.450421\n",
      "exploration/Rewards Max                                 1.57539\n",
      "exploration/Rewards Min                                -2.01526\n",
      "exploration/Returns Mean                             -320.196\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -320.196\n",
      "exploration/Returns Min                              -320.196\n",
      "exploration/Actions Mean                                0.047021\n",
      "exploration/Actions Std                                 0.596677\n",
      "exploration/Actions Max                                 0.999177\n",
      "exploration/Actions Min                                -0.998142\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -320.196\n",
      "exploration/env_infos/final/reward_run Mean             0.0116421\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.0116421\n",
      "exploration/env_infos/final/reward_run Min              0.0116421\n",
      "exploration/env_infos/initial/reward_run Mean           0.140128\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.140128\n",
      "exploration/env_infos/initial/reward_run Min            0.140128\n",
      "exploration/env_infos/reward_run Mean                  -0.0780987\n",
      "exploration/env_infos/reward_run Std                    0.603118\n",
      "exploration/env_infos/reward_run Max                    1.62113\n",
      "exploration/env_infos/reward_run Min                   -2.20067\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.110965\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.110965\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.110965\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.235929\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.235929\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.235929\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.214941\n",
      "exploration/env_infos/reward_ctrl Std                   0.0703415\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0300627\n",
      "exploration/env_infos/reward_ctrl Min                  -0.463561\n",
      "exploration/env_infos/final/height Mean                -0.541282\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.541282\n",
      "exploration/env_infos/final/height Min                 -0.541282\n",
      "exploration/env_infos/initial/height Mean              -0.0304385\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0304385\n",
      "exploration/env_infos/initial/height Min               -0.0304385\n",
      "exploration/env_infos/height Mean                      -0.396819\n",
      "exploration/env_infos/height Std                        0.221558\n",
      "exploration/env_infos/height Max                        0.244717\n",
      "exploration/env_infos/height Min                       -0.58281\n",
      "exploration/env_infos/final/reward_angular Mean        -0.533958\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -0.533958\n",
      "exploration/env_infos/final/reward_angular Min         -0.533958\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.805993\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.805993\n",
      "exploration/env_infos/initial/reward_angular Min       -0.805993\n",
      "exploration/env_infos/reward_angular Mean               0.0494973\n",
      "exploration/env_infos/reward_angular Std                1.45244\n",
      "exploration/env_infos/reward_angular Max                5.82874\n",
      "exploration/env_infos/reward_angular Min               -5.08299\n",
      "evaluation/num steps total                         100000\n",
      "evaluation/num paths total                            100\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.0721732\n",
      "evaluation/Rewards Std                                  0.0549156\n",
      "evaluation/Rewards Max                                  2.23029\n",
      "evaluation/Rewards Min                                 -1.15315\n",
      "evaluation/Returns Mean                               -72.1732\n",
      "evaluation/Returns Std                                 33.5406\n",
      "evaluation/Returns Max                                 -2.80451\n",
      "evaluation/Returns Min                               -132.27\n",
      "evaluation/Actions Mean                                -0.00564848\n",
      "evaluation/Actions Std                                  0.216188\n",
      "evaluation/Actions Max                                  0.624586\n",
      "evaluation/Actions Min                                 -0.698241\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            -72.1732\n",
      "evaluation/env_infos/final/reward_run Mean              2.37877e-09\n",
      "evaluation/env_infos/final/reward_run Std               2.29825e-08\n",
      "evaluation/env_infos/final/reward_run Max               6.37078e-08\n",
      "evaluation/env_infos/final/reward_run Min              -6.44703e-08\n",
      "evaluation/env_infos/initial/reward_run Mean            0.0201256\n",
      "evaluation/env_infos/initial/reward_run Std             0.236632\n",
      "evaluation/env_infos/initial/reward_run Max             0.413834\n",
      "evaluation/env_infos/initial/reward_run Min            -0.446391\n",
      "evaluation/env_infos/reward_run Mean                   -0.000602864\n",
      "evaluation/env_infos/reward_run Std                     0.0220782\n",
      "evaluation/env_infos/reward_run Max                     0.613636\n",
      "evaluation/env_infos/reward_run Min                    -0.571742\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0280348\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0234701\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00116065\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0745849\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0470233\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0340578\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.000954478\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.112539\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0280614\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0234818\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.000350416\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.112539\n",
      "evaluation/env_infos/final/height Mean                 -0.136275\n",
      "evaluation/env_infos/final/height Std                   0.00494495\n",
      "evaluation/env_infos/final/height Max                  -0.129729\n",
      "evaluation/env_infos/final/height Min                  -0.143688\n",
      "evaluation/env_infos/initial/height Mean               -0.0124556\n",
      "evaluation/env_infos/initial/height Std                 0.0643072\n",
      "evaluation/env_infos/initial/height Max                 0.0855688\n",
      "evaluation/env_infos/initial/height Min                -0.108862\n",
      "evaluation/env_infos/height Mean                       -0.135874\n",
      "evaluation/env_infos/height Std                         0.00779947\n",
      "evaluation/env_infos/height Max                         0.0855688\n",
      "evaluation/env_infos/height Min                        -0.167234\n",
      "evaluation/env_infos/final/reward_angular Mean         -8.38415e-09\n",
      "evaluation/env_infos/final/reward_angular Std           1.41549e-07\n",
      "evaluation/env_infos/final/reward_angular Max           2.92214e-07\n",
      "evaluation/env_infos/final/reward_angular Min          -4.17491e-07\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.32081\n",
      "evaluation/env_infos/initial/reward_angular Std         0.990875\n",
      "evaluation/env_infos/initial/reward_angular Max         3.00527\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.32222\n",
      "evaluation/env_infos/reward_angular Mean                0.000990563\n",
      "evaluation/env_infos/reward_angular Std                 0.0797978\n",
      "evaluation/env_infos/reward_angular Max                 3.00527\n",
      "evaluation/env_infos/reward_angular Min                -1.73512\n",
      "time/data storing (s)                                   0.0280484\n",
      "time/evaluation sampling (s)                           22.9254\n",
      "time/exploration sampling (s)                           1.04334\n",
      "time/logging (s)                                        0.248212\n",
      "time/saving (s)                                         0.0288302\n",
      "time/training (s)                                       3.2852\n",
      "time/epoch (s)                                         27.559\n",
      "time/total (s)                                        114.8\n",
      "Epoch                                                   3\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 09:55:31.488565 PDT | [gher-halfcheetahhard-SAC-10e-1000s-disc0.99_2021_05_25_09_53_11_0000--s-10] Epoch 4 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  12000\n",
      "trainer/QF1 Loss                                        0.637471\n",
      "trainer/QF2 Loss                                        0.636004\n",
      "trainer/Policy Loss                                    -6.86356\n",
      "trainer/Q1 Predictions Mean                             3.15402\n",
      "trainer/Q1 Predictions Std                              0.79944\n",
      "trainer/Q1 Predictions Max                              6.43922\n",
      "trainer/Q1 Predictions Min                              1.49698\n",
      "trainer/Q2 Predictions Mean                             3.04824\n",
      "trainer/Q2 Predictions Std                              0.825413\n",
      "trainer/Q2 Predictions Max                              6.02284\n",
      "trainer/Q2 Predictions Min                              1.49766\n",
      "trainer/Q Targets Mean                                  3.11392\n",
      "trainer/Q Targets Std                                   1.08899\n",
      "trainer/Q Targets Max                                   7.41976\n",
      "trainer/Q Targets Min                                   0.315072\n",
      "trainer/Log Pis Mean                                   -3.4874\n",
      "trainer/Log Pis Std                                     1.15368\n",
      "trainer/Log Pis Max                                     0.741341\n",
      "trainer/Log Pis Min                                    -7.42431\n",
      "trainer/Policy mu Mean                                 -0.00800349\n",
      "trainer/Policy mu Std                                   0.345037\n",
      "trainer/Policy mu Max                                   0.954612\n",
      "trainer/Policy mu Min                                  -1.14258\n",
      "trainer/Policy log std Mean                            -0.120852\n",
      "trainer/Policy log std Std                              0.0544201\n",
      "trainer/Policy log std Max                             -0.0353873\n",
      "trainer/Policy log std Min                             -0.37927\n",
      "trainer/Alpha                                           0.30435\n",
      "trainer/Alpha Loss                                    -11.2583\n",
      "exploration/num steps total                          6000\n",
      "exploration/num paths total                             6\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.122398\n",
      "exploration/Rewards Std                                 0.963509\n",
      "exploration/Rewards Max                                 2.79736\n",
      "exploration/Rewards Min                                -3.46888\n",
      "exploration/Returns Mean                             -122.398\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -122.398\n",
      "exploration/Returns Min                              -122.398\n",
      "exploration/Actions Mean                                0.161121\n",
      "exploration/Actions Std                                 0.592336\n",
      "exploration/Actions Max                                 0.997897\n",
      "exploration/Actions Min                                -0.994122\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -122.398\n",
      "exploration/env_infos/final/reward_run Mean             0.82197\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.82197\n",
      "exploration/env_infos/final/reward_run Min              0.82197\n",
      "exploration/env_infos/initial/reward_run Mean           0.776857\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.776857\n",
      "exploration/env_infos/initial/reward_run Min            0.776857\n",
      "exploration/env_infos/reward_run Mean                  -0.0826832\n",
      "exploration/env_infos/reward_run Std                    0.613365\n",
      "exploration/env_infos/reward_run Max                    2.2578\n",
      "exploration/env_infos/reward_run Min                   -2.11423\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.314968\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.314968\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.314968\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.320281\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.320281\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.320281\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.226093\n",
      "exploration/env_infos/reward_ctrl Std                   0.0733089\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0344251\n",
      "exploration/env_infos/reward_ctrl Min                  -0.440474\n",
      "exploration/env_infos/final/height Mean                -0.111669\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.111669\n",
      "exploration/env_infos/final/height Min                 -0.111669\n",
      "exploration/env_infos/initial/height Mean               0.0630666\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0630666\n",
      "exploration/env_infos/initial/height Min                0.0630666\n",
      "exploration/env_infos/height Mean                      -0.108144\n",
      "exploration/env_infos/height Std                        0.0853486\n",
      "exploration/env_infos/height Max                        0.187719\n",
      "exploration/env_infos/height Min                       -0.354495\n",
      "exploration/env_infos/final/reward_angular Mean        -0.210591\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -0.210591\n",
      "exploration/env_infos/final/reward_angular Min         -0.210591\n",
      "exploration/env_infos/initial/reward_angular Mean      -2.08727\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -2.08727\n",
      "exploration/env_infos/initial/reward_angular Min       -2.08727\n",
      "exploration/env_infos/reward_angular Mean              -0.0201499\n",
      "exploration/env_infos/reward_angular Std                1.50664\n",
      "exploration/env_infos/reward_angular Max                4.86963\n",
      "exploration/env_infos/reward_angular Min               -4.84567\n",
      "evaluation/num steps total                         125000\n",
      "evaluation/num paths total                            125\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.0774583\n",
      "evaluation/Rewards Std                                  0.0560421\n",
      "evaluation/Rewards Max                                  2.12653\n",
      "evaluation/Rewards Min                                 -1.16299\n",
      "evaluation/Returns Mean                               -77.4583\n",
      "evaluation/Returns Std                                 33.2105\n",
      "evaluation/Returns Max                                 -4.03266\n",
      "evaluation/Returns Min                               -137.491\n",
      "evaluation/Actions Mean                                 0.0391783\n",
      "evaluation/Actions Std                                  0.255717\n",
      "evaluation/Actions Max                                  0.626931\n",
      "evaluation/Actions Min                                 -0.744666\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            -77.4583\n",
      "evaluation/env_infos/final/reward_run Mean              1.07557e-09\n",
      "evaluation/env_infos/final/reward_run Std               1.44901e-08\n",
      "evaluation/env_infos/final/reward_run Max               3.98795e-08\n",
      "evaluation/env_infos/final/reward_run Min              -3.81557e-08\n",
      "evaluation/env_infos/initial/reward_run Mean            0.0774974\n",
      "evaluation/env_infos/initial/reward_run Std             0.256475\n",
      "evaluation/env_infos/initial/reward_run Max             0.460078\n",
      "evaluation/env_infos/initial/reward_run Min            -0.483976\n",
      "evaluation/env_infos/reward_run Mean                   -0.000538932\n",
      "evaluation/env_infos/reward_run Std                     0.0239729\n",
      "evaluation/env_infos/reward_run Max                     0.534812\n",
      "evaluation/env_infos/reward_run Min                    -0.633213\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0401616\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0302462\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0013265\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0977289\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0591961\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0416752\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.000916484\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.139948\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0401555\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0302491\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.000331367\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.139948\n",
      "evaluation/env_infos/final/height Mean                 -0.137964\n",
      "evaluation/env_infos/final/height Std                   0.00761805\n",
      "evaluation/env_infos/final/height Max                  -0.123084\n",
      "evaluation/env_infos/final/height Min                  -0.149836\n",
      "evaluation/env_infos/initial/height Mean               -0.0165285\n",
      "evaluation/env_infos/initial/height Std                 0.062448\n",
      "evaluation/env_infos/initial/height Max                 0.0868966\n",
      "evaluation/env_infos/initial/height Min                -0.108896\n",
      "evaluation/env_infos/height Mean                       -0.137539\n",
      "evaluation/env_infos/height Std                         0.00962099\n",
      "evaluation/env_infos/height Max                         0.0868966\n",
      "evaluation/env_infos/height Min                        -0.17396\n",
      "evaluation/env_infos/final/reward_angular Mean          1.67296e-08\n",
      "evaluation/env_infos/final/reward_angular Std           4.96276e-08\n",
      "evaluation/env_infos/final/reward_angular Max           2.00465e-07\n",
      "evaluation/env_infos/final/reward_angular Min          -4.59841e-08\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.183863\n",
      "evaluation/env_infos/initial/reward_angular Std         1.15057\n",
      "evaluation/env_infos/initial/reward_angular Max         2.55191\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.09026\n",
      "evaluation/env_infos/reward_angular Mean               -2.64613e-05\n",
      "evaluation/env_infos/reward_angular Std                 0.0599437\n",
      "evaluation/env_infos/reward_angular Max                 2.55191\n",
      "evaluation/env_infos/reward_angular Min                -2.09026\n",
      "time/data storing (s)                                   0.0328571\n",
      "time/evaluation sampling (s)                           23.3474\n",
      "time/exploration sampling (s)                           1.02048\n",
      "time/logging (s)                                        0.238402\n",
      "time/saving (s)                                         0.0278469\n",
      "time/training (s)                                       3.4648\n",
      "time/epoch (s)                                         28.1318\n",
      "time/total (s)                                        143.069\n",
      "Epoch                                                   4\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 09:56:00.075435 PDT | [gher-halfcheetahhard-SAC-10e-1000s-disc0.99_2021_05_25_09_53_11_0000--s-10] Epoch 5 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  14000\n",
      "trainer/QF1 Loss                                        0.436374\n",
      "trainer/QF2 Loss                                        0.453915\n",
      "trainer/Policy Loss                                    -6.56053\n",
      "trainer/Q1 Predictions Mean                             3.08042\n",
      "trainer/Q1 Predictions Std                              0.845518\n",
      "trainer/Q1 Predictions Max                              6.62725\n",
      "trainer/Q1 Predictions Min                              1.06732\n",
      "trainer/Q2 Predictions Mean                             3.1471\n",
      "trainer/Q2 Predictions Std                              0.843647\n",
      "trainer/Q2 Predictions Max                              6.69535\n",
      "trainer/Q2 Predictions Min                              1.35076\n",
      "trainer/Q Targets Mean                                  3.07741\n",
      "trainer/Q Targets Std                                   1.1143\n",
      "trainer/Q Targets Max                                   9.03059\n",
      "trainer/Q Targets Min                                   0.126627\n",
      "trainer/Log Pis Mean                                   -3.18035\n",
      "trainer/Log Pis Std                                     1.61031\n",
      "trainer/Log Pis Max                                     3.91731\n",
      "trainer/Log Pis Min                                   -10.5862\n",
      "trainer/Policy mu Mean                                  0.0963475\n",
      "trainer/Policy mu Std                                   0.468965\n",
      "trainer/Policy mu Max                                   1.2309\n",
      "trainer/Policy mu Min                                  -1.83919\n",
      "trainer/Policy log std Mean                            -0.18269\n",
      "trainer/Policy log std Std                              0.0736086\n",
      "trainer/Policy log std Max                             -0.0726937\n",
      "trainer/Policy log std Min                             -0.554472\n",
      "trainer/Alpha                                           0.228302\n",
      "trainer/Alpha Loss                                    -13.534\n",
      "exploration/num steps total                          7000\n",
      "exploration/num paths total                             7\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.123065\n",
      "exploration/Rewards Std                                 0.709555\n",
      "exploration/Rewards Max                                 2.71976\n",
      "exploration/Rewards Min                                -2.38701\n",
      "exploration/Returns Mean                             -123.065\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -123.065\n",
      "exploration/Returns Min                              -123.065\n",
      "exploration/Actions Mean                                0.233767\n",
      "exploration/Actions Std                                 0.562732\n",
      "exploration/Actions Max                                 0.998121\n",
      "exploration/Actions Min                                -0.997235\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -123.065\n",
      "exploration/env_infos/final/reward_run Mean             0.791662\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.791662\n",
      "exploration/env_infos/final/reward_run Min              0.791662\n",
      "exploration/env_infos/initial/reward_run Mean           0.404396\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.404396\n",
      "exploration/env_infos/initial/reward_run Min            0.404396\n",
      "exploration/env_infos/reward_run Mean                  -0.0940285\n",
      "exploration/env_infos/reward_run Std                    0.6683\n",
      "exploration/env_infos/reward_run Max                    1.74842\n",
      "exploration/env_infos/reward_run Min                   -2.39395\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.372815\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.372815\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.372815\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.157264\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.157264\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.157264\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.222789\n",
      "exploration/env_infos/reward_ctrl Std                   0.0740233\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0454037\n",
      "exploration/env_infos/reward_ctrl Min                  -0.47362\n",
      "exploration/env_infos/final/height Mean                -0.160636\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.160636\n",
      "exploration/env_infos/final/height Min                 -0.160636\n",
      "exploration/env_infos/initial/height Mean              -0.0667489\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0667489\n",
      "exploration/env_infos/initial/height Min               -0.0667489\n",
      "exploration/env_infos/height Mean                      -0.109853\n",
      "exploration/env_infos/height Std                        0.082034\n",
      "exploration/env_infos/height Max                        0.154443\n",
      "exploration/env_infos/height Min                       -0.380596\n",
      "exploration/env_infos/final/reward_angular Mean        -1.35949\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -1.35949\n",
      "exploration/env_infos/final/reward_angular Min         -1.35949\n",
      "exploration/env_infos/initial/reward_angular Mean       1.16907\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        1.16907\n",
      "exploration/env_infos/initial/reward_angular Min        1.16907\n",
      "exploration/env_infos/reward_angular Mean              -0.0147039\n",
      "exploration/env_infos/reward_angular Std                1.46172\n",
      "exploration/env_infos/reward_angular Max                4.90829\n",
      "exploration/env_infos/reward_angular Min               -6.41065\n",
      "evaluation/num steps total                         150000\n",
      "evaluation/num paths total                            150\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.0973034\n",
      "evaluation/Rewards Std                                  0.0758937\n",
      "evaluation/Rewards Max                                  2.10115\n",
      "evaluation/Rewards Min                                 -1.39821\n",
      "evaluation/Returns Mean                               -97.3034\n",
      "evaluation/Returns Std                                 34.6976\n",
      "evaluation/Returns Max                                -11.7539\n",
      "evaluation/Returns Min                               -162.854\n",
      "evaluation/Actions Mean                                 0.0835118\n",
      "evaluation/Actions Std                                  0.353318\n",
      "evaluation/Actions Max                                  0.818634\n",
      "evaluation/Actions Min                                 -0.874053\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            -97.3034\n",
      "evaluation/env_infos/final/reward_run Mean              0.0075389\n",
      "evaluation/env_infos/final/reward_run Std               0.0650528\n",
      "evaluation/env_infos/final/reward_run Max               0.274481\n",
      "evaluation/env_infos/final/reward_run Min              -0.161704\n",
      "evaluation/env_infos/initial/reward_run Mean            0.145964\n",
      "evaluation/env_infos/initial/reward_run Std             0.273031\n",
      "evaluation/env_infos/initial/reward_run Max             0.585393\n",
      "evaluation/env_infos/initial/reward_run Min            -0.368844\n",
      "evaluation/env_infos/reward_run Mean                    0.00182883\n",
      "evaluation/env_infos/reward_run Std                     0.0732415\n",
      "evaluation/env_infos/reward_run Max                     0.735039\n",
      "evaluation/env_infos/reward_run Min                    -0.715143\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0790789\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0512944\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00893994\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.175969\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.098949\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0616017\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0102744\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.222314\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0790846\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0513032\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00293882\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.222314\n",
      "evaluation/env_infos/final/height Mean                 -0.148458\n",
      "evaluation/env_infos/final/height Std                   0.0175694\n",
      "evaluation/env_infos/final/height Max                  -0.115554\n",
      "evaluation/env_infos/final/height Min                  -0.179734\n",
      "evaluation/env_infos/initial/height Mean               -0.0113718\n",
      "evaluation/env_infos/initial/height Std                 0.0553132\n",
      "evaluation/env_infos/initial/height Max                 0.0906347\n",
      "evaluation/env_infos/initial/height Min                -0.0952789\n",
      "evaluation/env_infos/height Mean                       -0.148902\n",
      "evaluation/env_infos/height Std                         0.0185141\n",
      "evaluation/env_infos/height Max                         0.0906347\n",
      "evaluation/env_infos/height Min                        -0.214579\n",
      "evaluation/env_infos/final/reward_angular Mean          0.0165929\n",
      "evaluation/env_infos/final/reward_angular Std           0.0921324\n",
      "evaluation/env_infos/final/reward_angular Max           0.41005\n",
      "evaluation/env_infos/final/reward_angular Min          -0.157207\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.110189\n",
      "evaluation/env_infos/initial/reward_angular Std         1.36044\n",
      "evaluation/env_infos/initial/reward_angular Max         2.89778\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.9119\n",
      "evaluation/env_infos/reward_angular Mean                0.000225143\n",
      "evaluation/env_infos/reward_angular Std                 0.116177\n",
      "evaluation/env_infos/reward_angular Max                 2.89778\n",
      "evaluation/env_infos/reward_angular Min                -1.98608\n",
      "time/data storing (s)                                   0.0311574\n",
      "time/evaluation sampling (s)                           23.3361\n",
      "time/exploration sampling (s)                           1.25883\n",
      "time/logging (s)                                        0.22928\n",
      "time/saving (s)                                         0.0281329\n",
      "time/training (s)                                       3.54298\n",
      "time/epoch (s)                                         28.4264\n",
      "time/total (s)                                        171.646\n",
      "Epoch                                                   5\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 09:56:28.070746 PDT | [gher-halfcheetahhard-SAC-10e-1000s-disc0.99_2021_05_25_09_53_11_0000--s-10] Epoch 6 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  16000\n",
      "trainer/QF1 Loss                                        0.519998\n",
      "trainer/QF2 Loss                                        0.582024\n",
      "trainer/Policy Loss                                    -6.22224\n",
      "trainer/Q1 Predictions Mean                             3.11674\n",
      "trainer/Q1 Predictions Std                              1.15703\n",
      "trainer/Q1 Predictions Max                              7.25252\n",
      "trainer/Q1 Predictions Min                              1.08608\n",
      "trainer/Q2 Predictions Mean                             3.14922\n",
      "trainer/Q2 Predictions Std                              1.13123\n",
      "trainer/Q2 Predictions Max                              6.99562\n",
      "trainer/Q2 Predictions Min                              1.17361\n",
      "trainer/Q Targets Mean                                  3.15433\n",
      "trainer/Q Targets Std                                   1.29413\n",
      "trainer/Q Targets Max                                   8.56962\n",
      "trainer/Q Targets Min                                  -0.772606\n",
      "trainer/Log Pis Mean                                   -2.77802\n",
      "trainer/Log Pis Std                                     1.77605\n",
      "trainer/Log Pis Max                                     2.60474\n",
      "trainer/Log Pis Min                                    -8.42073\n",
      "trainer/Policy mu Mean                                  0.116364\n",
      "trainer/Policy mu Std                                   0.576353\n",
      "trainer/Policy mu Max                                   1.42148\n",
      "trainer/Policy mu Min                                  -1.64645\n",
      "trainer/Policy log std Mean                            -0.206473\n",
      "trainer/Policy log std Std                              0.0972257\n",
      "trainer/Policy log std Max                             -0.00426184\n",
      "trainer/Policy log std Min                             -0.471339\n",
      "trainer/Alpha                                           0.172858\n",
      "trainer/Alpha Loss                                    -15.3839\n",
      "exploration/num steps total                          8000\n",
      "exploration/num paths total                             8\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.0922026\n",
      "exploration/Rewards Std                                 1.00373\n",
      "exploration/Rewards Max                                 3.20852\n",
      "exploration/Rewards Min                                -3.42042\n",
      "exploration/Returns Mean                              -92.2026\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               -92.2026\n",
      "exploration/Returns Min                               -92.2026\n",
      "exploration/Actions Mean                                0.391198\n",
      "exploration/Actions Std                                 0.548287\n",
      "exploration/Actions Max                                 0.998686\n",
      "exploration/Actions Min                                -0.984407\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           -92.2026\n",
      "exploration/env_infos/final/reward_run Mean            -0.167623\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.167623\n",
      "exploration/env_infos/final/reward_run Min             -0.167623\n",
      "exploration/env_infos/initial/reward_run Mean           0.218324\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.218324\n",
      "exploration/env_infos/initial/reward_run Min            0.218324\n",
      "exploration/env_infos/reward_run Mean                  -0.0456324\n",
      "exploration/env_infos/reward_run Std                    0.443611\n",
      "exploration/env_infos/reward_run Max                    1.37514\n",
      "exploration/env_infos/reward_run Min                   -1.59531\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.303342\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.303342\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.303342\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.144693\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.144693\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.144693\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.272193\n",
      "exploration/env_infos/reward_ctrl Std                   0.0745709\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0474698\n",
      "exploration/env_infos/reward_ctrl Min                  -0.490725\n",
      "exploration/env_infos/final/height Mean                -0.251261\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.251261\n",
      "exploration/env_infos/final/height Min                 -0.251261\n",
      "exploration/env_infos/initial/height Mean              -0.0690564\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0690564\n",
      "exploration/env_infos/initial/height Min               -0.0690564\n",
      "exploration/env_infos/height Mean                      -0.184197\n",
      "exploration/env_infos/height Std                        0.0771391\n",
      "exploration/env_infos/height Max                        0.0183199\n",
      "exploration/env_infos/height Min                       -0.429121\n",
      "exploration/env_infos/final/reward_angular Mean        -1.21857\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -1.21857\n",
      "exploration/env_infos/final/reward_angular Min         -1.21857\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.27611\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.27611\n",
      "exploration/env_infos/initial/reward_angular Min       -1.27611\n",
      "exploration/env_infos/reward_angular Mean              -0.00732463\n",
      "exploration/env_infos/reward_angular Std                1.07178\n",
      "exploration/env_infos/reward_angular Max                3.61907\n",
      "exploration/env_infos/reward_angular Min               -3.44595\n",
      "evaluation/num steps total                         175000\n",
      "evaluation/num paths total                            175\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.115497\n",
      "evaluation/Rewards Std                                  0.0797829\n",
      "evaluation/Rewards Max                                  2.86577\n",
      "evaluation/Rewards Min                                 -0.823058\n",
      "evaluation/Returns Mean                              -115.497\n",
      "evaluation/Returns Std                                 44.7576\n",
      "evaluation/Returns Max                                -31.733\n",
      "evaluation/Returns Min                               -205.872\n",
      "evaluation/Actions Mean                                 0.152449\n",
      "evaluation/Actions Std                                  0.426116\n",
      "evaluation/Actions Max                                  0.873049\n",
      "evaluation/Actions Min                                 -0.840488\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -115.497\n",
      "evaluation/env_infos/final/reward_run Mean             -0.00936607\n",
      "evaluation/env_infos/final/reward_run Std               0.0459125\n",
      "evaluation/env_infos/final/reward_run Max               9.19454e-05\n",
      "evaluation/env_infos/final/reward_run Min              -0.23429\n",
      "evaluation/env_infos/initial/reward_run Mean            0.172792\n",
      "evaluation/env_infos/initial/reward_run Std             0.391682\n",
      "evaluation/env_infos/initial/reward_run Max             0.765095\n",
      "evaluation/env_infos/initial/reward_run Min            -0.653593\n",
      "evaluation/env_infos/reward_run Mean                    0.000123428\n",
      "evaluation/env_infos/reward_run Std                     0.0500193\n",
      "evaluation/env_infos/reward_run Max                     0.886876\n",
      "evaluation/env_infos/reward_run Min                    -0.787383\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.123116\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0627097\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0104075\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.230465\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.126281\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0571603\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0191099\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.214526\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.122889\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0628883\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0062355\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.236508\n",
      "evaluation/env_infos/final/height Mean                 -0.159795\n",
      "evaluation/env_infos/final/height Std                   0.0152839\n",
      "evaluation/env_infos/final/height Max                  -0.129117\n",
      "evaluation/env_infos/final/height Min                  -0.183226\n",
      "evaluation/env_infos/initial/height Mean               -0.0198125\n",
      "evaluation/env_infos/initial/height Std                 0.0570266\n",
      "evaluation/env_infos/initial/height Max                 0.077624\n",
      "evaluation/env_infos/initial/height Min                -0.101376\n",
      "evaluation/env_infos/height Mean                       -0.159461\n",
      "evaluation/env_infos/height Std                         0.0168246\n",
      "evaluation/env_infos/height Max                         0.077624\n",
      "evaluation/env_infos/height Min                        -0.225448\n",
      "evaluation/env_infos/final/reward_angular Mean          0.0061414\n",
      "evaluation/env_infos/final/reward_angular Std           0.0301368\n",
      "evaluation/env_infos/final/reward_angular Max           0.153781\n",
      "evaluation/env_infos/final/reward_angular Min          -0.000122843\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.317415\n",
      "evaluation/env_infos/initial/reward_angular Std         1.61398\n",
      "evaluation/env_infos/initial/reward_angular Max         3.43035\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.15392\n",
      "evaluation/env_infos/reward_angular Mean               -0.000121744\n",
      "evaluation/env_infos/reward_angular Std                 0.102804\n",
      "evaluation/env_infos/reward_angular Max                 3.43035\n",
      "evaluation/env_infos/reward_angular Min                -2.15392\n",
      "time/data storing (s)                                   0.0286554\n",
      "time/evaluation sampling (s)                           23.1411\n",
      "time/exploration sampling (s)                           1.01373\n",
      "time/logging (s)                                        0.241005\n",
      "time/saving (s)                                         0.0278743\n",
      "time/training (s)                                       3.40344\n",
      "time/epoch (s)                                         27.8558\n",
      "time/total (s)                                        199.652\n",
      "Epoch                                                   6\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 09:56:57.813239 PDT | [gher-halfcheetahhard-SAC-10e-1000s-disc0.99_2021_05_25_09_53_11_0000--s-10] Epoch 7 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  18000\n",
      "trainer/QF1 Loss                                        0.656282\n",
      "trainer/QF2 Loss                                        0.768053\n",
      "trainer/Policy Loss                                    -6.0697\n",
      "trainer/Q1 Predictions Mean                             3.1968\n",
      "trainer/Q1 Predictions Std                              1.12377\n",
      "trainer/Q1 Predictions Max                              9.12399\n",
      "trainer/Q1 Predictions Min                              1.21287\n",
      "trainer/Q2 Predictions Mean                             3.14261\n",
      "trainer/Q2 Predictions Std                              1.15483\n",
      "trainer/Q2 Predictions Max                              9.49885\n",
      "trainer/Q2 Predictions Min                              1.05354\n",
      "trainer/Q Targets Mean                                  3.24032\n",
      "trainer/Q Targets Std                                   1.53054\n",
      "trainer/Q Targets Max                                  11.3606\n",
      "trainer/Q Targets Min                                  -0.617253\n",
      "trainer/Log Pis Mean                                   -2.56937\n",
      "trainer/Log Pis Std                                     2.00432\n",
      "trainer/Log Pis Max                                     4.15586\n",
      "trainer/Log Pis Min                                    -8.08714\n",
      "trainer/Policy mu Mean                                  0.21702\n",
      "trainer/Policy mu Std                                   0.587149\n",
      "trainer/Policy mu Max                                   2.5408\n",
      "trainer/Policy mu Min                                  -2.04933\n",
      "trainer/Policy log std Mean                            -0.216859\n",
      "trainer/Policy log std Std                              0.105988\n",
      "trainer/Policy log std Max                              0.00890229\n",
      "trainer/Policy log std Min                             -0.643069\n",
      "trainer/Alpha                                           0.132142\n",
      "trainer/Alpha Loss                                    -17.3207\n",
      "exploration/num steps total                          9000\n",
      "exploration/num paths total                             9\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.329716\n",
      "exploration/Rewards Std                                 0.924999\n",
      "exploration/Rewards Max                                 3.43165\n",
      "exploration/Rewards Min                                -3.14463\n",
      "exploration/Returns Mean                             -329.716\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -329.716\n",
      "exploration/Returns Min                              -329.716\n",
      "exploration/Actions Mean                               -0.0461077\n",
      "exploration/Actions Std                                 0.605487\n",
      "exploration/Actions Max                                 0.996356\n",
      "exploration/Actions Min                                -0.996813\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -329.716\n",
      "exploration/env_infos/final/reward_run Mean             0.211197\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.211197\n",
      "exploration/env_infos/final/reward_run Min              0.211197\n",
      "exploration/env_infos/initial/reward_run Mean          -0.00880407\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.00880407\n",
      "exploration/env_infos/initial/reward_run Min           -0.00880407\n",
      "exploration/env_infos/reward_run Mean                  -0.135969\n",
      "exploration/env_infos/reward_run Std                    0.621936\n",
      "exploration/env_infos/reward_run Max                    1.63567\n",
      "exploration/env_infos/reward_run Min                   -2.14806\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.298649\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.298649\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.298649\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.117748\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.117748\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.117748\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.221244\n",
      "exploration/env_infos/reward_ctrl Std                   0.0757463\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0351495\n",
      "exploration/env_infos/reward_ctrl Min                  -0.445601\n",
      "exploration/env_infos/final/height Mean                -0.538581\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.538581\n",
      "exploration/env_infos/final/height Min                 -0.538581\n",
      "exploration/env_infos/initial/height Mean               0.0806145\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0806145\n",
      "exploration/env_infos/initial/height Min                0.0806145\n",
      "exploration/env_infos/height Mean                      -0.371709\n",
      "exploration/env_infos/height Std                        0.250813\n",
      "exploration/env_infos/height Max                        0.323484\n",
      "exploration/env_infos/height Min                       -0.585118\n",
      "exploration/env_infos/final/reward_angular Mean         1.75871\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          1.75871\n",
      "exploration/env_infos/final/reward_angular Min          1.75871\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.463005\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.463005\n",
      "exploration/env_infos/initial/reward_angular Min       -0.463005\n",
      "exploration/env_infos/reward_angular Mean               0.0411388\n",
      "exploration/env_infos/reward_angular Std                1.57534\n",
      "exploration/env_infos/reward_angular Max                6.17774\n",
      "exploration/env_infos/reward_angular Min               -4.82282\n",
      "evaluation/num steps total                         200000\n",
      "evaluation/num paths total                            200\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.139393\n",
      "evaluation/Rewards Std                                  0.22098\n",
      "evaluation/Rewards Max                                  3.94529\n",
      "evaluation/Rewards Min                                 -1.57726\n",
      "evaluation/Returns Mean                              -139.393\n",
      "evaluation/Returns Std                                 73.9585\n",
      "evaluation/Returns Max                                 -5.59559\n",
      "evaluation/Returns Min                               -333.342\n",
      "evaluation/Actions Mean                                 0.209424\n",
      "evaluation/Actions Std                                  0.407985\n",
      "evaluation/Actions Max                                  0.953663\n",
      "evaluation/Actions Min                                 -0.918337\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -139.393\n",
      "evaluation/env_infos/final/reward_run Mean             -0.00951095\n",
      "evaluation/env_infos/final/reward_run Std               0.18668\n",
      "evaluation/env_infos/final/reward_run Max               0.418499\n",
      "evaluation/env_infos/final/reward_run Min              -0.671277\n",
      "evaluation/env_infos/initial/reward_run Mean            0.270129\n",
      "evaluation/env_infos/initial/reward_run Std             0.338098\n",
      "evaluation/env_infos/initial/reward_run Max             0.814799\n",
      "evaluation/env_infos/initial/reward_run Min            -0.383696\n",
      "evaluation/env_infos/reward_run Mean                    0.0122109\n",
      "evaluation/env_infos/reward_run Std                     0.164731\n",
      "evaluation/env_infos/reward_run Max                     1.18312\n",
      "evaluation/env_infos/reward_run Min                    -1.66609\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.126804\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0633344\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00307788\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.233009\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.144674\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0497387\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0343471\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.220012\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.126186\n",
      "evaluation/env_infos/reward_ctrl Std                    0.062696\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00200721\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.30551\n",
      "evaluation/env_infos/final/height Mean                 -0.223276\n",
      "evaluation/env_infos/final/height Std                   0.14316\n",
      "evaluation/env_infos/final/height Max                  -0.102799\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean               -0.00936038\n",
      "evaluation/env_infos/initial/height Std                 0.054937\n",
      "evaluation/env_infos/initial/height Max                 0.0819984\n",
      "evaluation/env_infos/initial/height Min                -0.0995274\n",
      "evaluation/env_infos/height Mean                       -0.22058\n",
      "evaluation/env_infos/height Std                         0.139622\n",
      "evaluation/env_infos/height Max                         0.289454\n",
      "evaluation/env_infos/height Min                        -0.586124\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.113453\n",
      "evaluation/env_infos/final/reward_angular Std           0.581444\n",
      "evaluation/env_infos/final/reward_angular Max           0.492126\n",
      "evaluation/env_infos/final/reward_angular Min          -2.72839\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.433023\n",
      "evaluation/env_infos/initial/reward_angular Std         1.34152\n",
      "evaluation/env_infos/initial/reward_angular Max         2.786\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.83347\n",
      "evaluation/env_infos/reward_angular Mean                0.00913997\n",
      "evaluation/env_infos/reward_angular Std                 0.483017\n",
      "evaluation/env_infos/reward_angular Max                 4.39015\n",
      "evaluation/env_infos/reward_angular Min                -3.43343\n",
      "time/data storing (s)                                   0.0320901\n",
      "time/evaluation sampling (s)                           23.7527\n",
      "time/exploration sampling (s)                           1.09108\n",
      "time/logging (s)                                        0.240035\n",
      "time/saving (s)                                         0.0276975\n",
      "time/training (s)                                       4.42975\n",
      "time/epoch (s)                                         29.5733\n",
      "time/total (s)                                        229.393\n",
      "Epoch                                                   7\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 09:57:25.651645 PDT | [gher-halfcheetahhard-SAC-10e-1000s-disc0.99_2021_05_25_09_53_11_0000--s-10] Epoch 8 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  20000\n",
      "trainer/QF1 Loss                                        0.535525\n",
      "trainer/QF2 Loss                                        0.591329\n",
      "trainer/Policy Loss                                    -5.5509\n",
      "trainer/Q1 Predictions Mean                             3.21789\n",
      "trainer/Q1 Predictions Std                              1.44796\n",
      "trainer/Q1 Predictions Max                              9.24395\n",
      "trainer/Q1 Predictions Min                              0.393115\n",
      "trainer/Q2 Predictions Mean                             3.24091\n",
      "trainer/Q2 Predictions Std                              1.49067\n",
      "trainer/Q2 Predictions Max                             10.1086\n",
      "trainer/Q2 Predictions Min                              0.327907\n",
      "trainer/Q Targets Mean                                  3.32276\n",
      "trainer/Q Targets Std                                   1.69661\n",
      "trainer/Q Targets Max                                   9.17699\n",
      "trainer/Q Targets Min                                  -1.0701\n",
      "trainer/Log Pis Mean                                   -2.03721\n",
      "trainer/Log Pis Std                                     2.23536\n",
      "trainer/Log Pis Max                                     7.14745\n",
      "trainer/Log Pis Min                                    -7.24488\n",
      "trainer/Policy mu Mean                                  0.181471\n",
      "trainer/Policy mu Std                                   0.660782\n",
      "trainer/Policy mu Max                                   3.04155\n",
      "trainer/Policy mu Min                                  -2.04848\n",
      "trainer/Policy log std Mean                            -0.240139\n",
      "trainer/Policy log std Std                              0.111577\n",
      "trainer/Policy log std Max                              0.0544966\n",
      "trainer/Policy log std Min                             -0.600691\n",
      "trainer/Alpha                                           0.101136\n",
      "trainer/Alpha Loss                                    -18.3941\n",
      "exploration/num steps total                         10000\n",
      "exploration/num paths total                            10\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.459925\n",
      "exploration/Rewards Std                                 0.362179\n",
      "exploration/Rewards Max                                 1.20304\n",
      "exploration/Rewards Min                                -1.71892\n",
      "exploration/Returns Mean                             -459.925\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -459.925\n",
      "exploration/Returns Min                              -459.925\n",
      "exploration/Actions Mean                                0.0307236\n",
      "exploration/Actions Std                                 0.601817\n",
      "exploration/Actions Max                                 0.997951\n",
      "exploration/Actions Min                                -0.998531\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -459.925\n",
      "exploration/env_infos/final/reward_run Mean            -0.707363\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.707363\n",
      "exploration/env_infos/final/reward_run Min             -0.707363\n",
      "exploration/env_infos/initial/reward_run Mean           0.927682\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.927682\n",
      "exploration/env_infos/initial/reward_run Min            0.927682\n",
      "exploration/env_infos/reward_run Mean                  -0.0675631\n",
      "exploration/env_infos/reward_run Std                    0.574301\n",
      "exploration/env_infos/reward_run Max                    2.18542\n",
      "exploration/env_infos/reward_run Min                   -1.88061\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.179215\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.179215\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.179215\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.27624\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.27624\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.27624\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.217877\n",
      "exploration/env_infos/reward_ctrl Std                   0.073786\n",
      "exploration/env_infos/reward_ctrl Max                  -0.029139\n",
      "exploration/env_infos/reward_ctrl Min                  -0.450647\n",
      "exploration/env_infos/final/height Mean                -0.533057\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.533057\n",
      "exploration/env_infos/final/height Min                 -0.533057\n",
      "exploration/env_infos/initial/height Mean              -0.0328815\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0328815\n",
      "exploration/env_infos/initial/height Min               -0.0328815\n",
      "exploration/env_infos/height Mean                      -0.469089\n",
      "exploration/env_infos/height Std                        0.178685\n",
      "exploration/env_infos/height Max                        0.318771\n",
      "exploration/env_infos/height Min                       -0.586103\n",
      "exploration/env_infos/final/reward_angular Mean        -0.505344\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -0.505344\n",
      "exploration/env_infos/final/reward_angular Min         -0.505344\n",
      "exploration/env_infos/initial/reward_angular Mean       1.65628\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        1.65628\n",
      "exploration/env_infos/initial/reward_angular Min        1.65628\n",
      "exploration/env_infos/reward_angular Mean               0.0401901\n",
      "exploration/env_infos/reward_angular Std                1.34083\n",
      "exploration/env_infos/reward_angular Max                6.15586\n",
      "exploration/env_infos/reward_angular Min               -5.063\n",
      "evaluation/num steps total                         225000\n",
      "evaluation/num paths total                            225\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.149642\n",
      "evaluation/Rewards Std                                  0.236788\n",
      "evaluation/Rewards Max                                  2.91164\n",
      "evaluation/Rewards Min                                 -4.17869\n",
      "evaluation/Returns Mean                              -149.642\n",
      "evaluation/Returns Std                                 74.8189\n",
      "evaluation/Returns Max                                -15.0182\n",
      "evaluation/Returns Min                               -350.691\n",
      "evaluation/Actions Mean                                 0.222124\n",
      "evaluation/Actions Std                                  0.447814\n",
      "evaluation/Actions Max                                  0.996018\n",
      "evaluation/Actions Min                                 -0.979624\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -149.642\n",
      "evaluation/env_infos/final/reward_run Mean              0.0119837\n",
      "evaluation/env_infos/final/reward_run Std               0.058612\n",
      "evaluation/env_infos/final/reward_run Max               0.299123\n",
      "evaluation/env_infos/final/reward_run Min              -1.44056e-07\n",
      "evaluation/env_infos/initial/reward_run Mean            0.172573\n",
      "evaluation/env_infos/initial/reward_run Std             0.371222\n",
      "evaluation/env_infos/initial/reward_run Max             0.67715\n",
      "evaluation/env_infos/initial/reward_run Min            -0.572052\n",
      "evaluation/env_infos/reward_run Mean                   -0.0269154\n",
      "evaluation/env_infos/reward_run Std                     0.229638\n",
      "evaluation/env_infos/reward_run Max                     1.49933\n",
      "evaluation/env_infos/reward_run Min                    -3.1031\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.146432\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0864689\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0217932\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.397346\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.170843\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0667293\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0363674\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.285599\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.149926\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0854835\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00683301\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.418051\n",
      "evaluation/env_infos/final/height Mean                 -0.208689\n",
      "evaluation/env_infos/final/height Std                   0.154067\n",
      "evaluation/env_infos/final/height Max                   0.079541\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean               -0.0305602\n",
      "evaluation/env_infos/initial/height Std                 0.0383933\n",
      "evaluation/env_infos/initial/height Max                 0.048797\n",
      "evaluation/env_infos/initial/height Min                -0.0863675\n",
      "evaluation/env_infos/height Mean                       -0.20447\n",
      "evaluation/env_infos/height Std                         0.15112\n",
      "evaluation/env_infos/height Max                         0.383101\n",
      "evaluation/env_infos/height Min                        -0.582914\n",
      "evaluation/env_infos/final/reward_angular Mean          0.000320647\n",
      "evaluation/env_infos/final/reward_angular Std           0.00283386\n",
      "evaluation/env_infos/final/reward_angular Max           0.01326\n",
      "evaluation/env_infos/final/reward_angular Min          -0.00524515\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.376187\n",
      "evaluation/env_infos/initial/reward_angular Std         1.70369\n",
      "evaluation/env_infos/initial/reward_angular Max         3.40581\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.2893\n",
      "evaluation/env_infos/reward_angular Mean                0.00995911\n",
      "evaluation/env_infos/reward_angular Std                 0.295458\n",
      "evaluation/env_infos/reward_angular Max                 5.86887\n",
      "evaluation/env_infos/reward_angular Min                -4.06232\n",
      "time/data storing (s)                                   0.029389\n",
      "time/evaluation sampling (s)                           22.3322\n",
      "time/exploration sampling (s)                           1.04966\n",
      "time/logging (s)                                        0.235793\n",
      "time/saving (s)                                         0.0315111\n",
      "time/training (s)                                       3.98863\n",
      "time/epoch (s)                                         27.6671\n",
      "time/total (s)                                        257.226\n",
      "Epoch                                                   8\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 09:57:53.245693 PDT | [gher-halfcheetahhard-SAC-10e-1000s-disc0.99_2021_05_25_09_53_11_0000--s-10] Epoch 9 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  22000\n",
      "trainer/QF1 Loss                                        0.587825\n",
      "trainer/QF2 Loss                                        0.657973\n",
      "trainer/Policy Loss                                    -4.80865\n",
      "trainer/Q1 Predictions Mean                             3.1652\n",
      "trainer/Q1 Predictions Std                              1.40976\n",
      "trainer/Q1 Predictions Max                              8.60939\n",
      "trainer/Q1 Predictions Min                              0.279964\n",
      "trainer/Q2 Predictions Mean                             3.11256\n",
      "trainer/Q2 Predictions Std                              1.40854\n",
      "trainer/Q2 Predictions Max                              8.1646\n",
      "trainer/Q2 Predictions Min                              0.480346\n",
      "trainer/Q Targets Mean                                  3.06475\n",
      "trainer/Q Targets Std                                   1.6122\n",
      "trainer/Q Targets Max                                   7.89965\n",
      "trainer/Q Targets Min                                  -1.74136\n",
      "trainer/Log Pis Mean                                   -1.33771\n",
      "trainer/Log Pis Std                                     2.81695\n",
      "trainer/Log Pis Max                                    11.0419\n",
      "trainer/Log Pis Min                                    -8.07828\n",
      "trainer/Policy mu Mean                                  0.0963795\n",
      "trainer/Policy mu Std                                   0.796898\n",
      "trainer/Policy mu Max                                   2.84164\n",
      "trainer/Policy mu Min                                  -3.78032\n",
      "trainer/Policy log std Mean                            -0.289994\n",
      "trainer/Policy log std Std                              0.140415\n",
      "trainer/Policy log std Max                              0.0152031\n",
      "trainer/Policy log std Min                             -0.803131\n",
      "trainer/Alpha                                           0.0777442\n",
      "trainer/Alpha Loss                                    -18.7241\n",
      "exploration/num steps total                         11000\n",
      "exploration/num paths total                            11\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.336346\n",
      "exploration/Rewards Std                                 1.01894\n",
      "exploration/Rewards Max                                 3.09086\n",
      "exploration/Rewards Min                                -3.96911\n",
      "exploration/Returns Mean                             -336.346\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -336.346\n",
      "exploration/Returns Min                              -336.346\n",
      "exploration/Actions Mean                               -0.102477\n",
      "exploration/Actions Std                                 0.642018\n",
      "exploration/Actions Max                                 0.998491\n",
      "exploration/Actions Min                                -0.999916\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -336.346\n",
      "exploration/env_infos/final/reward_run Mean             0.183367\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.183367\n",
      "exploration/env_infos/final/reward_run Min              0.183367\n",
      "exploration/env_infos/initial/reward_run Mean           0.400525\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.400525\n",
      "exploration/env_infos/initial/reward_run Min            0.400525\n",
      "exploration/env_infos/reward_run Mean                  -0.0726847\n",
      "exploration/env_infos/reward_run Std                    0.739994\n",
      "exploration/env_infos/reward_run Max                    2.12668\n",
      "exploration/env_infos/reward_run Min                   -2.14916\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.318291\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.318291\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.318291\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.211834\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.211834\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.211834\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.253613\n",
      "exploration/env_infos/reward_ctrl Std                   0.0927475\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0483661\n",
      "exploration/env_infos/reward_ctrl Min                  -0.530523\n",
      "exploration/env_infos/final/height Mean                -0.516758\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.516758\n",
      "exploration/env_infos/final/height Min                 -0.516758\n",
      "exploration/env_infos/initial/height Mean               0.0258082\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0258082\n",
      "exploration/env_infos/initial/height Min                0.0258082\n",
      "exploration/env_infos/height Mean                      -0.474996\n",
      "exploration/env_infos/height Std                        0.169148\n",
      "exploration/env_infos/height Max                        0.392361\n",
      "exploration/env_infos/height Min                       -0.584605\n",
      "exploration/env_infos/final/reward_angular Mean         0.509825\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.509825\n",
      "exploration/env_infos/final/reward_angular Min          0.509825\n",
      "exploration/env_infos/initial/reward_angular Mean       0.243747\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        0.243747\n",
      "exploration/env_infos/initial/reward_angular Min        0.243747\n",
      "exploration/env_infos/reward_angular Mean               0.099761\n",
      "exploration/env_infos/reward_angular Std                1.54018\n",
      "exploration/env_infos/reward_angular Max                4.89841\n",
      "exploration/env_infos/reward_angular Min               -5.51368\n",
      "evaluation/num steps total                         250000\n",
      "evaluation/num paths total                            250\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.165623\n",
      "evaluation/Rewards Std                                  0.192629\n",
      "evaluation/Rewards Max                                  3.46726\n",
      "evaluation/Rewards Min                                 -2.33887\n",
      "evaluation/Returns Mean                              -165.623\n",
      "evaluation/Returns Std                                 69.1665\n",
      "evaluation/Returns Max                                -24.2011\n",
      "evaluation/Returns Min                               -325.344\n",
      "evaluation/Actions Mean                                 0.259898\n",
      "evaluation/Actions Std                                  0.51091\n",
      "evaluation/Actions Max                                  0.986213\n",
      "evaluation/Actions Min                                 -0.998097\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -165.623\n",
      "evaluation/env_infos/final/reward_run Mean             -0.0425428\n",
      "evaluation/env_infos/final/reward_run Std               0.189211\n",
      "evaluation/env_infos/final/reward_run Max               1.10758e-05\n",
      "evaluation/env_infos/final/reward_run Min              -0.964611\n",
      "evaluation/env_infos/initial/reward_run Mean            0.062451\n",
      "evaluation/env_infos/initial/reward_run Std             0.38772\n",
      "evaluation/env_infos/initial/reward_run Max             0.846279\n",
      "evaluation/env_infos/initial/reward_run Min            -0.688425\n",
      "evaluation/env_infos/reward_run Mean                   -0.012644\n",
      "evaluation/env_infos/reward_run Std                     0.159617\n",
      "evaluation/env_infos/reward_run Max                     1.57256\n",
      "evaluation/env_infos/reward_run Min                    -2.01201\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.194054\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0842325\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.034116\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.322983\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.180888\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0691269\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0720111\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.341913\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.197146\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0840693\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0193592\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.48254\n",
      "evaluation/env_infos/final/height Mean                 -0.228405\n",
      "evaluation/env_infos/final/height Std                   0.140204\n",
      "evaluation/env_infos/final/height Max                   0.0825902\n",
      "evaluation/env_infos/final/height Min                  -0.577277\n",
      "evaluation/env_infos/initial/height Mean               -0.0162938\n",
      "evaluation/env_infos/initial/height Std                 0.0593911\n",
      "evaluation/env_infos/initial/height Max                 0.0781191\n",
      "evaluation/env_infos/initial/height Min                -0.118176\n",
      "evaluation/env_infos/height Mean                       -0.221584\n",
      "evaluation/env_infos/height Std                         0.140052\n",
      "evaluation/env_infos/height Max                         0.408082\n",
      "evaluation/env_infos/height Min                        -0.582705\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.0108694\n",
      "evaluation/env_infos/final/reward_angular Std           0.133997\n",
      "evaluation/env_infos/final/reward_angular Max           0.319666\n",
      "evaluation/env_infos/final/reward_angular Min          -0.59131\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.290335\n",
      "evaluation/env_infos/initial/reward_angular Std         1.42222\n",
      "evaluation/env_infos/initial/reward_angular Max         2.74404\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.56022\n",
      "evaluation/env_infos/reward_angular Mean                0.00889732\n",
      "evaluation/env_infos/reward_angular Std                 0.3087\n",
      "evaluation/env_infos/reward_angular Max                 4.65827\n",
      "evaluation/env_infos/reward_angular Min                -4.76906\n",
      "time/data storing (s)                                   0.0323648\n",
      "time/evaluation sampling (s)                           22.3249\n",
      "time/exploration sampling (s)                           1.07951\n",
      "time/logging (s)                                        0.22887\n",
      "time/saving (s)                                         0.0532366\n",
      "time/training (s)                                       3.69588\n",
      "time/epoch (s)                                         27.4148\n",
      "time/total (s)                                        284.812\n",
      "Epoch                                                   9\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doodad not detected\n",
      "objc[14985]: Class GLFWApplicationDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a212b8778) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a21339740). One of the two will be used. Which one is undefined.\n",
      "objc[14985]: Class GLFWWindowDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a212b8700) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a21339768). One of the two will be used. Which one is undefined.\n",
      "objc[14985]: Class GLFWContentView is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a212b87a0) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a213397b8). One of the two will be used. Which one is undefined.\n",
      "objc[14985]: Class GLFWWindow is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a212b8818) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a21339830). One of the two will be used. Which one is undefined.\n",
      "2021-05-25 09:57:59.235078 PDT | Variant:\n",
      "2021-05-25 09:57:59.235750 PDT | {\n",
      "  \"seed\": 10,\n",
      "  \"algorithm\": \"SAC\",\n",
      "  \"env_name\": \"halfcheetahhard\",\n",
      "  \"algo_kwargs\": {\n",
      "    \"batch_size\": 256,\n",
      "    \"num_epochs\": 10,\n",
      "    \"num_eval_steps_per_epoch\": 25000,\n",
      "    \"num_expl_steps_per_train_loop\": 1000,\n",
      "    \"num_trains_per_train_loop\": 100,\n",
      "    \"min_num_steps_before_training\": 1000,\n",
      "    \"max_path_length\": 1000,\n",
      "    \"num_train_loops_per_epoch\": 1\n",
      "  },\n",
      "  \"trainer_kwargs\": {\n",
      "    \"discount\": 0.99,\n",
      "    \"soft_target_tau\": 0.005,\n",
      "    \"target_update_period\": 1,\n",
      "    \"policy_lr\": 0.003,\n",
      "    \"qf_lr\": 0.003,\n",
      "    \"reward_scale\": 1,\n",
      "    \"use_automatic_entropy_tuning\": true\n",
      "  },\n",
      "  \"replay_buffer_kwargs\": {\n",
      "    \"max_replay_buffer_size\": 1000000,\n",
      "    \"latent_dim\": 4,\n",
      "    \"approx_irl\": false,\n",
      "    \"plot\": false\n",
      "  },\n",
      "  \"relabeler_kwargs\": {\n",
      "    \"relabel\": true,\n",
      "    \"use_adv\": true,\n",
      "    \"n_sampled_latents\": 100,\n",
      "    \"n_to_take\": 1,\n",
      "    \"cache\": false\n",
      "  },\n",
      "  \"qf_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      256,\n",
      "      256\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"policy_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      256,\n",
      "      256\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"path_collector_kwargs\": {\n",
      "    \"save_videos\": false\n",
      "  },\n",
      "  \"use_advantages\": true,\n",
      "  \"proper_advantages\": true,\n",
      "  \"plot\": false,\n",
      "  \"test\": false,\n",
      "  \"gpu\": 0,\n",
      "  \"mode\": \"here_no_doodad\",\n",
      "  \"local_docker\": false,\n",
      "  \"insert_time\": false,\n",
      "  \"latent_shape_multiplier\": 1\n",
      "}\n",
      "halfcheetah\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "2021-05-25 09:58:27.573800 PDT | [gher-halfcheetahhard-SAC-10e-1000s-disc0.99_2021_05_25_09_57_59_0000--s-10] Epoch 0 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  4000\n",
      "trainer/QF1 Loss                                      15.2055\n",
      "trainer/QF2 Loss                                      15.1195\n",
      "trainer/Policy Loss                                   -4.03378\n",
      "trainer/Q1 Predictions Mean                           -0.00733524\n",
      "trainer/Q1 Predictions Std                             0.00562048\n",
      "trainer/Q1 Predictions Max                             0.0044136\n",
      "trainer/Q1 Predictions Min                            -0.0337755\n",
      "trainer/Q2 Predictions Mean                            0.00400123\n",
      "trainer/Q2 Predictions Std                             0.004844\n",
      "trainer/Q2 Predictions Max                             0.0220214\n",
      "trainer/Q2 Predictions Min                            -0.00790488\n",
      "trainer/Q Targets Mean                                 3.77747\n",
      "trainer/Q Targets Std                                  0.939209\n",
      "trainer/Q Targets Max                                  7.128\n",
      "trainer/Q Targets Min                                  0.96589\n",
      "trainer/Log Pis Mean                                  -4.04123\n",
      "trainer/Log Pis Std                                    0.507524\n",
      "trainer/Log Pis Max                                   -2.36976\n",
      "trainer/Log Pis Min                                   -5.47938\n",
      "trainer/Policy mu Mean                                -0.000946604\n",
      "trainer/Policy mu Std                                  0.00247213\n",
      "trainer/Policy mu Max                                  0.00749793\n",
      "trainer/Policy mu Min                                 -0.00913525\n",
      "trainer/Policy log std Mean                           -0.00136505\n",
      "trainer/Policy log std Std                             0.00169087\n",
      "trainer/Policy log std Max                             0.00344665\n",
      "trainer/Policy log std Min                            -0.00954913\n",
      "trainer/Alpha                                          0.997005\n",
      "trainer/Alpha Loss                                    -0\n",
      "exploration/num steps total                         2000\n",
      "exploration/num paths total                            2\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.175912\n",
      "exploration/Rewards Std                                0.469454\n",
      "exploration/Rewards Max                                1.72958\n",
      "exploration/Rewards Min                               -1.68558\n",
      "exploration/Returns Mean                            -175.912\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -175.912\n",
      "exploration/Returns Min                             -175.912\n",
      "exploration/Actions Mean                               0.00118257\n",
      "exploration/Actions Std                                0.62711\n",
      "exploration/Actions Max                                0.999105\n",
      "exploration/Actions Min                               -0.999803\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -175.912\n",
      "exploration/env_infos/final/reward_run Mean            0.105579\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max             0.105579\n",
      "exploration/env_infos/final/reward_run Min             0.105579\n",
      "exploration/env_infos/initial/reward_run Mean         -0.376076\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max          -0.376076\n",
      "exploration/env_infos/initial/reward_run Min          -0.376076\n",
      "exploration/env_infos/reward_run Mean                 -0.0749444\n",
      "exploration/env_infos/reward_run Std                   0.679637\n",
      "exploration/env_infos/reward_run Max                   2.24466\n",
      "exploration/env_infos/reward_run Min                  -2.2117\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.112622\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.112622\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.112622\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.144561\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.144561\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.144561\n",
      "exploration/env_infos/reward_ctrl Mean                -0.235961\n",
      "exploration/env_infos/reward_ctrl Std                  0.0750458\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0182057\n",
      "exploration/env_infos/reward_ctrl Min                 -0.475816\n",
      "exploration/env_infos/final/height Mean               -0.129134\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.129134\n",
      "exploration/env_infos/final/height Min                -0.129134\n",
      "exploration/env_infos/initial/height Mean              0.078789\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max               0.078789\n",
      "exploration/env_infos/initial/height Min               0.078789\n",
      "exploration/env_infos/height Mean                     -0.0606269\n",
      "exploration/env_infos/height Std                       0.0914617\n",
      "exploration/env_infos/height Max                       0.292498\n",
      "exploration/env_infos/height Min                      -0.362025\n",
      "exploration/env_infos/final/reward_angular Mean       -0.405134\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max        -0.405134\n",
      "exploration/env_infos/final/reward_angular Min        -0.405134\n",
      "exploration/env_infos/initial/reward_angular Mean      0.317207\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       0.317207\n",
      "exploration/env_infos/initial/reward_angular Min       0.317207\n",
      "exploration/env_infos/reward_angular Mean             -0.00820329\n",
      "exploration/env_infos/reward_angular Std               1.69091\n",
      "exploration/env_infos/reward_angular Max               7.15301\n",
      "exploration/env_infos/reward_angular Min              -5.01457\n",
      "evaluation/num steps total                         25000\n",
      "evaluation/num paths total                            25\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.061855\n",
      "evaluation/Rewards Std                                 0.0460776\n",
      "evaluation/Rewards Max                                 1.24597\n",
      "evaluation/Rewards Min                                -1.43252\n",
      "evaluation/Returns Mean                              -61.855\n",
      "evaluation/Returns Std                                38.0549\n",
      "evaluation/Returns Max                                -0.591149\n",
      "evaluation/Returns Min                              -127.344\n",
      "evaluation/Actions Mean                               -0.000323015\n",
      "evaluation/Actions Std                                 0.00121845\n",
      "evaluation/Actions Max                                 0.00394262\n",
      "evaluation/Actions Min                                -0.00353879\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           -61.855\n",
      "evaluation/env_infos/final/reward_run Mean            -3.98878e-17\n",
      "evaluation/env_infos/final/reward_run Std              2.05441e-16\n",
      "evaluation/env_infos/final/reward_run Max              5.55112e-16\n",
      "evaluation/env_infos/final/reward_run Min             -5.55112e-16\n",
      "evaluation/env_infos/initial/reward_run Mean          -0.00581568\n",
      "evaluation/env_infos/initial/reward_run Std            0.0982681\n",
      "evaluation/env_infos/initial/reward_run Max            0.157834\n",
      "evaluation/env_infos/initial/reward_run Min           -0.229432\n",
      "evaluation/env_infos/reward_run Mean                  -0.000142429\n",
      "evaluation/env_infos/reward_run Std                    0.0134433\n",
      "evaluation/env_infos/reward_run Max                    0.381201\n",
      "evaluation/env_infos/reward_run Min                   -0.328708\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -9.52177e-07\n",
      "evaluation/env_infos/final/reward_ctrl Std             9.0322e-08\n",
      "evaluation/env_infos/final/reward_ctrl Max            -8.07404e-07\n",
      "evaluation/env_infos/final/reward_ctrl Min            -1.10654e-06\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -1.00888e-06\n",
      "evaluation/env_infos/initial/reward_ctrl Std           9.54105e-08\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -8.7194e-07\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -1.18277e-06\n",
      "evaluation/env_infos/reward_ctrl Mean                 -9.53386e-07\n",
      "evaluation/env_infos/reward_ctrl Std                   9.82057e-08\n",
      "evaluation/env_infos/reward_ctrl Max                  -5.4041e-07\n",
      "evaluation/env_infos/reward_ctrl Min                  -3.76371e-06\n",
      "evaluation/env_infos/final/height Mean                -0.132891\n",
      "evaluation/env_infos/final/height Std                  3.5865e-05\n",
      "evaluation/env_infos/final/height Max                 -0.132829\n",
      "evaluation/env_infos/final/height Min                 -0.132952\n",
      "evaluation/env_infos/initial/height Mean              -0.00683044\n",
      "evaluation/env_infos/initial/height Std                0.0516747\n",
      "evaluation/env_infos/initial/height Max                0.0848204\n",
      "evaluation/env_infos/initial/height Min               -0.0949906\n",
      "evaluation/env_infos/height Mean                      -0.132408\n",
      "evaluation/env_infos/height Std                        0.00587538\n",
      "evaluation/env_infos/height Max                        0.0848204\n",
      "evaluation/env_infos/height Min                       -0.143135\n",
      "evaluation/env_infos/final/reward_angular Mean        -3.82372e-17\n",
      "evaluation/env_infos/final/reward_angular Std          1.40955e-15\n",
      "evaluation/env_infos/final/reward_angular Max          3.53314e-15\n",
      "evaluation/env_infos/final/reward_angular Min         -2.77579e-15\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.0243883\n",
      "evaluation/env_infos/initial/reward_angular Std        0.248839\n",
      "evaluation/env_infos/initial/reward_angular Max        0.957859\n",
      "evaluation/env_infos/initial/reward_angular Min       -0.334714\n",
      "evaluation/env_infos/reward_angular Mean               0.00115929\n",
      "evaluation/env_infos/reward_angular Std                0.0377466\n",
      "evaluation/env_infos/reward_angular Max                1.69036\n",
      "evaluation/env_infos/reward_angular Min               -0.732174\n",
      "time/data storing (s)                                  0.197269\n",
      "time/evaluation sampling (s)                          22.4532\n",
      "time/exploration sampling (s)                          1.01463\n",
      "time/logging (s)                                       0.227089\n",
      "time/saving (s)                                        0.0715252\n",
      "time/training (s)                                      3.19666\n",
      "time/epoch (s)                                        27.1604\n",
      "time/total (s)                                        31.8917\n",
      "Epoch                                                  0\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 09:58:54.954008 PDT | [gher-halfcheetahhard-SAC-10e-1000s-disc0.99_2021_05_25_09_57_59_0000--s-10] Epoch 1 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  6000\n",
      "trainer/QF1 Loss                                       0.51048\n",
      "trainer/QF2 Loss                                       0.522122\n",
      "trainer/Policy Loss                                   -7.34182\n",
      "trainer/Q1 Predictions Mean                            3.28883\n",
      "trainer/Q1 Predictions Std                             0.583941\n",
      "trainer/Q1 Predictions Max                             4.92577\n",
      "trainer/Q1 Predictions Min                             1.6399\n",
      "trainer/Q2 Predictions Mean                            3.28576\n",
      "trainer/Q2 Predictions Std                             0.579971\n",
      "trainer/Q2 Predictions Max                             4.87368\n",
      "trainer/Q2 Predictions Min                             1.60498\n",
      "trainer/Q Targets Mean                                 3.3149\n",
      "trainer/Q Targets Std                                  0.825886\n",
      "trainer/Q Targets Max                                  6.04823\n",
      "trainer/Q Targets Min                                  0.847681\n",
      "trainer/Log Pis Mean                                  -4.05038\n",
      "trainer/Log Pis Std                                    0.44113\n",
      "trainer/Log Pis Max                                   -2.92519\n",
      "trainer/Log Pis Min                                   -6.54472\n",
      "trainer/Policy mu Mean                                -0.111959\n",
      "trainer/Policy mu Std                                  0.0564847\n",
      "trainer/Policy mu Max                                 -0.000463085\n",
      "trainer/Policy mu Min                                 -0.297595\n",
      "trainer/Policy log std Mean                           -0.116547\n",
      "trainer/Policy log std Std                             0.0238978\n",
      "trainer/Policy log std Max                            -0.0593668\n",
      "trainer/Policy log std Min                            -0.216531\n",
      "trainer/Alpha                                          0.738768\n",
      "trainer/Alpha Loss                                    -3.01286\n",
      "exploration/num steps total                         3000\n",
      "exploration/num paths total                            3\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.227952\n",
      "exploration/Rewards Std                                0.193186\n",
      "exploration/Rewards Max                                0.326003\n",
      "exploration/Rewards Min                               -0.843052\n",
      "exploration/Returns Mean                            -227.952\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -227.952\n",
      "exploration/Returns Min                             -227.952\n",
      "exploration/Actions Mean                              -0.07407\n",
      "exploration/Actions Std                                0.593729\n",
      "exploration/Actions Max                                0.993564\n",
      "exploration/Actions Min                               -0.998743\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -227.952\n",
      "exploration/env_infos/final/reward_run Mean            0.0163164\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max             0.0163164\n",
      "exploration/env_infos/final/reward_run Min             0.0163164\n",
      "exploration/env_infos/initial/reward_run Mean         -0.354067\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max          -0.354067\n",
      "exploration/env_infos/initial/reward_run Min          -0.354067\n",
      "exploration/env_infos/reward_run Mean                 -0.107958\n",
      "exploration/env_infos/reward_run Std                   0.679363\n",
      "exploration/env_infos/reward_run Max                   1.84208\n",
      "exploration/env_infos/reward_run Min                  -2.46542\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.19375\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.19375\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.19375\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.229846\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.229846\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.229846\n",
      "exploration/env_infos/reward_ctrl Mean                -0.2148\n",
      "exploration/env_infos/reward_ctrl Std                  0.0723967\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0110127\n",
      "exploration/env_infos/reward_ctrl Min                 -0.506945\n",
      "exploration/env_infos/final/height Mean               -0.118463\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.118463\n",
      "exploration/env_infos/final/height Min                -0.118463\n",
      "exploration/env_infos/initial/height Mean              0.0213046\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max               0.0213046\n",
      "exploration/env_infos/initial/height Min               0.0213046\n",
      "exploration/env_infos/height Mean                     -0.0830155\n",
      "exploration/env_infos/height Std                       0.0764621\n",
      "exploration/env_infos/height Max                       0.217647\n",
      "exploration/env_infos/height Min                      -0.29957\n",
      "exploration/env_infos/final/reward_angular Mean        1.90985\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max         1.90985\n",
      "exploration/env_infos/final/reward_angular Min         1.90985\n",
      "exploration/env_infos/initial/reward_angular Mean      0.847628\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       0.847628\n",
      "exploration/env_infos/initial/reward_angular Min       0.847628\n",
      "exploration/env_infos/reward_angular Mean             -0.01899\n",
      "exploration/env_infos/reward_angular Std               1.63545\n",
      "exploration/env_infos/reward_angular Max               5.40384\n",
      "exploration/env_infos/reward_angular Min              -5.39176\n",
      "evaluation/num steps total                         50000\n",
      "evaluation/num paths total                            50\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.0636887\n",
      "evaluation/Rewards Std                                 0.0501368\n",
      "evaluation/Rewards Max                                 1.37953\n",
      "evaluation/Rewards Min                                -1.56089\n",
      "evaluation/Returns Mean                              -63.6887\n",
      "evaluation/Returns Std                                38.2841\n",
      "evaluation/Returns Max                                -1.94242\n",
      "evaluation/Returns Min                              -128.826\n",
      "evaluation/Actions Mean                               -0.0761291\n",
      "evaluation/Actions Std                                 0.0371019\n",
      "evaluation/Actions Max                                -0.0047696\n",
      "evaluation/Actions Min                                -0.148255\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           -63.6887\n",
      "evaluation/env_infos/final/reward_run Mean             1.96214e-11\n",
      "evaluation/env_infos/final/reward_run Std              4.08524e-09\n",
      "evaluation/env_infos/final/reward_run Max              1.46869e-08\n",
      "evaluation/env_infos/final/reward_run Min             -1.41963e-08\n",
      "evaluation/env_infos/initial/reward_run Mean          -0.128246\n",
      "evaluation/env_infos/initial/reward_run Std            0.133516\n",
      "evaluation/env_infos/initial/reward_run Max            0.0966144\n",
      "evaluation/env_infos/initial/reward_run Min           -0.440612\n",
      "evaluation/env_infos/reward_run Mean                  -0.000314333\n",
      "evaluation/env_infos/reward_run Std                    0.0186408\n",
      "evaluation/env_infos/reward_run Max                    0.301059\n",
      "evaluation/env_infos/reward_run Min                   -0.600385\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.00430017\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.000445517\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.00339972\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.00479989\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.00444026\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.00047341\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.00348864\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.00525548\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.00430332\n",
      "evaluation/env_infos/reward_ctrl Std                   0.000447058\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00310018\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.00575716\n",
      "evaluation/env_infos/final/height Mean                -0.133615\n",
      "evaluation/env_infos/final/height Std                  0.000237871\n",
      "evaluation/env_infos/final/height Max                 -0.132869\n",
      "evaluation/env_infos/final/height Min                 -0.133971\n",
      "evaluation/env_infos/initial/height Mean               0.00902137\n",
      "evaluation/env_infos/initial/height Std                0.0452197\n",
      "evaluation/env_infos/initial/height Max                0.0654409\n",
      "evaluation/env_infos/initial/height Min               -0.0864262\n",
      "evaluation/env_infos/height Mean                      -0.133166\n",
      "evaluation/env_infos/height Std                        0.0064309\n",
      "evaluation/env_infos/height Max                        0.0654409\n",
      "evaluation/env_infos/height Min                       -0.155082\n",
      "evaluation/env_infos/final/reward_angular Mean        -3.75372e-09\n",
      "evaluation/env_infos/final/reward_angular Std          1.32533e-08\n",
      "evaluation/env_infos/final/reward_angular Max          3.4754e-15\n",
      "evaluation/env_infos/final/reward_angular Min         -5.99644e-08\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.262841\n",
      "evaluation/env_infos/initial/reward_angular Std        0.328762\n",
      "evaluation/env_infos/initial/reward_angular Max        1.33572\n",
      "evaluation/env_infos/initial/reward_angular Min       -0.456149\n",
      "evaluation/env_infos/reward_angular Mean               0.00152382\n",
      "evaluation/env_infos/reward_angular Std                0.0523978\n",
      "evaluation/env_infos/reward_angular Max                1.87385\n",
      "evaluation/env_infos/reward_angular Min               -1.32987\n",
      "time/data storing (s)                                  0.196639\n",
      "time/evaluation sampling (s)                          22.411\n",
      "time/exploration sampling (s)                          1.06054\n",
      "time/logging (s)                                       0.227827\n",
      "time/saving (s)                                        0.0267384\n",
      "time/training (s)                                      3.22818\n",
      "time/epoch (s)                                        27.1509\n",
      "time/total (s)                                        59.2719\n",
      "Epoch                                                  1\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 09:59:22.561244 PDT | [gher-halfcheetahhard-SAC-10e-1000s-disc0.99_2021_05_25_09_57_59_0000--s-10] Epoch 2 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  8000\n",
      "trainer/QF1 Loss                                       0.511092\n",
      "trainer/QF2 Loss                                       0.501994\n",
      "trainer/Policy Loss                                   -7.08222\n",
      "trainer/Q1 Predictions Mean                            3.06998\n",
      "trainer/Q1 Predictions Std                             0.577551\n",
      "trainer/Q1 Predictions Max                             4.94326\n",
      "trainer/Q1 Predictions Min                             1.73197\n",
      "trainer/Q2 Predictions Mean                            3.08338\n",
      "trainer/Q2 Predictions Std                             0.568663\n",
      "trainer/Q2 Predictions Max                             4.98871\n",
      "trainer/Q2 Predictions Min                             1.72394\n",
      "trainer/Q Targets Mean                                 3.16377\n",
      "trainer/Q Targets Std                                  0.850951\n",
      "trainer/Q Targets Max                                  6.48097\n",
      "trainer/Q Targets Min                                 -0.233349\n",
      "trainer/Log Pis Mean                                  -3.96764\n",
      "trainer/Log Pis Std                                    0.524319\n",
      "trainer/Log Pis Max                                   -2.43879\n",
      "trainer/Log Pis Min                                   -5.80079\n",
      "trainer/Policy mu Mean                                -0.130665\n",
      "trainer/Policy mu Std                                  0.076531\n",
      "trainer/Policy mu Max                                  0.0205084\n",
      "trainer/Policy mu Min                                 -0.366463\n",
      "trainer/Policy log std Mean                           -0.118275\n",
      "trainer/Policy log std Std                             0.0243445\n",
      "trainer/Policy log std Max                            -0.0465655\n",
      "trainer/Policy log std Min                            -0.213298\n",
      "trainer/Alpha                                          0.547777\n",
      "trainer/Alpha Loss                                    -5.96961\n",
      "exploration/num steps total                         4000\n",
      "exploration/num paths total                            4\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.13844\n",
      "exploration/Rewards Std                                0.519151\n",
      "exploration/Rewards Max                                1.71683\n",
      "exploration/Rewards Min                               -1.88969\n",
      "exploration/Returns Mean                            -138.44\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -138.44\n",
      "exploration/Returns Min                             -138.44\n",
      "exploration/Actions Mean                              -0.0940936\n",
      "exploration/Actions Std                                0.592433\n",
      "exploration/Actions Max                                0.995881\n",
      "exploration/Actions Min                               -0.996709\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -138.44\n",
      "exploration/env_infos/final/reward_run Mean            0.00260388\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max             0.00260388\n",
      "exploration/env_infos/final/reward_run Min             0.00260388\n",
      "exploration/env_infos/initial/reward_run Mean         -0.212023\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max          -0.212023\n",
      "exploration/env_infos/initial/reward_run Min          -0.212023\n",
      "exploration/env_infos/reward_run Mean                  0.0102469\n",
      "exploration/env_infos/reward_run Std                   0.607815\n",
      "exploration/env_infos/reward_run Max                   2.18661\n",
      "exploration/env_infos/reward_run Min                  -2.00341\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.156643\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.156643\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.156643\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.194976\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.194976\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.194976\n",
      "exploration/env_infos/reward_ctrl Mean                -0.215899\n",
      "exploration/env_infos/reward_ctrl Std                  0.0751497\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0325105\n",
      "exploration/env_infos/reward_ctrl Min                 -0.443853\n",
      "exploration/env_infos/final/height Mean               -0.56805\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.56805\n",
      "exploration/env_infos/final/height Min                -0.56805\n",
      "exploration/env_infos/initial/height Mean              0.0734158\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max               0.0734158\n",
      "exploration/env_infos/initial/height Min               0.0734158\n",
      "exploration/env_infos/height Mean                     -0.212673\n",
      "exploration/env_infos/height Std                       0.243826\n",
      "exploration/env_infos/height Max                       0.230022\n",
      "exploration/env_infos/height Min                      -0.581601\n",
      "exploration/env_infos/final/reward_angular Mean        0.256756\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max         0.256756\n",
      "exploration/env_infos/final/reward_angular Min         0.256756\n",
      "exploration/env_infos/initial/reward_angular Mean      1.15607\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       1.15607\n",
      "exploration/env_infos/initial/reward_angular Min       1.15607\n",
      "exploration/env_infos/reward_angular Mean              0.0546331\n",
      "exploration/env_infos/reward_angular Std               1.45038\n",
      "exploration/env_infos/reward_angular Max               4.88269\n",
      "exploration/env_infos/reward_angular Min              -5.41828\n",
      "evaluation/num steps total                         75000\n",
      "evaluation/num paths total                            75\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.0653186\n",
      "evaluation/Rewards Std                                 0.0512621\n",
      "evaluation/Rewards Max                                 1.11717\n",
      "evaluation/Rewards Min                                -1.26531\n",
      "evaluation/Returns Mean                              -65.3186\n",
      "evaluation/Returns Std                                38.1381\n",
      "evaluation/Returns Max                                -4.67129\n",
      "evaluation/Returns Min                              -129.121\n",
      "evaluation/Actions Mean                               -0.100933\n",
      "evaluation/Actions Std                                 0.05528\n",
      "evaluation/Actions Max                                -0.0121461\n",
      "evaluation/Actions Min                                -0.20155\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           -65.3186\n",
      "evaluation/env_infos/final/reward_run Mean            -6.0558e-10\n",
      "evaluation/env_infos/final/reward_run Std              5.53145e-09\n",
      "evaluation/env_infos/final/reward_run Max              8.18896e-09\n",
      "evaluation/env_infos/final/reward_run Min             -2.63077e-08\n",
      "evaluation/env_infos/initial/reward_run Mean          -0.0984872\n",
      "evaluation/env_infos/initial/reward_run Std            0.122742\n",
      "evaluation/env_infos/initial/reward_run Max            0.0834893\n",
      "evaluation/env_infos/initial/reward_run Min           -0.413806\n",
      "evaluation/env_infos/reward_run Mean                  -8.42123e-05\n",
      "evaluation/env_infos/reward_run Std                    0.0181993\n",
      "evaluation/env_infos/reward_run Max                    0.369262\n",
      "evaluation/env_infos/reward_run Min                   -0.42077\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.00794416\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.000611316\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.00659836\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.008613\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.00825846\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.000757397\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.00696546\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0093897\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.00794601\n",
      "evaluation/env_infos/reward_ctrl Std                   0.000615603\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00534291\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.0102248\n",
      "evaluation/env_infos/final/height Mean                -0.134008\n",
      "evaluation/env_infos/final/height Std                  0.000633396\n",
      "evaluation/env_infos/final/height Max                 -0.133001\n",
      "evaluation/env_infos/final/height Min                 -0.135354\n",
      "evaluation/env_infos/initial/height Mean              -0.00825867\n",
      "evaluation/env_infos/initial/height Std                0.0631078\n",
      "evaluation/env_infos/initial/height Max                0.0921929\n",
      "evaluation/env_infos/initial/height Min               -0.0907801\n",
      "evaluation/env_infos/height Mean                      -0.133608\n",
      "evaluation/env_infos/height Std                        0.00618519\n",
      "evaluation/env_infos/height Max                        0.0921929\n",
      "evaluation/env_infos/height Min                       -0.154536\n",
      "evaluation/env_infos/final/reward_angular Mean        -3.35342e-09\n",
      "evaluation/env_infos/final/reward_angular Std          1.68522e-08\n",
      "evaluation/env_infos/final/reward_angular Max          1.76686e-08\n",
      "evaluation/env_infos/final/reward_angular Min         -8.32402e-08\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.503696\n",
      "evaluation/env_infos/initial/reward_angular Std        0.387493\n",
      "evaluation/env_infos/initial/reward_angular Max        1.51414\n",
      "evaluation/env_infos/initial/reward_angular Min       -0.415699\n",
      "evaluation/env_infos/reward_angular Mean               0.00203018\n",
      "evaluation/env_infos/reward_angular Std                0.053462\n",
      "evaluation/env_infos/reward_angular Max                2.21633\n",
      "evaluation/env_infos/reward_angular Min               -1.39476\n",
      "time/data storing (s)                                  0.205097\n",
      "time/evaluation sampling (s)                          22.614\n",
      "time/exploration sampling (s)                          1.09758\n",
      "time/logging (s)                                       0.228399\n",
      "time/saving (s)                                        0.0264274\n",
      "time/training (s)                                      3.31801\n",
      "time/epoch (s)                                        27.4895\n",
      "time/total (s)                                        86.8788\n",
      "Epoch                                                  2\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 09:59:49.771694 PDT | [gher-halfcheetahhard-SAC-10e-1000s-disc0.99_2021_05_25_09_57_59_0000--s-10] Epoch 3 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  10000\n",
      "trainer/QF1 Loss                                        0.47572\n",
      "trainer/QF2 Loss                                        0.486954\n",
      "trainer/Policy Loss                                    -7.17815\n",
      "trainer/Q1 Predictions Mean                             3.26409\n",
      "trainer/Q1 Predictions Std                              0.67746\n",
      "trainer/Q1 Predictions Max                              5.94879\n",
      "trainer/Q1 Predictions Min                              1.82104\n",
      "trainer/Q2 Predictions Mean                             3.24111\n",
      "trainer/Q2 Predictions Std                              0.684397\n",
      "trainer/Q2 Predictions Max                              5.69973\n",
      "trainer/Q2 Predictions Min                              1.7624\n",
      "trainer/Q Targets Mean                                  3.24619\n",
      "trainer/Q Targets Std                                   0.858209\n",
      "trainer/Q Targets Max                                   6.74631\n",
      "trainer/Q Targets Min                                   0.768221\n",
      "trainer/Log Pis Mean                                   -3.8791\n",
      "trainer/Log Pis Std                                     0.710187\n",
      "trainer/Log Pis Max                                    -1.84084\n",
      "trainer/Log Pis Min                                    -5.92709\n",
      "trainer/Policy mu Mean                                 -0.189417\n",
      "trainer/Policy mu Std                                   0.119838\n",
      "trainer/Policy mu Max                                   0.048491\n",
      "trainer/Policy mu Min                                  -0.531408\n",
      "trainer/Policy log std Mean                            -0.125308\n",
      "trainer/Policy log std Std                              0.0289136\n",
      "trainer/Policy log std Max                             -0.0623796\n",
      "trainer/Policy log std Min                             -0.245425\n",
      "trainer/Alpha                                           0.406146\n",
      "trainer/Alpha Loss                                     -8.87192\n",
      "exploration/num steps total                          5000\n",
      "exploration/num paths total                             5\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.13309\n",
      "exploration/Rewards Std                                 0.55503\n",
      "exploration/Rewards Max                                 1.81851\n",
      "exploration/Rewards Min                                -2.03602\n",
      "exploration/Returns Mean                             -133.09\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -133.09\n",
      "exploration/Returns Min                              -133.09\n",
      "exploration/Actions Mean                               -0.117328\n",
      "exploration/Actions Std                                 0.585706\n",
      "exploration/Actions Max                                 0.997555\n",
      "exploration/Actions Min                                -0.99829\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -133.09\n",
      "exploration/env_infos/final/reward_run Mean             0.542069\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.542069\n",
      "exploration/env_infos/final/reward_run Min              0.542069\n",
      "exploration/env_infos/initial/reward_run Mean          -0.165834\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.165834\n",
      "exploration/env_infos/initial/reward_run Min           -0.165834\n",
      "exploration/env_infos/reward_run Mean                   0.0294466\n",
      "exploration/env_infos/reward_run Std                    0.588567\n",
      "exploration/env_infos/reward_run Max                    1.92453\n",
      "exploration/env_infos/reward_run Min                   -2.1288\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.24571\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.24571\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.24571\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.178716\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.178716\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.178716\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.21409\n",
      "exploration/env_infos/reward_ctrl Std                   0.0721811\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0241207\n",
      "exploration/env_infos/reward_ctrl Min                  -0.437262\n",
      "exploration/env_infos/final/height Mean                -0.545954\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.545954\n",
      "exploration/env_infos/final/height Min                 -0.545954\n",
      "exploration/env_infos/initial/height Mean              -0.0583575\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0583575\n",
      "exploration/env_infos/initial/height Min               -0.0583575\n",
      "exploration/env_infos/height Mean                      -0.280217\n",
      "exploration/env_infos/height Std                        0.279424\n",
      "exploration/env_infos/height Max                        0.348881\n",
      "exploration/env_infos/height Min                       -0.582558\n",
      "exploration/env_infos/final/reward_angular Mean        -1.63466\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -1.63466\n",
      "exploration/env_infos/final/reward_angular Min         -1.63466\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.0582959\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.0582959\n",
      "exploration/env_infos/initial/reward_angular Min       -0.0582959\n",
      "exploration/env_infos/reward_angular Mean               0.0642176\n",
      "exploration/env_infos/reward_angular Std                1.37628\n",
      "exploration/env_infos/reward_angular Max                5.206\n",
      "exploration/env_infos/reward_angular Min               -4.5616\n",
      "evaluation/num steps total                         100000\n",
      "evaluation/num paths total                            100\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.0683005\n",
      "evaluation/Rewards Std                                  0.051727\n",
      "evaluation/Rewards Max                                  0.836016\n",
      "evaluation/Rewards Min                                 -1.36336\n",
      "evaluation/Returns Mean                               -68.3005\n",
      "evaluation/Returns Std                                 38.7327\n",
      "evaluation/Returns Max                                 -1.75298\n",
      "evaluation/Returns Min                               -132.858\n",
      "evaluation/Actions Mean                                -0.131858\n",
      "evaluation/Actions Std                                  0.0723222\n",
      "evaluation/Actions Max                                 -0.00317013\n",
      "evaluation/Actions Min                                 -0.276494\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            -68.3005\n",
      "evaluation/env_infos/final/reward_run Mean              7.15508e-10\n",
      "evaluation/env_infos/final/reward_run Std               3.89398e-09\n",
      "evaluation/env_infos/final/reward_run Max               1.97116e-08\n",
      "evaluation/env_infos/final/reward_run Min              -1.82394e-09\n",
      "evaluation/env_infos/initial/reward_run Mean           -0.129689\n",
      "evaluation/env_infos/initial/reward_run Std             0.147437\n",
      "evaluation/env_infos/initial/reward_run Max             0.191654\n",
      "evaluation/env_infos/initial/reward_run Min            -0.403354\n",
      "evaluation/env_infos/reward_run Mean                    3.01221e-05\n",
      "evaluation/env_infos/reward_run Std                     0.0204774\n",
      "evaluation/env_infos/reward_run Max                     0.441328\n",
      "evaluation/env_infos/reward_run Min                    -0.520354\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0135628\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00251607\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00874069\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0178926\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.015648\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00307634\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00970097\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0210079\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0135702\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0025235\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.007063\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.0231545\n",
      "evaluation/env_infos/final/height Mean                 -0.136198\n",
      "evaluation/env_infos/final/height Std                   0.00203295\n",
      "evaluation/env_infos/final/height Max                  -0.132211\n",
      "evaluation/env_infos/final/height Min                  -0.140164\n",
      "evaluation/env_infos/initial/height Mean                0.00264512\n",
      "evaluation/env_infos/initial/height Std                 0.061005\n",
      "evaluation/env_infos/initial/height Max                 0.081821\n",
      "evaluation/env_infos/initial/height Min                -0.09316\n",
      "evaluation/env_infos/height Mean                       -0.135801\n",
      "evaluation/env_infos/height Std                         0.00688543\n",
      "evaluation/env_infos/height Max                         0.081821\n",
      "evaluation/env_infos/height Min                        -0.1593\n",
      "evaluation/env_infos/final/reward_angular Mean         -4.46365e-10\n",
      "evaluation/env_infos/final/reward_angular Std           6.00602e-09\n",
      "evaluation/env_infos/final/reward_angular Max           1.49695e-08\n",
      "evaluation/env_infos/final/reward_angular Min          -2.61286e-08\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.669821\n",
      "evaluation/env_infos/initial/reward_angular Std         0.482189\n",
      "evaluation/env_infos/initial/reward_angular Max         1.61852\n",
      "evaluation/env_infos/initial/reward_angular Min        -0.280503\n",
      "evaluation/env_infos/reward_angular Mean                0.00229513\n",
      "evaluation/env_infos/reward_angular Std                 0.0623383\n",
      "evaluation/env_infos/reward_angular Max                 2.58563\n",
      "evaluation/env_infos/reward_angular Min                -0.957731\n",
      "time/data storing (s)                                   0.203893\n",
      "time/evaluation sampling (s)                           22.1345\n",
      "time/exploration sampling (s)                           1.11943\n",
      "time/logging (s)                                        0.225662\n",
      "time/saving (s)                                         0.0272731\n",
      "time/training (s)                                       3.37624\n",
      "time/epoch (s)                                         27.087\n",
      "time/total (s)                                        114.086\n",
      "Epoch                                                   3\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:00:16.492479 PDT | [gher-halfcheetahhard-SAC-10e-1000s-disc0.99_2021_05_25_09_57_59_0000--s-10] Epoch 4 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  12000\n",
      "trainer/QF1 Loss                                        0.536263\n",
      "trainer/QF2 Loss                                        0.538897\n",
      "trainer/Policy Loss                                    -7.12601\n",
      "trainer/Q1 Predictions Mean                             3.21675\n",
      "trainer/Q1 Predictions Std                              0.661183\n",
      "trainer/Q1 Predictions Max                              6.51619\n",
      "trainer/Q1 Predictions Min                              1.81037\n",
      "trainer/Q2 Predictions Mean                             3.24064\n",
      "trainer/Q2 Predictions Std                              0.660781\n",
      "trainer/Q2 Predictions Max                              6.63175\n",
      "trainer/Q2 Predictions Min                              1.99161\n",
      "trainer/Q Targets Mean                                  3.21354\n",
      "trainer/Q Targets Std                                   0.982209\n",
      "trainer/Q Targets Max                                   7.22473\n",
      "trainer/Q Targets Min                                  -0.102374\n",
      "trainer/Log Pis Mean                                   -3.82569\n",
      "trainer/Log Pis Std                                     0.898587\n",
      "trainer/Log Pis Max                                    -0.396334\n",
      "trainer/Log Pis Min                                    -8.90338\n",
      "trainer/Policy mu Mean                                 -0.111479\n",
      "trainer/Policy mu Std                                   0.219397\n",
      "trainer/Policy mu Max                                   0.822911\n",
      "trainer/Policy mu Min                                  -0.926468\n",
      "trainer/Policy log std Mean                            -0.0897728\n",
      "trainer/Policy log std Std                              0.0257166\n",
      "trainer/Policy log std Max                              0.0210895\n",
      "trainer/Policy log std Min                             -0.164696\n",
      "trainer/Alpha                                           0.301113\n",
      "trainer/Alpha Loss                                    -11.7643\n",
      "exploration/num steps total                          6000\n",
      "exploration/num paths total                             6\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.297218\n",
      "exploration/Rewards Std                                 0.901484\n",
      "exploration/Rewards Max                                 2.73194\n",
      "exploration/Rewards Min                                -3.32542\n",
      "exploration/Returns Mean                             -297.218\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -297.218\n",
      "exploration/Returns Min                              -297.218\n",
      "exploration/Actions Mean                               -0.041244\n",
      "exploration/Actions Std                                 0.603095\n",
      "exploration/Actions Max                                 0.997645\n",
      "exploration/Actions Min                                -0.995979\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -297.218\n",
      "exploration/env_infos/final/reward_run Mean            -0.838519\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.838519\n",
      "exploration/env_infos/final/reward_run Min             -0.838519\n",
      "exploration/env_infos/initial/reward_run Mean           0.665122\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.665122\n",
      "exploration/env_infos/initial/reward_run Min            0.665122\n",
      "exploration/env_infos/reward_run Mean                   0.00836292\n",
      "exploration/env_infos/reward_run Std                    0.609387\n",
      "exploration/env_infos/reward_run Max                    1.57428\n",
      "exploration/env_infos/reward_run Min                   -2.00049\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.305787\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.305787\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.305787\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.168567\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.168567\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.168567\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.219255\n",
      "exploration/env_infos/reward_ctrl Std                   0.0742226\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0221987\n",
      "exploration/env_infos/reward_ctrl Min                  -0.510243\n",
      "exploration/env_infos/final/height Mean                -0.554988\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.554988\n",
      "exploration/env_infos/final/height Min                 -0.554988\n",
      "exploration/env_infos/initial/height Mean              -0.09172\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.09172\n",
      "exploration/env_infos/initial/height Min               -0.09172\n",
      "exploration/env_infos/height Mean                      -0.374522\n",
      "exploration/env_infos/height Std                        0.241508\n",
      "exploration/env_infos/height Max                        0.241055\n",
      "exploration/env_infos/height Min                       -0.589059\n",
      "exploration/env_infos/final/reward_angular Mean        -1.06423\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -1.06423\n",
      "exploration/env_infos/final/reward_angular Min         -1.06423\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.273694\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.273694\n",
      "exploration/env_infos/initial/reward_angular Min       -0.273694\n",
      "exploration/env_infos/reward_angular Mean               0.0428703\n",
      "exploration/env_infos/reward_angular Std                1.42375\n",
      "exploration/env_infos/reward_angular Max                4.9838\n",
      "exploration/env_infos/reward_angular Min               -4.86798\n",
      "evaluation/num steps total                         125000\n",
      "evaluation/num paths total                            125\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.0637435\n",
      "evaluation/Rewards Std                                  0.0491813\n",
      "evaluation/Rewards Max                                  1.10921\n",
      "evaluation/Rewards Min                                 -1.27887\n",
      "evaluation/Returns Mean                               -63.7435\n",
      "evaluation/Returns Std                                 33.886\n",
      "evaluation/Returns Max                                 -4.52243\n",
      "evaluation/Returns Min                               -122.298\n",
      "evaluation/Actions Mean                                -0.0295225\n",
      "evaluation/Actions Std                                  0.143589\n",
      "evaluation/Actions Max                                  0.434799\n",
      "evaluation/Actions Min                                 -0.454853\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            -63.7435\n",
      "evaluation/env_infos/final/reward_run Mean              5.09821e-09\n",
      "evaluation/env_infos/final/reward_run Std               5.0114e-08\n",
      "evaluation/env_infos/final/reward_run Max               1.24381e-07\n",
      "evaluation/env_infos/final/reward_run Min              -1.83266e-07\n",
      "evaluation/env_infos/initial/reward_run Mean            0.0830176\n",
      "evaluation/env_infos/initial/reward_run Std             0.287268\n",
      "evaluation/env_infos/initial/reward_run Max             0.585696\n",
      "evaluation/env_infos/initial/reward_run Min            -0.502313\n",
      "evaluation/env_infos/reward_run Mean                   -0.000190675\n",
      "evaluation/env_infos/reward_run Std                     0.0273579\n",
      "evaluation/env_infos/reward_run Max                     0.627838\n",
      "evaluation/env_infos/reward_run Min                    -0.719749\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0128763\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00887484\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00204699\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0406738\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.023423\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0159745\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00241148\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0669654\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0128936\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00889606\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00116373\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.0669654\n",
      "evaluation/env_infos/final/height Mean                 -0.127682\n",
      "evaluation/env_infos/final/height Std                   0.00432431\n",
      "evaluation/env_infos/final/height Max                  -0.123359\n",
      "evaluation/env_infos/final/height Min                  -0.139807\n",
      "evaluation/env_infos/initial/height Mean               -0.00423625\n",
      "evaluation/env_infos/initial/height Std                 0.0530372\n",
      "evaluation/env_infos/initial/height Max                 0.0691639\n",
      "evaluation/env_infos/initial/height Min                -0.0940655\n",
      "evaluation/env_infos/height Mean                       -0.127243\n",
      "evaluation/env_infos/height Std                         0.00726948\n",
      "evaluation/env_infos/height Max                         0.0691639\n",
      "evaluation/env_infos/height Min                        -0.168371\n",
      "evaluation/env_infos/final/reward_angular Mean         -5.15511e-09\n",
      "evaluation/env_infos/final/reward_angular Std           6.03187e-08\n",
      "evaluation/env_infos/final/reward_angular Max           8.76928e-08\n",
      "evaluation/env_infos/final/reward_angular Min          -1.86175e-07\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.278203\n",
      "evaluation/env_infos/initial/reward_angular Std         0.573409\n",
      "evaluation/env_infos/initial/reward_angular Max         1.21988\n",
      "evaluation/env_infos/initial/reward_angular Min        -0.653123\n",
      "evaluation/env_infos/reward_angular Mean                0.00140842\n",
      "evaluation/env_infos/reward_angular Std                 0.0587379\n",
      "evaluation/env_infos/reward_angular Max                 2.28465\n",
      "evaluation/env_infos/reward_angular Min                -1.62675\n",
      "time/data storing (s)                                   0.201701\n",
      "time/evaluation sampling (s)                           21.8549\n",
      "time/exploration sampling (s)                           1.0332\n",
      "time/logging (s)                                        0.233972\n",
      "time/saving (s)                                         0.0266381\n",
      "time/training (s)                                       3.24344\n",
      "time/epoch (s)                                         26.5938\n",
      "time/total (s)                                        140.814\n",
      "Epoch                                                   4\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:00:43.653155 PDT | [gher-halfcheetahhard-SAC-10e-1000s-disc0.99_2021_05_25_09_57_59_0000--s-10] Epoch 5 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  14000\n",
      "trainer/QF1 Loss                                        0.747745\n",
      "trainer/QF2 Loss                                        0.754249\n",
      "trainer/Policy Loss                                    -7.197\n",
      "trainer/Q1 Predictions Mean                             3.42201\n",
      "trainer/Q1 Predictions Std                              0.914695\n",
      "trainer/Q1 Predictions Max                              7.21009\n",
      "trainer/Q1 Predictions Min                              1.86781\n",
      "trainer/Q2 Predictions Mean                             3.41754\n",
      "trainer/Q2 Predictions Std                              0.916415\n",
      "trainer/Q2 Predictions Max                              7.25541\n",
      "trainer/Q2 Predictions Min                              1.77258\n",
      "trainer/Q Targets Mean                                  3.23675\n",
      "trainer/Q Targets Std                                   0.996781\n",
      "trainer/Q Targets Max                                   6.74985\n",
      "trainer/Q Targets Min                                   0.569904\n",
      "trainer/Log Pis Mean                                   -3.73372\n",
      "trainer/Log Pis Std                                     1.10131\n",
      "trainer/Log Pis Max                                     0.696366\n",
      "trainer/Log Pis Min                                    -8.05456\n",
      "trainer/Policy mu Mean                                 -0.15438\n",
      "trainer/Policy mu Std                                   0.295394\n",
      "trainer/Policy mu Max                                   0.573239\n",
      "trainer/Policy mu Min                                  -1.40393\n",
      "trainer/Policy log std Mean                            -0.112649\n",
      "trainer/Policy log std Std                              0.036755\n",
      "trainer/Policy log std Max                             -0.0233739\n",
      "trainer/Policy log std Min                             -0.262717\n",
      "trainer/Alpha                                           0.224034\n",
      "trainer/Alpha Loss                                    -14.5326\n",
      "exploration/num steps total                          7000\n",
      "exploration/num paths total                             7\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.327887\n",
      "exploration/Rewards Std                                 0.59289\n",
      "exploration/Rewards Max                                 2.31985\n",
      "exploration/Rewards Min                                -1.99839\n",
      "exploration/Returns Mean                             -327.887\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -327.887\n",
      "exploration/Returns Min                              -327.887\n",
      "exploration/Actions Mean                               -0.179138\n",
      "exploration/Actions Std                                 0.596977\n",
      "exploration/Actions Max                                 0.997115\n",
      "exploration/Actions Min                                -0.998899\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -327.887\n",
      "exploration/env_infos/final/reward_run Mean            -0.553058\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.553058\n",
      "exploration/env_infos/final/reward_run Min             -0.553058\n",
      "exploration/env_infos/initial/reward_run Mean           0.123326\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.123326\n",
      "exploration/env_infos/initial/reward_run Min            0.123326\n",
      "exploration/env_infos/reward_run Mean                  -0.0666445\n",
      "exploration/env_infos/reward_run Std                    0.710869\n",
      "exploration/env_infos/reward_run Max                    2.82202\n",
      "exploration/env_infos/reward_run Min                   -3.32084\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.352008\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.352008\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.352008\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.217867\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.217867\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.217867\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.233083\n",
      "exploration/env_infos/reward_ctrl Std                   0.0755647\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0348664\n",
      "exploration/env_infos/reward_ctrl Min                  -0.492326\n",
      "exploration/env_infos/final/height Mean                -0.541818\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.541818\n",
      "exploration/env_infos/final/height Min                 -0.541818\n",
      "exploration/env_infos/initial/height Mean              -0.0339607\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0339607\n",
      "exploration/env_infos/initial/height Min               -0.0339607\n",
      "exploration/env_infos/height Mean                      -0.257954\n",
      "exploration/env_infos/height Std                        0.25014\n",
      "exploration/env_infos/height Max                        0.245598\n",
      "exploration/env_infos/height Min                       -0.58212\n",
      "exploration/env_infos/final/reward_angular Mean        -1.37323\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -1.37323\n",
      "exploration/env_infos/final/reward_angular Min         -1.37323\n",
      "exploration/env_infos/initial/reward_angular Mean       2.67371\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        2.67371\n",
      "exploration/env_infos/initial/reward_angular Min        2.67371\n",
      "exploration/env_infos/reward_angular Mean              -0.0715942\n",
      "exploration/env_infos/reward_angular Std                1.5352\n",
      "exploration/env_infos/reward_angular Max                6.72876\n",
      "exploration/env_infos/reward_angular Min               -5.42711\n",
      "evaluation/num steps total                         150000\n",
      "evaluation/num paths total                            150\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.0685807\n",
      "evaluation/Rewards Std                                  0.161098\n",
      "evaluation/Rewards Max                                  1.82283\n",
      "evaluation/Rewards Min                                 -1.82883\n",
      "evaluation/Returns Mean                               -68.5807\n",
      "evaluation/Returns Std                                 32.2654\n",
      "evaluation/Returns Max                                 -8.81344\n",
      "evaluation/Returns Min                               -129.105\n",
      "evaluation/Actions Mean                                -0.0367056\n",
      "evaluation/Actions Std                                  0.166985\n",
      "evaluation/Actions Max                                  0.379412\n",
      "evaluation/Actions Min                                 -0.703487\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            -68.5807\n",
      "evaluation/env_infos/final/reward_run Mean              0.0217351\n",
      "evaluation/env_infos/final/reward_run Std               0.100285\n",
      "evaluation/env_infos/final/reward_run Max               0.388038\n",
      "evaluation/env_infos/final/reward_run Min              -0.182743\n",
      "evaluation/env_infos/initial/reward_run Mean           -0.0203769\n",
      "evaluation/env_infos/initial/reward_run Std             0.32156\n",
      "evaluation/env_infos/initial/reward_run Max             0.404459\n",
      "evaluation/env_infos/initial/reward_run Min            -0.629715\n",
      "evaluation/env_infos/reward_run Mean                    0.00440949\n",
      "evaluation/env_infos/reward_run Std                     0.0841415\n",
      "evaluation/env_infos/reward_run Max                     0.626553\n",
      "evaluation/env_infos/reward_run Min                    -0.787949\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0170032\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.018519\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.000170468\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0727327\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.03462\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0420491\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.000479868\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.166787\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0175388\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0187225\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.000133998\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.166787\n",
      "evaluation/env_infos/final/height Mean                 -0.13841\n",
      "evaluation/env_infos/final/height Std                   0.00844667\n",
      "evaluation/env_infos/final/height Max                  -0.127299\n",
      "evaluation/env_infos/final/height Min                  -0.157187\n",
      "evaluation/env_infos/initial/height Mean               -0.00683551\n",
      "evaluation/env_infos/initial/height Std                 0.0549776\n",
      "evaluation/env_infos/initial/height Max                 0.0749012\n",
      "evaluation/env_infos/initial/height Min                -0.0887581\n",
      "evaluation/env_infos/height Mean                       -0.136617\n",
      "evaluation/env_infos/height Std                         0.00987987\n",
      "evaluation/env_infos/height Max                         0.0749012\n",
      "evaluation/env_infos/height Min                        -0.191281\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.137028\n",
      "evaluation/env_infos/final/reward_angular Std           0.529334\n",
      "evaluation/env_infos/final/reward_angular Max           0.443995\n",
      "evaluation/env_infos/final/reward_angular Min          -2.28991\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.314337\n",
      "evaluation/env_infos/initial/reward_angular Std         0.836901\n",
      "evaluation/env_infos/initial/reward_angular Max         2.14796\n",
      "evaluation/env_infos/initial/reward_angular Min        -0.913752\n",
      "evaluation/env_infos/reward_angular Mean                0.000563942\n",
      "evaluation/env_infos/reward_angular Std                 0.527176\n",
      "evaluation/env_infos/reward_angular Max                 2.43343\n",
      "evaluation/env_infos/reward_angular Min                -2.32681\n",
      "time/data storing (s)                                   0.199675\n",
      "time/evaluation sampling (s)                           22.1906\n",
      "time/exploration sampling (s)                           0.987608\n",
      "time/logging (s)                                        0.227293\n",
      "time/saving (s)                                         0.0271507\n",
      "time/training (s)                                       3.3785\n",
      "time/epoch (s)                                         27.0108\n",
      "time/total (s)                                        167.967\n",
      "Epoch                                                   5\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:01:10.679692 PDT | [gher-halfcheetahhard-SAC-10e-1000s-disc0.99_2021_05_25_09_57_59_0000--s-10] Epoch 6 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  16000\n",
      "trainer/QF1 Loss                                        0.618858\n",
      "trainer/QF2 Loss                                        0.638064\n",
      "trainer/Policy Loss                                    -7.36215\n",
      "trainer/Q1 Predictions Mean                             3.54905\n",
      "trainer/Q1 Predictions Std                              0.936871\n",
      "trainer/Q1 Predictions Max                              6.48759\n",
      "trainer/Q1 Predictions Min                              1.43216\n",
      "trainer/Q2 Predictions Mean                             3.57137\n",
      "trainer/Q2 Predictions Std                              0.91441\n",
      "trainer/Q2 Predictions Max                              6.3373\n",
      "trainer/Q2 Predictions Min                              1.75238\n",
      "trainer/Q Targets Mean                                  3.28462\n",
      "trainer/Q Targets Std                                   1.0607\n",
      "trainer/Q Targets Max                                   6.72206\n",
      "trainer/Q Targets Min                                   0.29093\n",
      "trainer/Log Pis Mean                                   -3.73215\n",
      "trainer/Log Pis Std                                     0.971648\n",
      "trainer/Log Pis Max                                    -0.607943\n",
      "trainer/Log Pis Min                                    -6.77212\n",
      "trainer/Policy mu Mean                                  0.0233976\n",
      "trainer/Policy mu Std                                   0.293347\n",
      "trainer/Policy mu Max                                   1.05429\n",
      "trainer/Policy mu Min                                  -1.23054\n",
      "trainer/Policy log std Mean                            -0.0985227\n",
      "trainer/Policy log std Std                              0.0472278\n",
      "trainer/Policy log std Max                              0.0375583\n",
      "trainer/Policy log std Min                             -0.357001\n",
      "trainer/Alpha                                           0.166674\n",
      "trainer/Alpha Loss                                    -17.4088\n",
      "exploration/num steps total                          8000\n",
      "exploration/num paths total                             8\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.246212\n",
      "exploration/Rewards Std                                 0.866891\n",
      "exploration/Rewards Max                                 2.53109\n",
      "exploration/Rewards Min                                -3.36633\n",
      "exploration/Returns Mean                             -246.212\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -246.212\n",
      "exploration/Returns Min                              -246.212\n",
      "exploration/Actions Mean                                0.122555\n",
      "exploration/Actions Std                                 0.609619\n",
      "exploration/Actions Max                                 0.998795\n",
      "exploration/Actions Min                                -0.998332\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -246.212\n",
      "exploration/env_infos/final/reward_run Mean             0.428111\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.428111\n",
      "exploration/env_infos/final/reward_run Min              0.428111\n",
      "exploration/env_infos/initial/reward_run Mean           0.208693\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.208693\n",
      "exploration/env_infos/initial/reward_run Min            0.208693\n",
      "exploration/env_infos/reward_run Mean                  -0.172487\n",
      "exploration/env_infos/reward_run Std                    0.67674\n",
      "exploration/env_infos/reward_run Max                    1.50483\n",
      "exploration/env_infos/reward_run Min                   -2.84202\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.248302\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.248302\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.248302\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0886929\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0886929\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0886929\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.231993\n",
      "exploration/env_infos/reward_ctrl Std                   0.0734715\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0454025\n",
      "exploration/env_infos/reward_ctrl Min                  -0.485716\n",
      "exploration/env_infos/final/height Mean                -0.158408\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.158408\n",
      "exploration/env_infos/final/height Min                 -0.158408\n",
      "exploration/env_infos/initial/height Mean               0.083167\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.083167\n",
      "exploration/env_infos/initial/height Min                0.083167\n",
      "exploration/env_infos/height Mean                      -0.0630676\n",
      "exploration/env_infos/height Std                        0.101629\n",
      "exploration/env_infos/height Max                        0.257688\n",
      "exploration/env_infos/height Min                       -0.362717\n",
      "exploration/env_infos/final/reward_angular Mean        -3.39133\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -3.39133\n",
      "exploration/env_infos/final/reward_angular Min         -3.39133\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.278717\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.278717\n",
      "exploration/env_infos/initial/reward_angular Min       -0.278717\n",
      "exploration/env_infos/reward_angular Mean              -0.0153421\n",
      "exploration/env_infos/reward_angular Std                1.77889\n",
      "exploration/env_infos/reward_angular Max                5.02404\n",
      "exploration/env_infos/reward_angular Min               -5.92634\n",
      "evaluation/num steps total                         175000\n",
      "evaluation/num paths total                            175\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.0867431\n",
      "evaluation/Rewards Std                                  0.124793\n",
      "evaluation/Rewards Max                                  2.58589\n",
      "evaluation/Rewards Min                                 -1.70972\n",
      "evaluation/Returns Mean                               -86.7431\n",
      "evaluation/Returns Std                                 40.6635\n",
      "evaluation/Returns Max                                -18.3987\n",
      "evaluation/Returns Min                               -159.293\n",
      "evaluation/Actions Mean                                 0.0400706\n",
      "evaluation/Actions Std                                  0.227888\n",
      "evaluation/Actions Max                                  0.752624\n",
      "evaluation/Actions Min                                 -0.692885\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            -86.7431\n",
      "evaluation/env_infos/final/reward_run Mean              0.0185199\n",
      "evaluation/env_infos/final/reward_run Std               0.0676561\n",
      "evaluation/env_infos/final/reward_run Max               0.21961\n",
      "evaluation/env_infos/final/reward_run Min              -0.108948\n",
      "evaluation/env_infos/initial/reward_run Mean            0.0686919\n",
      "evaluation/env_infos/initial/reward_run Std             0.317116\n",
      "evaluation/env_infos/initial/reward_run Max             0.550132\n",
      "evaluation/env_infos/initial/reward_run Min            -0.559044\n",
      "evaluation/env_infos/reward_run Mean                   -0.00440058\n",
      "evaluation/env_infos/reward_run Std                     0.0830993\n",
      "evaluation/env_infos/reward_run Max                     0.776655\n",
      "evaluation/env_infos/reward_run Min                    -0.73603\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0318549\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0209406\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00273795\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0769263\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.051277\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0359845\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00535721\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.131502\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0321231\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0208252\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00112883\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.14125\n",
      "evaluation/env_infos/final/height Mean                 -0.16218\n",
      "evaluation/env_infos/final/height Std                   0.00843602\n",
      "evaluation/env_infos/final/height Max                  -0.147129\n",
      "evaluation/env_infos/final/height Min                  -0.17484\n",
      "evaluation/env_infos/initial/height Mean               -0.0142875\n",
      "evaluation/env_infos/initial/height Std                 0.0531584\n",
      "evaluation/env_infos/initial/height Max                 0.0901313\n",
      "evaluation/env_infos/initial/height Min                -0.103548\n",
      "evaluation/env_infos/height Mean                       -0.161716\n",
      "evaluation/env_infos/height Std                         0.0106921\n",
      "evaluation/env_infos/height Max                         0.0901313\n",
      "evaluation/env_infos/height Min                        -0.194455\n",
      "evaluation/env_infos/final/reward_angular Mean          0.0250402\n",
      "evaluation/env_infos/final/reward_angular Std           0.151362\n",
      "evaluation/env_infos/final/reward_angular Max           0.600276\n",
      "evaluation/env_infos/final/reward_angular Min          -0.256493\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.188335\n",
      "evaluation/env_infos/initial/reward_angular Std         1.14132\n",
      "evaluation/env_infos/initial/reward_angular Max         2.94458\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.53555\n",
      "evaluation/env_infos/reward_angular Mean                0.0016128\n",
      "evaluation/env_infos/reward_angular Std                 0.220359\n",
      "evaluation/env_infos/reward_angular Max                 2.94458\n",
      "evaluation/env_infos/reward_angular Min                -1.62738\n",
      "time/data storing (s)                                   0.207527\n",
      "time/evaluation sampling (s)                           21.987\n",
      "time/exploration sampling (s)                           1.02225\n",
      "time/logging (s)                                        0.233153\n",
      "time/saving (s)                                         0.02692\n",
      "time/training (s)                                       3.41011\n",
      "time/epoch (s)                                         26.8869\n",
      "time/total (s)                                        194.998\n",
      "Epoch                                                   6\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:01:38.629944 PDT | [gher-halfcheetahhard-SAC-10e-1000s-disc0.99_2021_05_25_09_57_59_0000--s-10] Epoch 7 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  18000\n",
      "trainer/QF1 Loss                                        0.756707\n",
      "trainer/QF2 Loss                                        0.795742\n",
      "trainer/Policy Loss                                    -6.67505\n",
      "trainer/Q1 Predictions Mean                             3.60044\n",
      "trainer/Q1 Predictions Std                              1.14055\n",
      "trainer/Q1 Predictions Max                              8.0607\n",
      "trainer/Q1 Predictions Min                              1.19896\n",
      "trainer/Q2 Predictions Mean                             3.6494\n",
      "trainer/Q2 Predictions Std                              1.13211\n",
      "trainer/Q2 Predictions Max                              8.0222\n",
      "trainer/Q2 Predictions Min                              1.34251\n",
      "trainer/Q Targets Mean                                  3.34327\n",
      "trainer/Q Targets Std                                   1.32525\n",
      "trainer/Q Targets Max                                   7.85943\n",
      "trainer/Q Targets Min                                  -0.399517\n",
      "trainer/Log Pis Mean                                   -2.78122\n",
      "trainer/Log Pis Std                                     1.92783\n",
      "trainer/Log Pis Max                                     3.53539\n",
      "trainer/Log Pis Min                                    -8.67364\n",
      "trainer/Policy mu Mean                                 -0.0494815\n",
      "trainer/Policy mu Std                                   0.612858\n",
      "trainer/Policy mu Max                                   1.82012\n",
      "trainer/Policy mu Min                                  -2.52114\n",
      "trainer/Policy log std Mean                            -0.194157\n",
      "trainer/Policy log std Std                              0.100554\n",
      "trainer/Policy log std Max                              0.0211623\n",
      "trainer/Policy log std Min                             -0.722651\n",
      "trainer/Alpha                                           0.12546\n",
      "trainer/Alpha Loss                                    -18.2034\n",
      "exploration/num steps total                          9000\n",
      "exploration/num paths total                             9\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.119874\n",
      "exploration/Rewards Std                                 0.466115\n",
      "exploration/Rewards Max                                 1.16502\n",
      "exploration/Rewards Min                                -1.8599\n",
      "exploration/Returns Mean                             -119.874\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -119.874\n",
      "exploration/Returns Min                              -119.874\n",
      "exploration/Actions Mean                                0.136391\n",
      "exploration/Actions Std                                 0.617787\n",
      "exploration/Actions Max                                 0.998884\n",
      "exploration/Actions Min                                -0.997019\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -119.874\n",
      "exploration/env_infos/final/reward_run Mean             0.453613\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.453613\n",
      "exploration/env_infos/final/reward_run Min              0.453613\n",
      "exploration/env_infos/initial/reward_run Mean          -0.0183583\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.0183583\n",
      "exploration/env_infos/initial/reward_run Min           -0.0183583\n",
      "exploration/env_infos/reward_run Mean                   0.0118233\n",
      "exploration/env_infos/reward_run Std                    0.569179\n",
      "exploration/env_infos/reward_run Max                    1.51284\n",
      "exploration/env_infos/reward_run Min                   -2.42251\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.402289\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.402289\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.402289\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.412188\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.412188\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.412188\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.240158\n",
      "exploration/env_infos/reward_ctrl Std                   0.0795444\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0261887\n",
      "exploration/env_infos/reward_ctrl Min                  -0.468041\n",
      "exploration/env_infos/final/height Mean                -0.545732\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.545732\n",
      "exploration/env_infos/final/height Min                 -0.545732\n",
      "exploration/env_infos/initial/height Mean              -0.0325457\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0325457\n",
      "exploration/env_infos/initial/height Min               -0.0325457\n",
      "exploration/env_infos/height Mean                      -0.0982426\n",
      "exploration/env_infos/height Std                        0.165385\n",
      "exploration/env_infos/height Max                        0.285481\n",
      "exploration/env_infos/height Min                       -0.583063\n",
      "exploration/env_infos/final/reward_angular Mean         0.763285\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.763285\n",
      "exploration/env_infos/final/reward_angular Min          0.763285\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.65687\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.65687\n",
      "exploration/env_infos/initial/reward_angular Min       -1.65687\n",
      "exploration/env_infos/reward_angular Mean               0.0791574\n",
      "exploration/env_infos/reward_angular Std                1.58727\n",
      "exploration/env_infos/reward_angular Max                6.23627\n",
      "exploration/env_infos/reward_angular Min               -4.68579\n",
      "evaluation/num steps total                         200000\n",
      "evaluation/num paths total                            200\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.132777\n",
      "evaluation/Rewards Std                                  0.0759706\n",
      "evaluation/Rewards Max                                  2.78251\n",
      "evaluation/Rewards Min                                 -1.0638\n",
      "evaluation/Returns Mean                              -132.777\n",
      "evaluation/Returns Std                                 43.3456\n",
      "evaluation/Returns Max                                -36.1507\n",
      "evaluation/Returns Min                               -218.046\n",
      "evaluation/Actions Mean                                 0.0433408\n",
      "evaluation/Actions Std                                  0.471419\n",
      "evaluation/Actions Max                                  0.954842\n",
      "evaluation/Actions Min                                 -0.9414\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -132.777\n",
      "evaluation/env_infos/final/reward_run Mean             -9.66807e-10\n",
      "evaluation/env_infos/final/reward_run Std               4.57797e-08\n",
      "evaluation/env_infos/final/reward_run Max               9.6599e-08\n",
      "evaluation/env_infos/final/reward_run Min              -9.02821e-08\n",
      "evaluation/env_infos/initial/reward_run Mean            0.112187\n",
      "evaluation/env_infos/initial/reward_run Std             0.396827\n",
      "evaluation/env_infos/initial/reward_run Max             0.720408\n",
      "evaluation/env_infos/initial/reward_run Min            -0.592392\n",
      "evaluation/env_infos/reward_run Mean                   -6.69451e-05\n",
      "evaluation/env_infos/reward_run Std                     0.0322021\n",
      "evaluation/env_infos/reward_run Max                     0.783915\n",
      "evaluation/env_infos/reward_run Min                    -0.826743\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.134407\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.077043\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0284697\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.249385\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.156359\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0834053\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0422939\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.30674\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.134469\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0770754\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0117695\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.319782\n",
      "evaluation/env_infos/final/height Mean                 -0.193995\n",
      "evaluation/env_infos/final/height Std                   0.0263918\n",
      "evaluation/env_infos/final/height Max                  -0.15233\n",
      "evaluation/env_infos/final/height Min                  -0.260997\n",
      "evaluation/env_infos/initial/height Mean               -0.0209212\n",
      "evaluation/env_infos/initial/height Std                 0.054776\n",
      "evaluation/env_infos/initial/height Max                 0.0834331\n",
      "evaluation/env_infos/initial/height Min                -0.111832\n",
      "evaluation/env_infos/height Mean                       -0.19355\n",
      "evaluation/env_infos/height Std                         0.0275912\n",
      "evaluation/env_infos/height Max                         0.0834331\n",
      "evaluation/env_infos/height Min                        -0.325015\n",
      "evaluation/env_infos/final/reward_angular Mean          5.94637e-09\n",
      "evaluation/env_infos/final/reward_angular Std           1.10523e-07\n",
      "evaluation/env_infos/final/reward_angular Max           3.56811e-07\n",
      "evaluation/env_infos/final/reward_angular Min          -2.29958e-07\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.338421\n",
      "evaluation/env_infos/initial/reward_angular Std         1.75986\n",
      "evaluation/env_infos/initial/reward_angular Max         3.51222\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.69801\n",
      "evaluation/env_infos/reward_angular Mean                0.00058145\n",
      "evaluation/env_infos/reward_angular Std                 0.0919566\n",
      "evaluation/env_infos/reward_angular Max                 3.51222\n",
      "evaluation/env_infos/reward_angular Min                -2.69801\n",
      "time/data storing (s)                                   0.195234\n",
      "time/evaluation sampling (s)                           22.2004\n",
      "time/exploration sampling (s)                           1.02697\n",
      "time/logging (s)                                        0.231959\n",
      "time/saving (s)                                         0.0266338\n",
      "time/training (s)                                       4.11208\n",
      "time/epoch (s)                                         27.7933\n",
      "time/total (s)                                        222.947\n",
      "Epoch                                                   7\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:02:06.186646 PDT | [gher-halfcheetahhard-SAC-10e-1000s-disc0.99_2021_05_25_09_57_59_0000--s-10] Epoch 8 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  20000\n",
      "trainer/QF1 Loss                                        0.6313\n",
      "trainer/QF2 Loss                                        0.630432\n",
      "trainer/Policy Loss                                    -6.04155\n",
      "trainer/Q1 Predictions Mean                             3.38944\n",
      "trainer/Q1 Predictions Std                              1.08124\n",
      "trainer/Q1 Predictions Max                              6.88109\n",
      "trainer/Q1 Predictions Min                              1.23364\n",
      "trainer/Q2 Predictions Mean                             3.35135\n",
      "trainer/Q2 Predictions Std                              1.05996\n",
      "trainer/Q2 Predictions Max                              7.26463\n",
      "trainer/Q2 Predictions Min                              1.14589\n",
      "trainer/Q Targets Mean                                  3.4985\n",
      "trainer/Q Targets Std                                   1.32146\n",
      "trainer/Q Targets Max                                   8.29782\n",
      "trainer/Q Targets Min                                  -0.130543\n",
      "trainer/Log Pis Mean                                   -2.39954\n",
      "trainer/Log Pis Std                                     1.97155\n",
      "trainer/Log Pis Max                                     3.99013\n",
      "trainer/Log Pis Min                                    -7.55537\n",
      "trainer/Policy mu Mean                                  0.0514352\n",
      "trainer/Policy mu Std                                   0.662141\n",
      "trainer/Policy mu Max                                   2.13216\n",
      "trainer/Policy mu Min                                  -1.80697\n",
      "trainer/Policy log std Mean                            -0.198457\n",
      "trainer/Policy log std Std                              0.098702\n",
      "trainer/Policy log std Max                              0.0131893\n",
      "trainer/Policy log std Min                             -0.651384\n",
      "trainer/Alpha                                           0.0958923\n",
      "trainer/Alpha Loss                                    -19.671\n",
      "exploration/num steps total                         10000\n",
      "exploration/num paths total                            10\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.342359\n",
      "exploration/Rewards Std                                 0.682113\n",
      "exploration/Rewards Max                                 3.05695\n",
      "exploration/Rewards Min                                -3.24711\n",
      "exploration/Returns Mean                             -342.359\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -342.359\n",
      "exploration/Returns Min                              -342.359\n",
      "exploration/Actions Mean                                0.413533\n",
      "exploration/Actions Std                                 0.572082\n",
      "exploration/Actions Max                                 0.999265\n",
      "exploration/Actions Min                                -0.996953\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -342.359\n",
      "exploration/env_infos/final/reward_run Mean            -0.582043\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.582043\n",
      "exploration/env_infos/final/reward_run Min             -0.582043\n",
      "exploration/env_infos/initial/reward_run Mean           0.663165\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.663165\n",
      "exploration/env_infos/initial/reward_run Min            0.663165\n",
      "exploration/env_infos/reward_run Mean                  -0.122804\n",
      "exploration/env_infos/reward_run Std                    0.455484\n",
      "exploration/env_infos/reward_run Max                    1.31981\n",
      "exploration/env_infos/reward_run Min                   -1.57574\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.339511\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.339511\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.339511\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.301138\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.301138\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.301138\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.298972\n",
      "exploration/env_infos/reward_ctrl Std                   0.0701669\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0527345\n",
      "exploration/env_infos/reward_ctrl Min                  -0.49228\n",
      "exploration/env_infos/final/height Mean                -0.528892\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.528892\n",
      "exploration/env_infos/final/height Min                 -0.528892\n",
      "exploration/env_infos/initial/height Mean              -0.0189135\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0189135\n",
      "exploration/env_infos/initial/height Min               -0.0189135\n",
      "exploration/env_infos/height Mean                      -0.408545\n",
      "exploration/env_infos/height Std                        0.214591\n",
      "exploration/env_infos/height Max                        0.130418\n",
      "exploration/env_infos/height Min                       -0.583651\n",
      "exploration/env_infos/final/reward_angular Mean         0.42306\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.42306\n",
      "exploration/env_infos/final/reward_angular Min          0.42306\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.71489\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.71489\n",
      "exploration/env_infos/initial/reward_angular Min       -1.71489\n",
      "exploration/env_infos/reward_angular Mean               0.0776685\n",
      "exploration/env_infos/reward_angular Std                0.880495\n",
      "exploration/env_infos/reward_angular Max                4.02612\n",
      "exploration/env_infos/reward_angular Min               -4.08536\n",
      "evaluation/num steps total                         225000\n",
      "evaluation/num paths total                            225\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.129252\n",
      "evaluation/Rewards Std                                  0.135403\n",
      "evaluation/Rewards Max                                  2.20297\n",
      "evaluation/Rewards Min                                 -1.77547\n",
      "evaluation/Returns Mean                              -129.252\n",
      "evaluation/Returns Std                                 44.5757\n",
      "evaluation/Returns Max                                -23.3847\n",
      "evaluation/Returns Min                               -201.906\n",
      "evaluation/Actions Mean                                 0.110585\n",
      "evaluation/Actions Std                                  0.495035\n",
      "evaluation/Actions Max                                  0.965692\n",
      "evaluation/Actions Min                                 -0.954759\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -129.252\n",
      "evaluation/env_infos/final/reward_run Mean             -0.0334905\n",
      "evaluation/env_infos/final/reward_run Std               0.157931\n",
      "evaluation/env_infos/final/reward_run Max               0.413177\n",
      "evaluation/env_infos/final/reward_run Min              -0.570943\n",
      "evaluation/env_infos/initial/reward_run Mean            0.201164\n",
      "evaluation/env_infos/initial/reward_run Std             0.405678\n",
      "evaluation/env_infos/initial/reward_run Max             0.799415\n",
      "evaluation/env_infos/initial/reward_run Min            -0.42766\n",
      "evaluation/env_infos/reward_run Mean                    0.00816566\n",
      "evaluation/env_infos/reward_run Std                     0.18296\n",
      "evaluation/env_infos/reward_run Max                     1.20197\n",
      "evaluation/env_infos/reward_run Min                    -1.10154\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.151981\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0820215\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0289385\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.284515\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.171578\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0787429\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0474282\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.319586\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.154373\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0804896\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00662415\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.366147\n",
      "evaluation/env_infos/final/height Mean                 -0.178096\n",
      "evaluation/env_infos/final/height Std                   0.0446509\n",
      "evaluation/env_infos/final/height Max                  -0.0735348\n",
      "evaluation/env_infos/final/height Min                  -0.306462\n",
      "evaluation/env_infos/initial/height Mean               -0.012485\n",
      "evaluation/env_infos/initial/height Std                 0.0555195\n",
      "evaluation/env_infos/initial/height Max                 0.0747204\n",
      "evaluation/env_infos/initial/height Min                -0.106474\n",
      "evaluation/env_infos/height Mean                       -0.177817\n",
      "evaluation/env_infos/height Std                         0.0417314\n",
      "evaluation/env_infos/height Max                         0.0747204\n",
      "evaluation/env_infos/height Min                        -0.411202\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.0825659\n",
      "evaluation/env_infos/final/reward_angular Std           0.462588\n",
      "evaluation/env_infos/final/reward_angular Max           0.441621\n",
      "evaluation/env_infos/final/reward_angular Min          -1.81916\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.242343\n",
      "evaluation/env_infos/initial/reward_angular Std         1.54954\n",
      "evaluation/env_infos/initial/reward_angular Max         3.04896\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.38663\n",
      "evaluation/env_infos/reward_angular Mean                0.00355242\n",
      "evaluation/env_infos/reward_angular Std                 0.414573\n",
      "evaluation/env_infos/reward_angular Max                 3.04896\n",
      "evaluation/env_infos/reward_angular Min                -2.92666\n",
      "time/data storing (s)                                   0.199655\n",
      "time/evaluation sampling (s)                           21.9226\n",
      "time/exploration sampling (s)                           1.04258\n",
      "time/logging (s)                                        0.23604\n",
      "time/saving (s)                                         0.027542\n",
      "time/training (s)                                       3.97267\n",
      "time/epoch (s)                                         27.4011\n",
      "time/total (s)                                        250.507\n",
      "Epoch                                                   8\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:02:33.547986 PDT | [gher-halfcheetahhard-SAC-10e-1000s-disc0.99_2021_05_25_09_57_59_0000--s-10] Epoch 9 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  22000\n",
      "trainer/QF1 Loss                                        0.729046\n",
      "trainer/QF2 Loss                                        0.817169\n",
      "trainer/Policy Loss                                    -5.27208\n",
      "trainer/Q1 Predictions Mean                             3.38068\n",
      "trainer/Q1 Predictions Std                              1.20798\n",
      "trainer/Q1 Predictions Max                              8.63646\n",
      "trainer/Q1 Predictions Min                              1.44756\n",
      "trainer/Q2 Predictions Mean                             3.34635\n",
      "trainer/Q2 Predictions Std                              1.15232\n",
      "trainer/Q2 Predictions Max                              8.41776\n",
      "trainer/Q2 Predictions Min                              1.33058\n",
      "trainer/Q Targets Mean                                  3.4622\n",
      "trainer/Q Targets Std                                   1.50635\n",
      "trainer/Q Targets Max                                  10.2522\n",
      "trainer/Q Targets Min                                   0.618852\n",
      "trainer/Log Pis Mean                                   -1.61305\n",
      "trainer/Log Pis Std                                     2.23752\n",
      "trainer/Log Pis Max                                     4.19918\n",
      "trainer/Log Pis Min                                    -7.55467\n",
      "trainer/Policy mu Mean                                  0.0171423\n",
      "trainer/Policy mu Std                                   0.79389\n",
      "trainer/Policy mu Max                                   2.23547\n",
      "trainer/Policy mu Min                                  -2.71418\n",
      "trainer/Policy log std Mean                            -0.244133\n",
      "trainer/Policy log std Std                              0.149911\n",
      "trainer/Policy log std Max                              0.055548\n",
      "trainer/Policy log std Min                             -0.938146\n",
      "trainer/Alpha                                           0.0745247\n",
      "trainer/Alpha Loss                                    -19.7501\n",
      "exploration/num steps total                         11000\n",
      "exploration/num paths total                            11\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.35336\n",
      "exploration/Rewards Std                                 0.124453\n",
      "exploration/Rewards Max                                 0.412037\n",
      "exploration/Rewards Min                                -0.930388\n",
      "exploration/Returns Mean                             -353.36\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -353.36\n",
      "exploration/Returns Min                              -353.36\n",
      "exploration/Actions Mean                                0.347445\n",
      "exploration/Actions Std                                 0.570715\n",
      "exploration/Actions Max                                 0.998299\n",
      "exploration/Actions Min                                -0.98742\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -353.36\n",
      "exploration/env_infos/final/reward_run Mean             0.189251\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.189251\n",
      "exploration/env_infos/final/reward_run Min              0.189251\n",
      "exploration/env_infos/initial/reward_run Mean           0.337454\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.337454\n",
      "exploration/env_infos/initial/reward_run Min            0.337454\n",
      "exploration/env_infos/reward_run Mean                   0.0150351\n",
      "exploration/env_infos/reward_run Std                    0.414262\n",
      "exploration/env_infos/reward_run Max                    2.26143\n",
      "exploration/env_infos/reward_run Min                   -1.92002\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.327942\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.327942\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.327942\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.236299\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.236299\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.236299\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.267861\n",
      "exploration/env_infos/reward_ctrl Std                   0.0756034\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0697153\n",
      "exploration/env_infos/reward_ctrl Min                  -0.481177\n",
      "exploration/env_infos/final/height Mean                -0.570686\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.570686\n",
      "exploration/env_infos/final/height Min                 -0.570686\n",
      "exploration/env_infos/initial/height Mean               0.0146879\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0146879\n",
      "exploration/env_infos/initial/height Min                0.0146879\n",
      "exploration/env_infos/height Mean                      -0.473688\n",
      "exploration/env_infos/height Std                        0.217013\n",
      "exploration/env_infos/height Max                        0.34881\n",
      "exploration/env_infos/height Min                       -0.583605\n",
      "exploration/env_infos/final/reward_angular Mean         0.106289\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.106289\n",
      "exploration/env_infos/final/reward_angular Min          0.106289\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.61176\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.61176\n",
      "exploration/env_infos/initial/reward_angular Min       -1.61176\n",
      "exploration/env_infos/reward_angular Mean               0.0771873\n",
      "exploration/env_infos/reward_angular Std                1.01333\n",
      "exploration/env_infos/reward_angular Max                4.85413\n",
      "exploration/env_infos/reward_angular Min               -5.59998\n",
      "evaluation/num steps total                         250000\n",
      "evaluation/num paths total                            250\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.137387\n",
      "evaluation/Rewards Std                                  0.259149\n",
      "evaluation/Rewards Max                                  2.84197\n",
      "evaluation/Rewards Min                                 -1.7223\n",
      "evaluation/Returns Mean                              -137.387\n",
      "evaluation/Returns Std                                 52.0851\n",
      "evaluation/Returns Max                                -32.3667\n",
      "evaluation/Returns Min                               -216.515\n",
      "evaluation/Actions Mean                                 0.102843\n",
      "evaluation/Actions Std                                  0.516884\n",
      "evaluation/Actions Max                                  0.964435\n",
      "evaluation/Actions Min                                 -0.96514\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -137.387\n",
      "evaluation/env_infos/final/reward_run Mean              0.00646132\n",
      "evaluation/env_infos/final/reward_run Std               0.0582176\n",
      "evaluation/env_infos/final/reward_run Max               0.195052\n",
      "evaluation/env_infos/final/reward_run Min              -0.149484\n",
      "evaluation/env_infos/initial/reward_run Mean            0.119484\n",
      "evaluation/env_infos/initial/reward_run Std             0.425254\n",
      "evaluation/env_infos/initial/reward_run Max             1.01129\n",
      "evaluation/env_infos/initial/reward_run Min            -0.54542\n",
      "evaluation/env_infos/reward_run Mean                    0.0102415\n",
      "evaluation/env_infos/reward_run Std                     0.0877591\n",
      "evaluation/env_infos/reward_run Max                     1.40393\n",
      "evaluation/env_infos/reward_run Min                    -0.855943\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.167558\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.083619\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0315087\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.296722\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.186544\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0975967\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0239676\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.372941\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.166647\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0841036\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00708537\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.422039\n",
      "evaluation/env_infos/final/height Mean                 -0.195411\n",
      "evaluation/env_infos/final/height Std                   0.02317\n",
      "evaluation/env_infos/final/height Max                  -0.147509\n",
      "evaluation/env_infos/final/height Min                  -0.247889\n",
      "evaluation/env_infos/initial/height Mean               -0.0186976\n",
      "evaluation/env_infos/initial/height Std                 0.0515174\n",
      "evaluation/env_infos/initial/height Max                 0.0603774\n",
      "evaluation/env_infos/initial/height Min                -0.106238\n",
      "evaluation/env_infos/height Mean                       -0.193655\n",
      "evaluation/env_infos/height Std                         0.026255\n",
      "evaluation/env_infos/height Max                         0.0603774\n",
      "evaluation/env_infos/height Min                        -0.366644\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.0382593\n",
      "evaluation/env_infos/final/reward_angular Std           0.833319\n",
      "evaluation/env_infos/final/reward_angular Max           1.52865\n",
      "evaluation/env_infos/final/reward_angular Min          -3.75577\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.383913\n",
      "evaluation/env_infos/initial/reward_angular Std         1.68768\n",
      "evaluation/env_infos/initial/reward_angular Max         2.59155\n",
      "evaluation/env_infos/initial/reward_angular Min        -3.06504\n",
      "evaluation/env_infos/reward_angular Mean               -0.000726109\n",
      "evaluation/env_infos/reward_angular Std                 0.836766\n",
      "evaluation/env_infos/reward_angular Max                 4.89164\n",
      "evaluation/env_infos/reward_angular Min                -4.53434\n",
      "time/data storing (s)                                   0.202106\n",
      "time/evaluation sampling (s)                           21.9899\n",
      "time/exploration sampling (s)                           1.05073\n",
      "time/logging (s)                                        0.236209\n",
      "time/saving (s)                                         0.0526009\n",
      "time/training (s)                                       3.66434\n",
      "time/epoch (s)                                         27.1959\n",
      "time/total (s)                                        277.868\n",
      "Epoch                                                   9\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doodad not detected\n",
      "objc[15033]: Class GLFWApplicationDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a22cd7778) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a22d58740). One of the two will be used. Which one is undefined.\n",
      "objc[15033]: Class GLFWWindowDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a22cd7700) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a22d58768). One of the two will be used. Which one is undefined.\n",
      "objc[15033]: Class GLFWContentView is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a22cd77a0) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a22d587b8). One of the two will be used. Which one is undefined.\n",
      "objc[15033]: Class GLFWWindow is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a22cd7818) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a22d58830). One of the two will be used. Which one is undefined.\n",
      "2021-05-25 10:02:39.360811 PDT | Variant:\n",
      "2021-05-25 10:02:39.361432 PDT | {\n",
      "  \"seed\": 10,\n",
      "  \"algorithm\": \"SAC\",\n",
      "  \"env_name\": \"halfcheetahhard\",\n",
      "  \"algo_kwargs\": {\n",
      "    \"batch_size\": 256,\n",
      "    \"num_epochs\": 10,\n",
      "    \"num_eval_steps_per_epoch\": 25000,\n",
      "    \"num_expl_steps_per_train_loop\": 1000,\n",
      "    \"num_trains_per_train_loop\": 100,\n",
      "    \"min_num_steps_before_training\": 1000,\n",
      "    \"max_path_length\": 1000,\n",
      "    \"num_train_loops_per_epoch\": 1\n",
      "  },\n",
      "  \"trainer_kwargs\": {\n",
      "    \"discount\": 0.99,\n",
      "    \"soft_target_tau\": 0.005,\n",
      "    \"target_update_period\": 1,\n",
      "    \"policy_lr\": 0.003,\n",
      "    \"qf_lr\": 0.003,\n",
      "    \"reward_scale\": 1,\n",
      "    \"use_automatic_entropy_tuning\": true\n",
      "  },\n",
      "  \"replay_buffer_kwargs\": {\n",
      "    \"max_replay_buffer_size\": 1000000,\n",
      "    \"latent_dim\": 4,\n",
      "    \"approx_irl\": true,\n",
      "    \"plot\": false\n",
      "  },\n",
      "  \"relabeler_kwargs\": {\n",
      "    \"relabel\": true,\n",
      "    \"use_adv\": true,\n",
      "    \"n_sampled_latents\": 100,\n",
      "    \"n_to_take\": 1,\n",
      "    \"cache\": true\n",
      "  },\n",
      "  \"qf_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      256,\n",
      "      256\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"policy_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      256,\n",
      "      256\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"path_collector_kwargs\": {\n",
      "    \"save_videos\": false\n",
      "  },\n",
      "  \"use_advantages\": true,\n",
      "  \"proper_advantages\": true,\n",
      "  \"plot\": false,\n",
      "  \"test\": false,\n",
      "  \"gpu\": 0,\n",
      "  \"mode\": \"here_no_doodad\",\n",
      "  \"local_docker\": false,\n",
      "  \"insert_time\": false,\n",
      "  \"latent_shape_multiplier\": 1\n",
      "}\n",
      "halfcheetah\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "2021-05-25 10:03:07.954203 PDT | [gher-halfcheetahhard-SAC-10e-1000s-disc0.99_2021_05_25_10_02_39_0000--s-10] Epoch 0 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  4000\n",
      "trainer/QF1 Loss                                      16.6802\n",
      "trainer/QF2 Loss                                      16.5885\n",
      "trainer/Policy Loss                                   -4.03334\n",
      "trainer/Q1 Predictions Mean                           -0.00737131\n",
      "trainer/Q1 Predictions Std                             0.00557663\n",
      "trainer/Q1 Predictions Max                             0.00463407\n",
      "trainer/Q1 Predictions Min                            -0.0336251\n",
      "trainer/Q2 Predictions Mean                            0.00415807\n",
      "trainer/Q2 Predictions Std                             0.0048979\n",
      "trainer/Q2 Predictions Max                             0.0203202\n",
      "trainer/Q2 Predictions Min                            -0.00775022\n",
      "trainer/Q Targets Mean                                 3.92335\n",
      "trainer/Q Targets Std                                  1.10908\n",
      "trainer/Q Targets Max                                  7.1054\n",
      "trainer/Q Targets Min                                  1.18374\n",
      "trainer/Log Pis Mean                                  -4.04084\n",
      "trainer/Log Pis Std                                    0.508067\n",
      "trainer/Log Pis Max                                   -2.36323\n",
      "trainer/Log Pis Min                                   -5.48163\n",
      "trainer/Policy mu Mean                                -0.000934237\n",
      "trainer/Policy mu Std                                  0.002431\n",
      "trainer/Policy mu Max                                  0.00722954\n",
      "trainer/Policy mu Min                                 -0.00893305\n",
      "trainer/Policy log std Mean                           -0.00130592\n",
      "trainer/Policy log std Std                             0.00165964\n",
      "trainer/Policy log std Max                             0.00340147\n",
      "trainer/Policy log std Min                            -0.00757277\n",
      "trainer/Alpha                                          0.997005\n",
      "trainer/Alpha Loss                                    -0\n",
      "exploration/num steps total                         2000\n",
      "exploration/num paths total                            2\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.136377\n",
      "exploration/Rewards Std                                0.476496\n",
      "exploration/Rewards Max                                1.4954\n",
      "exploration/Rewards Min                               -1.58775\n",
      "exploration/Returns Mean                            -136.377\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -136.377\n",
      "exploration/Returns Min                             -136.377\n",
      "exploration/Actions Mean                               0.00119015\n",
      "exploration/Actions Std                                0.62711\n",
      "exploration/Actions Max                                0.9991\n",
      "exploration/Actions Min                               -0.999803\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -136.377\n",
      "exploration/env_infos/final/reward_run Mean           -1.17973\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max            -1.17973\n",
      "exploration/env_infos/final/reward_run Min            -1.17973\n",
      "exploration/env_infos/initial/reward_run Mean         -0.374905\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max          -0.374905\n",
      "exploration/env_infos/initial/reward_run Min          -0.374905\n",
      "exploration/env_infos/reward_run Mean                 -0.166443\n",
      "exploration/env_infos/reward_run Std                   0.640606\n",
      "exploration/env_infos/reward_run Max                   1.61939\n",
      "exploration/env_infos/reward_run Min                  -2.0604\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.112643\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.112643\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.112643\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.14458\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.14458\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.14458\n",
      "exploration/env_infos/reward_ctrl Mean                -0.235961\n",
      "exploration/env_infos/reward_ctrl Std                  0.0750451\n",
      "exploration/env_infos/reward_ctrl Max                 -0.018194\n",
      "exploration/env_infos/reward_ctrl Min                 -0.475831\n",
      "exploration/env_infos/final/height Mean                0.0706641\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                 0.0706641\n",
      "exploration/env_infos/final/height Min                 0.0706641\n",
      "exploration/env_infos/initial/height Mean             -0.0710219\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max              -0.0710219\n",
      "exploration/env_infos/initial/height Min              -0.0710219\n",
      "exploration/env_infos/height Mean                     -0.063873\n",
      "exploration/env_infos/height Std                       0.0876588\n",
      "exploration/env_infos/height Max                       0.20319\n",
      "exploration/env_infos/height Min                      -0.360702\n",
      "exploration/env_infos/final/reward_angular Mean       -2.15328\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max        -2.15328\n",
      "exploration/env_infos/final/reward_angular Min        -2.15328\n",
      "exploration/env_infos/initial/reward_angular Mean     -0.122531\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max      -0.122531\n",
      "exploration/env_infos/initial/reward_angular Min      -0.122531\n",
      "exploration/env_infos/reward_angular Mean              0.0139977\n",
      "exploration/env_infos/reward_angular Std               1.69039\n",
      "exploration/env_infos/reward_angular Max               7.79906\n",
      "exploration/env_infos/reward_angular Min              -5.07586\n",
      "evaluation/num steps total                         25000\n",
      "evaluation/num paths total                            25\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.0615483\n",
      "evaluation/Rewards Std                                 0.0463997\n",
      "evaluation/Rewards Max                                 1.32204\n",
      "evaluation/Rewards Min                                -1.09337\n",
      "evaluation/Returns Mean                              -61.5483\n",
      "evaluation/Returns Std                                38.1997\n",
      "evaluation/Returns Max                                -1.6116\n",
      "evaluation/Returns Min                              -127.487\n",
      "evaluation/Actions Mean                               -0.00032298\n",
      "evaluation/Actions Std                                 0.00121843\n",
      "evaluation/Actions Max                                 0.00337558\n",
      "evaluation/Actions Min                                -0.0036283\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           -61.5483\n",
      "evaluation/env_infos/final/reward_run Mean             2.26598e-18\n",
      "evaluation/env_infos/final/reward_run Std              2.22662e-16\n",
      "evaluation/env_infos/final/reward_run Max              8.32667e-16\n",
      "evaluation/env_infos/final/reward_run Min             -5.55112e-16\n",
      "evaluation/env_infos/initial/reward_run Mean           0.0173362\n",
      "evaluation/env_infos/initial/reward_run Std            0.108441\n",
      "evaluation/env_infos/initial/reward_run Max            0.244668\n",
      "evaluation/env_infos/initial/reward_run Min           -0.173308\n",
      "evaluation/env_infos/reward_run Mean                  -0.000210638\n",
      "evaluation/env_infos/reward_run Std                    0.0146511\n",
      "evaluation/env_infos/reward_run Max                    0.400855\n",
      "evaluation/env_infos/reward_run Min                   -0.377249\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -9.52177e-07\n",
      "evaluation/env_infos/final/reward_ctrl Std             9.0322e-08\n",
      "evaluation/env_infos/final/reward_ctrl Max            -8.07404e-07\n",
      "evaluation/env_infos/final/reward_ctrl Min            -1.10654e-06\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -1.00256e-06\n",
      "evaluation/env_infos/initial/reward_ctrl Std           1.05861e-07\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -8.13546e-07\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -1.20276e-06\n",
      "evaluation/env_infos/reward_ctrl Mean                 -9.53335e-07\n",
      "evaluation/env_infos/reward_ctrl Std                   9.70461e-08\n",
      "evaluation/env_infos/reward_ctrl Max                  -6.32376e-07\n",
      "evaluation/env_infos/reward_ctrl Min                  -3.2053e-06\n",
      "evaluation/env_infos/final/height Mean                -0.132891\n",
      "evaluation/env_infos/final/height Std                  3.5865e-05\n",
      "evaluation/env_infos/final/height Max                 -0.132829\n",
      "evaluation/env_infos/final/height Min                 -0.132952\n",
      "evaluation/env_infos/initial/height Mean              -0.00512014\n",
      "evaluation/env_infos/initial/height Std                0.0600862\n",
      "evaluation/env_infos/initial/height Max                0.0882252\n",
      "evaluation/env_infos/initial/height Min               -0.0959662\n",
      "evaluation/env_infos/height Mean                      -0.132401\n",
      "evaluation/env_infos/height Std                        0.00616916\n",
      "evaluation/env_infos/height Max                        0.0882252\n",
      "evaluation/env_infos/height Min                       -0.141504\n",
      "evaluation/env_infos/final/reward_angular Mean        -6.84683e-18\n",
      "evaluation/env_infos/final/reward_angular Std          1.55547e-15\n",
      "evaluation/env_infos/final/reward_angular Max          3.28736e-15\n",
      "evaluation/env_infos/final/reward_angular Min         -2.5144e-15\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.074782\n",
      "evaluation/env_infos/initial/reward_angular Std        0.354898\n",
      "evaluation/env_infos/initial/reward_angular Max        1.18051\n",
      "evaluation/env_infos/initial/reward_angular Min       -0.50039\n",
      "evaluation/env_infos/reward_angular Mean               0.00114103\n",
      "evaluation/env_infos/reward_angular Std                0.0450042\n",
      "evaluation/env_infos/reward_angular Max                2.18753\n",
      "evaluation/env_infos/reward_angular Min               -1.38548\n",
      "time/data storing (s)                                  0.0689242\n",
      "time/evaluation sampling (s)                          22.2747\n",
      "time/exploration sampling (s)                          1.08234\n",
      "time/logging (s)                                       0.24479\n",
      "time/saving (s)                                        0.471237\n",
      "time/training (s)                                      3.47003\n",
      "time/epoch (s)                                        27.612\n",
      "time/total (s)                                        32.0328\n",
      "Epoch                                                  0\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shanliyu/School/CSE257/own/generalized-hindsight/rlkit/data_management/task_relabeling_replay_buffer.py:401: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \"relabeled_latents\": np.array(self.relabeled_latents),\n",
      "2021-05-25 10:03:34.848560 PDT | [gher-halfcheetahhard-SAC-10e-1000s-disc0.99_2021_05_25_10_02_39_0000--s-10] Epoch 1 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  6000\n",
      "trainer/QF1 Loss                                       0.743806\n",
      "trainer/QF2 Loss                                       0.766513\n",
      "trainer/Policy Loss                                   -7.38367\n",
      "trainer/Q1 Predictions Mean                            3.34326\n",
      "trainer/Q1 Predictions Std                             0.614642\n",
      "trainer/Q1 Predictions Max                             5.34839\n",
      "trainer/Q1 Predictions Min                             1.35027\n",
      "trainer/Q2 Predictions Mean                            3.34056\n",
      "trainer/Q2 Predictions Std                             0.607186\n",
      "trainer/Q2 Predictions Max                             5.23391\n",
      "trainer/Q2 Predictions Min                             1.43813\n",
      "trainer/Q Targets Mean                                 3.38564\n",
      "trainer/Q Targets Std                                  0.951928\n",
      "trainer/Q Targets Max                                  6.50674\n",
      "trainer/Q Targets Min                                 -0.0394452\n",
      "trainer/Log Pis Mean                                  -4.02402\n",
      "trainer/Log Pis Std                                    0.532243\n",
      "trainer/Log Pis Max                                   -2.63937\n",
      "trainer/Log Pis Min                                   -6.76312\n",
      "trainer/Policy mu Mean                                -0.130934\n",
      "trainer/Policy mu Std                                  0.0964565\n",
      "trainer/Policy mu Max                                 -0.000436069\n",
      "trainer/Policy mu Min                                 -0.474782\n",
      "trainer/Policy log std Mean                           -0.119606\n",
      "trainer/Policy log std Std                             0.0234164\n",
      "trainer/Policy log std Max                            -0.0809322\n",
      "trainer/Policy log std Min                            -0.23082\n",
      "trainer/Alpha                                          0.738881\n",
      "trainer/Alpha Loss                                    -3.00347\n",
      "exploration/num steps total                         3000\n",
      "exploration/num paths total                            3\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.228972\n",
      "exploration/Rewards Std                                0.192434\n",
      "exploration/Rewards Max                                0.352685\n",
      "exploration/Rewards Min                               -0.946785\n",
      "exploration/Returns Mean                            -228.972\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -228.972\n",
      "exploration/Returns Min                             -228.972\n",
      "exploration/Actions Mean                              -0.0861007\n",
      "exploration/Actions Std                                0.59439\n",
      "exploration/Actions Max                                0.993912\n",
      "exploration/Actions Min                               -0.998908\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -228.972\n",
      "exploration/env_infos/final/reward_run Mean            0.710108\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max             0.710108\n",
      "exploration/env_infos/final/reward_run Min             0.710108\n",
      "exploration/env_infos/initial/reward_run Mean         -0.201483\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max          -0.201483\n",
      "exploration/env_infos/initial/reward_run Min          -0.201483\n",
      "exploration/env_infos/reward_run Mean                 -0.149074\n",
      "exploration/env_infos/reward_run Std                   0.654058\n",
      "exploration/env_infos/reward_run Max                   1.89179\n",
      "exploration/env_infos/reward_run Min                  -2.44908\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.204652\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.204652\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.204652\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.230986\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.230986\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.230986\n",
      "exploration/env_infos/reward_ctrl Mean                -0.216427\n",
      "exploration/env_infos/reward_ctrl Std                  0.0720325\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0160272\n",
      "exploration/env_infos/reward_ctrl Min                 -0.500249\n",
      "exploration/env_infos/final/height Mean               -0.0224901\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.0224901\n",
      "exploration/env_infos/final/height Min                -0.0224901\n",
      "exploration/env_infos/initial/height Mean              0.0289295\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max               0.0289295\n",
      "exploration/env_infos/initial/height Min               0.0289295\n",
      "exploration/env_infos/height Mean                     -0.0837714\n",
      "exploration/env_infos/height Std                       0.0781987\n",
      "exploration/env_infos/height Max                       0.265628\n",
      "exploration/env_infos/height Min                      -0.299167\n",
      "exploration/env_infos/final/reward_angular Mean       -0.0517604\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max        -0.0517604\n",
      "exploration/env_infos/final/reward_angular Min        -0.0517604\n",
      "exploration/env_infos/initial/reward_angular Mean      1.03437\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       1.03437\n",
      "exploration/env_infos/initial/reward_angular Min       1.03437\n",
      "exploration/env_infos/reward_angular Mean             -0.0153459\n",
      "exploration/env_infos/reward_angular Std               1.63607\n",
      "exploration/env_infos/reward_angular Max               4.59191\n",
      "exploration/env_infos/reward_angular Min              -5.2682\n",
      "evaluation/num steps total                         50000\n",
      "evaluation/num paths total                            50\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.0632997\n",
      "evaluation/Rewards Std                                 0.0534852\n",
      "evaluation/Rewards Max                                 1.22113\n",
      "evaluation/Rewards Min                                -2.18749\n",
      "evaluation/Returns Mean                              -63.2997\n",
      "evaluation/Returns Std                                36.9538\n",
      "evaluation/Returns Max                                -3.96147\n",
      "evaluation/Returns Min                              -125.662\n",
      "evaluation/Actions Mean                               -0.0916443\n",
      "evaluation/Actions Std                                 0.0650338\n",
      "evaluation/Actions Max                                -0.00698582\n",
      "evaluation/Actions Min                                -0.281051\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           -63.2997\n",
      "evaluation/env_infos/final/reward_run Mean             1.02067e-16\n",
      "evaluation/env_infos/final/reward_run Std              2.54846e-16\n",
      "evaluation/env_infos/final/reward_run Max              8.32667e-16\n",
      "evaluation/env_infos/final/reward_run Min             -5.55112e-16\n",
      "evaluation/env_infos/initial/reward_run Mean          -0.0405053\n",
      "evaluation/env_infos/initial/reward_run Std            0.172324\n",
      "evaluation/env_infos/initial/reward_run Max            0.330372\n",
      "evaluation/env_infos/initial/reward_run Min           -0.418189\n",
      "evaluation/env_infos/reward_run Mean                  -0.000100539\n",
      "evaluation/env_infos/reward_run Std                    0.0190179\n",
      "evaluation/env_infos/reward_run Max                    0.330372\n",
      "evaluation/env_infos/reward_run Min                   -0.536875\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.00756925\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.00116305\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.00562747\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.00919956\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.00746772\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.00121161\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.00530501\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.00918547\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.00757684\n",
      "evaluation/env_infos/reward_ctrl Std                   0.00116853\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00511199\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.0143009\n",
      "evaluation/env_infos/final/height Mean                -0.130147\n",
      "evaluation/env_infos/final/height Std                  0.000177086\n",
      "evaluation/env_infos/final/height Max                 -0.129879\n",
      "evaluation/env_infos/final/height Min                 -0.130414\n",
      "evaluation/env_infos/initial/height Mean              -0.00180861\n",
      "evaluation/env_infos/initial/height Std                0.0540812\n",
      "evaluation/env_infos/initial/height Max                0.0830823\n",
      "evaluation/env_infos/initial/height Min               -0.093423\n",
      "evaluation/env_infos/height Mean                      -0.129737\n",
      "evaluation/env_infos/height Std                        0.00598333\n",
      "evaluation/env_infos/height Max                        0.0830823\n",
      "evaluation/env_infos/height Min                       -0.154083\n",
      "evaluation/env_infos/final/reward_angular Mean         1.2895e-16\n",
      "evaluation/env_infos/final/reward_angular Std          1.535e-15\n",
      "evaluation/env_infos/final/reward_angular Max          3.16185e-15\n",
      "evaluation/env_infos/final/reward_angular Min         -2.82832e-15\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.511987\n",
      "evaluation/env_infos/initial/reward_angular Std        0.32327\n",
      "evaluation/env_infos/initial/reward_angular Max        1.23924\n",
      "evaluation/env_infos/initial/reward_angular Min        0.00330403\n",
      "evaluation/env_infos/reward_angular Mean               0.00221283\n",
      "evaluation/env_infos/reward_angular Std                0.0584708\n",
      "evaluation/env_infos/reward_angular Max                2.27082\n",
      "evaluation/env_infos/reward_angular Min               -0.925278\n",
      "time/data storing (s)                                  0.0683762\n",
      "time/evaluation sampling (s)                          21.7468\n",
      "time/exploration sampling (s)                          1.01421\n",
      "time/logging (s)                                       0.237878\n",
      "time/saving (s)                                        0.332789\n",
      "time/training (s)                                      3.25012\n",
      "time/epoch (s)                                        26.6502\n",
      "time/total (s)                                        58.9197\n",
      "Epoch                                                  1\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:04:02.687435 PDT | [gher-halfcheetahhard-SAC-10e-1000s-disc0.99_2021_05_25_10_02_39_0000--s-10] Epoch 2 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  8000\n",
      "trainer/QF1 Loss                                       0.852367\n",
      "trainer/QF2 Loss                                       0.85493\n",
      "trainer/Policy Loss                                   -7.31409\n",
      "trainer/Q1 Predictions Mean                            3.28253\n",
      "trainer/Q1 Predictions Std                             0.732813\n",
      "trainer/Q1 Predictions Max                             5.62163\n",
      "trainer/Q1 Predictions Min                             1.45355\n",
      "trainer/Q2 Predictions Mean                            3.31075\n",
      "trainer/Q2 Predictions Std                             0.723922\n",
      "trainer/Q2 Predictions Max                             5.6136\n",
      "trainer/Q2 Predictions Min                             1.51262\n",
      "trainer/Q Targets Mean                                 3.1993\n",
      "trainer/Q Targets Std                                  1.13343\n",
      "trainer/Q Targets Max                                  6.97577\n",
      "trainer/Q Targets Min                                 -2.35241\n",
      "trainer/Log Pis Mean                                  -3.9342\n",
      "trainer/Log Pis Std                                    0.613627\n",
      "trainer/Log Pis Max                                   -1.95606\n",
      "trainer/Log Pis Min                                   -6.07042\n",
      "trainer/Policy mu Mean                                -0.12736\n",
      "trainer/Policy mu Std                                  0.142507\n",
      "trainer/Policy mu Max                                  0.171919\n",
      "trainer/Policy mu Min                                 -0.766841\n",
      "trainer/Policy log std Mean                           -0.132104\n",
      "trainer/Policy log std Std                             0.0325939\n",
      "trainer/Policy log std Max                            -0.0732969\n",
      "trainer/Policy log std Min                            -0.280719\n",
      "trainer/Alpha                                          0.548086\n",
      "trainer/Alpha Loss                                    -5.94407\n",
      "exploration/num steps total                         4000\n",
      "exploration/num paths total                            4\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.0801393\n",
      "exploration/Rewards Std                                0.546897\n",
      "exploration/Rewards Max                                1.60755\n",
      "exploration/Rewards Min                               -2.11331\n",
      "exploration/Returns Mean                             -80.1393\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              -80.1393\n",
      "exploration/Returns Min                              -80.1393\n",
      "exploration/Actions Mean                              -0.104866\n",
      "exploration/Actions Std                                0.590479\n",
      "exploration/Actions Max                                0.997102\n",
      "exploration/Actions Min                               -0.997529\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          -80.1393\n",
      "exploration/env_infos/final/reward_run Mean           -0.248803\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max            -0.248803\n",
      "exploration/env_infos/final/reward_run Min            -0.248803\n",
      "exploration/env_infos/initial/reward_run Mean         -0.102436\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max          -0.102436\n",
      "exploration/env_infos/initial/reward_run Min          -0.102436\n",
      "exploration/env_infos/reward_run Mean                 -0.0261349\n",
      "exploration/env_infos/reward_run Std                   0.666405\n",
      "exploration/env_infos/reward_run Max                   1.82474\n",
      "exploration/env_infos/reward_run Min                  -2.26154\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.147797\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.147797\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.147797\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.215675\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.215675\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.215675\n",
      "exploration/env_infos/reward_ctrl Mean                -0.215797\n",
      "exploration/env_infos/reward_ctrl Std                  0.07424\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0218283\n",
      "exploration/env_infos/reward_ctrl Min                 -0.454896\n",
      "exploration/env_infos/final/height Mean               -0.0500108\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.0500108\n",
      "exploration/env_infos/final/height Min                -0.0500108\n",
      "exploration/env_infos/initial/height Mean             -0.0549325\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max              -0.0549325\n",
      "exploration/env_infos/initial/height Min              -0.0549325\n",
      "exploration/env_infos/height Mean                     -0.0752075\n",
      "exploration/env_infos/height Std                       0.0825511\n",
      "exploration/env_infos/height Max                       0.207062\n",
      "exploration/env_infos/height Min                      -0.324674\n",
      "exploration/env_infos/final/reward_angular Mean        0.204123\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max         0.204123\n",
      "exploration/env_infos/final/reward_angular Min         0.204123\n",
      "exploration/env_infos/initial/reward_angular Mean      1.27606\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       1.27606\n",
      "exploration/env_infos/initial/reward_angular Min       1.27606\n",
      "exploration/env_infos/reward_angular Mean              0.00102233\n",
      "exploration/env_infos/reward_angular Std               1.59487\n",
      "exploration/env_infos/reward_angular Max               4.42326\n",
      "exploration/env_infos/reward_angular Min              -5.02551\n",
      "evaluation/num steps total                         75000\n",
      "evaluation/num paths total                            75\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.0649087\n",
      "evaluation/Rewards Std                                 0.0486764\n",
      "evaluation/Rewards Max                                 1.36335\n",
      "evaluation/Rewards Min                                -1.08438\n",
      "evaluation/Returns Mean                              -64.9087\n",
      "evaluation/Returns Std                                36.98\n",
      "evaluation/Returns Max                                -5.70788\n",
      "evaluation/Returns Min                              -129.322\n",
      "evaluation/Actions Mean                               -0.102002\n",
      "evaluation/Actions Std                                 0.111392\n",
      "evaluation/Actions Max                                 0.076981\n",
      "evaluation/Actions Min                                -0.489476\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           -64.9087\n",
      "evaluation/env_infos/final/reward_run Mean             2.89282e-09\n",
      "evaluation/env_infos/final/reward_run Std              9.43671e-09\n",
      "evaluation/env_infos/final/reward_run Max              3.94696e-08\n",
      "evaluation/env_infos/final/reward_run Min             -1.08345e-08\n",
      "evaluation/env_infos/initial/reward_run Mean          -0.120389\n",
      "evaluation/env_infos/initial/reward_run Std            0.124892\n",
      "evaluation/env_infos/initial/reward_run Max            0.104161\n",
      "evaluation/env_infos/initial/reward_run Min           -0.371461\n",
      "evaluation/env_infos/reward_run Mean                  -0.000263023\n",
      "evaluation/env_infos/reward_run Std                    0.019027\n",
      "evaluation/env_infos/reward_run Max                    0.280509\n",
      "evaluation/env_infos/reward_run Min                   -0.608039\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0136845\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.00765167\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.00615294\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.0293803\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0149953\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0091306\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.00613883\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0325018\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0136876\n",
      "evaluation/env_infos/reward_ctrl Std                   0.00765388\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00565734\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.0347089\n",
      "evaluation/env_infos/final/height Mean                -0.130372\n",
      "evaluation/env_infos/final/height Std                  0.00656969\n",
      "evaluation/env_infos/final/height Max                 -0.120567\n",
      "evaluation/env_infos/final/height Min                 -0.138482\n",
      "evaluation/env_infos/initial/height Mean              -0.0141268\n",
      "evaluation/env_infos/initial/height Std                0.0488786\n",
      "evaluation/env_infos/initial/height Max                0.0687628\n",
      "evaluation/env_infos/initial/height Min               -0.0990248\n",
      "evaluation/env_infos/height Mean                      -0.129999\n",
      "evaluation/env_infos/height Std                        0.00848659\n",
      "evaluation/env_infos/height Max                        0.0687628\n",
      "evaluation/env_infos/height Min                       -0.159155\n",
      "evaluation/env_infos/final/reward_angular Mean        -8.26656e-10\n",
      "evaluation/env_infos/final/reward_angular Std          7.99463e-09\n",
      "evaluation/env_infos/final/reward_angular Max          2.42775e-08\n",
      "evaluation/env_infos/final/reward_angular Min         -2.27891e-08\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.532027\n",
      "evaluation/env_infos/initial/reward_angular Std        0.403996\n",
      "evaluation/env_infos/initial/reward_angular Max        1.62279\n",
      "evaluation/env_infos/initial/reward_angular Min       -0.373245\n",
      "evaluation/env_infos/reward_angular Mean               0.00199084\n",
      "evaluation/env_infos/reward_angular Std                0.0545126\n",
      "evaluation/env_infos/reward_angular Max                1.64456\n",
      "evaluation/env_infos/reward_angular Min               -1.41155\n",
      "time/data storing (s)                                  0.0778783\n",
      "time/evaluation sampling (s)                          22.4327\n",
      "time/exploration sampling (s)                          1.0882\n",
      "time/logging (s)                                       0.247095\n",
      "time/saving (s)                                        0.439839\n",
      "time/training (s)                                      3.43749\n",
      "time/epoch (s)                                        27.7232\n",
      "time/total (s)                                        86.7668\n",
      "Epoch                                                  2\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:04:31.755705 PDT | [gher-halfcheetahhard-SAC-10e-1000s-disc0.99_2021_05_25_10_02_39_0000--s-10] Epoch 3 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  10000\n",
      "trainer/QF1 Loss                                        0.778313\n",
      "trainer/QF2 Loss                                        0.760984\n",
      "trainer/Policy Loss                                    -6.93587\n",
      "trainer/Q1 Predictions Mean                             3.04099\n",
      "trainer/Q1 Predictions Std                              0.789674\n",
      "trainer/Q1 Predictions Max                              8.35476\n",
      "trainer/Q1 Predictions Min                              0.999759\n",
      "trainer/Q2 Predictions Mean                             3.05199\n",
      "trainer/Q2 Predictions Std                              0.790416\n",
      "trainer/Q2 Predictions Max                              8.16598\n",
      "trainer/Q2 Predictions Min                              1.00307\n",
      "trainer/Q Targets Mean                                  3.33167\n",
      "trainer/Q Targets Std                                   1.17311\n",
      "trainer/Q Targets Max                                   9.92709\n",
      "trainer/Q Targets Min                                   0.522334\n",
      "trainer/Log Pis Mean                                   -3.82663\n",
      "trainer/Log Pis Std                                     0.830307\n",
      "trainer/Log Pis Max                                    -1.08163\n",
      "trainer/Log Pis Min                                    -5.8566\n",
      "trainer/Policy mu Mean                                 -0.184225\n",
      "trainer/Policy mu Std                                   0.192251\n",
      "trainer/Policy mu Max                                   0.186793\n",
      "trainer/Policy mu Min                                  -1.05845\n",
      "trainer/Policy log std Mean                            -0.11142\n",
      "trainer/Policy log std Std                              0.0382386\n",
      "trainer/Policy log std Max                             -0.0342935\n",
      "trainer/Policy log std Min                             -0.288679\n",
      "trainer/Alpha                                           0.407457\n",
      "trainer/Alpha Loss                                     -8.79344\n",
      "exploration/num steps total                          5000\n",
      "exploration/num paths total                             5\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.102629\n",
      "exploration/Rewards Std                                 0.590485\n",
      "exploration/Rewards Max                                 2.20847\n",
      "exploration/Rewards Min                                -2.26132\n",
      "exploration/Returns Mean                             -102.629\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -102.629\n",
      "exploration/Returns Min                              -102.629\n",
      "exploration/Actions Mean                               -0.0837937\n",
      "exploration/Actions Std                                 0.594991\n",
      "exploration/Actions Max                                 0.997352\n",
      "exploration/Actions Min                                -0.998424\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -102.629\n",
      "exploration/env_infos/final/reward_run Mean             0.615216\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.615216\n",
      "exploration/env_infos/final/reward_run Min              0.615216\n",
      "exploration/env_infos/initial/reward_run Mean          -0.0133224\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.0133224\n",
      "exploration/env_infos/initial/reward_run Min           -0.0133224\n",
      "exploration/env_infos/reward_run Mean                  -0.00839036\n",
      "exploration/env_infos/reward_run Std                    0.60468\n",
      "exploration/env_infos/reward_run Max                    1.72013\n",
      "exploration/env_infos/reward_run Min                   -2.03022\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.227419\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.227419\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.227419\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.181719\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.181719\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.181719\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.216621\n",
      "exploration/env_infos/reward_ctrl Std                   0.0714614\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0108772\n",
      "exploration/env_infos/reward_ctrl Min                  -0.436711\n",
      "exploration/env_infos/final/height Mean                -0.534543\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.534543\n",
      "exploration/env_infos/final/height Min                 -0.534543\n",
      "exploration/env_infos/initial/height Mean              -0.00383065\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.00383065\n",
      "exploration/env_infos/initial/height Min               -0.00383065\n",
      "exploration/env_infos/height Mean                      -0.309813\n",
      "exploration/env_infos/height Std                        0.244704\n",
      "exploration/env_infos/height Max                        0.176417\n",
      "exploration/env_infos/height Min                       -0.58278\n",
      "exploration/env_infos/final/reward_angular Mean        -1.42156\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -1.42156\n",
      "exploration/env_infos/final/reward_angular Min         -1.42156\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.313818\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.313818\n",
      "exploration/env_infos/initial/reward_angular Min       -0.313818\n",
      "exploration/env_infos/reward_angular Mean               0.0720715\n",
      "exploration/env_infos/reward_angular Std                1.4892\n",
      "exploration/env_infos/reward_angular Max                6.00799\n",
      "exploration/env_infos/reward_angular Min               -5.35569\n",
      "evaluation/num steps total                         100000\n",
      "evaluation/num paths total                            100\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.0758176\n",
      "evaluation/Rewards Std                                  0.0564992\n",
      "evaluation/Rewards Max                                  1.14766\n",
      "evaluation/Rewards Min                                 -1.55094\n",
      "evaluation/Returns Mean                               -75.8176\n",
      "evaluation/Returns Std                                 38.3501\n",
      "evaluation/Returns Max                                 -7.83325\n",
      "evaluation/Returns Min                               -139.723\n",
      "evaluation/Actions Mean                                -0.150163\n",
      "evaluation/Actions Std                                  0.15254\n",
      "evaluation/Actions Max                                  0.0988208\n",
      "evaluation/Actions Min                                 -0.634796\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            -75.8176\n",
      "evaluation/env_infos/final/reward_run Mean             -2.47033e-09\n",
      "evaluation/env_infos/final/reward_run Std               1.11121e-08\n",
      "evaluation/env_infos/final/reward_run Max               9.94568e-09\n",
      "evaluation/env_infos/final/reward_run Min              -5.52985e-08\n",
      "evaluation/env_infos/initial/reward_run Mean           -0.227\n",
      "evaluation/env_infos/initial/reward_run Std             0.128943\n",
      "evaluation/env_infos/initial/reward_run Max            -0.0422307\n",
      "evaluation/env_infos/initial/reward_run Min            -0.54181\n",
      "evaluation/env_infos/reward_run Mean                   -0.000230916\n",
      "evaluation/env_infos/reward_run Std                     0.0240615\n",
      "evaluation/env_infos/reward_run Max                     0.391164\n",
      "evaluation/env_infos/reward_run Min                    -0.587887\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0274744\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0183572\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0082239\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0621299\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0339703\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0252369\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00830893\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0853256\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0274906\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0183674\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00761193\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.0853256\n",
      "evaluation/env_infos/final/height Mean                 -0.143229\n",
      "evaluation/env_infos/final/height Std                   0.00805394\n",
      "evaluation/env_infos/final/height Max                  -0.133051\n",
      "evaluation/env_infos/final/height Min                  -0.159195\n",
      "evaluation/env_infos/initial/height Mean               -0.01234\n",
      "evaluation/env_infos/initial/height Std                 0.0524668\n",
      "evaluation/env_infos/initial/height Max                 0.0849976\n",
      "evaluation/env_infos/initial/height Min                -0.104949\n",
      "evaluation/env_infos/height Mean                       -0.142897\n",
      "evaluation/env_infos/height Std                         0.010141\n",
      "evaluation/env_infos/height Max                         0.0849976\n",
      "evaluation/env_infos/height Min                        -0.187085\n",
      "evaluation/env_infos/final/reward_angular Mean         -7.96698e-09\n",
      "evaluation/env_infos/final/reward_angular Std           1.77785e-08\n",
      "evaluation/env_infos/final/reward_angular Max           1.32364e-15\n",
      "evaluation/env_infos/final/reward_angular Min          -6.37256e-08\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.720935\n",
      "evaluation/env_infos/initial/reward_angular Std         0.454819\n",
      "evaluation/env_infos/initial/reward_angular Max         2.21294\n",
      "evaluation/env_infos/initial/reward_angular Min        -0.11833\n",
      "evaluation/env_infos/reward_angular Mean                0.002755\n",
      "evaluation/env_infos/reward_angular Std                 0.0728537\n",
      "evaluation/env_infos/reward_angular Max                 2.21294\n",
      "evaluation/env_infos/reward_angular Min                -1.09192\n",
      "time/data storing (s)                                   0.0730403\n",
      "time/evaluation sampling (s)                           23.381\n",
      "time/exploration sampling (s)                           1.0919\n",
      "time/logging (s)                                        0.233982\n",
      "time/saving (s)                                         0.554816\n",
      "time/training (s)                                       3.59043\n",
      "time/epoch (s)                                         28.9251\n",
      "time/total (s)                                        115.821\n",
      "Epoch                                                   3\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:04:59.313552 PDT | [gher-halfcheetahhard-SAC-10e-1000s-disc0.99_2021_05_25_10_02_39_0000--s-10] Epoch 4 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  12000\n",
      "trainer/QF1 Loss                                        0.674375\n",
      "trainer/QF2 Loss                                        0.671835\n",
      "trainer/Policy Loss                                    -6.75074\n",
      "trainer/Q1 Predictions Mean                             3.12413\n",
      "trainer/Q1 Predictions Std                              0.799901\n",
      "trainer/Q1 Predictions Max                              6.99621\n",
      "trainer/Q1 Predictions Min                              1.21742\n",
      "trainer/Q2 Predictions Mean                             3.11529\n",
      "trainer/Q2 Predictions Std                              0.821791\n",
      "trainer/Q2 Predictions Max                              6.97004\n",
      "trainer/Q2 Predictions Min                              1.05253\n",
      "trainer/Q Targets Mean                                  3.0546\n",
      "trainer/Q Targets Std                                   1.20782\n",
      "trainer/Q Targets Max                                   7.13414\n",
      "trainer/Q Targets Min                                  -1.46664\n",
      "trainer/Log Pis Mean                                   -3.46471\n",
      "trainer/Log Pis Std                                     1.31914\n",
      "trainer/Log Pis Max                                     1.65658\n",
      "trainer/Log Pis Min                                    -9.03913\n",
      "trainer/Policy mu Mean                                 -0.150721\n",
      "trainer/Policy mu Std                                   0.351773\n",
      "trainer/Policy mu Max                                   1.01379\n",
      "trainer/Policy mu Min                                  -1.66898\n",
      "trainer/Policy log std Mean                            -0.120015\n",
      "trainer/Policy log std Std                              0.0489382\n",
      "trainer/Policy log std Max                             -0.0380612\n",
      "trainer/Policy log std Min                             -0.377566\n",
      "trainer/Alpha                                           0.303563\n",
      "trainer/Alpha Loss                                    -11.256\n",
      "exploration/num steps total                          6000\n",
      "exploration/num paths total                             6\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.116997\n",
      "exploration/Rewards Std                                 1.03355\n",
      "exploration/Rewards Max                                 3.46695\n",
      "exploration/Rewards Min                                -3.52064\n",
      "exploration/Returns Mean                             -116.997\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -116.997\n",
      "exploration/Returns Min                              -116.997\n",
      "exploration/Actions Mean                               -0.136584\n",
      "exploration/Actions Std                                 0.597409\n",
      "exploration/Actions Max                                 0.995703\n",
      "exploration/Actions Min                                -0.997237\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -116.997\n",
      "exploration/env_infos/final/reward_run Mean            -0.314251\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.314251\n",
      "exploration/env_infos/final/reward_run Min             -0.314251\n",
      "exploration/env_infos/initial/reward_run Mean           0.646271\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.646271\n",
      "exploration/env_infos/initial/reward_run Min            0.646271\n",
      "exploration/env_infos/reward_run Mean                  -0.0633345\n",
      "exploration/env_infos/reward_run Std                    0.658297\n",
      "exploration/env_infos/reward_run Max                    1.96981\n",
      "exploration/env_infos/reward_run Min                   -2.41861\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.26821\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.26821\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.26821\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.17788\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.17788\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.17788\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.225332\n",
      "exploration/env_infos/reward_ctrl Std                   0.0759711\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0180479\n",
      "exploration/env_infos/reward_ctrl Min                  -0.530547\n",
      "exploration/env_infos/final/height Mean                -0.571479\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.571479\n",
      "exploration/env_infos/final/height Min                 -0.571479\n",
      "exploration/env_infos/initial/height Mean               0.0074968\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0074968\n",
      "exploration/env_infos/initial/height Min                0.0074968\n",
      "exploration/env_infos/height Mean                      -0.135353\n",
      "exploration/env_infos/height Std                        0.199154\n",
      "exploration/env_infos/height Max                        0.222859\n",
      "exploration/env_infos/height Min                       -0.582419\n",
      "exploration/env_infos/final/reward_angular Mean         0.16824\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.16824\n",
      "exploration/env_infos/final/reward_angular Min          0.16824\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.381488\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.381488\n",
      "exploration/env_infos/initial/reward_angular Min       -0.381488\n",
      "exploration/env_infos/reward_angular Mean               0.0657009\n",
      "exploration/env_infos/reward_angular Std                1.61063\n",
      "exploration/env_infos/reward_angular Max                5.24682\n",
      "exploration/env_infos/reward_angular Min               -5.48474\n",
      "evaluation/num steps total                         125000\n",
      "evaluation/num paths total                            125\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.0712745\n",
      "evaluation/Rewards Std                                  0.0536755\n",
      "evaluation/Rewards Max                                  2.1386\n",
      "evaluation/Rewards Min                                 -1.36382\n",
      "evaluation/Returns Mean                               -71.2745\n",
      "evaluation/Returns Std                                 31.796\n",
      "evaluation/Returns Max                                 -9.18686\n",
      "evaluation/Returns Min                               -126.691\n",
      "evaluation/Actions Mean                                -0.0195462\n",
      "evaluation/Actions Std                                  0.23112\n",
      "evaluation/Actions Max                                  0.691919\n",
      "evaluation/Actions Min                                 -0.831722\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            -71.2745\n",
      "evaluation/env_infos/final/reward_run Mean              3.10784e-09\n",
      "evaluation/env_infos/final/reward_run Std               6.72129e-08\n",
      "evaluation/env_infos/final/reward_run Max               2.24093e-07\n",
      "evaluation/env_infos/final/reward_run Min              -1.29983e-07\n",
      "evaluation/env_infos/initial/reward_run Mean            0.0131249\n",
      "evaluation/env_infos/initial/reward_run Std             0.294852\n",
      "evaluation/env_infos/initial/reward_run Max             0.65123\n",
      "evaluation/env_infos/initial/reward_run Min            -0.584092\n",
      "evaluation/env_infos/reward_run Mean                   -0.000125465\n",
      "evaluation/env_infos/reward_run Std                     0.0287302\n",
      "evaluation/env_infos/reward_run Max                     0.803087\n",
      "evaluation/env_infos/reward_run Min                    -0.802472\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0322688\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0297504\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.000632729\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.105465\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0519604\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0467257\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00206093\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.161321\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.032279\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0297898\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.000632302\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.161321\n",
      "evaluation/env_infos/final/height Mean                 -0.131105\n",
      "evaluation/env_infos/final/height Std                   0.00888431\n",
      "evaluation/env_infos/final/height Max                  -0.110869\n",
      "evaluation/env_infos/final/height Min                  -0.148264\n",
      "evaluation/env_infos/initial/height Mean               -0.0205242\n",
      "evaluation/env_infos/initial/height Std                 0.0520697\n",
      "evaluation/env_infos/initial/height Max                 0.0489552\n",
      "evaluation/env_infos/initial/height Min                -0.106885\n",
      "evaluation/env_infos/height Mean                       -0.13078\n",
      "evaluation/env_infos/height Std                         0.010258\n",
      "evaluation/env_infos/height Max                         0.0489552\n",
      "evaluation/env_infos/height Min                        -0.185319\n",
      "evaluation/env_infos/final/reward_angular Mean         -8.7849e-09\n",
      "evaluation/env_infos/final/reward_angular Std           1.05412e-07\n",
      "evaluation/env_infos/final/reward_angular Max           1.5347e-07\n",
      "evaluation/env_infos/final/reward_angular Min          -4.32852e-07\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.214308\n",
      "evaluation/env_infos/initial/reward_angular Std         1.01444\n",
      "evaluation/env_infos/initial/reward_angular Max         2.36575\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.17393\n",
      "evaluation/env_infos/reward_angular Mean                0.00109111\n",
      "evaluation/env_infos/reward_angular Std                 0.0660697\n",
      "evaluation/env_infos/reward_angular Max                 2.48436\n",
      "evaluation/env_infos/reward_angular Min                -1.17393\n",
      "time/data storing (s)                                   0.073322\n",
      "time/evaluation sampling (s)                           22.1009\n",
      "time/exploration sampling (s)                           1.03337\n",
      "time/logging (s)                                        0.227637\n",
      "time/saving (s)                                         0.642549\n",
      "time/training (s)                                       3.33425\n",
      "time/epoch (s)                                         27.412\n",
      "time/total (s)                                        143.372\n",
      "Epoch                                                   4\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:05:27.314605 PDT | [gher-halfcheetahhard-SAC-10e-1000s-disc0.99_2021_05_25_10_02_39_0000--s-10] Epoch 5 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  14000\n",
      "trainer/QF1 Loss                                        0.651759\n",
      "trainer/QF2 Loss                                        0.662258\n",
      "trainer/Policy Loss                                    -6.77202\n",
      "trainer/Q1 Predictions Mean                             3.15381\n",
      "trainer/Q1 Predictions Std                              0.869619\n",
      "trainer/Q1 Predictions Max                              5.94279\n",
      "trainer/Q1 Predictions Min                              0.943441\n",
      "trainer/Q2 Predictions Mean                             3.14131\n",
      "trainer/Q2 Predictions Std                              0.827561\n",
      "trainer/Q2 Predictions Max                              5.61035\n",
      "trainer/Q2 Predictions Min                              0.73231\n",
      "trainer/Q Targets Mean                                  2.98923\n",
      "trainer/Q Targets Std                                   1.17215\n",
      "trainer/Q Targets Max                                   7.09112\n",
      "trainer/Q Targets Min                                  -1.35734\n",
      "trainer/Log Pis Mean                                   -3.30114\n",
      "trainer/Log Pis Std                                     1.47916\n",
      "trainer/Log Pis Max                                     1.51592\n",
      "trainer/Log Pis Min                                    -6.67839\n",
      "trainer/Policy mu Mean                                 -0.068392\n",
      "trainer/Policy mu Std                                   0.442745\n",
      "trainer/Policy mu Max                                   1.47981\n",
      "trainer/Policy mu Min                                  -1.23143\n",
      "trainer/Policy log std Mean                            -0.175039\n",
      "trainer/Policy log std Std                              0.0645908\n",
      "trainer/Policy log std Max                             -0.0541848\n",
      "trainer/Policy log std Min                             -0.458117\n",
      "trainer/Alpha                                           0.227438\n",
      "trainer/Alpha Loss                                    -13.7473\n",
      "exploration/num steps total                          7000\n",
      "exploration/num paths total                             7\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.164032\n",
      "exploration/Rewards Std                                 0.590066\n",
      "exploration/Rewards Max                                 1.61774\n",
      "exploration/Rewards Min                                -1.71077\n",
      "exploration/Returns Mean                             -164.032\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -164.032\n",
      "exploration/Returns Min                              -164.032\n",
      "exploration/Actions Mean                               -0.110917\n",
      "exploration/Actions Std                                 0.587054\n",
      "exploration/Actions Max                                 0.995321\n",
      "exploration/Actions Min                                -0.998651\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -164.032\n",
      "exploration/env_infos/final/reward_run Mean            -0.144207\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.144207\n",
      "exploration/env_infos/final/reward_run Min             -0.144207\n",
      "exploration/env_infos/initial/reward_run Mean          -0.550808\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.550808\n",
      "exploration/env_infos/initial/reward_run Min           -0.550808\n",
      "exploration/env_infos/reward_run Mean                  -0.028723\n",
      "exploration/env_infos/reward_run Std                    0.75675\n",
      "exploration/env_infos/reward_run Max                    2.48399\n",
      "exploration/env_infos/reward_run Min                   -2.59109\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.335947\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.335947\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.335947\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.252651\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.252651\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.252651\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.214161\n",
      "exploration/env_infos/reward_ctrl Std                   0.0745579\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0287584\n",
      "exploration/env_infos/reward_ctrl Min                  -0.441293\n",
      "exploration/env_infos/final/height Mean                -0.529189\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.529189\n",
      "exploration/env_infos/final/height Min                 -0.529189\n",
      "exploration/env_infos/initial/height Mean              -0.0654903\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0654903\n",
      "exploration/env_infos/initial/height Min               -0.0654903\n",
      "exploration/env_infos/height Mean                      -0.0998852\n",
      "exploration/env_infos/height Std                        0.127984\n",
      "exploration/env_infos/height Max                        0.31034\n",
      "exploration/env_infos/height Min                       -0.576657\n",
      "exploration/env_infos/final/reward_angular Mean        -1.2046\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -1.2046\n",
      "exploration/env_infos/final/reward_angular Min         -1.2046\n",
      "exploration/env_infos/initial/reward_angular Mean       3.12939\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        3.12939\n",
      "exploration/env_infos/initial/reward_angular Min        3.12939\n",
      "exploration/env_infos/reward_angular Mean               0.0612807\n",
      "exploration/env_infos/reward_angular Std                1.59545\n",
      "exploration/env_infos/reward_angular Max                5.04262\n",
      "exploration/env_infos/reward_angular Min               -4.68497\n",
      "evaluation/num steps total                         150000\n",
      "evaluation/num paths total                            150\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.0916381\n",
      "evaluation/Rewards Std                                  0.0664807\n",
      "evaluation/Rewards Max                                  1.93555\n",
      "evaluation/Rewards Min                                 -1.36627\n",
      "evaluation/Returns Mean                               -91.6381\n",
      "evaluation/Returns Std                                 32.2965\n",
      "evaluation/Returns Max                                -18.9686\n",
      "evaluation/Returns Min                               -146.275\n",
      "evaluation/Actions Mean                                 0.0292535\n",
      "evaluation/Actions Std                                  0.337004\n",
      "evaluation/Actions Max                                  0.765868\n",
      "evaluation/Actions Min                                 -0.735235\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            -91.6381\n",
      "evaluation/env_infos/final/reward_run Mean              0.0109908\n",
      "evaluation/env_infos/final/reward_run Std               0.0571776\n",
      "evaluation/env_infos/final/reward_run Max               0.210627\n",
      "evaluation/env_infos/final/reward_run Min              -0.138002\n",
      "evaluation/env_infos/initial/reward_run Mean            0.0481963\n",
      "evaluation/env_infos/initial/reward_run Std             0.451353\n",
      "evaluation/env_infos/initial/reward_run Max             0.84208\n",
      "evaluation/env_infos/initial/reward_run Min            -0.674938\n",
      "evaluation/env_infos/reward_run Mean                    0.00107276\n",
      "evaluation/env_infos/reward_run Std                     0.0807572\n",
      "evaluation/env_infos/reward_run Max                     0.919628\n",
      "evaluation/env_infos/reward_run Min                    -1.00974\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0687449\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0430863\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00237116\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.142022\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0881701\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0530392\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00732899\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.184165\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0686565\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0433308\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.000442921\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.185637\n",
      "evaluation/env_infos/final/height Mean                 -0.152548\n",
      "evaluation/env_infos/final/height Std                   0.0406522\n",
      "evaluation/env_infos/final/height Max                  -0.123248\n",
      "evaluation/env_infos/final/height Min                  -0.346168\n",
      "evaluation/env_infos/initial/height Mean               -0.032351\n",
      "evaluation/env_infos/initial/height Std                 0.0534567\n",
      "evaluation/env_infos/initial/height Max                 0.0758293\n",
      "evaluation/env_infos/initial/height Min                -0.100578\n",
      "evaluation/env_infos/height Mean                       -0.152248\n",
      "evaluation/env_infos/height Std                         0.0407788\n",
      "evaluation/env_infos/height Max                         0.0758293\n",
      "evaluation/env_infos/height Min                        -0.408201\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.00269545\n",
      "evaluation/env_infos/final/reward_angular Std           0.0674287\n",
      "evaluation/env_infos/final/reward_angular Max           0.202125\n",
      "evaluation/env_infos/final/reward_angular Min          -0.198602\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.17657\n",
      "evaluation/env_infos/initial/reward_angular Std         1.23553\n",
      "evaluation/env_infos/initial/reward_angular Max         1.79277\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.9926\n",
      "evaluation/env_infos/reward_angular Mean                0.00127673\n",
      "evaluation/env_infos/reward_angular Std                 0.0939898\n",
      "evaluation/env_infos/reward_angular Max                 1.79277\n",
      "evaluation/env_infos/reward_angular Min                -1.9926\n",
      "time/data storing (s)                                   0.0773512\n",
      "time/evaluation sampling (s)                           22.3021\n",
      "time/exploration sampling (s)                           1.10174\n",
      "time/logging (s)                                        0.232948\n",
      "time/saving (s)                                         0.745022\n",
      "time/training (s)                                       3.40747\n",
      "time/epoch (s)                                         27.8667\n",
      "time/total (s)                                        171.377\n",
      "Epoch                                                   5\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:05:56.358958 PDT | [gher-halfcheetahhard-SAC-10e-1000s-disc0.99_2021_05_25_10_02_39_0000--s-10] Epoch 6 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  16000\n",
      "trainer/QF1 Loss                                        0.53774\n",
      "trainer/QF2 Loss                                        0.519711\n",
      "trainer/Policy Loss                                    -6.5484\n",
      "trainer/Q1 Predictions Mean                             3.2432\n",
      "trainer/Q1 Predictions Std                              1.09611\n",
      "trainer/Q1 Predictions Max                              6.94537\n",
      "trainer/Q1 Predictions Min                              0.704928\n",
      "trainer/Q2 Predictions Mean                             3.22139\n",
      "trainer/Q2 Predictions Std                              1.10234\n",
      "trainer/Q2 Predictions Max                              6.78165\n",
      "trainer/Q2 Predictions Min                              0.603459\n",
      "trainer/Q Targets Mean                                  3.3539\n",
      "trainer/Q Targets Std                                   1.38338\n",
      "trainer/Q Targets Max                                   7.29192\n",
      "trainer/Q Targets Min                                  -1.10934\n",
      "trainer/Log Pis Mean                                   -2.87473\n",
      "trainer/Log Pis Std                                     1.75447\n",
      "trainer/Log Pis Max                                     3.92382\n",
      "trainer/Log Pis Min                                    -6.30094\n",
      "trainer/Policy mu Mean                                  0.000794585\n",
      "trainer/Policy mu Std                                   0.55741\n",
      "trainer/Policy mu Max                                   1.4723\n",
      "trainer/Policy mu Min                                  -1.44608\n",
      "trainer/Policy log std Mean                            -0.206259\n",
      "trainer/Policy log std Std                              0.0899399\n",
      "trainer/Policy log std Max                             -0.00900124\n",
      "trainer/Policy log std Min                             -0.510564\n",
      "trainer/Alpha                                           0.172206\n",
      "trainer/Alpha Loss                                    -15.5867\n",
      "exploration/num steps total                          8000\n",
      "exploration/num paths total                             8\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.238908\n",
      "exploration/Rewards Std                                 0.467425\n",
      "exploration/Rewards Max                                 1.49505\n",
      "exploration/Rewards Min                                -2.12385\n",
      "exploration/Returns Mean                             -238.908\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -238.908\n",
      "exploration/Returns Min                              -238.908\n",
      "exploration/Actions Mean                                0.398323\n",
      "exploration/Actions Std                                 0.546327\n",
      "exploration/Actions Max                                 0.998484\n",
      "exploration/Actions Min                                -0.990649\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -238.908\n",
      "exploration/env_infos/final/reward_run Mean            -0.451255\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.451255\n",
      "exploration/env_infos/final/reward_run Min             -0.451255\n",
      "exploration/env_infos/initial/reward_run Mean           0.461022\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.461022\n",
      "exploration/env_infos/initial/reward_run Min            0.461022\n",
      "exploration/env_infos/reward_run Mean                   0.0337076\n",
      "exploration/env_infos/reward_run Std                    0.379002\n",
      "exploration/env_infos/reward_run Max                    1.32655\n",
      "exploration/env_infos/reward_run Min                   -1.9239\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.319314\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.319314\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.319314\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.166665\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.166665\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.166665\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.27428\n",
      "exploration/env_infos/reward_ctrl Std                   0.0758771\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0625168\n",
      "exploration/env_infos/reward_ctrl Min                  -0.471729\n",
      "exploration/env_infos/final/height Mean                -0.179001\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.179001\n",
      "exploration/env_infos/final/height Min                 -0.179001\n",
      "exploration/env_infos/initial/height Mean               0.0199911\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0199911\n",
      "exploration/env_infos/initial/height Min                0.0199911\n",
      "exploration/env_infos/height Mean                      -0.282136\n",
      "exploration/env_infos/height Std                        0.0867378\n",
      "exploration/env_infos/height Max                        0.0199911\n",
      "exploration/env_infos/height Min                       -0.458\n",
      "exploration/env_infos/final/reward_angular Mean        -2.01344\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -2.01344\n",
      "exploration/env_infos/final/reward_angular Min         -2.01344\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.27795\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.27795\n",
      "exploration/env_infos/initial/reward_angular Min       -1.27795\n",
      "exploration/env_infos/reward_angular Mean              -0.00946082\n",
      "exploration/env_infos/reward_angular Std                1.10644\n",
      "exploration/env_infos/reward_angular Max                3.63024\n",
      "exploration/env_infos/reward_angular Min               -3.66443\n",
      "evaluation/num steps total                         175000\n",
      "evaluation/num paths total                            175\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.14341\n",
      "evaluation/Rewards Std                                  0.110074\n",
      "evaluation/Rewards Max                                  2.29713\n",
      "evaluation/Rewards Min                                 -1.54264\n",
      "evaluation/Returns Mean                              -143.41\n",
      "evaluation/Returns Std                                 68.2814\n",
      "evaluation/Returns Max                                -37.3812\n",
      "evaluation/Returns Min                               -341.28\n",
      "evaluation/Actions Mean                                 0.0619582\n",
      "evaluation/Actions Std                                  0.492557\n",
      "evaluation/Actions Max                                  0.915968\n",
      "evaluation/Actions Min                                 -0.83224\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -143.41\n",
      "evaluation/env_infos/final/reward_run Mean             -2.27837e-07\n",
      "evaluation/env_infos/final/reward_run Std               1.45173e-05\n",
      "evaluation/env_infos/final/reward_run Max               5.04589e-05\n",
      "evaluation/env_infos/final/reward_run Min              -5.2021e-05\n",
      "evaluation/env_infos/initial/reward_run Mean            0.125073\n",
      "evaluation/env_infos/initial/reward_run Std             0.442776\n",
      "evaluation/env_infos/initial/reward_run Max             0.812923\n",
      "evaluation/env_infos/initial/reward_run Min            -0.646771\n",
      "evaluation/env_infos/reward_run Mean                    0.00268457\n",
      "evaluation/env_infos/reward_run Std                     0.0484625\n",
      "evaluation/env_infos/reward_run Max                     1.05579\n",
      "evaluation/env_infos/reward_run Min                    -0.794698\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.147776\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0919223\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00259264\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.316875\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.146799\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0890917\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00567927\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.304213\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.147871\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0920284\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00160692\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.335263\n",
      "evaluation/env_infos/final/height Mean                 -0.220356\n",
      "evaluation/env_infos/final/height Std                   0.100983\n",
      "evaluation/env_infos/final/height Max                  -0.133223\n",
      "evaluation/env_infos/final/height Min                  -0.413262\n",
      "evaluation/env_infos/initial/height Mean               -0.0205031\n",
      "evaluation/env_infos/initial/height Std                 0.0432817\n",
      "evaluation/env_infos/initial/height Max                 0.0715924\n",
      "evaluation/env_infos/initial/height Min                -0.0787673\n",
      "evaluation/env_infos/height Mean                       -0.219445\n",
      "evaluation/env_infos/height Std                         0.10094\n",
      "evaluation/env_infos/height Max                         0.0715924\n",
      "evaluation/env_infos/height Min                        -0.44813\n",
      "evaluation/env_infos/final/reward_angular Mean          1.34068e-06\n",
      "evaluation/env_infos/final/reward_angular Std           6.32924e-05\n",
      "evaluation/env_infos/final/reward_angular Max           0.000258055\n",
      "evaluation/env_infos/final/reward_angular Min          -0.00017691\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.194017\n",
      "evaluation/env_infos/initial/reward_angular Std         1.60251\n",
      "evaluation/env_infos/initial/reward_angular Max         2.73446\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.25737\n",
      "evaluation/env_infos/reward_angular Mean                0.00477842\n",
      "evaluation/env_infos/reward_angular Std                 0.117707\n",
      "evaluation/env_infos/reward_angular Max                 2.73446\n",
      "evaluation/env_infos/reward_angular Min                -2.25737\n",
      "time/data storing (s)                                   0.0770135\n",
      "time/evaluation sampling (s)                           23.2264\n",
      "time/exploration sampling (s)                           1.04948\n",
      "time/logging (s)                                        0.233979\n",
      "time/saving (s)                                         0.857612\n",
      "time/training (s)                                       3.43736\n",
      "time/epoch (s)                                         28.8818\n",
      "time/total (s)                                        200.422\n",
      "Epoch                                                   6\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:06:25.139789 PDT | [gher-halfcheetahhard-SAC-10e-1000s-disc0.99_2021_05_25_10_02_39_0000--s-10] Epoch 7 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  18000\n",
      "trainer/QF1 Loss                                        0.536057\n",
      "trainer/QF2 Loss                                        0.547271\n",
      "trainer/Policy Loss                                    -6.65946\n",
      "trainer/Q1 Predictions Mean                             3.49932\n",
      "trainer/Q1 Predictions Std                              1.23845\n",
      "trainer/Q1 Predictions Max                              7.39008\n",
      "trainer/Q1 Predictions Min                              1.05298\n",
      "trainer/Q2 Predictions Mean                             3.50949\n",
      "trainer/Q2 Predictions Std                              1.26555\n",
      "trainer/Q2 Predictions Max                              7.71064\n",
      "trainer/Q2 Predictions Min                              1.04071\n",
      "trainer/Q Targets Mean                                  3.4322\n",
      "trainer/Q Targets Std                                   1.41254\n",
      "trainer/Q Targets Max                                   9.73416\n",
      "trainer/Q Targets Min                                   0.0921724\n",
      "trainer/Log Pis Mean                                   -2.76409\n",
      "trainer/Log Pis Std                                     1.8866\n",
      "trainer/Log Pis Max                                     3.74837\n",
      "trainer/Log Pis Min                                    -8.07718\n",
      "trainer/Policy mu Mean                                 -0.128029\n",
      "trainer/Policy mu Std                                   0.588057\n",
      "trainer/Policy mu Max                                   1.39576\n",
      "trainer/Policy mu Min                                  -1.87157\n",
      "trainer/Policy log std Mean                            -0.209316\n",
      "trainer/Policy log std Std                              0.0956104\n",
      "trainer/Policy log std Max                              0.026254\n",
      "trainer/Policy log std Min                             -0.617633\n",
      "trainer/Alpha                                           0.131228\n",
      "trainer/Alpha Loss                                    -17.7743\n",
      "exploration/num steps total                          9000\n",
      "exploration/num paths total                             9\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.343902\n",
      "exploration/Rewards Std                                 0.452104\n",
      "exploration/Rewards Max                                 1.50625\n",
      "exploration/Rewards Min                                -1.59101\n",
      "exploration/Returns Mean                             -343.902\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -343.902\n",
      "exploration/Returns Min                              -343.902\n",
      "exploration/Actions Mean                                0.109786\n",
      "exploration/Actions Std                                 0.587916\n",
      "exploration/Actions Max                                 0.998084\n",
      "exploration/Actions Min                                -0.997428\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -343.902\n",
      "exploration/env_infos/final/reward_run Mean             0.256421\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.256421\n",
      "exploration/env_infos/final/reward_run Min              0.256421\n",
      "exploration/env_infos/initial/reward_run Mean           0.0816649\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.0816649\n",
      "exploration/env_infos/initial/reward_run Min            0.0816649\n",
      "exploration/env_infos/reward_run Mean                  -0.00480965\n",
      "exploration/env_infos/reward_run Std                    0.5378\n",
      "exploration/env_infos/reward_run Max                    1.80573\n",
      "exploration/env_infos/reward_run Min                   -1.69407\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.323406\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.323406\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.323406\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.33974\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.33974\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.33974\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.214619\n",
      "exploration/env_infos/reward_ctrl Std                   0.0749774\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0138508\n",
      "exploration/env_infos/reward_ctrl Min                  -0.416817\n",
      "exploration/env_infos/final/height Mean                -0.56875\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.56875\n",
      "exploration/env_infos/final/height Min                 -0.56875\n",
      "exploration/env_infos/initial/height Mean              -0.06569\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.06569\n",
      "exploration/env_infos/initial/height Min               -0.06569\n",
      "exploration/env_infos/height Mean                      -0.441476\n",
      "exploration/env_infos/height Std                        0.196663\n",
      "exploration/env_infos/height Max                        0.0857534\n",
      "exploration/env_infos/height Min                       -0.582974\n",
      "exploration/env_infos/final/reward_angular Mean         1.51335\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          1.51335\n",
      "exploration/env_infos/final/reward_angular Min          1.51335\n",
      "exploration/env_infos/initial/reward_angular Mean      -2.50022\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -2.50022\n",
      "exploration/env_infos/initial/reward_angular Min       -2.50022\n",
      "exploration/env_infos/reward_angular Mean               0.0818563\n",
      "exploration/env_infos/reward_angular Std                1.21202\n",
      "exploration/env_infos/reward_angular Max                5.21187\n",
      "exploration/env_infos/reward_angular Min               -4.06055\n",
      "evaluation/num steps total                         200000\n",
      "evaluation/num paths total                            200\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.136461\n",
      "evaluation/Rewards Std                                  0.0750717\n",
      "evaluation/Rewards Max                                  2.28525\n",
      "evaluation/Rewards Min                                 -0.989143\n",
      "evaluation/Returns Mean                              -136.461\n",
      "evaluation/Returns Std                                 45.8616\n",
      "evaluation/Returns Max                                -29.3389\n",
      "evaluation/Returns Min                               -203.285\n",
      "evaluation/Actions Mean                                -0.0303068\n",
      "evaluation/Actions Std                                  0.530843\n",
      "evaluation/Actions Max                                  0.930464\n",
      "evaluation/Actions Min                                 -0.955206\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -136.461\n",
      "evaluation/env_infos/final/reward_run Mean             -0.0023943\n",
      "evaluation/env_infos/final/reward_run Std               0.011729\n",
      "evaluation/env_infos/final/reward_run Max               2.49364e-08\n",
      "evaluation/env_infos/final/reward_run Min              -0.0598545\n",
      "evaluation/env_infos/initial/reward_run Mean           -0.0801644\n",
      "evaluation/env_infos/initial/reward_run Std             0.46083\n",
      "evaluation/env_infos/initial/reward_run Max             0.747209\n",
      "evaluation/env_infos/initial/reward_run Min            -0.914167\n",
      "evaluation/env_infos/reward_run Mean                    0.000159404\n",
      "evaluation/env_infos/reward_run Std                     0.0508896\n",
      "evaluation/env_infos/reward_run Max                     0.797002\n",
      "evaluation/env_infos/reward_run Min                    -1.10477\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.169597\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0949974\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00160869\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.320799\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.159377\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.09133\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0018364\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.30743\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.169628\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0947788\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00151312\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.339947\n",
      "evaluation/env_infos/final/height Mean                 -0.167677\n",
      "evaluation/env_infos/final/height Std                   0.0228407\n",
      "evaluation/env_infos/final/height Max                  -0.120856\n",
      "evaluation/env_infos/final/height Min                  -0.205444\n",
      "evaluation/env_infos/initial/height Mean               -0.0111441\n",
      "evaluation/env_infos/initial/height Std                 0.0553455\n",
      "evaluation/env_infos/initial/height Max                 0.0894902\n",
      "evaluation/env_infos/initial/height Min                -0.0893722\n",
      "evaluation/env_infos/height Mean                       -0.16784\n",
      "evaluation/env_infos/height Std                         0.0237723\n",
      "evaluation/env_infos/height Max                         0.0894902\n",
      "evaluation/env_infos/height Min                        -0.262471\n",
      "evaluation/env_infos/final/reward_angular Mean          0.00726219\n",
      "evaluation/env_infos/final/reward_angular Std           0.035576\n",
      "evaluation/env_infos/final/reward_angular Max           0.181548\n",
      "evaluation/env_infos/final/reward_angular Min          -5.03473e-08\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.077248\n",
      "evaluation/env_infos/initial/reward_angular Std         1.65549\n",
      "evaluation/env_infos/initial/reward_angular Max         2.69811\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.07162\n",
      "evaluation/env_infos/reward_angular Mean                0.000972604\n",
      "evaluation/env_infos/reward_angular Std                 0.0956772\n",
      "evaluation/env_infos/reward_angular Max                 2.69811\n",
      "evaluation/env_infos/reward_angular Min                -2.07162\n",
      "time/data storing (s)                                   0.0857664\n",
      "time/evaluation sampling (s)                           22.2095\n",
      "time/exploration sampling (s)                           1.02558\n",
      "time/logging (s)                                        0.232476\n",
      "time/saving (s)                                         0.951306\n",
      "time/training (s)                                       4.11402\n",
      "time/epoch (s)                                         28.6187\n",
      "time/total (s)                                        229.2\n",
      "Epoch                                                   7\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:06:54.448476 PDT | [gher-halfcheetahhard-SAC-10e-1000s-disc0.99_2021_05_25_10_02_39_0000--s-10] Epoch 8 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  20000\n",
      "trainer/QF1 Loss                                        0.661419\n",
      "trainer/QF2 Loss                                        0.700122\n",
      "trainer/Policy Loss                                    -5.7838\n",
      "trainer/Q1 Predictions Mean                             3.46513\n",
      "trainer/Q1 Predictions Std                              1.41852\n",
      "trainer/Q1 Predictions Max                              7.74684\n",
      "trainer/Q1 Predictions Min                              0.910088\n",
      "trainer/Q2 Predictions Mean                             3.44127\n",
      "trainer/Q2 Predictions Std                              1.48484\n",
      "trainer/Q2 Predictions Max                              8.39418\n",
      "trainer/Q2 Predictions Min                              0.871825\n",
      "trainer/Q Targets Mean                                  3.5316\n",
      "trainer/Q Targets Std                                   1.62855\n",
      "trainer/Q Targets Max                                  10.4788\n",
      "trainer/Q Targets Min                                   0.409878\n",
      "trainer/Log Pis Mean                                   -1.87042\n",
      "trainer/Log Pis Std                                     2.32169\n",
      "trainer/Log Pis Max                                     7.73329\n",
      "trainer/Log Pis Min                                    -6.54005\n",
      "trainer/Policy mu Mean                                 -0.119085\n",
      "trainer/Policy mu Std                                   0.734781\n",
      "trainer/Policy mu Max                                   2.34556\n",
      "trainer/Policy mu Min                                  -2.4014\n",
      "trainer/Policy log std Mean                            -0.257732\n",
      "trainer/Policy log std Std                              0.105327\n",
      "trainer/Policy log std Max                              0.0993097\n",
      "trainer/Policy log std Min                             -0.826296\n",
      "trainer/Alpha                                           0.100439\n",
      "trainer/Alpha Loss                                    -18.0673\n",
      "exploration/num steps total                         10000\n",
      "exploration/num paths total                            10\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.303205\n",
      "exploration/Rewards Std                                 0.713587\n",
      "exploration/Rewards Max                                 1.96056\n",
      "exploration/Rewards Min                                -3.33432\n",
      "exploration/Returns Mean                             -303.205\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -303.205\n",
      "exploration/Returns Min                              -303.205\n",
      "exploration/Actions Mean                                0.280853\n",
      "exploration/Actions Std                                 0.581423\n",
      "exploration/Actions Max                                 0.998244\n",
      "exploration/Actions Min                                -0.996993\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -303.205\n",
      "exploration/env_infos/final/reward_run Mean            -0.978531\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.978531\n",
      "exploration/env_infos/final/reward_run Min             -0.978531\n",
      "exploration/env_infos/initial/reward_run Mean           0.629955\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.629955\n",
      "exploration/env_infos/initial/reward_run Min            0.629955\n",
      "exploration/env_infos/reward_run Mean                  -0.0825263\n",
      "exploration/env_infos/reward_run Std                    0.528906\n",
      "exploration/env_infos/reward_run Max                    1.89241\n",
      "exploration/env_infos/reward_run Min                   -1.78536\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.268267\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.268267\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.268267\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.278636\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.278636\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.278636\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.250158\n",
      "exploration/env_infos/reward_ctrl Std                   0.0768456\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0450924\n",
      "exploration/env_infos/reward_ctrl Min                  -0.4823\n",
      "exploration/env_infos/final/height Mean                -0.427615\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.427615\n",
      "exploration/env_infos/final/height Min                 -0.427615\n",
      "exploration/env_infos/initial/height Mean              -0.0117363\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0117363\n",
      "exploration/env_infos/initial/height Min               -0.0117363\n",
      "exploration/env_infos/height Mean                      -0.39116\n",
      "exploration/env_infos/height Std                        0.210832\n",
      "exploration/env_infos/height Max                        0.120124\n",
      "exploration/env_infos/height Min                       -0.589287\n",
      "exploration/env_infos/final/reward_angular Mean        -0.281127\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -0.281127\n",
      "exploration/env_infos/final/reward_angular Min         -0.281127\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.69013\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.69013\n",
      "exploration/env_infos/initial/reward_angular Min       -1.69013\n",
      "exploration/env_infos/reward_angular Mean               0.0440453\n",
      "exploration/env_infos/reward_angular Std                0.914901\n",
      "exploration/env_infos/reward_angular Max                3.53999\n",
      "exploration/env_infos/reward_angular Min               -2.77252\n",
      "evaluation/num steps total                         225000\n",
      "evaluation/num paths total                            225\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.144563\n",
      "evaluation/Rewards Std                                  0.16347\n",
      "evaluation/Rewards Max                                  2.63674\n",
      "evaluation/Rewards Min                                 -2.35602\n",
      "evaluation/Returns Mean                              -144.563\n",
      "evaluation/Returns Std                                 57.6285\n",
      "evaluation/Returns Max                                -22.6227\n",
      "evaluation/Returns Min                               -287.428\n",
      "evaluation/Actions Mean                                -0.0335912\n",
      "evaluation/Actions Std                                  0.552833\n",
      "evaluation/Actions Max                                  0.953046\n",
      "evaluation/Actions Min                                 -0.966266\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -144.563\n",
      "evaluation/env_infos/final/reward_run Mean             -0.0655568\n",
      "evaluation/env_infos/final/reward_run Std               0.342873\n",
      "evaluation/env_infos/final/reward_run Max               0.118883\n",
      "evaluation/env_infos/final/reward_run Min              -1.74129\n",
      "evaluation/env_infos/initial/reward_run Mean           -0.0655334\n",
      "evaluation/env_infos/initial/reward_run Std             0.475504\n",
      "evaluation/env_infos/initial/reward_run Max             0.628644\n",
      "evaluation/env_infos/initial/reward_run Min            -0.999795\n",
      "evaluation/env_infos/reward_run Mean                   -0.0234246\n",
      "evaluation/env_infos/reward_run Std                     0.203347\n",
      "evaluation/env_infos/reward_run Max                     1.45572\n",
      "evaluation/env_infos/reward_run Min                    -2.64649\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.182852\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.105363\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00739416\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.374041\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.200051\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0963705\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0221631\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.38056\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.184052\n",
      "evaluation/env_infos/reward_ctrl Std                    0.105953\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00321175\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.38814\n",
      "evaluation/env_infos/final/height Mean                 -0.170643\n",
      "evaluation/env_infos/final/height Std                   0.0852619\n",
      "evaluation/env_infos/final/height Max                  -0.119283\n",
      "evaluation/env_infos/final/height Min                  -0.576958\n",
      "evaluation/env_infos/initial/height Mean               -0.00571162\n",
      "evaluation/env_infos/initial/height Std                 0.0444284\n",
      "evaluation/env_infos/initial/height Max                 0.071524\n",
      "evaluation/env_infos/initial/height Min                -0.0734475\n",
      "evaluation/env_infos/height Mean                       -0.164436\n",
      "evaluation/env_infos/height Std                         0.0803764\n",
      "evaluation/env_infos/height Max                         0.23259\n",
      "evaluation/env_infos/height Min                        -0.577998\n",
      "evaluation/env_infos/final/reward_angular Mean          0.00804076\n",
      "evaluation/env_infos/final/reward_angular Std           0.037297\n",
      "evaluation/env_infos/final/reward_angular Max           0.190429\n",
      "evaluation/env_infos/final/reward_angular Min          -0.000780546\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.262605\n",
      "evaluation/env_infos/initial/reward_angular Std         1.71248\n",
      "evaluation/env_infos/initial/reward_angular Max         3.5908\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.08018\n",
      "evaluation/env_infos/reward_angular Mean                0.00631828\n",
      "evaluation/env_infos/reward_angular Std                 0.232983\n",
      "evaluation/env_infos/reward_angular Max                 3.5908\n",
      "evaluation/env_infos/reward_angular Min                -3.14439\n",
      "time/data storing (s)                                   0.0831256\n",
      "time/evaluation sampling (s)                           22.7112\n",
      "time/exploration sampling (s)                           1.03722\n",
      "time/logging (s)                                        0.24029\n",
      "time/saving (s)                                         1.04292\n",
      "time/training (s)                                       4.02429\n",
      "time/epoch (s)                                         29.139\n",
      "time/total (s)                                        258.516\n",
      "Epoch                                                   8\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:07:24.015288 PDT | [gher-halfcheetahhard-SAC-10e-1000s-disc0.99_2021_05_25_10_02_39_0000--s-10] Epoch 9 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  22000\n",
      "trainer/QF1 Loss                                        0.775429\n",
      "trainer/QF2 Loss                                        0.778427\n",
      "trainer/Policy Loss                                    -5.13137\n",
      "trainer/Q1 Predictions Mean                             3.65603\n",
      "trainer/Q1 Predictions Std                              1.76639\n",
      "trainer/Q1 Predictions Max                             10.0465\n",
      "trainer/Q1 Predictions Min                              0.406873\n",
      "trainer/Q2 Predictions Mean                             3.6162\n",
      "trainer/Q2 Predictions Std                              1.75922\n",
      "trainer/Q2 Predictions Max                              9.7219\n",
      "trainer/Q2 Predictions Min                              0.539754\n",
      "trainer/Q Targets Mean                                  3.4601\n",
      "trainer/Q Targets Std                                   1.9258\n",
      "trainer/Q Targets Max                                  13.4437\n",
      "trainer/Q Targets Min                                  -1.51123\n",
      "trainer/Log Pis Mean                                   -1.01917\n",
      "trainer/Log Pis Std                                     2.87467\n",
      "trainer/Log Pis Max                                    11.7716\n",
      "trainer/Log Pis Min                                    -6.75501\n",
      "trainer/Policy mu Mean                                  0.04203\n",
      "trainer/Policy mu Std                                   0.851622\n",
      "trainer/Policy mu Max                                   3.60823\n",
      "trainer/Policy mu Min                                  -2.47888\n",
      "trainer/Policy log std Mean                            -0.329258\n",
      "trainer/Policy log std Std                              0.127039\n",
      "trainer/Policy log std Max                             -0.0309145\n",
      "trainer/Policy log std Min                             -0.951365\n",
      "trainer/Alpha                                           0.0777538\n",
      "trainer/Alpha Loss                                    -17.911\n",
      "exploration/num steps total                         11000\n",
      "exploration/num paths total                            11\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.243743\n",
      "exploration/Rewards Std                                 0.210418\n",
      "exploration/Rewards Max                                 0.395651\n",
      "exploration/Rewards Min                                -0.98224\n",
      "exploration/Returns Mean                             -243.743\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -243.743\n",
      "exploration/Returns Min                              -243.743\n",
      "exploration/Actions Mean                                0.133087\n",
      "exploration/Actions Std                                 0.602217\n",
      "exploration/Actions Max                                 0.998106\n",
      "exploration/Actions Min                                -0.995637\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -243.743\n",
      "exploration/env_infos/final/reward_run Mean             0.236723\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.236723\n",
      "exploration/env_infos/final/reward_run Min              0.236723\n",
      "exploration/env_infos/initial/reward_run Mean           0.0381664\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.0381664\n",
      "exploration/env_infos/initial/reward_run Min            0.0381664\n",
      "exploration/env_infos/reward_run Mean                  -0.281319\n",
      "exploration/env_infos/reward_run Std                    0.772228\n",
      "exploration/env_infos/reward_run Max                    2.81636\n",
      "exploration/env_infos/reward_run Min                   -3.14586\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.258316\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.258316\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.258316\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.21028\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.21028\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.21028\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.228227\n",
      "exploration/env_infos/reward_ctrl Std                   0.0765333\n",
      "exploration/env_infos/reward_ctrl Max                  -0.042218\n",
      "exploration/env_infos/reward_ctrl Min                  -0.438333\n",
      "exploration/env_infos/final/height Mean                -0.532294\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.532294\n",
      "exploration/env_infos/final/height Min                 -0.532294\n",
      "exploration/env_infos/initial/height Mean              -0.0483574\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0483574\n",
      "exploration/env_infos/initial/height Min               -0.0483574\n",
      "exploration/env_infos/height Mean                      -0.270712\n",
      "exploration/env_infos/height Std                        0.242374\n",
      "exploration/env_infos/height Max                        0.374183\n",
      "exploration/env_infos/height Min                       -0.583402\n",
      "exploration/env_infos/final/reward_angular Mean         0.961622\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.961622\n",
      "exploration/env_infos/final/reward_angular Min          0.961622\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.982564\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.982564\n",
      "exploration/env_infos/initial/reward_angular Min       -0.982564\n",
      "exploration/env_infos/reward_angular Mean               0.0759825\n",
      "exploration/env_infos/reward_angular Std                1.72256\n",
      "exploration/env_infos/reward_angular Max                5.19944\n",
      "exploration/env_infos/reward_angular Min               -6.45871\n",
      "evaluation/num steps total                         250000\n",
      "evaluation/num paths total                            250\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.140842\n",
      "evaluation/Rewards Std                                  0.379187\n",
      "evaluation/Rewards Max                                  4.41835\n",
      "evaluation/Rewards Min                                 -4.87396\n",
      "evaluation/Returns Mean                              -140.842\n",
      "evaluation/Returns Std                                 79.9805\n",
      "evaluation/Returns Max                                  3.58227\n",
      "evaluation/Returns Min                               -306.43\n",
      "evaluation/Actions Mean                                 0.131355\n",
      "evaluation/Actions Std                                  0.552458\n",
      "evaluation/Actions Max                                  0.997598\n",
      "evaluation/Actions Min                                 -0.995037\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -140.842\n",
      "evaluation/env_infos/final/reward_run Mean             -0.0451105\n",
      "evaluation/env_infos/final/reward_run Std               0.460756\n",
      "evaluation/env_infos/final/reward_run Max               0.851442\n",
      "evaluation/env_infos/final/reward_run Min              -2.00938\n",
      "evaluation/env_infos/initial/reward_run Mean            0.0401896\n",
      "evaluation/env_infos/initial/reward_run Std             0.428524\n",
      "evaluation/env_infos/initial/reward_run Max             0.87399\n",
      "evaluation/env_infos/initial/reward_run Min            -0.881654\n",
      "evaluation/env_infos/reward_run Mean                   -0.130989\n",
      "evaluation/env_infos/reward_run Std                     0.465091\n",
      "evaluation/env_infos/reward_run Max                     2.38364\n",
      "evaluation/env_infos/reward_run Min                    -3.67676\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.194801\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0852579\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0740378\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.437289\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.229299\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.111237\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0280743\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.437197\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.193478\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0942611\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0041485\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.499926\n",
      "evaluation/env_infos/final/height Mean                 -0.243667\n",
      "evaluation/env_infos/final/height Std                   0.147684\n",
      "evaluation/env_infos/final/height Max                  -0.0294396\n",
      "evaluation/env_infos/final/height Min                  -0.579704\n",
      "evaluation/env_infos/initial/height Mean               -0.00834717\n",
      "evaluation/env_infos/initial/height Std                 0.0546907\n",
      "evaluation/env_infos/initial/height Max                 0.084783\n",
      "evaluation/env_infos/initial/height Min                -0.0943825\n",
      "evaluation/env_infos/height Mean                       -0.222084\n",
      "evaluation/env_infos/height Std                         0.147175\n",
      "evaluation/env_infos/height Max                         0.527375\n",
      "evaluation/env_infos/height Min                        -0.585116\n",
      "evaluation/env_infos/final/reward_angular Mean          0.0975331\n",
      "evaluation/env_infos/final/reward_angular Std           0.702602\n",
      "evaluation/env_infos/final/reward_angular Max           2.78736\n",
      "evaluation/env_infos/final/reward_angular Min          -1.69172\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.00284597\n",
      "evaluation/env_infos/initial/reward_angular Std         1.50687\n",
      "evaluation/env_infos/initial/reward_angular Max         2.8137\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.5953\n",
      "evaluation/env_infos/reward_angular Mean                0.0187198\n",
      "evaluation/env_infos/reward_angular Std                 0.658179\n",
      "evaluation/env_infos/reward_angular Max                 5.66183\n",
      "evaluation/env_infos/reward_angular Min                -8.55481\n",
      "time/data storing (s)                                   0.0832649\n",
      "time/evaluation sampling (s)                           21.9055\n",
      "time/exploration sampling (s)                           1.02547\n",
      "time/logging (s)                                        0.236323\n",
      "time/saving (s)                                         2.27075\n",
      "time/training (s)                                       3.85751\n",
      "time/epoch (s)                                         29.3788\n",
      "time/total (s)                                        288.078\n",
      "Epoch                                                   9\n",
      "-------------------------------------------------  ---------------\n"
     ]
    }
   ],
   "source": [
    "!python launch_gher.py --epochs 100 --env halfcheetahhard\n",
    "!python launch_gher.py --epochs 100 --relabel --n_sampled_latents 1 --env halfcheetahhard\n",
    "!python launch_gher.py --epochs 100 --relabel --n_sampled_latents 100 --use_advantages --env halfcheetahhard\n",
    "!python launch_gher.py --epochs 100 --relabel --n_sampled_latents 100 --use_advantages --env halfcheetahhard --cache --irl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19fb7b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
